0
This paper analyzes the effects of structural variation of sentences on parsing performances .
This paper analyzes the effect of the structural variation of sentences on parsing performance .
1
The analysis of the experimental results will illustrate the necessity for handling various sentence constructions by fundamental improvement of parsers such as re-construction of feature designs .
Analysis of the experimental results illustrates the need to handle different sentence constructions through fundamental improvement of the parsers such as re-construction of feature designs .
2
Parsing is a fundamental natural language processing task and essential for various NLP applications .
Parsing is a fundamental natural language processing task and essential to various NLP applications .
3
Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
4
This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions which were not extensively studied in the recent parsing research : imperatives and questions .
This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions that have not been extensively studied in recent parsing research : imperatives and questions .
5
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracies of a part-of-speech tagger for them .
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracy of their part-of-speech ( POS ) tagger .
6
Since domain adaptation has been an extensive research area in parsing research \CITE , a lot of ideas have been proposed , including un- / semi-supervised approaches \CITE and supervised approaches \CITE .
Since domain adaptation is an extensive research area in parsing research \CITE , many ideas have been proposed , including un- or semi-supervised approaches \CITE and supervised approaches \CITE .
7
Their main focus was on adapting parsing models trained with a specific genre of text ( in most cases Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
The main focus of these works is on adapting parsing models trained with a specific genre of text ( in most cases the Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
8
In our work , sentences are collected from the Brown corpus , which includes a wider range of types of questions and imperatives .
For our study , sentences were collected from the Brown corpus , which includes a wider range of types of questions and imperatives .
9
In the experiments , we will additionally use QuestionBank for comparison .
In the experiments , we also used QuestionBank for comparison .
10
All parsers assume that the input is already POS-tagged .
All parsers assumed that the input was already POS-tagged .
11
For the evaluation of the output from each of the MST and Malt parser , we used the labeled attachment accuracy excluding the punctuations .
To evaluate the output from each of the parsers , we used the labeled attachment accuracy excluding punctuation .
12
Because the Brown Corpus portion includes texts of literary works , it is expected that it inherently contains a larger number of imperatives and questions than the WSJ portion .
As the Brown Corpus portion includes texts of literary works , it is expected to contain inherently a larger number of imperatives and questions than the WSJ portion .
13
When they are embedded in another imperative or question , we only extracted the outermost one .
However , is these were embedded in another imperative or question , we only extracted the outermost one .
14
The number of sentences for each section is shown in Table \REF .
The numbers of sentences for each section are given in Table \REF .
15
As we will describe below , we additionally use QuestionBank in experiments .
As described below , we also used QuestionBank in the experiments .
16
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged not with " . " but with " ? " ( 2 ,051 sentences ) , and phrase labels annotated as POS ( one sentence ) .
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged with " ? " instead of " . " ( 2 ,051 sentences ) , and phrase labels annotated as the POS ( one sentence ) .
17
By observing the effects of parser or tagger adaptation to each domain , we would like to see the difficulties in parsing imperative and question sentences .
By observing the effect of the parser or tagger adaptation in each domain , we can identify the difficulties in parsing imperative and question sentences .
18
The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .
The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .
19
Figure \REF shows the POS tagging accuracy for the target domains given by changing the size of the target training data .
Figure \REF shows the POS tagging accuracy for the target domains for varying sizes of the target training data .
20
In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\% in Table \REF ) , it would not seem to be enough only to prepare much more training data .
To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\% in Table \REF ) , preparing much more training data does not appear to be enough .
21
Especially , the problem would be more serious for imperatives .
In particular , the problem is more serious for imperatives .
22
We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .
Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .
23
In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , " VB \SPEC " .
From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , " VB \SPEC " .
24
For Brown imperatives , the WSJ tagger gave two major tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
For Brown imperatives , the WSJ tagger gave two main tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
25
Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .
First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .
26
The WSJ tagger was trained on the domain mainly consisting of declarative sentences , and the training was based on N-gram sequences of words or POSs . The tagger therefore preferred to give noun phrase-derived tags to the beginning of a sentence .
Since The WSJ tagger was trained on a domain consisting mainly of declarative sentences , with the training based on N-gram sequences of words or POSs , preference was given to noun phrase-derived tags at the beginning of a sentence .
27
The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .
A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .
28
The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .
Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .
29
The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .
The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .
30
After the adaptation , while some of the errors such as special usage of wh-words , i.e. , " WDT \SPEC WP " , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .
After the adaptation , although some of the errors such as the special use of wh-words , i.e. , " WDT \SPEC WP " , were corrected , other kinds or errors related to the global change in sentence structure still remained .
31
In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
32
Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
33
For the QuestionBank , 25 to 35 points accuracy improvements were observed .
For QuestionBank , 25 to 35 percent improvement in accuracy was observed .
34
This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .
This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .
35
However , we have no more training data for Brown imperatives and questions . We should prepare more training data or explore approaches to enable us to sufficiently adapt parsers with small training data .
However , as there is no more training data for Brown imperatives and questions , we need to either prepare more training data or explore approaches that enable the parsers to be adapted with small amounts of training data .
36
Table \REF and \REF show the recall errors on labeled dependencies which were observed more than ten times for 100 analysis sentences of each domain .
Tables \REF and \REF give the recall errors on labeled dependencies , which were observed more than ten times for 100 analysis sentences in each domain .
37
Since ROOT dependencies , that is , heads of sentences would be critical to construction of sentences , we mainly focus on that type of errors .
Since ROOT dependencies , that is , heads of sentences , are critical to the construction of sentences , we focus mainly on this type of error .
38
For Brown imperatives and questions , we could observe that the reduction of ROOT dependency was prominent .
For Brown imperatives and questions , the reduction in ROOT dependencies was prominent .
39
When we focus on this type of errors , we could find that the WSJ parser could often make mistakes in parsing sentences which began or ended with the names of persons who were talk to .
On investigation , we found that the WSJ parser often made mistakes in parsing sentences which began or ended with the name of the person being addressed .
40
We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .
A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .
41
On the other hand , we also observed some still unsolved errors . We would show the two kinds of major errors among them .
On the other hand , we also observed some unsolved errors , of which we discuss two .
42
The parsing models based on the plausibility of constructions could hardly capture such sentences .
The parsing models based on the plausibility of constructions were not able to capture such sentences .
43
It would be difficult for the parsers to know where the main clause in such complex sentences .
It would be difficult for the parsers to know which is the main clause in such complex sentences .
44
In this section , we examined whether the parser adapted to one domain would be portable to the other domain .
In this section , we examine whether a parser adapted to one domain could be ported to another domain .
45
QuestionBankdoes not give function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
QuestionBank does not provide function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
46
Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .
As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .
47
However , we would be able to expect that sentence constructions would be basically common and portable between two domains , which would contribute to give correct boundary for phrases and therefore the correct dependencies in phrases would be introduced by the adaptation .
However , we would expect that sentence constructions are basically common and portable between two domains , which would provide a correct boundary for phrases and therefore , the correct dependencies in phrases would be introduced by the adaptation .
48
the difference from Table \REF was that the parsers and the tagger were adapted to another question domain .
These results differ from those in Table \REF in that the parsers and the tagger have been adapted to another question domain .
49
Table \REF could explain the result .
Table \REF could explain this result .
50
With Brown questions , we could learn wh-questions which QuestionBank mainly contain , while with QuestionBank , we could not we could not learn yes-no questions which more than half of Brown corpus contain .
Using Brown questions , many wh-questions were learnt , which is what QuestionBank mainly contains . On the other hand , despite yes-no questions constituting more than half the Brown corpus , these were not learnt using QuestionBank for training .
51
Through the experiments on various parsers we observed that simple supervised adaptation methods are insufficient to arrive at theparsing accuracy comparable to that of declarative sentences .
Through experiments with various parsers we observed that simple supervised adaptation methods are insufficient to achieve parsing accuracy comparable with that of declarative sentences .
52
This observation holds both for POS tagging and syntactic parsing , and itindicates that we need fundamental improvement of parsers , such as re-constructing feature designs or changing parsing models .
This observation holds both for POS tagging and syntactic parsing , and indicates that the parsers need to be fundamentally improved , such as re-constructing feature designs or changing parsing models .
53
While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .
While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .
54
Since the process is fundamental and indispensable , we need to explore how word segmentation affects Natural Language Processing applications .
Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .
55
We also examine an unsupervised morphological analyzer and its results .
In addition , we examine an unsupervised morphological analyzer , and its results .
56
Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .
However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .
57
This work aims to empirically compare representative word segmentation methods in terms of SMT quality .
This work aims at empirically comparing representative word segmentation methods in terms of the SMT quality .
58
We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .
We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .
59
Furthermore , their policies about word segmentation definitions are very much different .
Furthermore , their policies concerning word segmentation definitions vary significantly .
60
We also investigate such morphological analysis accuracy and word definition problems in our experiments .
In our experiments , we further investigate such morphological analysis accuracies and word definition problems .
61
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is hard to independently evaluate the effects of word segmentation on training data .
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .
62
However , our preliminary experiments showed that the results obtained with this method were not independent from word segmentation of training data .
However , our preliminary experiments indicated that the results obtained with this method were not independent from word segmentation of the training data .
63
And the best results were obtained when we use the same word segmentation as the training data .
The best results were obtained when we used the same word segmentation as the training data .
64
In order to manage such a problem , we use one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .
In order to manage this issue , we used one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .
65
In this case , the evaluation scores created by BLEU and RIBES are not comparative due to the differences of Japanese word definitions between the outputs of word segmentation methods .
In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .
66
And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .
Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .
67
On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT were very much worse than the supervised morphological analyzers .
On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT performed much poorer than the supervised morphological analyzers .
68
- Long sentences were translated worse than the other word segmentation outputs .
- Long sentences were not translated as well as other word segmentation outputs .
69
The reasons of the CHAR results are yet to be analyzed in details .
The reasons for the CHAR results are yet to be analyzed in detail .
70
This work focused on how the difference of word segmentation affects SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
71
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , they were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation .
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , the data was insufficient for discussing the relative advantages and disadvantages of word segmentation , with accuracy .
72
However , problems with their approaches are the disregard of the interdependencies of word senses , and the limited applicability to those word senses for which training instances are served .
However , the problem with these approaches , is that they disregard the interdependencies of word senses , and that it is limited in its applicability to the word senses for which training instances are served .**[<-This sentence is a bit confusing]
73
Also , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can even work for words that do not appear in the training data , and these combined features help relieve the data sparseness problem .
Furthermore , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can also work for words that do not appear in the training data ; these combined features help relieve the data sparseness problem .
74
We also present an in-depth analysis on the effectiveness of the sense dependency features with intuitive examples .
We also present an in-depth analysis of the effectiveness of the sense dependency features by using intuitive examples .
75
One major obstacle to large-scale and precise WSD is the data sparseness problem caused by the fine-grainedness of the sense distinction .
One major obstacle for large-scale and precise WSD is solving the data sparseness problem caused by the fine-grained nature of sense distinction .
76
Although the use of the global information has succeeded in dramatically increase the performance of WSD , there are much room left to examine the effectiveness of local or syntactic information .
Although the use of global information has succeeded in dramatically increasing the performance of WSD , there is much room left to examine the effectiveness of local or syntactic information .
77
Although the use of local and syntactic information has been common in WSD , traditional approaches to WSD are based on the individual classification framework for each word , in which each word 's sense is treated independently , regardless of any interdependencies nor cooccurrences of word senses .
Although the use of local and syntactic information has been common in WSD , traditional approaches to WSD are based on the individual classification framework for each word ; each word 's sense is treated independently , regardless of any interdependencies or cooccurrences of word senses .
78
We focus on the use of the interdependency of word senses , so that we can directly address the issue of semantic ambiguity of a whole sentence arose from the interaction of each word 's sense ambiguity .
We focus on using the interdependency of word senses , so that we can directly address the issue of semantic ambiguity in a whole sentence that arose from the interaction of each word 's sense ambiguity . **[ <- this part is confusing .]
79
Specifically , we assume that there exist strong sense dependencies between a syntactic head and its dependents in the dependency tree , rather than between neighboring words in the sentence .
Specifically , we assume that are strong sense dependencies between a syntactic head , and its dependents in the dependency tree , rather than between neighboring words of a sentence .
80
This is to relieve the data sparseness problem caused by the explosion of the number of features , which is roughly squared by the combination of two word senses .
This is to relieve the data sparseness problem caused by the explosion in the number of features , which is roughly squared by the combination of two word senses .
81
The WordNet is a broad-coverage machine-readable dictionary ( MRD ) for English , which contains about 150 ,000 words .
The WordNet is a broad-coverage machine-readable dictionary ( MRD ) for English , containing about 150 ,000 words .
82
Note that since the organizations of adjectives and adverbs are far different from those of nouns and verbs , we use this hierarchical information for only nouns and verbs .
Note that since the organizations of adjectives and adverbs are very different from those of nouns and verbs , we use this hierarchical information for only nouns and verbs .
83
Since the data sparsity has been a significant problem in WSD , the sense frequency information is necessary to achieve good performance .
Since data sparsity has been a significant issue in WSD , the sense frequency information is necessary in achieving a good performance .
84
The first sense classifier is known as a strong baseline in WSD , which can be even considered to be a good alternative to WSD .
The first sense classifier is known as a strong baseline in WSD , which can even be considered as a good alternative to WSD .
85
On the other hand , the traditional approach to the supervised WSD is to solve an independent classification problem for each word .
On the contrary , the traditional approach to supervised WSD is to solve an independent classification problem for each word .
86
Note additionally that they do not mention how and how much they contribute to the improvement of supervised WSD .
Note additionally , that they do not mention the means or the quality of contribution in improving supervised WSD .
87
In Section 1 , we presented one of the most significant problems in WSD - the data sparsity .
In Section 1 , we presented one of the most significant issues in WSD - the data sparsity problem .
88
This problem may even be magnified when we consider the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .
This problem may even be magnified , when taking into consideration the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .
89
This is reflected in the low inter-annotator agreement of sense tagging ( typically around 70% ) , which implies that WSD models are unlikely to perform better than this accuracy .
This is reflected in the low inter-annotator agreement of sense tagging ( typically around 70% ) , which implies that WSD models would be unlikely to perform better than the accuracy achieved .
90
In this case , the inter-annotator agreements are turned out to reach around 90% .
In this case , the inter-annotator agreements have reached nearly90% .
91
This approach has been taken in several hierarchical WSD methods , but never combined with the sense dependencies as we use .
This approach has been taken in several hierarchical WSD methods , but has never been combined with the sense dependencies in a way that have used them .
92
The conditional probability of a label sequence / MATH conditioned on a data sequence / MATH is given by / MATH	 , where / MATH and / MATH are the feature vectors for an edge and a vertex , / MATH and / MATH are the weight vectors for them , / MATH and / MATH are the set of components of / MATH associated with an edge / MATH and a vertex / MATH , and / MATH is the partition function which constrains the sum of all the probabilities to be 1 .
The conditional probability of a label sequence / MATH conditioned on a data sequence / MATH is given by / MATH	 , where / MATH and / MATH are the feature vectors for an edge and a vertex , / MATH and / MATH are the weight vectors , / MATH and / MATH are the set of components of / MATH associated with an edge / MATH and a vertex / MATH , and / MATH is the partition function that constrains the sum of all the probabilities to be 1 .
93
Hence , we can consider them appropriate for modeling the syntactic dependencies of word senses , which cannot be represented by linear structures .
Hence , we can consider them relevant in modeling the syntactic dependencies of word senses , which cannot be represented by linear structures .
94
In this section , we introduce a method to build graph structures on which CRFs are constructed .
In this section , we introduce the method of building graph structures on which CRFs are constructed .
95
For the linear-chain models , we do not need to parse a sentence .
For the linear-chain models , parsing a sentence is unnecessary .
96
Next , as the same reason for the tree-structured case , we remove from the graph those words that we do not need to disambiguate , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .
Next , as the same reason as for the tree-structured case , we remove those words that we do not need to disambiguate from the graph , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .
97
The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) but not related to natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .
The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) , but is unrelated to a natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .
98
Unlike in the synset-based model , we do not incorporate the syntactic information of the parent and child words , since it has been reported not to improve the performance .
Unlike in the synset-based model , we do not incorporate the syntactic information of the parent and child words , since it has not been reported to improve the performance .
99
In this section , we introduce corpora we use for the evaluation .
In this section , we introduce corpora that we have used for the evaluation .
100
For the evaluation on the Senseval data sets , all instances of the rest ( e.g. SEM-E ) is used for development and one of the Senseval data sets ( SE2 or SE3 ) is used for evaluation .
For the evaluation on the Senseval data sets , all instances of the rest ( e.g. SEM-E ) are used for development , and one of the Senseval data sets ( SE2 or SE3 ) is used for evaluation .
101
We can see from Table 7 that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .
From Table 7 , it can be seen that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .
102
These improvements seem small in terms of figures ; However , considering for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model only by 0 .37% on SEM , the further improvement of 0 .21% is considerable because this means our dependency model could handle 57% more instances over the first sense baseline .
These improvements seem insignificant in figures ; however , considering that for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model by only 0 .37% on SEM , the further improvement of 0 .21% is substantial , because it indicates that our dependency model could handle 57% more instances over the first sense baseline .
103
Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed poorer than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness regardless of the existence of the sense frequency information .
Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed worse than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness [of ...] regardless of the existence of the sense frequency information .
104
However , by the incorporation of the sense dependencies , the improvements with the sense ranking features are even smaller , and the deteriorations without them are even larger than in the tree-structured case .
Nonetheless , by the incorporation of the sense dependencies , the improvements with the sense ranking features was even less , and the deteriorations without them[define " them " ] were even more than in the tree-structured case .
105
From the phrase career as a player , we can assume that the correct sense of career can be either of two senses , and possibly there is a preference for career#2 , as captured by the largest-weighted dependency WS :career%1%2-( NMOD )-SS :noun .person ( / MATH ) between career and player .
From the phrase career as a player , we can assume that the correct sense of career can be either of two senses , with the possibility that there is a preference for career#2 , as captured by the largest-weighted dependency WS :career%1%2-( NMOD )-SS :noun .person ( / MATH ) between career and player .
106
Although our model was based on a simple framework and trained only on the SemCor corpus , the results we gained were promising , suggesting that our model still has a great potential for improvement .
Although our model was based on a simple framework , and was trained only on the SemCor corpus , the results that we gained were promising . They suggested that our model still has a great potential for improvement .
107
Generating short summary videos for rushes is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .
Generating short summary videos for rushes is a challenging task due to the difficulty in eliminating redundancy and determining the important objects and events to be placed in the summary .
108
Video summarization is a significant research that helps to meet these needs by developing a condensed version of a full length digital video with the most important contents \CITE .
Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \CITE .
109
For example , the maximum duration for the summary of a 30 minute length video is 36 seconds ( \MATH ) .
For example , the maximum duration for a summary of a 30 minute video is 36 seconds ( \MATH ) .
110
" ', with this length constraint , it is difficult to present it in a pleasant tempo and rhythm .
" ', with this length constraint , it would be difficult to present it in a pleasant tempo and rhythm .
111
On the contrary , smooth presentation of events consumes a lot number of frames , that decrease the recall .
On the other hand , a smooth presentation of these events would consume a large number of frames , which would decrease the recall .
112
In the other case , assume that we have selected appropriate segments , the total length of these segments are usually larger than that of the final summary .
In the other case , assuming that we have selected the appropriate segments , the total length of these segments is usually larger than that of the final summary .
113
Then the portion of each segment that has high motion is used to include into the final summary .
Then the portion of each segment that has the highest motion is included in the final summary .
114
The sum of the middle eight of these 16 values are used to define a cut between frames \MATH and \MATH if these values exceed a threshold \MATH .
The sum of the middle eight of these 16 values is used to define the cut between frames \MATH and \MATH if these values exceed the threshold \MATH .
115
If the value of the \MATH is smaller than threshold \MATH , then these sub-shot is defined as a color bar sub-shot .
If the value of the \MATH is smaller than the threshold \MATH , then these sub-shots are defined as a color bar sub-shot .
116
The clapper boards have many types , such as scale , rotation , and illumination changes .
There are many types of clapper boards , such as scale , rotation , and illumination changes .
117
If a result of the NDK algorithm returns a match between a keyframe with a query then we define the sub-shot is a clapper board sub-shot .
If the result of the NDK algorithm returns a match from a keyframe with a query then the sub-shot is defined as a clapper board sub-shot .
118
GreedyRSC , proposed in \CITE , is used to find clusters with high precision and the number of clusters is automatically determined .
GreedyRSC , proposed in \CITE , is used to find clusters at high precision and the number of clusters is automatically determined .
119
This system is adopted with some modifications from the system developed for the same task last year \CITE .
This system has some modifications from the system developed for the same task last year \CITE .
120
We found that this approach is more effective than the approach using one keyframe for one fragment since the more number of keyframes is used , the more information is available to make right decision .
We found that this approach is more effective than the approach using one keyframe for one fragment since the more keyframes that are used , the more information is available to make the right decision .
121
Since the length of these fragments is still larger than the maximum length of the final summary, we employ a simple strategy to shrink these fragments as follows .
Since the length of these fragments is still larger than the maximum length of the final summary, we use the following simple strategy to shrink these fragments .
122
Second, for each fragment, we extract the portion which is expanded from the central of the fragment .
Second, for each fragment, we extract the portion that is expanded from the central part of the fragment .
123
This portion covers a duration twice as much as the fragment quota by selecting frames with sampling rate of 2 frames .
This portion covers a duration twice the size of the fragment quota by selecting the frames with a sampling rate of two frames .
124
Specifically, we select frames \MATH, \MATH, ..., \MATH, \MATH, ..., \MATH, \MATH, where \MATH is the middle frame of the fragment, and \MATH is half of number of frames computed from the quota\MATH and frame rate ( 25fps ) \MATH :
Specifically, we select frames \MATH, \MATH, ..., \MATH, \MATH, ..., \MATH, \MATH, where \MATH is the middle frame of the fragment, and \MATH is half the number of frames computed from the quota\MATH and the frame rate ( 25fps ) \MATH :
125
We have tested our approaches with 40 videos of TRECVID 2008 test set .
We have tested our approaches on 40 videos from the TRECVID 2008 test set .
126
The summary videos generated by NII-1 have fewer duplications ( RE ), are presented in a smoother way ( TE ) and are easy to judge for inclusions ( TT ) .
The summary videos generated by NII-1 have less duplication ( RE ), are presented in a smoother way ( TE ), and are easy to judge for inclusions ( TT ) .
127
Compared to the other systems participating in this task of TRECVID 2008, NII-1 has good performance in measures such as DU and TT ( see Figure \REF and Figure \REF; while NII-2 achieves good performance in measure IN ( see Figure \REF ) .
Compared to the other systems participating in this task of TRECVID 2008, NII-1 performed better in such measures as DU and TT ( see Figure \REF and Figure \REF; while NII-2 performs well in the IN measure ( see Figure \REF ) .
128
Fragmentation is the case that samples of one cluster are put into several different clusters .
Fragmentation is where samples of one cluster are put into several different clusters .
129
Using all frames of one segment instead of using one keyframe as proposed in NII-2 is one of the efforts toward this direction .
Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .
130
Although the result is not very high as expected, we still believe that this approach is promising .
Although the results are not as high as expected, we still believe that this approach is promising .
131
This approach achieves good performance in recall and reasonable performance in usability score .
This approach is good for recall and has a reasonably good usability score .
132
Compared to other systems participating in TRECVID 2008 summarization task, NII-2 is among best systems that have good balance between recall and usability .
Compared to other systems participating in the TRECVID 2008 summarization task, NII-2 is among the best systems with a good balance between recall and usability .
133
Searching persons is one of the essential tasks required by users for image and video search engines .
Searching for images of people is one of the essential tasks required by users for image and video search engines .
134
In the proposed method , we treat the problem as a classification problem which input faces are classified as 'personX' ( the queried person ) or 'non-personX' and the faces are ranked based on their relevant score that is inferred from the classifier 's probability output .
In the proposed method , we treat this problem as a classification problem in which input faces are classified as 'person-X' ( the queried person ) or 'non-person-X' , and the faces are ranked based on their relevant score inferred from the classifier 's probability output .
135
With the rapid growing of digital technology , large image and video databases are available easier than ever to users .
With the rapid growth of digital technology , large image and video databases are more available than ever to users .
136
-The fact the retrieved face set consists of faces of several persons while no any label is given makes supervised learning methods as well as unsupervised learning methods such as \MATH -means inapplicable .
-The fact the retrieved face set consists of faces of several people with no label makes supervised learning methods as well as unsupervised learning methods such as , \MATH -means , inapplicable .
137
The main idea is to learn visual consistency assumed to exist among the results returned from current text-based search engines .
The main idea is to assume that there is visual consistency among the results returned from current text-based search engines .
138
By assuming that the number of faces of the queried person are larger than that of other persons , and these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph whose solution is available .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph with an available solution . //[Do graphs have solutions ? They just provide information .]
139
In another work \CITE , a clustering-based approach was proposed to associate names and faces in news photos .
In another work \CITE , a clustering-based approach was proposed for associating names and faces in news photos .
140
This dataset consists of approximately half a million news pictures and captions from Yahoo News over a period of roughly two years .
This dataset consisted of approximately half a million news pictures and captions from Yahoo News over a period of roughly two years .
141
We selected sixteen celebrities who are government leaders such as George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key persons such as John Paul II ( the Former Pope ) , Kofi Annan and Hans Blix ( UN ) . These persons are selected since their appearances are highly frequent in the dataset \CITE .
We selected sixteen government leaders including George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals such as John Paul II ( the Former Pope ) and Kofi Annan and Hans Blix ( UN ) since their images appeared frequently in the dataset \CITE .
142
For each person , variations of his name are collected . For example , George W . Bush , President Bush , U . S . President , etc are variations of U . S . President Bush .
For each person , variations of his name were collected . For example , George W . Bush , President Bush , U . S . President , etc are variations of U . S . President Bush .
143
We evaluated the retrieval performance with measures that are popularly used in information retrieval such as precision , recall and average precision .
We evaluated the retrieval performance with measures that are commonly used in information retrieval such as precision , recall , and average precision .
144
Given a queried person , assuming that \MATH is the total number of faces returned , \MATH is the number of relevant faces , \MATH is the number of relevant faces , we calculate recall and precision as follows :
Given a queried person , assuming that \MATH is the total number of faces returned , \MATH is the number of relevant faces , \MATH is the number of relevant faces , we calculated recall and precision as follows : //[Nrel and Nhit are exactly the same here . They should be different .]
145
In addition , to evaluate performance of multiple queries , we used mean average precision that is the mean of average precisions computed from queries .
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .
146
It indicates that DBO-based method outperforms the others .
It indicated that the DBO-based method outperformed the others .
147
This suggests that the input face sets are quite dense .
This suggests that the input face sets were quite dense .
148
To select one subset , we set \MATH and \MATH which means 20% of highest ranked faces are used for \MATH and 30% of lowest ranked faces are used for \MATH .
To select one subset , we set \MATH and \MATH which means 20% of the highest ranked faces were used for \MATH and 30% of the lowest ranked faces were used for \MATH .
149
The subsets \MATH and \MATH are generated by randomly selecting with replacement 70% samples of \MATH amd \MATH .
The subsets \MATH and \MATH were generated by randomly selecting with replacement 70% samples of \MATH and \MATH .[�gWith replacement�h does not make sense here . I am not sure what you want to say .]
150
The performance of different methods shown in Figure \REF indicates that our proposed method outperforms the distance-based outliers detection method and has comparable performance with the supervised method using 5% annotation data .
The performance of different methods shown in Figure \REF indicates that our proposed method outperformed the distance-based outlier detection method and performed comparable to the supervised method using 5% annotation data .
151
We present a method to effectively rank faces retrieved by text-based correlation methods when searching a specific person .
We presented a method for effectively ranking faces retrieved using text-based correlation methods when searching for a specific person .
152
This classifier is used to rank input faces for the next step .
This classifier was used to rank input faces for the next step .
153
To get initial rank for the first step , we propose to use common outliers detection method .
To obtain the initial rank for the first step , we proposed using a common outlier detection method .
154
- Classification : the extracted features is passed through a classifier which is trained beforehand to classify the input pattern associated with these features as a face or a non-face .
- Classification : the extracted features are passed through a classifier that has been previously trained to classify the input pattern associated with these features as a face or a non-face . //[trained / programmed ?]
155
Since the number of processed patterns is large while the vast majority of them are non-face , a single classifier based systems such as neural network \CITE and support vector machines \CITE are usually slow .
Since the vast majority of processed patterns are non-face , the single classifier based systems , such as the neural network \CITE and the support vector machines \CITE , are usually slow .
156
The accurate classifier described in \CITE requires about five thousand original face patterns and hundreds of million non-face patterns extracted from 9 ,500 non-face images .
The accurate classifier described in \CITE requires about five thousand original face patterns and hundreds of millions of non-face patterns extracted from 9 ,500 non-face images .
157
Another learning method which has been used widely in many object detection systems is AdaBoost and its variants .
Another learning method that has been widely used in many object detection systems is AdaBoost and its variants .
158
At run time a user outlines a face in a frame of the video , and the face tracks within the movie are then ranked according to the similarity to the outlined query face in the manner of Google .
At run time a user outlines a face in a video frame , and the face tracks within the movie are then ranked according to their similarity to the outlined query face in the same way as Google .
159
For instance , shots containing a particular person can be retrieved by a keyword like " Bush " or " Julia Roberts " instead of an outlined query face as used in [5] .
For instance , shots containing a particular person can be retrieved by a keyword like " Bush " or " Julia Roberts " instead of the use of an outlined query face as used in [5] .
160
However , a real-time face tracker will become necessary if the target archive is established from too large quantities of videos , e.g. 24-hour continuous video recording that needs daily structuring .
However , a real-time face tracker will become necessary if a target archive is established from too large a quantity of videos , e.g. 24-hour continuous video recording that needs daily structuring .
161
Can the system cope with varying illumination , facial expression , scale , pose , camerawork , occlusion and large head motion ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who are moving from indoor to outdoor environment .
Can the system cope with varying illuminations , facial expressions , scales , poses , camerawork , occlusion , and large head motions ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who is moving from an indoor to an outdoor environment .
162
Different from non-broadcast video , e.g. video used for HCI , faces appearing in broadcast video varies from large close-up faces to small faces taken by a long-shot .
Different from non-broadcast video , e.g. video used for HCI , faces appearing in broadcast video vary from large close-up faces to small faces taken by a long-shot .
163
Pose variation , i.e. head rotations including pitch , roll and yaw , is another influencing factor , which can cause disappearance of part of the face .
Pose variations , i.e. head rotations including the pitch , roll , and yaw , is another influencing factor , which can cause disappearances of parts of faces .
164
Disappearance of part of the face is also apt to happen due to occlusion by other objects , and motion information may be distracted by alternate motion of them .
The partial disappearance of a face is also apt to happen due to occlusion by other objects , and motion information may be distracted by an alternate motion .
165
This problem is difficult to solve due to a fixed threshold .
This problem is difficult to solve because it has a fixed threshold .
166
For each tracked face , three steps are involved that are initialization , tracking and a stopping procedure , as illustrated in Figure 2 .
For each tracked face , three steps are involved , which are the initialization , tracking , and stopping procedures , as illustrated in Fig. 2 .
167
Most facial-feature-based face trackers [6 , 10] are only tested by using non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .
Most facial-feature-based face trackers [6 , 10] have been tested using only non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .
168
One example of appearance-based face tracker is [1] that has been introduced above .
One example of an appearance-based face tracker is [1] , which was introduced above .
169
Another example is proposed by Li et al [9] , which uses a multi-view face detector to detect and track faces of different poses .
Another example was proposed by Li et al. [9] , which uses a multi-view face detector to detect and track faces from different poses .
170
During the tracking procedure , face tracking systems usually employ a motion model that describes how the image of the target might change for different possible motions of the face to track .
During the tracking procedure , face tracking systems usually use a motion model that describes how the image of the target might change for different possible motions of the face to track .
171
But note that most 3D-based and mesh-based face trackers require relatively clear appearance , high resolution , and limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
However , it must be noted that most 3D-based and mesh-based face trackers require a relatively clear appearance , high resolution , and a limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
172
If none of the two eyes are in the face region , it will be determined as drifting and the tracking process will be stopped .
If neither of the eyes is in the face region , it will be determined as drifting and the tracking process will be stopped .
173
A general evaluation criterion , in terms of speed , robustness and accuracy , is needed for performance comparison between face trackers of different purposes .
A general evaluation criterion , in terms of speed , robustness , and accuracy , is needed for a performance comparison between the face trackers with different purposes .
174
To get the final strong classifier , we use the idea of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
To obtain the final strong classifier , we use the [idea / concept?] of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
175
For examples , as described in \CITE , [Reference numbers generally should not be grammatically part of the sentence .
For examples , as described by Fergus et al. \CITE , [Reference numbers generally should not be grammatically part of the sentence .
176
The good point of the methods \CITE is they are fully unsupervised .
An advantage of these methods \CITE is they are fully unsupervised .
177
However , the bad point is no model is learned to predict new images of the same category .
However , a disadvantage is that no model is learned for predicting new images of the same category .
178
This leads the number of collected images is reduced .
This leads to the reduction in the number of collected images .
179
One approach to re-rank these faces is to do clustering based on visual similarity .
One approach of re-ranking these faces is to cluster based on visual similarity .
180
We use the idea of density-based clustering described in \CITE to solve this problem .
We use the idea of density-based clustering described by Ester et al. and Breunig et al. \CITE to solve this problem . //idea / concept?
181
One limitation of the local density score based ranking is it could not handle the case that faces of another person have strong association in \MATH-neighbor set ( for example , many duplicates ) .
One limitation of the local density score based ranking is it cannot handle faces of another person strongly associated in the \MATH-neighbor set ( for example , many duplicates ) .
182
We used the dataset described in \CITE for our experiments .
We used the dataset described by Berg et al. \CITE for our experiments .
183
This is very naive method in which no prior knowledge between names and faces is used .
This is a rather naive method in which no prior knowledge between names and faces is used .
184
Our approach is beneficial in the case multiple faces residing in the returned image as shown in Figure \REF .
Our approach is beneficial when there are several faces in a returned image , as shown in Figure \REF .
185
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as continuous frames from a single camera at a time .
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .
186
By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
187
Nevertheless , they are sensitive to changes of illumination and motion .
Nevertheless , they are sensitive to changes in illumination and motion .
188
Recent works \CITE use machine learning methods for making decision and show impressive results on test videos of TRECVID \CITE which is a de-facto benchmark for evaluation of various techniques in shot boundary detection .
Recent works \CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .
189
In this study , we propose a new approach inspired from natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem of text segmentation .
In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .
190
The remaining of the paper is organized as follows .
The remainder of this paper is organized as follows .
191
To label a frame in video , firstly we extract features for that frame and then use a classifier , that has been trained by annotated frames in advance , to classify it into one of six categories mentioned above .
To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .
192
We use two typical features that are color moments , edge direction histogram for representing visual information of each frame .
We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .
193
The basic steps to compute edge orientation histogram feature are as follows :
The basic steps for computing the edge orientation histogram features are as follows :
194
It has been very efficiently proved in many pattern recognition applications \CITE .
They have been very efficiently proved to be useful in many pattern recognition applications \CITE .
195
In the binary classification case , the objective of the SVM is to find a best separating hyperplane with a maximum margin .
In the case of binary classification , the objective of the SVM is to find the best separating hyperplane with a maximum margin .
196
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) and NORM-FRM ( normal frame which does not belong to any shot transitions ) .
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) , and NORM-FRM ( normal frame that does not belong to any shot transitions ) .
197
A gradual transition usually has the pattern " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' and a cut transition usually has the pattern " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " ' .
A gradual transition usually has a " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' pattern and a cut transition usually has a " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " 'pattern .
198
Since the classifier occasionally produce false predictions due to variations caused by photo flashes , rapid camera movement and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many truth shot boundaries .
Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .
199
We divided 8 videos , each 30-minute length , into two sets : training set and testing set .
We divided eight videos , each 30-minute long , into two sets : a training set and a test set .
200
We used \MATH grid for dividing the input image into sub-images .
We used \MATH grid to divide the input image into sub-images .
201
In Table \REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
202
The number of dimensions of feature vectors using GCM and EOH is 20 while that of feature vectors using GCM+EOH is 40 .
The number of dimensions of the feature vectors using GCM and EOH was 20 , while that of feature vectors using GCM+EOH was 40 .
203
Our system achieves high precision and recall for the CUT transition and the result is comparable with the third-ranked system .
Our system achieves a high precision and recall for the CUT transition and this result is comparable to the third-ranked system .
204
Many previous shot boundary detectors usually divided the system into sub-systems in which special treatments were proposed to handle different types of shot transitions .
Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .
205
Experiments on various videos of TRECVID 2003 have shown that our approach is effective .
The experiments we conducted on various videos from TRECVID 2003 have shown that our approach is effective .
206
Recently , boosting is used widely in object detection applications because of its impressive performance in both speed and accuracy .
Recently , boosting has come to be used widely in object detection applications because of its impressive performance in both speed and accuracy .
207
However , learning weak classifiers which is one of the most significant tasks in using boosting is left for users .
However , learning weak classifiers , which is one of the most significant tasks in using boosting , is left to users . //learning / training / identifying / finding?<--Here and throughout , I am not sure that " learning " is the best word choice . If you change it here , it should be changed throughout .
208
Experiments have shown that strong classifiers learned by Ent-Boost can achieve good performance , and have compact storage space .
Experiments showed that strong classifiers learned by Ent-Boost can achieve good performance and be stored compactly . //[be stored compactly / achieve compact storage?]
209
Boosting [3] and its variants [4] ,[5] ,[6] ,[7] ,[8] ,[9] ,[10] have recently gained a lot of attentions from researchers because of its excellent performance .
Boosting [3] and its variants [4] ? [10] have recently gained much attention from researchers because of their excellent performance .
210
Performances of weak classifiers are integrated into the final form of the strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .
The performances of these weak classifiers are integrated into the final form of a strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .
211
Generally , for efficient computation , the dimension of the input space of weak classifiers is reduced to much lower than that of the strong classifier .
Generally , for efficient computation , the dimensions of the input space of weak classifiers are reduced be to much smaller than those of the strong classifier[s?] .
212
The first trend is dealing with the problem of how to design features for best representation of the target object .
The first trend is dealing with the problem of how to design features for best representing the target object .
213
This leads weak classifiers are too weak to boost when handling complex data sets .
This leads weak classifiers to be too weak to boost when handling complex data sets .
214
Normally , it has been done by trials and errors [6] ,[17] - a tedious task .
Normally , it has been done by trial and error [6] , [17] ? a tedious task .
215
It is therefore necessary to have a deterministic method to choose this number of bins automatically and optimally .
A deterministic method is therefore needed to automatically and optimally choose the number of bins .
216
It is a supervised discretization method which takes into account class information and data distribution , so it is generic and can be applied for any kinds of input data .
It is a supervised discretization method that takes into account class information and data distribution , so it is generic and can be applied to any kind of input data .
217
This method also proposes designing weak classifiers that partition the input space into subspaces so that its predictions are unique in each subspace .
Do you mean that the creators of this system proposed this?] designing weak classifiers that partition the input space into subspaces so that the predictions are unique in each subspace .
218
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by above measurements give comparable performance .
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by the above measures give similar performances . //[measurements / measures?]
219
However , it will be shown in section 5 , these performances are affected seriously if different subspace splitting methods are used .
However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .
220
However , instead of using equal-width binning method like Real AdaBoost [6] ,[17] which is hard to know the suitable number of bins in advance , we use entropy-based discretization method [19] to split the input space into subspaces .
However , instead of using the equal-width binning method used in Real AdaBoost [6] , [17] which has a hard time predicting the suitable number of bins in advance , we use the entropy-based discretization method [19] to split the input space into subspaces .
221
This subspace splitting process is totally automatically in which the stopping criteria of splitting process is determined through using Minimum Description Length Principles ( MDLP ) ( see the next section ) .
This subspace splitting process is totally automatic ; the stopping criteria of the splitting process are determined using minimum description length principles ( MDLP ) . This process will be described in greater detail in the next section .
222
This section gives a brief introduction on automatic subspace splitting using entropy-based discretization .
This section briefly describes automatic subspace splitting using entropy-based discretization .
223
A higher arity can make the understanding of an attribute more difficult , while a very low arity may affect predictive accuracy negatively .
A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .
224
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to send before partition is more than the length of the message required to send after partition .
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to be sent before the partition is more than the length of the message required to be sent after the partition .
225
The 10 ,000 patterns in each set are divided into a training set of 6 ,000 patterns and a test set of 4 ,000 examples .
The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 examples .
226
It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
These consisted of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
227
The feature value is defined as the difference of sum of the pixels within rectangles ( cf . Figure 3 ) .
The feature value was defined as the difference of the sum of the pixels within rectangles ( Figure 3 ) .
228
The curves indicate that the performances of Real AdaBoost and Ent-Boost are better than that of AdaBoost .
The curves indicate that the performances of Real AdaBoost and Ent-Boost were better than that of AdaBoost .
229
In addition , the performance of Real AdaBoost classifiers varies when using different number of bins .
In addition , the performance of Real AdaBoost classifiers varied when using different numbers of bins .
230
As for storage space , the Ent-Boost based classifier only employs 6 .79 bins on average which is much smaller than that of Real AdaBoost-based classifiers .
As for storage space , the Ent-Boost-based classifier only uses 6 .79 bins on average , which is much fewer than the number used by Real AdaBoost-based classifiers .
231
We have presented Ent-Boost , a variant of AdaBoost , which uses entropy measure for automatic subspace splitting and optimal weak classifier selection .
We have described Ent-Boost , a variant of AdaBoost , which uses entropy measures for automatic subspace splitting and optimal weak classifier selection .
232
By considering the class information and the distribution of the input data in splitting process , this method is generic and can be applied to other applications .
Because it considers the class information and the distribution of the input data in the splitting process , this method is generic and can be used for other applications .
233
It does not select a feature similar to already selected ones , even if it is individual powerful , as selecting it might not increase much information about the target class [7] .
It doesThese methods do not select a feature similar to ones already selected ones , even if itthe feature is individually powerful , as because selecting it might not do much to increase much information about the target class [7] .
234
However , binarizing features is not a suitable way to handle highly complex data for which it is hard to find the best threshold .
However , binarizing features is not a suitable way to handle highly complex data for which it is hard to finding the best threshold is difficult .
235
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
236
A multi-stage approach --- which is fast , robust and easy to train --- for a face-detection system is proposed .
A multi-stage approach that is fast , robust , and easy to train is proposed for a face-detection system .
237
First , a new stage is added to detect face candidate regions more quickly by using a larger window size and larger moving step size .
First , a new stage has been added to detect face candidate regions more quickly by using a larger window size and larger moving step size .
238
The proposed multi-stage-based system is shown to run faster than the original AdaBoost-based system while maintaining comparable accuracy .
The proposed multi-stage-based system has been shown to run faster than the original AdaBoost-based system while maintaining comparable accuracy .
239
Discriminative and informative features usually increase detection rate and reduce complexity of the training procedure [17] .
Discriminative and informative features usually increase detection rates and reduce the complexity of training procedures [17] .
240
In a typical face detector which is scale-free and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .
In a typical face detector that is scale- and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .
241
To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers is proposed [8] ,[1] ,[9] ,[20] ,[21] ,[11] .
To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers has been proposed [8] , [1] , [9] , [20] , [21] , [11] .
242
By this way , the complexity of classifiers is adapted corresponding to the difficulty in the input patterns .
In this way , the complexity of classifiers can be adapted corresponding to the difficulty in the input patterns . / / [is / can be?]
243
In [8] , non linear SVM classifiers using pixel-based features are arranged into a sequence with increasing number of support vectors , or in [9] , linear SVM classifiers trained at different resolutions are used for rejection and a reduced set of principle component analysis ( PCA )-based features are used with the non linear SVM at the classification stage in order to reduce computation time .
In [8] , nonlinear SVM classifiers using pixel-based features were arranged into a sequence by increasing the number of support vectors , while in [9] , linear SVM classifiers trained at different resolutions were used for rejection and a reduced set of principle component analysis ( PCA )-based features were used with the nonlinear SVM at the classification stage in order to reduce computation time .
244
Using about 10 features of the first two layers , more than 90\% of non-face patterns are rejected .
Using about 10 features of the first two layers , more than 90\% of non-face patterns were rejected .
245
-Thirdly , Haar-wavelet features used for all stages are informative [22] and evaluated extremely fast due to the introduction of the integral image .
-Haar wavelet features used for all stages are informative [22] and can be evaluated extremely quickly due to the introduction of the integral image .
246
In our experiment , with 20 ,000 training samples and 134 ,736 features , the average training time for choosing one feature associated with the weak classifier is about 30 minutes on a PC ( Pentium 4 , 2 .8 MHz , 512-MB RAM ) .
In our experiment , with 20 ,000 training samples and 134 ,736 features , the average training time for choosing one feature associated with the weak classifier was about 30 minutes on a PC ( Pentium 4 , 2 .8 MHz , 512-MB RAM ) .
247
Because the complexity of the training sets varies through layers in the cascade , it is undetermined how to choose these parameters automatically and optimally .
Because the complexity of the training sets varies throughout the layers in the cascade , a way to choose these parameters automatically and optimally has not been determined .
248
The contribution of this approach is three fold :
The contribution of this approach is threefold :
249
-First , to detect the face candidate regions , a new stage ( using a larger window size and a larger moving step size ) is added .
-First , to detect the face candidate regions , a new stage ( using a larger window size and a larger moving step size ) has been added .
250
Furthermore , it is unnecessary to re-evaluate these features because they have been previously evaluated .
Furthermore , these features do not need to be re-evaluated because they have already been evaluated .
251
Third , the training time of AdaBoost classifiers is shortened by using simple sampling techniques to reduce the number of features in the feature set .
Third , the training time of AdaBoost classifiers has been shortened by using simple sampling techniques to reduce the number of features in the feature set .
252
Along with using several SVM classifiers instead of many AdaBoost classifiers in later layers , the total training time is reduced significantly .
Along with using several SVM classifiers instead of many AdaBoost classifiers in later layers , the total training time has been significantly reduced .
253
With their new proposal , weak classifiers are trained only once and features are selected by the direct feature selection method that directly maximizes the learning objective of the output classifier .
With the new proposal of Wu et al. , weak classifiers are trained only once and features are selected by the direct feature selection method , which directly maximizes the learning objective of the output classifier .
254
Another direction is to optimally build the cascade to improve the overall performance of the cascade .
Another direction is to optimally build the cascade to improve its overall performance .
255
Actually , our system can benefit from this approach when building the rejection stage and thus also reduce the training time much more .
However , our system can benefit from this approach when building the rejection stage and can thus reduce the training time even further .
256
This is done by taking advantages of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers are extremely fast in computation .
This is done by taking advantage of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers enable extremely fast computation .
257
The \MATH window is chosen in accordance with the idea from [5] stated that the classifier can be trained to be invariant to translation by up to \MATH of original window size .
The \MATH window is chosen in accordance with the idea in [5] that the classifier can be trained to be invariant to translation by up to \MATH of the original window size .
258
In our experiments , only 100 features are used and hence it is faster than using any pixel-based SVM classifiers [8] ,[9] .
In our experiments , only 100 features were used , making classification faster than it would have been using pixel-based SVM classifiers [8] , [9] .
259
Compared to AdaBoost classifiers , SVM classifiers run much slower in running because of the large number of support vectors and heavy kernel computation .
Compared to AdaBoost classifiers , SVM classifiers run much more slowly because of the large number of support vectors and the heavy kernel computation .
260
Face patterns for training the 36x36 classifiers are generated by selecting 36x36 windows containing the 24x24 face window of the input image .
Face patterns for training the 36x36 classifiers were generated by selecting 36x36 windows containing the 24x24 face window of the input image .
261
To train the cascade of 24x24 AdaBoost classifiers used in the rejection stage , the same 7 ,500 face patterns are used for all layers .
To train the cascade of 24x24 AdaBoost classifiers used in the rejection stage , the same 7 ,500 face patterns were used for all layers .
262
Non-face patterns of the training and the validating sets of the first layer in the cascade are selected randomly .
Non-face patterns of the training and the validating sets of the first layer in the cascade were selected randomly .
263
For each layer classifier , 7 ,500 non-face patterns are used for training and 7 ,500 other non-face patterns are used for validating .
For each layer classifier , 7 ,500 non-face patterns were used for training and 7 ,500 other non-face patterns were used for validating .
264
The minimum of the detection rate is \MATH , the maximum of the false positive rate is \MATH and the maximum of the number of features in each layer is 200 .
The minimum of the detection rate was \MATH , the maximum of the false positive rate was \MATH , and the maximum of the number of features in each layer was 200 .
265
We carried out experiments to compare the performance of classifiers trained on these two feature sets : the full feature set \MATH containing 134 ,736 features and the reduced feature set \MATH containing 14 ,807 features ( excluding features with the small size ) .
We carried out experiments to compare the performance of classifiers trained on these two feature sets : the full feature set \MATH , containing 134 ,736 features and the reduced feature set \MATH , containing 14 ,807 features ( excluding features of small size ) .
266
Two classifiers are trained up to the maximum of 200 features .
Two classifiers were trained up to the maximum of 200 features .
267
Rejection performance is evaluated through the false positive rate on a validation test set which contains 500 ,000 non-face patterns .
Rejection performance was evaluated through the false positive rate on a validation test set that contains 500 ,000 non-face patterns .
268
All non-face patterns are selected randomly from the training set mentioned above .
All non-face patterns were selected randomly from the training set mentioned above .
269
The result shown in Figure 10 indicates that the performances of these two classifiers are no different , especially when the number of features is large enough , for example , more than 50 .
The results shown in Figure 10 indicate that the performances of these two classifiers were no different , especially when the number of features was large enough , for example , more than 50 .
270
Our another experiment has shown that , for similar performance , the AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than that trained on the full feature set .
Another experiment we conducted showed that , for similar performance , an AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than one trained on the full feature set . / / [Do you need a reference here , or is this still talking about the experiments you report in this paper?]
271
Since a 36x36 face sample contains a lot of background outside the 24x24 face region while the classifier is required to be fast and to keep all possible face regions , training parameters are set as follows : the minimum detection rate of \MATH and maximum of false positive rate of \MATH .
Since a 36x36 face sample contains a large proportion of background outside the 24x24 face region and the classifier is required to be fast and to keep all possible face regions , a minimum detection rate of \MATH and a maximum of false positive rate of \MATH were set as the training parameters .
272
It is somehow similar to features of the first 24x24 layer classifier as shown in Figure 11( b ) .
They are somehow similar to the features of the first 24x24 layer classifier as shown in Figure 11( b ) . / / [somehow?This sounds vague . How are they similar?]
273
For comparison of the performance of SVM classifiers , 2 ,450 face patterns and 7 ,500 non-face patterns which are separated from the training set ( section 6 .1 ) were used .
For comparison of the performance of SVM classifiers , 2 ,450 face patterns and 7 ,500 non-face patterns that were separated from the training set ( section 6 .1 ) were used .
274
To determine how many features is robust enough , we used the 200-feature set selected in layer 17 to generate different subsets of features with different number of features .
To determine the number of features is that would be sufficiently robust , we used the 200-feature set selected in layer 17 to generate different subsets of features with different numbers of features .
275
The speeds of SVM classifiers using 100 , 125 and 175 features are not importantly different because their difference in terms of number of features and number of support vectors is inconsiderable .
The speeds of SVM classifiers using 100 , 125 , and 175 features were not importantly different because their difference in terms of number of features and number of support vectors were not large enough to have a significant impact .
276
Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and , thus , can save significant training time ( which is approximate 27 times in total ) .
Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and can thus save significant training time ; the training time needed using the new system is approximately 27 times shorter / approximately 27 rounds of training are needed in the new system . / / <--I think that the first choice here is your intended meaning , but please check carefully .
277
The fraction of the remaining patterns on these two tables indicates that most of the non-face patterns , i.e. , \MATH , are rejected by the first stage , the cascade of 36x36 AdaBoost classifiers .
The fraction of the remaining patterns on these two tables indicates that most of the non-face patterns , i.e. , \MATH , were rejected by the first stage , the cascade of 36x36 AdaBoost classifiers .
278
It also shows that most of the processing time used by the AdaBoost+SVM system , \MATH , is used for SVM classifiers .
It also shows that most of the processing time used by the AdaBoost+SVM system , \MATH , was used for SVM classifiers .
279
First , the cascade of 36x36 AdaBoost classifiers rejects a lot of non-face patterns extremely fast while slow SVM classifiers only process a very small number of the remaining patterns .
First , the cascade of 36x36 AdaBoost classifiers rejected many of non-face patterns extremely quickly , while slow SVM classifiers only processed a very small number of the remaining patterns .
280
Second , many images in the MIT+CMU test set contain large portion of background which was mentioned in [9] which said the ratio of non-face to face patterns is about 50 ,000 to 1 .
Second , many images in the MIT+CMU test set contain large portion of background , which [9] mentioned has a ratio of non-face to face patterns of about 50 ,000 to 1 .
281
Experimental results showed that the AdaBoost+SVM system runs faster than that of the original AdaBoost on \MATH of total number of images in this test set .
Experimental results showed that the AdaBoost+SVM system ran faster than that of the original AdaBoost on \MATH of the total number of images in this test set .
282
Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers do not affect so much in final performance because it might also be rejected by 24x24 classifiers in later layers .
Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers did not severely affect the final performance because they might also be rejected by 24x24 classifiers in later layers .
283
Discriminant Haar wavelet features selected from AdaBoost are used for all stage classifier to take advantages from their efficient representation and fast evaluation .
Discriminant Haar wavelet features selected from AdaBoost are used for all stage classifiers to take advantage of their efficient representation and fast evaluation .
284
to improve the retrieval performance of image search engines that use textual information for indexing , it is necessary to utilize visual information .
It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing .
285
One popular approach is to learn visual consistency among the images returned by these search engines .
One popular approach has been to learn visual consistency between images returned by these search engines .
286
The key contribution of this paper is to introduce a query-dependent feature to represent this relevancy and an unsupervised method to collect training samples for learning the generic classifier .
The key contribution of this research is to introduce a query-dependent feature to represent this relevance and an unsupervised method of collecting training samples to learn the generic classifier .
287
Image search is essential for many search engines .
Image searches are essential for many search engines .
288
However , content-based image understanding is a challenging and unsolved problem .
However , understanding content-based images remains a challenging and unsolved problem .
289
One popular approach \CITE combining both text and visual information is to use text information to quickly retrieve a set of candidates and then do post-processing (i . e . re-rank) on this set to improve the precision .
One popular approach \CITE combining both text and visual information has been to use text information to quickly retrieve a set of candidates and then do post-processing ( i . e . , re-ranking ) on this set to improve precision .
290
There are two ways for post-processing : The first way \CITE is to build a ranker or a classifier specific to the given query using the returned images .
There are two ways of doing post-processing : The first \CITE has been to build a ranker or a classifier specific to the given query using the returned images .
291
The second way \CITE is to build a generic classifier once and then use it for all new queries .
The second way \CITE has been to build a generic classifier once and then use it for all new queries .
292
We follow the latter way for the problem of face retrieval in which the system enables users to search persons's appearance by their names .
We pursued the latter way to solve the problem with face retrieval in which the system enables users to search people's appearances by their names .
293
this classifier is independent with the identity of faces , so it can be shared for multiple queries (cf . Figure \REF) .
As this classifier is independent of the identity of faces , it can be shared for multiple queries ( cf . Figure \REF ) .
294
experimental results show that the relevance classifier that is independent with underlying ranking algorithm of existing search engines can significantly boost the performance .
The experimental results demonstrated that the relevance classifier that is independent of the underlying ranking algorithms of existing search engines could significantly boost performance .
295
Specifically , We detect and group faces of persons appearing in video programs in face tracks in which each face track contains of the faces of one person .
We specifically detected and grouped faces of people appearing in video programs in face tracks in which each face track contained the faces of one person .
296
To distinguish face tracks of different persons , we assume that if multiple faces are detected at different locations in one frame , they are of different persons (cf . Figure \REF) .
To distinguish the face tracks of different people , we assumed that if multiple faces were detected at different locations in one frame , they would be of different people ( cf .
297
To enlarge the number of such face tracks , We use video programs of multiple genres and channels .
We used video programs from multiple genres and channels to increase the number of such face tracks .
298
From these faces , We can artificially generate face sets similar to the sets returned by search engines given person names .
We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .
299
Since we know the relevance of these faces to the artificial sets , the labels of each face can be easily generated and no human intervention is needed for this process .
Since we knew the relevance of these faces to the artificial sets , the labels for each face could be easily generated and no human intervention was needed in this process .
300
Note that the label of faces in our approach is not identity of that face . It is the relevance between the face and the associated query .
Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .
301
Collecting training sets from such external sources as video archives is easy and efficient because : firstly , a large number of videos can be easy to obtain .
Collecting training sets from such external sources as video archives is easy and efficient because , first , a large number of videos can be easily obtained .
302
For example , people can record broadcast videos of different channels in a certain period .
For example , people can record broadcast videos from different channels within a certain period .
303
There are different approaches described in \CITE for re-ranking images containing general objects and faces returned from text-based search engines .
There have been different approaches \CITE to re-ranking images containing general objects and faces returned from text-based search engines .
304
Work such as \CITE extend topics models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or Hierarchical Dirichlet Process to learn generative model based classifiers .
Work \CITE has extended to topics on models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or the Hierarchical Dirichlet Process to learn generative model-based classifiers .
305
These models can handle noisy image data in some degree .
These models can handle noisy image data to some degree .
306
In addition , how to select the best topic associated with the input query for identifying target label is still challenging \CITE .
In addition , how the best topic is selected associated with the input query to identify the target label is still a difficult issue \CITE .
307
In \CITE , Textual information is used to build a text ranker to re-rank the returned images \CITE .
Textual information has been used to build a text ranker to re-rank the returned images \CITE .
308
The top images in this ranked list are used as positive samples to train visual classifiers using SVM (Support vector machines) .
The top images in this ranked list were used as positive samples to train visual classifiers using SVM ( Support vector machines ) .
309
This method makes the training data cleaner that leads to performance improvement .
This method made the training data cleaner and led to improved performance .
310
In \CITE , A multiple instance learning framework is used to learn category models from images associated with keywords \CITE .
A multiple-instance learning framework has been used to learn category models from images associated with keywords \CITE .
311
Negative bags are collected from image sets corresponding to unrelated keywords .
Negative bags were collected from image sets corresponding to unrelated keywords .
312
The learned model is used to re-rank the images .
The learned model was used to re-rank the images .
313
For re-ranking faces , work described in \CITE use Gaussian mixture models to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
Gaussian mixture models have been used for re-ranking faces to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
314
In \CITE , Discriminative approach based models such as SVM and linear discriminant analysis are used instead of Gaussian mixture models \CITE .
Discriminative-approach-based models such as SVM and linear discriminant analysis have been used instead of Gaussian mixture models \CITE .
315
In \CITE , A densest graph based method is used for finding the face group relevant to the query \CITE .
A densest-graph-based method has been used for finding the face group relevant to the query \CITE .
316
In \CITE{Krapac10CVPR} , Only one generic classifier is built in advance \CITE and then used for all queries .
Only one generic classifier has been built in advance \CITE and then used for all queries .
317
This generic classifier is a relevance classifier that learns relevancy between an image and the query .
This generic classifier was a relevance classifier that learned relevance between an image and the query .
318
We extend it by two means : first , query-dependent features specific for faces are proposed , and second , the training data for learning the generic classifier is collected automatically by mining video archives .
We extended it in two ways : first , query-dependent features specific to faces are proposed , and second , the training data for learning the generic classifier are collected automatically by mining video archives .
319
The ranked list is then return to users as shown in Figure \REF( b ) .
The ranked list is then returned to users as shown in Figure \REF( b ) .
320
Meanwhile , to build the generic classifier which is independent with any \textit{'personX'} , each face is represented by the query-dependent feature .
Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .
321
Each feature is treated as binary indicating the presence or absence of the query terms in textual data associated with the input image , for example , filename , image title , and nearby text .
Each feature was treated as binary indicating the presence or absence of query terms in the textual data associated with the input image , e.g. , filenames , image titles , and nearby text .
322
Extending this query-dependent feature for using visual information is not trivial since we can not compute the presence and absence of the query term such as 'George Bush' in each face .
Extending this query-dependent feature to use visual information is not trivial since we cannot compute the presence or absence of query terms such as 'George Bush' in each face .
323
To be able to model the relevance between a face and the given query , We assume that there is visual consistency among faces returned by search engines for that query .
We assumed that there would be visual consistency between faces returned by search engines for a query to be able to model the relevance between a face and that given query .
324
This assumption is widely accepted in most of the work of this field \CITE .
This assumption is widely accepted in most of the work in this field \CITE .
325
This number of points \MATH is called the neighborhood score of \MATH and is defined as follows : \MATH where \MATH is the total number of points of the input dataset .
This number of points \MATH is called the neighborhood score of \MATH and is defined as : \MATH where \MATH is the total number of points in the input dataset .
326
A low value of \MATH indicates \MATH is a candidate of outliers , while a high value of \MATH indicates \MATH is a member of one strong association cluster .
A low value for \MATH indicates \MATH is a candidate of outliers , while a high value for \MATH indicates \MATH is a member of one strong association cluster .
327
In practice , it is difficult to know \MATH because it depends on underlying distribution of the input dataset .
In practice , it is difficult to know \MATH because this depends on the underlying distribution of the input dataset .
328
Points with larger values for \MATH have more sparse neighborhoods and are likely outliers than points belonging to dense clusters which usually have lower values of \MATH .
Points with larger values for \MATH have sparser neighborhoods and are more likely outliers than points belonging to dense clusters , which usually have lower values for \MATH .
329
In our framework , Each face is an sample , and non-outliers / outliers mean faces relevant / irrelevant to the query ( i .e . target person ) .
Each face in our framework is a sample , and non-outliers / outliers mean faces are relevant / irrelevant to the query ( i.e. , target person ) .
330
First , by mining video archives , we automatically collect a set of faces of \MATH different persons \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of persons; and
First , by mining video archives , we automatically collect a set of faces of \MATH different people \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of people .
331
Specifically , We use the following heuristics to pick a set of different persons appearing in video archives :
We specifically use the following heuristics to pick a set of different people appearing in video archives :
332
-If there are more than one face appearing in different locations in one frame , they likely belong to different persons .
-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .
333
-If two persons appear in video programs broadcast by different broadcast stations ( e .g . , CNN , MSNBC , and CCTV ) , they are likely different .
-If two people appear in video programs broadcast by different broadcast stations ( e.g. , CNN , MSNBC , and CCTV ) , they are likely to be different .
334
If we have large video archives , using these heuristics we can collect a sufficient number of training samples for learning the relevance classifier .
If we have large video archives , we can collect a sufficient number of training samples to learn the relevance classifier by using these heuristics .
335
To guarantee selected face tracks representing different persons , for one channel , only face tracks of one shot was picked .
To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .
336
As a result , there are 5 ,126 faces of 19 face tracks picked from the 7 channels corresponding to 19 different persons .
As a result , there were 5 ,126 faces of 19 face tracks selected from the seven channels corresponding to 19 different people .
337
It only knows any two face tracks represent different persons .
It only knew any two face tracks represented different people .
338
Using these face tracks , We generated 133 labeled sets described in Section \REF and used them for training the relevance classifier .
We generated the 133 labeled sets described in Section \REF using these face tracks and used them to train the relevance classifier .
339
These names are widely used in experiments such as \CITE .
These names have widely been used in experiments \CITE .
340
Specifically , for each face , 9 facial feature points were detected , and 4 more facial feature points were inferred from these 9 points .
Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .
341
In total , There were 13 feature points from which features are extracted .
There were a total of 13 feature points from which features were extracted .
342
The features are intensity values lying within the circle with radius of 15 pixels .
The features were intensity values lying within a circle with a radius of 15 pixels .
343
Figure \REF shows illustration of this feature .
Figure \REF illustrates this feature .
344
-DistScore-TrainGoogleImages : The training set is the set of annotated faces returned by Google Images Search for 23 person names .
-DistScore-TrainGoogleImages : The training set was the set of annotated faces returned by Google Images Search for 23 people�fs names .
345
-NNScore-TrainGoogleImages : The training set is the same as DistScore-TrainGoogleImages .
-NNScore-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
346
The training set is the set of annotated faces artificially generated by our method described in Section \REF .
The training set was the set of annotated faces artificially generated with our method described in Section \REF .
347
-NNScore-TrainTRECVID : The training set is the same as DistScore-TrainTRECVID .
-NNScore-TrainTRECVID : The training set was the same as DistScore-TrainTRECVID .
348
-Krapac[11]-TrainGoogleImages : The training set is the same as DistScore-TrainGoogleImages .
-Krapac[11]-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
349
Since this method was proposed to handle images , not for faces , we modified it for handling faces .
Since this method was proposed to handle images , not faces , we modified it to handle faces .
350
Therefore , we can use small number of features for reducing the computational cost .
Therefore , we could use small numbers of features to reduce the computational cost .
351
-Our proposed method DistScore-TrainTRECVID has comparable performance to the state of the art method in specific classifier-based approach Mensink[15]-Friends .
-The method of DistScore-TrainTRECVID we propose performed comparably to the state-of-the-art method in the specific classifier-based approach of Mensink[15]-Friends .
352
-Our proposed method DistScore-TrainTRECVID outperforms the method proposed by Krapac et al . customized for handling faces .
-Our proposed method DistScore-TrainTRECVID outperformed the method proposed by Krapac et al. , which was customized to handle faces .
353
The result of DistScore-TrainTRECVID on YahooNews Images set and Google Images set indicates that the relevance classifier of our proposed method is able to generalize well on different queries and independent with underlying ranking algorithms of search engines .
The results for DistScore-TrainTRECVID on the YahooNews Images set and Google Images set indicate that the relevance classifier with our proposed method was able to generalize well on different queries and was independent of underlying ranking algorithms used in search engines .
354
Figure \REF shows an example of re-ranking result of top-30 faces for the query John Paul that is one of the most difficult cases of the YahooNews Images set .
Figure \REF shows an example of re-ranking results for the top-30 faces for the query John Paul , which is one of the most difficult cases in the YahooNews Images set .
355
The result clearly shows that our proposed method outperforms the other state of the art methods .
The results clearly demonstrate that our proposed method outperformed the other state-of-the-art methods .
356
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and SASH \CITE can speed up the nearest neighbor search significantly .
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and a Self Adaptive Set of Histograms ( SASH ) \CITE could significantly speed up the nearest neighbor search .
357
We have presented a novel method for re-ranking face images returned by existing search engines .
We have presented a novel method of re-ranking face images returned by existing search engines .
358
Experiments shown that although our method is unsupervised and independent with underlying algorithms of existing search engines but successfully learns visual consistency among returned faces to boosts retrieval performance .
Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .
359
In the current digital environment , the mathematical content being published on the Web is increasing day by day . While more and more mathematical contents being available on the Web , retrieving mathematical contents becomes an important issue for many users .
The mathematical content being published on the Web is increasing day by day , and retrieving mathematical content has become an important issue for many users .
360
Therefore , users need specialized search systems to nd the formula that is relevant to their requirements .
Moreover , users need specialized search systems to find formulas that are relevant to their needs .
361
But full mathematical search is still not available .
But even this site does not provide a full mathematical search .
362
This site and some recent works done by Adeel et al. [2] and Yokoi and Aizawa [1] propose similarity search methods based on MathML but these works do not make use of the semantics of the formulas' surrounding text , which is considered to be important information sources .
This site and some recent work done by Adeel et al. [2] and Yokoi and Aizawa [1] employ similarity search methods based on MathML but they do not make use of the semantics of the formulas' surrounding text . //[ ? ? propose is unclear in the sense of a website .]
363
Relations between formulas and their name could also be used to correct errors in mathematical OCR systems , such as Infty [5] .
The relationship between formulas and their names can also be used to correct errors in mathematical OCR systems , such as Infty [5] .
364
Section 4 concludes the paper and gives avenues for future works .
Section 4 concludes the paper and gives avenues of future study .
365
At this point , we use some heuristics to provide an adequate solution for matching mathematical formulas with their names .
We used heuristics to ensure adequate matching of mathematical formulas with their names .
366
The experimental results have shown how helpful this information provides to the users of mathematical search .
The experimental results showed how helpful this information is to mathematical search users .
367
Experimental results on the Wolfram Function Site show that our approach achieves an improvement against the prior rule-based system .
Experimental results on the Wolfram Function Site show that our approach is an improvement over prior rule-based systems . // <Note : It seems that where were two prior systems that were compared . If not , you can go back to using a prior system .> .
368
- Second , the underlying mathematical meaning of an expression need to follow a semantic markup in a semantically rigorous way .
- Second , the underlying mathematical meaning of an expression needs to follow a semantic markup in a semantically rigorous way .
369
The probability distribution will be automatically learned from data that have both Presentation and Content MathML markup , that is the parallel markup MathML data .
The probability distribution is automatically learned from both Presentation and Content MathML markup data , that is , parallel markup MathML data .
370
We have two main contributions in this paper
There are two main contributions in this paper :
371
- Second , mathematics knowledge such as symbol 's meanings or structural relations is automatically learned while training , therefor it is not required mathematics experts nor human effort and it is also easier to update the system given more data .
- Second , mathematics knowledge such as a symbol 's meanings or structural relations is automatically learned while training ; therefore , the system requires no human effort or expertise , and it is easier to update with more data .
372
We set up another experiment to confirm the correlation between system performance and training set size and saw that increasing the size of training data actually boost the system performance .
We performed another experiment to assess the correlation between the system 's performance and training set size and found that increasing the size of the training data boosted the system 's performance .
373
Our experimental results show that the proposed approach works well on the mathematics semantic enrichment problem and it excels the previous work by providing significantly less error rate .
Our experimental results show that our approach works well in dealing with the mathematics semantic enrichment problem and it outperforms the previous work by making significantly fewer errors .
374
org Math \CITE , ASCIIMathML \CITE and OpenMath \CITE , but these markup can be converted to MathML using freely available tools .
org Math \CITE , ASCIIMathML \CITE , and OpenMath \CITE , but these markups can be converted into MathML by using freely available tools .
375
In this section , we list some works that related to exploit the meaning of mathematical expressions .
In this section , we list some of the work on exploiting the meanings of mathematical expressions .
376
SMT uses a very large data set of good translations , that is , a corpus of texts which have already been translated into other language , and then uses those texts to automatically infer a statistical model of translation .
SMT uses a very large data set of good translations , that is , a corpus of texts which have already been translated into another language , and it uses those texts to automatically infer a statistical model of translation .
377
Tree-based or syntax-based SMT can be used for tree-to-tree translation but it has two drawbacks when apply to the problem of translating from Presentation to Content MathML expression .
Tree-based or syntax-based SMT can be used for tree-to-tree translation but it has two drawbacks when it is applied to the problem of translating Presentation into Content MathML .
378
To overcome these limitations , we introduced two separated sets of rule : fragment rules and translation rules .
To overcome these limitations , we made two separate rule sets : fragment rules and translation rules .
379
The detail is described in the next section .
The details are described in the next section .
380
Another example are the pairs of parentheses , it is used to indicate that the expressions in the parentheses go together , while its structure already encoded that information .
Another example is pairs of parentheses ; these are used to indicate that the expressions in the parentheses go together , despite that their structure already encodes that information . // <The original is unclear . The rewrite is a guess . > .
381
For simplification , expressions with more than 200 content nodes also be removed .
For simplification , expressions with more than 200 content nodes are also removed .
382
Based on the aligned data , we use some heuristics to extract rules which we called " fragment rules " .
Based on the aligned data , we use heuristics to extract rules that we call " fragment rules " .
383
- Second , the fragment rule is applied to the expression until it could not be divided any further .
- Second , the fragment rule is applied to the expression until it cannot be divided any further .
384
- Third , the small sub-expressions in Presentation MathML markup will be translated into sub-expressions in Content MathML markup using translation rule set .
- Third , the small sub-expressions in Presentation MathML markup are translated into sub-expressions in Content MathML markup by using the translation rule set .
385
These datasets we used contain 205 , 653 mathematical expressions belong to 6 categories .
The datasets we used contain 205 , 653 mathematical expressions belonging to six categories .
386
Of the 10 subsets , a single subset is retained as the validation data for testing the model , and the remaining subsets are used as training data .
Of the ten subsets , a single subset was retained as the validation data for testing the model , and the remaining subsets were used as training data .
387
In the first experiment , the data is not compatible with SnuggleTeX since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .
In the first experiment , the data was not compatible with SnuggleTeX since SnuggleTeX uses ASCII MathML but the Wolfram Functions site does not .
388
In the experiments , we extend the conventional definition of " Translation Error Rate " and use a metric which is the combined version of
In the experiments , we extended the conventional definition of " Translation Error Rate " and used a metric which is a combined version of
389
Our system archived 24 percent TEDR less than the output using SnuggleTeX .
Our system had a 24 percent lower TEDR in comparison with SnuggleTeX .
390
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expression to a Content MathML expression has the significant improvement over a prior system .
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expressions to Content MathML expressions is a significant improvement over prior systems .
391
The experimental results confirm that this approach is helpful to the understanding of mathematical expressions .
The experimental results confirm that it would be helpful for automatic understanding of mathematical expressions .
392
Recent research shows a major part of difficult cases in event extraction for the biomedical domain are related to coreference .
Current research has shown that major difficulties in event extraction cases for the biomedical domain are related to coreference .
393
However , the shared task results showed that transferring coreference resolution methods developed for other domains to the biological domain was not straight forward , which is supposed to be caused by the domain differences in coreference phenomena .
However , the shared task results indicated that transferring coreference resolution methods developed for other domains to the biological domain was not straightforward , due to the domain differences in the coreference phenomena .
394
In particular , the domain-specific information is encoded into semantic classification modules whose output is used in different components of the coreference resolution .
In particular , the domain-specific information is encoded into semantic classification modules for which the output is used in different components of the coreference resolution .
395
Analysis of the experimental results showed that semantic classification using protein information has contributed to an increase in performance ( 2.3 % on the test data , and 4 .0% on the development data , in F-score ) .
Analysis of the experimental results revealed that semantic classification , using protein information , had contributed to an increase in performance by 2.3 % on the test data , and 4 .0% on the development data , in F-score .
396
Since such information is difficult to be transferred across different domains , we need to continue seeking for methods to exploit and use it in coreference resolution .
Since it is difficult to transfer domain-specific information across different domains , we need to continue to seek methods to exploit and use it in coreference resolution .
397
In the figure , protein names are highlighted in bold face , P4 - P10 , and targeted anaphoric expressions of the shared task , e.g. pronouns and definite noun phrases , are T29 , and T32 , of which the antecedents are indicated by arrows if found in the text .
In the figure , protein names P4 - P10 are highlighted in boldface ; the targeted anaphoric expressions of the shared task ( pronouns and definite noun phrases ) are T29 , and T32 , for which the antecedents are indicated by arrows , if found in the text .
398
Without knowing this coreference relation , it becomes hard to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is localization of p65 ( out of nucleus ) according to the framework of BioNLP-ST .
Without knowing this coreference relation , it becomes difficult to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is a localization of p65 ( out of nucleus ) , according to the framework of BioNLP-ST .
399
This is an encouraging result , since the authors make use of an external coreference resolution tool originally built for the news domain , without much domain adaptation on the main coreference resolution algorithm .
The results are promising , since the authors make use of an external coreference resolution tool originally built for the news domain , without much domain adaptation on the main coreference resolution algorithm .
400
However , the external coreference tool achieves much lower results on biological texts than that on news texts , from 66 .38% down to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .
However , the external coreference tool " s performance drops for biological texts than for news texts , from 66 .38% to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .
401
The analysis results in also showed that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type are 27 .5% F-score and 10 .1 F-score respectively , which are far less than that for relative pronoun ( the RELAT type ) 66 .2 % F-score .
An analysis of the results indicated that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type was 27 .5% F-score and 10 .1 F-score , respectively ; the scores were much lower than the F-score for relative pronouns ( the RELAT type ) , which yielded a 66 .2 % F-score .
402
Thus , it can be inferred that definite noun phrases and pronouns are more difficult to be resolved than relative pronouns .
Thus , it can be inferred that it is more difficult to resolve definite noun phrases and pronouns than relative pronouns .
403
In this paper , we compare the contributions of different features in coreference resolution , two simple types of domain-portable information : discourse preference and number-agreement , and domain-specific information which can be considered as more difficult to be transferred across different domains .
In this paper , we compare the contributions of different features in coreference resolution ; two simple types of domain-portable information : discourse preference and number-agreement , is compared , as well as domain-specific information , which is considered to be more difficult to be transferred across different domains .
404
Moreover , because the focus of our task is protein references , semantic filters can be used to filter out non-protein anaphors at this stage .
Moreover , because our task focuses on protein references , semantic filters can be used to filter out non-protein anaphors at this stage .
405
In practice , for definite noun phrase type of anaphors , this is done using a list of possible head words of protein references , and for pronouns , their context words are used .
In practice , for definite noun phrase type of anaphors , this is accomplished , by using a list of possible head-words of protein references ; for pronouns , their context words are used .
406
More details of the methods can be found in the following section .
More details of these methods can be found in the following section .
407
In theory , all expressions in the set of markables can become antecedent candidates , however too much candidates makes it difficult to achieve correct antecedent prediction .
In theory , all expressions in the set of markables can become antecedent candidates ; however , too many candidates makes it difficult to achieve correct antecedent prediction .
408
For example , the two expressions dominant negative form and its in our example in Table 1 , can not be coreferential with each other , since they are connected via the preposition of .
For example , the two expressions : dominant negative form and its in our example in Table 1 , cannot be coreferential with each other , since they are connected via the preposition of .
409
Another syntactic filter removes pronouns which are not in the same pronoun family as the anaphor .
Another syntactic filter removes pronouns that are not in the same pronoun family as the anaphor .
410
Step 4 - Antecedent predicion : selects the best candidate in the antecedent candidate set , and forms a response coreference link .
Step 4 - Antecedent prediction : The best candidate in the antecedent candidate set is selected , and a response coreference link is formed .
411
-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate which is not number conflict with anaphor is selected .
-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate , which does not conflict in number with the anaphor , is selected .
412
The rules are implemented using different features of expressions such as syntactic types of expression , head noun , semantic types , etc. , in a similar way to [ 22 ] .
The rules are implemented using different features of expressions , such as syntactic types of expressions , head noun , semantic types , etc. , in a similar way to [ 22 ] .
413
More details about the implementation of the main components of our system shown in Figure 2 are presented below .
More details concerning the implementation of the main components of our system shown in Figure 2 are presented below .
414
In this step , we want to filter out those pronouns and definite noun phrases that are not target of this task , comprised of two types : non-anaphoric expressions , and anaphoric expressions which do not point to proteins .
In this step , we want to filter out those pronouns and definite noun phrases that are not a target of this task . The expressions are comprised of two types : non-anaphoric expressions , and anaphoric expressions , which do not point to proteins .
415
Syntactic dependency relations The fact that arguments of some dependency relations such as poss-arg12 and prep-arg12 do not corefer with each other enables us to use them to correctly eliminate the number of antecedent candidates .
Syntactic dependency relations Since arguments of some dependency relations ( such as poss-arg12 and prep-arg12 ) do not corefer with each other , they can be used to correctly eliminate the number of antecedent candidates .
416
For this type of anaphors , any syntactic parser can be used to find the relations between relative pronouns and their arguments .
For these types of anaphors , any syntactic parser can be used to find the relation between relative pronouns and their arguments .
417
This is exactly what our system does .
Our system accomplishes this task .
418
By definition , two coreferential expressions refer to the same thing , which implies a semantic-constraint on coreference relationship .
By definition , two coreferential expressions are identical , which implies a semantic-constraint on coreference relationship .
419
Coreferential definite noun phrases in text are used in broader meaning of coreference .
Coreferential definite noun phrases in text are used to include a broader definition of coreference .
420
Distinguishing such non-anaphoric definite noun phrases from anaphoric ones is an uneasy task .
Distinguishing such non-anaphoric definite noun phrases from anaphoric ones is a difficult task .
421
Knowing their semantic type helps to filter out irrelevant candidate antecedents , increasing chance to pick up the right antecedent or the precision of antecedent prediction .
Knowing their semantic type helps to filter out irrelevant candidate antecedents , thereby increasing the chance of picking up the right antecedent , and increasing the precision of antecedent prediction .
422
Those candidates which are not agree with the anaphor in semantics are filtered out .
Those candidates that are not in agreement with the anaphor in semantics are filtered out .
423
Breaking down the system performance by types of anaphors gives us an insight into what have been solved by our methods , and what needs more improvement effort .
Breaking down the system performance by the different types of anaphors provides us with insight into what has been accomplished / solved by our methods , and also provides us with improvement opportunities .
424
The analysis results are given in section Discussions .
The analyses of the results are provided in the section entitled Discussions .
425
The combination of rule 1 , 2 and 3 resulted in 62 .4% fscore ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contribute to the increasement of 4 points Fscore on the development set , and 2 .3 points Fscore on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .
The combination of rule 1 , 2 and 3 resulted in a 62 .4% F-score ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contributes to a 4-point F-score increase in the development set , and 2 .3-point F-score increase on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .
426
In our system , domain-specific semantic information is ultilized at two places : anaphor selection and antecedent prediction .
In our system , domain-specific semantic information is utilized in two places : anaphor selection and antecedent prediction .
427
The effect of semantic information in antecedent prediction has been analyzed in above section .
The effect of semantic information in antecedent prediction has been analyzed in the sections above .
428
This is because the semantic filter is the only way to filter out definite noun phrase anaphors .
This decrease is due to the fact that the semantic filter is the only way to filter out definite noun phrase anaphors .
429
This is an encouraging sign to seek for a systematic method to exploit and include such contextual information in coreference resolution .
This gain is a good indication for seeking a systematic method to develop and include such contextual information in coreference resolution .
430
In furture , corpus annotation and evaluation scheme should be revised for the ease of automation of coreference resolution .
In future , revision of corpus annotation and evaluation schemes would benefit the ease of automation of coreference resolution .
431
Incorporation of recent works for coordination resolution like [ 20 ] should be useful to improve the performance .
Incorporation of recent works for coordination resolution like [ 20 ] should be useful for improving the performance .
432
However , from another perspective , the perspective of coreference data creation , we should revise the markable annotations , for the sake of automatic and robust markable detection .
However , from the perspective of coreference data creation , revision of the markable annotations would aid in automatic and robust markable detection .
433
The success of corpus-based methods has made syntactically annotated corpora important resources for natural language processing .
Syntactically annotated corpora have become important resources for natural language processing , due in part to the success of corpus-based methods .
434
This is also a concern of Vietnamese Treebank ( VTB ) , the first and the only publicly available syntactically annotated corpus so far for the Vietnamese language .
This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language .
435
Although word segmentation is straight-forward for space-delimited languages like English , this is not true for languages like Vietnamese of which no standard criterion for word segmentation exists .
Although word segmentation is straight-forward for space-delimited languages like English , this is not the case for languages like Vietnamese for which a standard criterion for word segmentation does not exist .
436
There are two possible reasons for this .
There are two possible reasons to explain this outcome .
437
Second , parsing Vietnamese is a diffcult problem by its own , and we need to seek new solutions to the problem .
The second reason is the difficulty of parsing Vietnamese ; we need to seek new solutions to address this problem .
438
For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) ; '' Word segmentation is expected to break down the sentence at the boundaries of these words , not to split `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .
For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words , `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) '' . Word segmentation is expected to break down the sentence at the boundaries of these words , instead of splitting `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .
439
In such context , the extracted words are more appropriate for building a dictionary than for corpus-based language processing , which are out of the focus of this paper .
In such a context , the extracted words are more appropriate for building a dictionary , rather than for corpus-based language processing , which are outside of the scope of this paper .
440
The disagreement is not only because of the different functions of blank spaces as mentioned above , but also because Vietnamese is not an inflectional language like English or Japanese , where morphological forms can be useful clues for word segmentation .
The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation .
441
While the similar problems also happen with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more diffcult because the modern Vietnamese writing system is based on Latin characters , which represents the pronunciation but not the meaning of words .
While similar problems also occur with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more difficult , because the modern Vietnamese writing system is based on Latin characters , which represent the pronunciation , but not the meaning of words .
442
The rest , which can be considered as the most diffcult and controversial cases of word segmentation , were used to create different versions of the VTB corpus representing different word segmentation criteria .
The rest , which can be considered as the most difficult and controversial instances of word segmentation , were used to create different versions of the VTB corpus , representing different word segmentation criteria .
443
Word segmentation in VTB aims to found a standard for word segmentation in a context of multi-level language processing .
Word segmentation in VTB aims at establishing a standard for word segmentation in a context of multi-level language processing .
444
Besides , we added a translation for each token when possible , so that the readers unfamiliar with Vietnamese can have an intuitive idea of how the compound words are formed .
Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed .
445
Our analysis is based on two types of inconsistency : variation and structural inconsistency , whose definitions and details are given below .
Our analysis is based on two types of inconsistencies : variation and structural inconsistency , which are defined below .
446
It is likely that structural inconsistency in word segmentation level makes the higher levels of processing , POS tagging and bracketing , become more complicated .
It is likely that structural inconsistency at the word segmentation level complicates the higher levels of processing , including POS tagging and bracketing .
447
The analysis results showed that compound nouns , compound verbs , and compound adjectives are the top diffcult cases of word segmentation .
The analysis revealed that compound nouns , compound verbs , and compound adjectives are the most difficult cases of word segmentation .
448
One of the reasons why the compound words are sometimes splitted , is because the tokens in those compound words have their own meanings , which seem to contribute to the whole meaning of the compounds .
One of the reasons why the compound words are sometimes split , is because the tokens in those compound words have their own meanings , which seem to contribute to the overall meaning of the compounds .
449
This can be seen through the examples given in Table 4 , where the meanings of tokens are given with a subscript .
This can be seen through the examples provided in Table 4 , where the meanings of tokens are given with a subscript .
450
This problem seems to have caused a lot of trouble for the annotators of VTB .
This scenario has proven to be problematic for the annotators of VTB .
451
Furthermore , observing the POS patterns in Table 5 and Table 6 , we can see the potential of structural inconsistency , in particular for closed-set POS tags .
Furthermore , by observing the POS patterns in Table 5 and Table 6 , we can see the potential for structural inconsistency , particularly for closed-set POS tags .
452
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these diffcult cases , to see whether the solution is fruitful for applications of the corpus .
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these cases , in order to see whether the solution is successful for applications of the corpus .
453
Checking structural inconsistency of these special characters including percentage% , hyphen - , and so on , we found quite a significant amount of inconsistent annotations .
By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations .
454
For example , the character % in " 30% " is splitted but is combined with the number in " 50 % " , which is considered as a structural inconsistency .
For example , the character , % , in " 30% " is split , but is combined with a number in " 50 % " , which is considered to be a structural inconsistency .
455
It does matter higher-levels of annotation such as POS tagging because we may need one or two different POS tags for different ways of annotation .
Higher-levels of annotation such as POS tagging is significant , because we may need one or two different POS tags for the different methods of annotation .
456
For example , hyphens in date expressions like " 5-4-1975 " , which means the date " April the fifth , 1975 , " are combined with the numbers .
For example , hyphens in date expressions like " 5-4-1975 " , which refers to the date , " the fifth of April , 1975 , " are combined with the numbers .
457
The difference is that we performed for document level , not for sentence level .
The difference is that we performed the task at the document level , instead of at the sentence level .
458
Since the VTB corpus is the first effort in building a treebank for Vietnamese , and is the only corpus publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building effcient NLP systems .
Since the VTB corpus is the first effort in building a treebank for Vietnamese , and is the only corpus that is publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building efficient NLP systems .
459
Although there are several approaches proposed to cope with this problem , their extremely high computational cost limits their scalability on largescale video datasets that may contain millions faces of hundreds characters .
Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .
460
In this paper , we introduce approaches for face retrieval which are scalable on such datasets while maintaining competitive performances with the state-of-the-art approaches .
In this paper , we introduce approaches to face retrieval that are scalable to such datasets while maintaining competitive performances with state-of-the-art approaches .
461
We use a point tracker for exploring the connections between detected faces belonging to the same character , then grouping them into one face-track .
We use a point tracker to explore the connections between detected faces belonging to the same character and , then group them into one face track .
462
As a result , we significantly reduce the computational cost for face-track matching while taking into account variability of faces in face-tracks for high matching accuracy .
As a result , we significantly reduce the computational cost of face-track matching while taking into account the variability of faces in face tracks to achieve high matching accuracy .
463
Their scales have not been considered in literature ever .
scales that have never been considered in the literature .
464
The experimental results demonstrate that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
The experimental results show that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
465
News videos play an important role in our sources of information nowadays because of their rich and important contents .
News videos play an important role as a source of information nowadays because of their rich and relevant contents .
466
With the advances of modern technology , a huge amount of news videos can be obtained easily .
With the advances in modern technology , a huge amount of news videos can be obtained easily .
467
Accordingly , it creates an urgent demand for retrieving useful information in such news video datasets .
Accordingly , this creates an urgent demand to retrieve useful information from such news video datasets .
468
Since most of the news is related to human , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
469
With the list , important events related to the character can be detected or summarized.
With such a list , important events related to the character can be found or summarized.
470
Thus , accurate and efficient approaches for face retrieval are always required.
Thus , accurate and efficient approaches to face retrieval are always required.
471
Generally , there are two principle steps in a face retrieval system .
Generally , a face retrieval system consists of two principal steps .
472
While conventional approaches consider single face images as the basic units for extracting and matching \CITE , recently proposed approaches sifted towards sets of face images called face-tracks .
Whereas conventional approaches consider single face images as the basic units in extracting and matching \CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .
473
There are several approaches introduced to deal with this problem \CITE .
Several approaches have been introduced to deal with this problem \CITE .
474
Working toward solving the above problems , our contributions in this paper is three-fold.
This paper provides a threefold contribution toward solving the above problems , .
475
The basic idea is to employ a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
The basic idea is to use a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
476
, In constrast to the approach in \CITE , which is failed to deal with specific problems of news video caused by sudden illumination change and partial occlusion , our approach is incorporated techniques to overcome the problems .
Our approach incorporates techniques to overcome specific problems with news video caused by sudden illumination change and partial occlusion , in contrast to the approach in \CITE , which failed to deal with , these problems .
477
We introduce an approach which significantly reduces the computational cost for face-track matching while maintaining a competitive performance compare to those of the state-of-the-art approaches .
We introduce an approach that significantly reduces the computational cost for face-track matching while maintaining a competitive performance with state-of-the-art approaches .
478
We investigated the problem of face-retrieval on news video datasets whose scales have not been considered in literature ever .
We investigated the problem of face retrieval in news video datasets whose scales have never been considered in the literature .
479
The remaining of this paper is organized as follows .
The remainder of this paper is organized as follows .
480
Then , in the second step , detected faces of the same character will be grouped by using either clustering approaches \CITE or tracking approaches \CITE .
, In the second step , the detected faces of the same character are grouped by using either clustering \CITE or tracking approaches \CITE .
481
A concatenated vector of the normalized color histograms represents the face .
A concatenated vector of the normalized color histogram represented the face .
482
They than use the trained statistical face model to incorporate identity evidence over a sequence .
They then used the trained statistical face model to incorporate identity evidence over a sequence .
483
Such a small number of samples for each character is not sufficient for stably evaluating a face matching or recognition approach , which is an important part of a face retrieval system .
Such a small number of samples for each character is not sufficient to stably evaluate a face-matching or recognition approach , which is an important part of a face retrieval system .
484
Because of all above mentioned reasons , we prepare new datasets for evaluating the approaches.
In view of all the above-mentioned considerations , we prepare new datasets for evaluating the approaches.
485
In the offline stage , face-tracks in all shots of videos are extracted using our face-track extraction approach ( described in Section 4 ) .
In the off-line stage , the face tracks in all video shots are extracted using our face-track extraction approach ( described in Section 4 ) .
486
A common strategy of existing approaches for face-track extraction consists of detecting faces in frames and grouping detected faces of the same character .
A common strategy in the existing approaches to face-track extraction consists in detecting faces in frames and grouping detected faces of the same character .
487
in \CITE propose to use KLT tracker for this purpose .
in \CITE proposed the use of a KLT tracker for this purpose .
488
Points which can not be propagated from one frame to the next are eliminated and replaced with new points .
Points that cannot be propagated from one frame to the next are eliminated and replaced with new points .
489
Given two faces in different frames , if the number of point tracks passing through both faces is larger than half of the total number of point tracks which are not in common to both faces , they are grouped into one face-track.
Given two faces in different frames , if the number of point tracks passing through both faces is larger than half of the total number of point tracks that are not common to both faces , the faces are grouped into one face track.
490
has demonstrated its efficiency and robustness on drama videos \CITE , directly applying the approach to news videos results poor performances due to following issues.
has shown its efficiency and robustness with drama videos \CITE , directly applying the approach to news videos results in poor performance due to the following issues.
491
New points are generated at the first frame of the shot or at a frame in which some existing points can not be propagated .
New points are generated at the first frame of the shot or at a frame in which some existing points cannot be propagated .
492
Based on above observed limitations of the approach in \CITE on news videos , we integrate techniques to bypass these liminations in our proposed approach for face-track extraction on news videos.
Based on the observed limitations of the approach in \CITE when applied to news videos , we integrate techniques to bypass these restrictions in our proposed approach to face-track extraction in news videos.
493
And , faces appeared in those frames are less informative for recognition since most of the facial identity characteristics are loss due to overlighting .
and that the , faces that appear in those frames are less informative for recognition because most of the facial identity characteristics are lost due to over-lighting .
494
, They can not enrich information of its corresponding face-track , but may add noise .
Thus , the faces cannot enrich the information on its corresponding face track , but may only add noise .
495
If the enumerated number is larger than half of the total number of points which are not in common to both faces , the faces is grouped into the face-track .
If the enumerated number is larger than half of the total number of points that are not common to both faces , the face is grouped into the face track .
496
A face which can not be grouped into any face-track is treated as an initial face of a new face-track .
A face that cannot be grouped into any face track is treated as the initial face of a new face track .
497
We remove all points which are inside the last appeared face of the face-track but not inside the current face , and vice versa .
We remove all points that are inside the last face that appeared on the face track but are not inside the current face , and vice versa .
498
Points which are shared by both faces are kept .
Points that are shared by both faces are kept .
499
By doing that , our tracking results through a long sequence of frames become more accurate and reliable .
By doing so , our tracking results over a long sequence of frames become more accurate and reliable .
500
When a face is partly and slowly occluded , our approach can discard incorrectly tracked points as well as reproduce points for the face after being occluded .
When a face is partly and slowly occluded , our approach can discard incorrectly tracked points and reproduce points for the face after it has been occluded .
501
, Although these existing approaches achive high accuracy on benchmark datasets , their expensive computational costs limits their practical applications on large-scale datasets .
However , although these approaches have shown high accuracy in benchmark datasets , their high computational costs limit their practical applications in large-scale datasets .
502
This motivate us to target an matching approach which is balanced between accuracy and computational cost .
This motivates us to target a matching approach that provides a good balance between accuracy and computational cost .
503
By doing that , the require computational cost can be reduced while a sufficient amount of information is kept for improving accuracy .
In doing so , the required computational cost can be reduced while keeping the amount of information sufficient to improve accuracy .
504
Let denote mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} are two mean faces of two face-track A and B , respectively , with N imposes the number of dimension of the feature space .
Let mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} denote the mean faces of face tracks A and B , respectively , with N representing the number of dimensions of the feature space .
505
By using k as a predefined parameter , k-Faces provides flexibility for users in balancing their expected accuracy and the cost which they can afford ( or time they can wait for the result ).
By using k as a predefined parameter , k-Faces provides users with flexibility in balancing the accuracy they expect and the cost they can afford ( or the time they can spend waiting for the result ).
506
The number of frames , faces , and face tracks are shown in Table 1 .
Table 1 shows the number of frames , faces , and face tracks .
507
In addition , the results also shows that our approach is superior to the approach by Everingham et al .
In addition , the results also show that our approach is superior to that of Everingham et al .
508
Obviously , extra cost is required .
obviously , requires extra cost .
509
However , our complexity is somehow linear to total number of face , because we consequently enlarge face-tracks following temporal order by checking new faces with only one last appeared face of each face-track .
However , our complexity is somehow linear to the total number of faces , because we consequently enlarge face tracks according to the temporal order by checking new faces with only the last face that appeared on each face track .
510
If this number is getting larger , the gap in speed between our approach and the approach by Everingham et al .
If the number of faces increases , the gap in speed between our approach and that by Everingham et al .
511
In this experiment , we show that our proposed techniques and solutions for the problems are robust and efficient enough for extracting face-tracks in real-world news videos by successfully extracting 94% of all face-tracks .
In this experiment , we show that our proposed techniques and solutions to the problems are robust and efficient enough for extracting face tracks in real-world news videos by successfully extracting 94% of all face tracks .
512
From our observations , one can use other complex techniques to handle the problems .
Based on our observations , other complex techniques can be applied to handle the problems .
513
However , a trade-o_ between completely obtaining 6% remaining face-tracks and an overly expensive computational cost should be considered with care.
However , the trade-off between obtaining the 6% remaining face tracks and incurring an overly high computational cost should be considered with care.
514
This dataset is observed from NHKNews7 channel in 11 years .
This dataset consists of observations from the NHK News 7 program over 11 years .
515
In the Table 4 , we compare our datasets with some public benchmark datasets .
Table 4 shows a , comparison between our datasets and some public benchmark datasets .
516
Compared to Youtube Faces dataset , although ours have less number of character ( or subjects ) , we provide much more face-tracks ( or video shots ) per character , .
Compared to the YouTube Faces dataset , , we provide much more face tracks ( or video shots ) per character , although our datasets have smaller numbers of characters ( or subjects ) .
517
However , due to copyright issues , face images in face-tracks can not be published .
However , due to copyright issues , the face images in the face tracks cannot be published .
518
Regarding to \CITE , if the pair-wise based approaches are representative for non-parametric sampled based approaches , MSM and CMSM are representative for approaches based on parametric model .
Regarding \CITE , if the pair-wise-based approaches are representative of nonparametric sample-based approaches , MSM and CMSM are representative of approaches based on a parametric model .
519
By doing that , the subspaces are expected to be better separatable .
In doing so , the subspaces are expected to be more separable .
520
All of these approaches had been shown their robustness on benchmark datasets , such as MoBo , HondaUCSD , and Youtube Faces .
All of these approaches have shown their robustness in benchmark datasets , such as MoBo , HondaUCSD , and YouTube Faces .
521
In particular , for each dataset , each face-track is alternatively picked out as a query facetrack , while the remaining face-tracks are used as the retrieved database .
In particular , in each dataset , a face track is alternatively picked out as a query face track , while the remaining face tracks are used as the retrieved database .
522
Besides MAP , we record processing times of the approaches on each dataset for efficiency comparison.
Besides the MAP , we record the processing times of the approaches in each dataset to compare their efficiency.
523
Figure 7 presents Mean Average Precision ( MAP ) of all evaluated approaches on our two datasets , Trecvid and NHKNews7 .
Figure 7 presents the mean average precision ( MAP ) of all the evaluated approaches in our two datasets , Trecvid and NHKNews7 .
524
The gap of MAPs between two datasets can be explained by following reasons .
The difference in the MAPs between the two datasets can be explained by following reasons .
525
Due to those reasons , matching faces in NHKNews7 becomes more challenging , .
For these reasons , matching faces in NHKNews7 becomes more challenging , which .
526
Among several distance types , L1 is the optimal one to be used with pair:min .
Among the distance types , L1 is the optimal for use with pair:min .
527
A reasonable replacement can be Euclidean distance .
A reasonable replacement is the Euclidean distance .
528
It causes the gap of MAPs between pair:min and pair:min .
This causes the difference in MAPs between pair:min and pair:min .
529
Since the average length of face-tracks on NHKNews7 is longer ( i.e. , each face-track contains more sample faces of a character ) , there is more chance that two face-tracks of the same character contain identical faces.
Because the average length of face tracks on NHKNews7 is longer ( i.e. , each face track contains more sample faces of a character ) , there is a greater chance that two face tracks of the same character contain identical faces.
530
On the contrary , k plays an important role in k-Faces.Temporal .
In contrast , k plays an important role in k-Faces.Temporal .
531
Since keep increasing k does not help to obtain imporant accuracy improvement but expensive computational cost , we select k = 20 to investigate the trade-off between accuracy and computational costs of k-Faces approaches compared to others .
Because further increasing k does not help improve accuracy but increases the computational cost , we select k = 20 for investigating the trade-off between the accuracy and computational cost of k-Faces approaches in comparison to others .
532
According to Table 5 , k-Faces.KMeans and k- Faces.Temporal achieve almost equal accuracy and consume the same amount of time for one query on both datasets .
As shown in Table 5 , k-Faces.KMeans and k- Faces.Temporal achieve almost equal accuracy and consume the same amount of time for one query in both datasets .
533
This suggest that , selecting presentative faces based on tempo .
This suggests that in terms of both accuracy and efficiency , selecting representative faces based on temporal sampling is better than that based on clustering .
534
Compared to state-of-the-art approaches , our k- Faces.Temporal is thousands times faster than the best approach , which is pair:min , and hundred times faster than MSM and CMSM on both datasets .
Compared to state-of-the-art approaches , our k- Faces.Temporal is thousands of times faster than the best approach , which is pair:min , and hundreds of times faster than MSM and CMSM in both datasets .
535
This is due to the fact that face-tracks on NHKNews7 dataset is larger than those on Trecvid dataset .
The reason for this is the fact that the face tracks in the NHKNews7 dataset are larger than those in the Trecvid dataset .
536
As expected , the results in this experiment demonstrate that our proposed approach is extremely efficient while archiving comparable performance with state-of-the-art approaches�f.
As expected , the results of this experiment show that our proposed approach is extremely efficient while achieving comparable performance with state-of-the-art approaches .
537
Based on that , we introduce techniques and solutions to bypass the problems for robust face-track extraction .
Based on these , we introduce techniques and solutions to overcome these problems to achieve robust face-track extraction .
538
Secondly , we present an approach for face-track matching which significantly reduces the computational cost and achive competitive performance compared to state-of-the-art approaches .
Second , we present an approach for face-track matching that significantly reduces the computational cost while achieving competitive performance compared with state-of-the-art approaches .
539
Thirdly , we prepare , evaluate state-of-the-art face retreival approaches , and publish real-world face-track datasets whose scale have not been considered in literature ever.
Third , we prepare datasets , evaluate state-of-the-art face retrieval approaches , and publish real-world face-track datasets of such scales that have never been considered in the literature.
540
As a result , the exponential growth of image repositories creates the urgent needs for searching images . Because of its importance and wide applications , image search has attracted more interest in recent years .
The resulting exponential growth of image repositories , however , has created an urgent need for effective ways of searching images .
541
The reason is because relevant items are not in the database .
The reason is that relevant items are not in the database .
542
The expected scheme can be described as follows ( see Figure \REF for an example ) .
The envisioned scheme can be described as follows ( see Figure \REF for an example ) .
543
Items with higher assigned numbers will be more recommended .
Items with larger assigned numbers will be more recommended .
544
First , there is a huge pool of candidate items in the initial query image .
First , there tends to be a huge pool of candidate items in the initial query image .
545
Furthermore , scanning over all regions in images of the database will inevitably be prohibitive , if not infeasible for practical purposes .
Furthermore , the cost of scanning all regions of the images of the database will inevitably be prohibitive for practical purposes .
546
By applying the approach instead of other naive sampling approach such as sliding windows , the number of items ( i.e. regions ) that need to be processed in each image dramatically reduces .
By applying this approach instead of a naive sampling approach such as sliding windows , we were able to dramatically reduce the number of items ( i.e. regions ) that need to be processed in each image .
547
In order to do that , we introduce a novel representation based on hierarchical structure to describe a set of region pairs and a corresponding function bounding the similarity scores of pairs over such a set .
In order to do that , we introduce a novel representation based on a hierarchical structure describing a set of region pairs and a corresponding function bounding the similarity scores of pairs over such a set .
548
With respect to discovering common items , Recommend-Me is related to recent studies on mining common items in image databases such as \CITE .
On the topic of discovering common items , Recommend-Me is related to recent studies on mining common items in image databases such as \CITE .
549
However , by doing that , extra costs for mining unnecessary items , which appear in the database but the initial query image , arise accordingly .
However , doing that incurs the extra cost of mining unnecessary items that appear in the database , but not in the initial query image .
550
From technical point of view , our solution is motivated by recent works for object localization and subimage retrieval based on branch-and-bound optimization \CITE .
From a technical point of view , our solution is motivated by recent work on object localization and subimage retrieval based on branch-and-bound optimization \CITE .
551
Details of our proposed approaches for finding region pairs with highest similarity scores are given in Section 3 .
The details of how we find region pairs with the highest similarity scores are given in Section 3 .
552
Section 5 concludes our paper .
Section 5 concludes the paper .
553
Using all possible rectangular regions in images as candidate items is overly expensive for further processing .
Using all possible rectangular regions in images as candidate items is overly expensive in the subsequent processing .
554
All region throughout the hierarchy is considered as candidate items .
All regions throughout the hierarchy are considered to be candidate items .
555
There is a pool of region pairs if we compare each region in the initial query image with each region in images of the database .
There will be a pool of region pairs if we compare each region in the initial query image with each region in images of the database .
556
They are perceived as the same item by human being .
These regions would be perceived as the same item by users .
557
A general branch-and-bound algorithm works by hierarchically dividing the parameter space into disjoint parts , known as branching step .
A general branch-and-bound algorithm works by hierarchically dividing the parameter space into disjoint parts ; this is called the branching step .
558
In the bounding step , each part is assigned an upper bound value that the quality function could take on any of the members of the part .
In the bounding step , each part is assigned an upper bound for which the quality function could take on any of the members of the part . //<The rewrite is grammatical but I don 't know what " the quality function could take on any of the members of the part " . Inparticular what are these members and can they be computed in a function ?>
559
- if each node is repsented by a histgoram \MATH with \MATH bins , the value at each bin of a child node is constrainted to be equal or smaller the value at the same bin of its parent node .
- if each node is represented by a histogram \MATH with \MATH bins , the value in each bin of a child node is constrained to be equal or smaller than the value in the same bin of its parent node .
560
Otherwise , given \MATH with \MATH are direct child nodes of \MATH , \MATH can be recursively defined as follows : \MATH
Otherwise , given \MATH with \MATH being direct child nodes of \MATH , \MATH can be recursively defined as follows : \MATH
561
Dividing the search space ( i.e. set of region pairs ) covered by \MATH is straightforward by utilizing the hierarchical structures \MATH , \MATH at certain nodes \MATH , \MATH .
Dividing up the search space ( i.e. set of region pairs ) covered by \MATH can be done straightforwardly by utilizing the hierarchical structures \MATH , \MATH at certain nodes \MATH , \MATH . //<The rewrite is better if it is correct .>
562
We select the larger one to be divided first .
We divide the larger one first .
563
The generated node is exactly the same as the non-leaf node it attach to , which now becomes a virtual node .
The generated node is exactly the same as the non-leaf node it is attached to , which now becomes a virtual node .
564
With a set containing regions of multiple images , we perform a two-stage organization procedure .
With a set containing regions from multiple images , we perform a two-stage organization procedure .
565
The value at each hitogram bin of a non-leaf nodes is the maximum of all values at the same bin of its child nodes .
The value at each histogram bin of a non-leaf node is the maximum of all values in the same bin of its child nodes .
566
Note that , because the hierarchical for regions of images in the database is independent of query , we construct it only one time .
Note that because the hierarchy of the regions of images in the database is independent of the query , we construct it only one time .
567
Based on those insights , we evaluate Recommend-Me system using two evaluation metrics : precision on introducing recommendation and rank of the first hit recommendation on the list .
Based on these insights , we evaluated the Recommend-Me system using two evaluation metrics : precision in pesenting recommendations and rank of the first hit recommendation on the list .
568
A codebook of 2000 visual words is built using standard K-Means algorithm .
A codebook of 2000 visual words was built using the standard K-Means algorithm .
569
%to cluster points on a set of random images .% Additionally , the set of interest points obtained by DoG in the query image is used to remove regions without any of such points inside .
%to cluster points on a set of random images .% Additionally , the set of points of interest obtained by the DoG in the query image was used to remove regions without any such points inside .
570
As mentioned above , we use the approach introduced in \CITE on different color channel for region selection .
As mentioned above , we used the approach first introduced in \CITE on different color channels for the region selection .
571
Given the set of regions in the initial query image , we build a graph in which two regions are connected if they highly overlap each other ( we use the approach of Pascal VOC with tighter threshold , 0 .8 ) .
Given the set of regions in the initial query image , we built a graph in which two regions were connected if they nearly overlapped each other ( we use the approach of Pascal VOC with a tighter threshold , 0 .8 ) .
572
One clique is one group of regions .
One clique was one group of regions .
573
The results show that Recommend-Me can successfully introduce hit recommendations to users with high precision ( approximately 80 .27\% ) .
The results show that Recommend-Me can make hit recommendations to users with high precision ( approximately 80 .27\% ) .
574
However , if users are interested in using them as hints to explore the database , they are still very much helpful .
However , if users are interested in using them as hints to explore the database , they may still be very helpful .
575
This results in a drop of average rank of the first hit recommendation .
This results in a drop in the average rank of the first hit recommendation .
576
Recommend-Me cannot provide any hit recommendation for around 20\% of initial query images due to the fact that our region comparision technique cannot deal with significant variations of items .
Recommend-Me cannot provide any hit recommendation for around 20\% of the initial query images due to the fact that our region comparison technique cannot deal with significant variations in the items .
577
It is the decline of efficiency improvement of Recommend-Me .
It is the decline in the efficiency improvement of Recommend-Me .
578
Its superiority is important for practical applications .
This advantage will be important for practical applications .
579
Such recommendations support users to select search query , to rapidly refine the initial query image or to explore the database .
Such recommendations help users to select the search query , to rapidly refine the initial query image or to explore the database .
580
An efficient solution to make Recommend-Me practical is presented .
An efficient solution to make Recommend-Me practical was also presented .
