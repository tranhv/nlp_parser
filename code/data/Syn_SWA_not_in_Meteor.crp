0
The analysis of the experimental results will illustrate the necessity for handling various sentence constructions by fundamental improvement of parsers such as re-construction of feature designs .
Analysis of the experimental results illustrates the need to handle different sentence constructions through fundamental improvement of the parsers such as re-construction of feature designs .
1
Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
2
In these constructions , words in some syntactic positions disappear or the orders of words change .
In these constructions , words in certain syntactic positions disappear or the order of the words changes .
3
Compared to domain adaptation , structural types of sentences have gained little attention to date .
Compared to domain adaptation , structural types of sentences have received little attention to date .
4
In the experiments , we will additionally use QuestionBank for comparison .
In the experiments , we also used QuestionBank for comparison .
5
While the Wall Street Journal ( WSJ ) treebank has extensively been used for parsing experiments , we use the treebank of the Brown Corpus in our experiments .
Although the WSJ treebank has been used extensively for parsing experiments , we used the treebank of the Brown Corpus in our experiments .
6
Because the Brown Corpus portion includes texts of literary works , it is expected that it inherently contains a larger number of imperatives and questions than the WSJ portion .
As the Brown Corpus portion includes texts of literary works , it is expected to contain inherently a larger number of imperatives and questions than the WSJ portion .
7
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " represents wh-questions , while " SQ " denotes yes / no questions .
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " denotes wh-questions , while " SQ " denotes yes / no questions .
8
When they are embedded in another imperative or question , we only extracted the outermost one .
However , is these were embedded in another imperative or question , we only extracted the outermost one .
9
Extracted sentences are post-processed so that they have natural sentence forms : first characters are capitalized , and question marks or periods are added when appropriate .
Extracted sentences were post-processed to fit the natural sentence form ; that is , with first characters capitalized and question marks or periods added as appropriate .
10
The number of sentences for each section is shown in Table \REF .
The numbers of sentences for each section are given in Table \REF .
11
As we will describe below , we additionally use QuestionBank in experiments .
As described below , we also used QuestionBank in the experiments .
12
Table \REF shows the POS tagging accuracies for the target domains .
Table \REF gives the POS tagging accuracy for the target domains .
13
The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .
The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .
14
This graph shows that for both types of sentences , first 300 training sentences improved the accuracy rapidly , and after that , the effect of adding training corpus declined .
This graph shows that for both types of sentences , the first 300 training sentences greatly improved the accuracy , but thereafter , the effect of adding training data declined .
15
In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\% in Table \REF ) , it would not seem to be enough only to prepare much more training data .
To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\% in Table \REF ) , preparing much more training data does not appear to be enough .
16
We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .
Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .
17
In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , " VB \SPEC " .
From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , " VB \SPEC " .
18
We then analyzed why each of such errors had occurred .
We then analyzed why each of these errors had occurred .
19
For Brown imperatives , the WSJ tagger gave two major tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
For Brown imperatives , the WSJ tagger gave two main tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
20
Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .
First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .
21
Secondly , main verbs in imperative sentences take base forms while main verbs in declarative sentences take the forms according to tense .
Second , the main verb in an imperative sentence takes a base form , whereas the main verb in a declarative sentence takes a form based on tense .
22
The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .
A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .
23
The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .
Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .
24
Table \REF shows that the above two types of errors did decrease to some extent . However , we can also observe that not a few mis-tags to verbs were still left after the adaptation .
Table \REF shows that after adaptation the above two types of errors decreased to some extent , although a few mistags of verbs still remained .
25
The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .
The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .
26
After the adaptation , while some of the errors such as special usage of wh-words , i.e. , " WDT \SPEC WP " , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .
After the adaptation , although some of the errors such as the special use of wh-words , i.e. , " WDT \SPEC WP " , were corrected , other kinds or errors related to the global change in sentence structure still remained .
27
In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
28
Table \REF shows the parsing accuracies of MST( first order ) , MST( second order ) , Malt , and Enju parser for WSJ , Brown overall , Brown imperatives and Brown questions .
Table \REF gives the parsing accuracy of MST ( first order ) , MST ( second order ) , Malt , and the Enju parser for WSJ , Brown overall , Brown imperatives , and Brown questions .
29
Figure \REF shows the parsing accuracies against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .
Figure \REF plots the parsing accuracy against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .
30
Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
31
When we adapted the parser model ( see fifth column in Table \REF ) , the parser could give two to four points higher accuracies for each of the Brown domains than the WSJ parser .
After adaptation ( see " Adapted " column in Table \REF ) , the parser achieved two to four percent higher accuracy for each of the Brown domains compared to the WSJ parser .
32
For the QuestionBank , 25 to 35 points accuracy improvements were observed .
For QuestionBank , 25 to 35 percent improvement in accuracy was observed .
33
Figure \REF shows that , the improvements increased according to the size of the training data , and the tendencies would not seem to converge .
Figure \REF shows that the improvement is proportional to the size of the training data and that this tendency does not seem to converge .
34
This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .
This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .
35
In order to capture the outline of the adaptation effects , we observed error reduction for the Malt parser .
To capture an overview of the adaptation effects , we observed the error reduction in the Malt parser .
36
Table \REF and \REF show the recall errors on labeled dependencies which were observed more than ten times for 100 analysis sentences of each domain .
Tables \REF and \REF give the recall errors on labeled dependencies , which were observed more than ten times for 100 analysis sentences in each domain .
37
For example in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser regarded the person name " Zion " as ROOT , and the main verb " See " as modifiers of the name .
For example , in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser mistook the name " Zion " to be ROOT , and the main verb " See " to be a modifier of the name .
38
For such sentences , the WSJ parser regarded the first " ! " or " ? " as ROOT , and " Hang " or " did " as the modifier of the marks .
For such sentences , the WSJ parser regarded the first " ! " or " ? " as ROOT , and " Hang " or " did " as the modifier of the punctuation .
39
We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .
A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .
40
However , the adapted parser could take in such doubtful construction and gave ROOT to the main verbs as the corpus required .
The adapted parser could handle this dubious construction and assigned ROOT to the main verbs as the corpus required .
41
First , Brown imperatives and questions , include many conversation sentences , and therefore rather flexible constructions could be observed especially for imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
First , Brown imperatives and questions , include many colloquial sentences , which have rather flexible constructions , especially imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
42
Second , when the different constructions of sentences were in one sentence , such as , the case where to-infinitive phrases or subordinate clauses precede imperatives and questions , the parser would often be confused .
Second , having different sentence constructions within a single sentence , such as , where a to-infinitive phrase or subordinate clause precedes an imperative or question , often confused the parser .
43
The parser would parse such complex sentences without partition into each construction , and therefore it would sometimes be confused .
These complex sentences were parsed without being partitioned into separate constructions , and as a result the parser sometimes became confused .
44
In this section , we examined whether the parser adapted to one domain would be portable to the other domain .
In this section , we examine whether a parser adapted to one domain could be ported to another domain .
45
QuestionBankdoes not give function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
QuestionBank does not provide function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
46
Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .
As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .
47
Table \REF shows the parsing or tagging accuracies of each parser and the POS tagger for Brown questions and QuestionBank .
Table \REF gives the parsing or tagging accuracy of each parser and the POS tagger for Brown questions and QuestionBank .
48
With Brown questions , we could learn wh-questions which QuestionBank mainly contain , while with QuestionBank , we could not we could not learn yes-no questions which more than half of Brown corpus contain .
Using Brown questions , many wh-questions were learnt , which is what QuestionBank mainly contains . On the other hand , despite yes-no questions constituting more than half the Brown corpus , these were not learnt using QuestionBank for training .
49
This type of problem would not be noticed so much when we were working mainly on declarative sentences .
This type of problem was not so obvious when we were working mainly with declarative sentences .
50
Following the present work , future work should include investigating parsing frameworks that are robust for sentences with various sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives , questions , and more .
Following on from this study , future work includes investigating parsing frameworks that are robust for sentences with different sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives and questions , among others .
51
While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .
While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .
52
The experiments revealed that supervised morphological analyzers were competitive , and considerably better than an unsupervised analyzer and a heuristic segmentation method .
The experimental results confirmed that supervised morphological analyzers were competitive with , and performed considerably better than an unsupervised analyzer and a heuristic segmentation method .
53
Several natural languages like Chinese and Japanese do not have to put spaces between words in their written forms .
Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .
54
Therefore , we investigate how Japanese word segmentation affects on SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .
Thus , we investigate how Japanese word segmentation affects SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .
55
Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .
However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .
56
This work aims to empirically compare representative word segmentation methods in terms of SMT quality .
This work aims at empirically comparing representative word segmentation methods in terms of the SMT quality .
57
We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .
We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .
58
Furthermore , their policies about word segmentation definitions are very much different .
Furthermore , their policies concerning word segmentation definitions vary significantly .
59
In the case of latticelm , as it has no supervised definition of words , it uses the expectation maximized length of words for every word depending on training data .
For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .
60
We also investigate such morphological analysis accuracy and word definition problems in our experiments .
In our experiments , we further investigate such morphological analysis accuracies and word definition problems .
61
Secondly , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .
Next , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .
62
Thirdly , sentence pairs that include a character " | " in English or Japanese are removed because it caused a problem with Moses .
Then , sentence pairs that include a character “|” in English or Japanese were removed , because it caused a problem with Moses .
63
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is hard to independently evaluate the effects of word segmentation on training data .
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .
64
However , our preliminary experiments showed that the results obtained with this method were not independent from word segmentation of training data .
However , our preliminary experiments indicated that the results obtained with this method were not independent from word segmentation of the training data .
65
The unsupervised morphological analyzer latticelm and one of heuristic methods CAT were worse than our expectations .
The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .
66
It is because Japanese-English translations are conducted without Head-Finalization .
This is because Japanese-English translations are conducted without Head-Finalization .
67
All supervised analyzers were better than the unsupervised and the both heuristic methods .
All supervised analyzers performed better than the unsupervised and the both heuristic methods .
68
The results were the worst scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS . The only one exception was in the case of the best 56 .55 BLEU in Characters on REUTERS .
The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .
69
And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .
Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .
70
Also , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can even work for words that do not appear in the training data , and these combined features help relieve the data sparseness problem .
Furthermore , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can also work for words that do not appear in the training data ; these combined features help relieve the data sparseness problem .
71
To solve this problem is of great interest from both practical and theoretical perspectives .
To solve this problem is of great interest from both a practical and theoretical viewpoint .
72
We confirm the appropriateness of this assumption by showing the superiority of the tree-structured models over the linear-chain models .
We confirm the validity of this assumption , by showing the superiority of the tree-structured models over the linear-chain models .
73
It also serves as an ontology , in which various kinds of meta data , relations among words and senses , and well-organized hierarchical classification of word senses are defined .
WordNet also serves as an ontology of various kinds of meta data , relations among words and senses , and a well-organized hierarchical classification of word senses that are defined .
74
Note that since the organizations of adjectives and adverbs are far different from those of nouns and verbs , we use this hierarchical information for only nouns and verbs .
Note that since the organizations of adjectives and adverbs are very different from those of nouns and verbs , we use this hierarchical information for only nouns and verbs .
75
Since the data sparsity has been a significant problem in WSD , the sense frequency information is necessary to achieve good performance .
Since data sparsity has been a significant issue in WSD , the sense frequency information is necessary in achieving a good performance .
76
For this reason , we use this first sense feature instead of the ranking feature , for the supersense-based evaluation .
For such a reason , we use this first sense feature instead of the ranking feature , for the supersense-based evaluation .
77
Also , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not explicitly examined thus far .
Furthermore , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not been explicitly examined thus far .
78
In Section 1 , we presented one of the most significant problems in WSD - the data sparsity .
In Section 1 , we presented one of the most significant issues in WSD - the data sparsity problem .
79
This method serves us much more training instances for each coarser sense , while we can no longer distinguish finer senses .
This method serves us many more training instances for each coarser sense , while we can no longer distinguish finer senses .
80
Thus , by the compaction of the trees , our model can capture more useful dependencies among word senses .
By the compaction of the trees , therefore , our model can capture more useful dependencies among word senses .
81
The detailed description of sense bigrams are given in Section 4 .7 .
The detailed description of sense bigrams are provided in Section 4 .7 .
82
These improvements seem small in terms of figures ; However , considering for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model only by 0 .37% on SEM , the further improvement of 0 .21% is considerable because this means our dependency model could handle 57% more instances over the first sense baseline .
These improvements seem insignificant in figures ; however , considering that for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model by only 0 .37% on SEM , the further improvement of 0 .21% is substantial , because it indicates that our dependency model could handle 57% more instances over the first sense baseline .
83
Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed poorer than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness regardless of the existence of the sense frequency information .
Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed worse than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness [of ...] regardless of the existence of the sense frequency information .
84
However , by the incorporation of the sense dependencies , the improvements with the sense ranking features are even smaller , and the deteriorations without them are even larger than in the tree-structured case .
Nonetheless , by the incorporation of the sense dependencies , the improvements with the sense ranking features was even less , and the deteriorations without them[define " them " ] were even more than in the tree-structured case .
85
While this verb-object dependency had a large excitatory weight , the corresponding verb-subject dependency had an inhibitory weight ( G1 :have( v )#2-( SBJ )-SS :noun .attribute ( / MATH ) ) , which means the dependency relationlabel also contributed to the result .
While this verb-object dependency had a large excitatory weight , the corresponding verb-subject dependency had an inhibitory weight ( G1 :have( v )#2-( SBJ )-SS :noun .attribute ( / MATH ) ) , which indicates that the dependency relationlabel also contributed to the result .
86
Another interesting result observed is that the noun-noun dependencies in coordination relations work remarkably strongly .
Through our result , we observed that the noun-noun dependencies in coordination relations work remarkably well .
87
The dependency tree structures was shown to be appropriate for modeling the dependencies of word senses , by the results that the tree-structured models outperformed the linear-chain models .
The dependency tree structures were shown to be appropriate in modeling the dependencies of word senses , because the results of the tree-structured models outperformed the [results of ?] linear-chain models .
88
In the analysis section , we presented an in-depth analysis of the observed instances , and saw that the noun-noun dependencies particularly contribute to the positive instances .
In the analysis section , we presented an in-depth analysis of the observed instances , and observed that the noun-noun dependencies particularly contribute to the positive instances .
89
This makes approaches using one keyframe for shot representation failed in doing clustering .
This makes approaches using one keyframe for a shot representation fail when trying to form a cluster .
90
In the first approach , one keyframe is used for shot representation in doing clustering; and sub-segments are selected using motion information for generating the summary .
In the first approach , one keyframe is used for representing a shot when forming a cluster; and sub-segments are selected using the motion information for generating the summary .
91
Video summarization is a significant research that helps to meet these needs by developing a condensed version of a full length digital video with the most important contents \CITE .
Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \CITE .
92
Each segment should be aligned such that it is a part of a scene .
Each segment should be aligned so that it is a part of a scene .
93
The question is how to determine the important part of the selected segment such that it conveys information of the scene as much as possible .
The question is how to determine the most important parts of the selected segments so that they convey as much of the information of the scene as possible .
94
From the definition , all rushes are unedited; therefore it must consist of hard cut only .
By definition , all rushes are unedited; therefore they must consist of hard cuts only .
95
The number of clusters is determined automatically by this method .
The number of clusters is determined automatically using this method .
96
The system NII-2 achieves higher recall ( IN ) than the system NII-1 since NII-1 only uses one keyframe for each sub-shot and has shorter duration ( DU ) for summary videos .
The NII-2system achieves a higher recall ( IN ) than the NII-1 system because NII-1 only uses one keyframe for each sub-shot and has a shorter duration ( DU ) for summary videos .
97
The summary videos generated by NII-1 have fewer duplications ( RE ), are presented in a smoother way ( TE ) and are easy to judge for inclusions ( TT ) .
The summary videos generated by NII-1 have less duplication ( RE ), are presented in a smoother way ( TE ), and are easy to judge for inclusions ( TT ) .
98
Compared to other systems listed in this list, our system NII-2 is one of the fastest systems .
Compared to the other systems listed in this table, our NII-2system is one of the fastest .
99
Using all frames of one segment instead of using one keyframe as proposed in NII-2 is one of the efforts toward this direction .
Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .
100
Although the result is not very high as expected, we still believe that this approach is promising .
Although the results are not as high as expected, we still believe that this approach is promising .
101
Experimental results on various face sets retrieved from the caption of news photos show that the retrieval performance is improved after each iteration leading the final performance outperforms the baseline algorithms .
Experimental results on various face sets retrieved from the captions of news photos show that the retrieval performance improved after each iteration with the final performance outperforming the baseline algorithms .
102
In this paper , we propose a method to solve the mentioned problem .
We propose a method to solve the above-mentioned problem .
103
The output is a rank list in which faces having larger number of neighbors within a distance are predicted as relevant ones and therefore are put on the top .
The output is a rank list in which faces with larger number of neighbors within a certain distance are considered as relevant and are therefore put at the top of the list . //[What do you mean by �gneighbors�h ? Do you mean the un-queried faces ? ]
104
Since the above ranking method is based on the number of neighbors , it is sensitive to the chosen distance .
Since the above ranking method is based on the number of neighbors , it is sensitive to the specified distance .
105
-Supervised learning methods such as SVM have strong theoretical background in finding optimal decision boundary even with existence of noisy data .
-Supervised learning methods such , as SVMs , provide a strong theoretical background in finding optimal decision boundary even with existence of noisy data .
106
Although the result was impressive , it is not easy to apply for our problem since a large number of irrelevant faces ( more than 12% ) are eliminated manually before doing clustering .
Although the result was impressive , it is not easy to apply it to our problem since a large number of irrelevant faces ( more than 12% ) are eliminated manually before performing clustering .
107
We evaluated the retrieval performance with measures that are popularly used in information retrieval such as precision , recall and average precision .
We evaluated the retrieval performance with measures that are commonly used in information retrieval such as precision , recall , and average precision .
108
We set the number of iterations for the Bag-Rank-SVM algorithm being 5 and set the number of iterations of the outer loop $T=30$ to see how much the final performance changes .
We set the number of iterations for the Bag-Rank-SVM algorithm at five and set the number of iterations of the outer loop $T=30$ to see how much the final performance changes .
109
We present a method to effectively rank faces retrieved by text-based correlation methods when searching a specific person .
We presented a method for effectively ranking faces retrieved using text-based correlation methods when searching for a specific person .
110
- Fastness : it should be fast for real-time processing which is an important factor in processing large video archives .
- Quickness : it should be fast in order to perform real-time processing , which is an important factor in processing large video archives .
111
Typically , detecting faces in an image includes the following steps :
Typically , detecting the faces in an image takes the following steps :
112
The number of patterns extracted from one 320x240 frame image is large , approximately 160 ,000 in which only a small number of patterns containing face .
The number of patterns extracted from a 320 x 240 frame image is large , approximately 160 ,000 , in which only a small number of patterns contain a face .
113
The most popular feature type is Haar wavelet since it is very fast to compute using the integral image \CITE .
The most popular feature type is the Haar wavelet because it is very fast to compute using the integral image \CITE .
114
- Classification : the extracted features is passed through a classifier which is trained beforehand to classify the input pattern associated with these features as a face or a non-face .
- Classification : the extracted features are passed through a classifier that has been previously trained to classify the input pattern associated with these features as a face or a non-face . //[trained / programmed ?]
115
In order to return one final detection per face , it is necessary to combine overlapping detections into a single detection .
In order to return a single final detection per face , it is necessary to combine the overlapping detections into a single detection .
116
In this structure , fast and simple classifiers are used as filters at the earliest stages to quickly reject a large number of non-face patterns and slower yet more accurate classifiers are then used for classifying face-like patterns .
In this structure , fast and simple classifiers are used as filters in the earliest stages to quickly reject a large number of the non-face patterns and then slower but more accurate classifiers are used for classifying the face-like patterns .
117
- Learning method selection : Basically , in the ideal case with proper settings , advanced learning methods such as neural network , support vector machines and AdaBoost produce similar performance .
- Learning method selection : Basically , in an ideal situation with the proper settings , the advanced learning methods , such as the neural network , support vector machines , and AdaBoost , can perform similarly .
118
Therefore , it is preferable to use support vector machines since the number of parameters is only two if using RBF kernel and many tools are available .
Therefore , it is preferable to use support vector machines because only two parameters are necessary if a RBF kernel is used and many tools are available .
119
This system uses a face tracker similar with [5] to extract a few hundred tracks of a particular character each in a single shot .
This system uses a face tracker similar to the one in [5] that can extract a few hundred tracks of each particular character in a single shot .
120
During the tracking procedure , face tracking systems usually employ a motion model that describes how the image of the target might change for different possible motions of the face to track .
During the tracking procedure , face tracking systems usually use a motion model that describes how the image of the target might change for different possible motions of the face to track .
121
Some researchers assume the face as a rigid 3D object , thus the motion model defines its aspect depending on its 3D position and orientation [10] .
Some researchers view a face as a rigid 3D object , thus the motion model defines its aspect depending on its 3D position and orientation [10] .
122
But note that most 3D-based and mesh-based face trackers require relatively clear appearance , high resolution , and limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
However , it must be noted that most 3D-based and mesh-based face trackers require a relatively clear appearance , high resolution , and a limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
123
If none of the two eyes are in the face region , it will be determined as drifting and the tracking process will be stopped .
If neither of the eyes is in the face region , it will be determined as drifting and the tracking process will be stopped .
124
Besides , most mesh-based trackers and top-down trackers are considered to be able to avoid drifting .
In addition , most mesh-based and top-down trackers are assumed to be able to avoid drifting .
125
This score is used to form a ranked list , in which faces having high density scores are considered relevant and are put at the top of the list .
This score is used to form a ranked list , in which faces with high-density scores are considered relevant and are put at the top .
126
To get the final strong classifier , we use the idea of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
To obtain the final strong classifier , we use the [idea / concept?] of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
127
It is better to use the authors�f names .]objects retrieved by an image search engine are re-ranked by extending the constellation model .
It is better to use the authors�f names .] objects retrieved using an image search engine are re-ranked by extending the constellation model .
128
Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to the curse of dimensionality .
Furthermore , choosing an optimal threshold to convert the initial graph into a binary one is difficult and rather ad hoc due to dimensionality .
129
-Step 4 : Improve this ranked list by Rank-By-Bagging-ProbSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet . You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
-Step 4 : Improve this ranked list using rank-by-bagging-probSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet. You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
130
Eigenfaces were computed from the original face set returned by the text-based query method .
Eigenfaces were computed from the original face set returned using the text-based query method .
131
This is very naive method in which no prior knowledge between names and faces is used .
This is a rather naive method in which no prior knowledge between names and faces is used .
132
This makes irrelevant faces that are near duplicates ( row 2 and row 3 in Figure \REF( b ) ) ranked higher than relevant faces .
This ranks irrelevant faces that are near duplicates ( rows 2 and 3 in Figure \REF( b ) ) higher than relevant faces .
133
Next , these images were processed as the steps described in section \REF : extracting faces , detecting eyes and doing normalization .
Next , these images were processed using the steps described in section \REF : extracting faces , detecting eyes , and doing normalization .
134
We evaluated the performance by calculating the precision at top 20 returned faces , which is popular for image search engines ; and recall and precision on all detected faces of the test set .
We evaluated the performance by calculating the precision of the top 20 returned faces , which is common for image search engines and recall and precision on all detected faces of the test set .
135
Figure \REF shows top 20 faces ranked by these two methods .
Figure \REF shows top 20 faces ranked using these two methods .
136
Our approach is beneficial in the case multiple faces residing in the returned image as shown in Figure \REF .
Our approach is beneficial when there are several faces in a returned image , as shown in Figure \REF .
137
Text segmentation based approaches that have been well studied in natural language processing can be adopted .
The text segmentation based approaches in natural language processing can be used .
138
By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
139
The remaining of the paper is organized as follows .
The remainder of this paper is organized as follows .
140
The \MATH SVM classifier is trained by positive samples being examples of the \MATH class and negative samples being examples of the other classes .
The \MATH SVM classifier is trained using positive samples as examples of the \MATH class and negative samples as the examples of the other classes .
141
To learn this classifier , we manually annotate frames in the training data .
To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?
142
We divided 8 videos , each 30-minute length , into two sets : training set and testing set .
We divided eight videos , each 30-minute long , into two sets : a training set and a test set .
143
In Table \REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
144
In regards to face detection , for example , the methods described in works [4] ,[5] ,[10] represent the state of the art in terms of both high accuracy and running speed .
In regard to face detection , for example , the methods described in [4] , [5] , and [10] are state-of-the-art in terms of both accuracy and running speed .
145
Typically , each weak classifier is any classifier whose performance is better than random guessing ( i.e. , error rate is less than 0 .5 ) .
Typically , a weak classifier is any classifier whose performance is better than random guessing ( i.e. , its error rate is less than 0 .5 ) .
146
Furthermore it might increase computation and training time , waste storage space which is critical in applications with limited resources , for example , face detection on mobile phones .
Furthermore , it might lengthen computation and training time and waste storage space , which is critical in applications with limited resources , for example , face detection on mobile phones .
147
Meanwhile choosing a small number of bins might not well approximate the real densities of the data distribution and thus influence selection of the best weak classifier .
Choosing a small number of bins , however , might not accurately approximate the real densities of the data distribution and could influence the selection of the best weak classifier .
148
Real AdaBoost is easy to implement ; however , in practical applications , designing and learning weak classifiers depend on specific applications .
Real AdaBoost is easy to implement , but in practical applications , designing and learning weak classifiers depend on specific applications .
149
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by above measurements give comparable performance .
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by the above measures give similar performances . //[measurements / measures?]
150
However , it will be shown in section 5 , these performances are affected seriously if different subspace splitting methods are used .
However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .
151
A higher arity can make the understanding of an attribute more difficult , while a very low arity may affect predictive accuracy negatively .
A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .
152
It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
These consisted of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
153
Overall , Ent-Boost has the best result .
Overall , Ent-Boost produced the best result .
154
As for storage space , the Ent-Boost based classifier only employs 6 .79 bins on average which is much smaller than that of Real AdaBoost-based classifiers .
As for storage space , the Ent-Boost-based classifier only uses 6 .79 bins on average , which is much fewer than the number used by Real AdaBoost-based classifiers .
155
We have presented Ent-Boost , a variant of AdaBoost , which uses entropy measure for automatic subspace splitting and optimal weak classifier selection .
We have described Ent-Boost , a variant of AdaBoost , which uses entropy measures for automatic subspace splitting and optimal weak classifier selection .
156
Furthermore , it overcomes the main limitation of Real AdaBoost which is hard to determine the suitable number of bins for subspace splitting .
Furthermore , this new boosting scheme overcomes the main limitation of Real AdaBoost , which is difficulty in determining the suitable number of bins for subspace splitting .
157
In this filter-based method , features are selected so that not only maximizing their relevance with the target class but also minimizing their mutual dependency .
In this filter-based method , features are selected so that not only to maximizeing their relevance with the target class but also to minimizeing their mutual dependency .
158
The filter-based approach is independent of any induction algorithm while the wrapper-based approach is associated with a specific induction algorithm to evaluate the goodness of the selected feature subset .
The filter-based approach is independent of any induction algorithm , while but the wrapper-based approach is associated with a specific induction algorithm to evaluate the quality of the selected feature subset . //[goodness / quality / appropriateness?<--If " goodness " is the word you would usually use in your field for this , it is fine , but I would suggest a different word choice otherwise . " Goodness " seems vague , so in what sense do you mean " good " ?]
159
The main idea of CMI-based methods is to select features which maximize their relevance with the target class and simultaneously minimize mutual dependency between selected ones .
The main goal of these CMI-based methods is to select features which that maximize their relevance with the target class and to simultaneously minimize mutual dependency between selected ones . //[idea / goal?]
160
Basically , discretization is a quantizing process that converts continuous values into discrete values .
Discretization is essentially a quantizing process that converts continuous values into discrete values .
161
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
162
It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
They consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape . //I�fm not 100 percent clear on what " they " points to here . " These Haar wavelet features , " perhaps? But can features consist of other kinds of features? You may want to clarify here .]
163
There are many kinds of features that have been used ranging from simple features such as intensity values [7] ,[5] and eigenspace [15] to complex features such as wavelets [16] ,[1] ,[12] , edge orientation histograms [17] ,[18] and Bayesian discriminating features ( BDF ) [19] .
Many kinds of features have been used , ranging from simple ones such as intensity values [7] , [5] and eigenspace [15] to complex ones such as wavelets [16] , [1] , [12] , edge orientation histograms [17] , [18] , and Bayesian discriminating features ( BDF ) [19] .
164
In [8] , non linear SVM classifiers using pixel-based features are arranged into a sequence with increasing number of support vectors , or in [9] , linear SVM classifiers trained at different resolutions are used for rejection and a reduced set of principle component analysis ( PCA )-based features are used with the non linear SVM at the classification stage in order to reduce computation time .
In [8] , nonlinear SVM classifiers using pixel-based features were arranged into a sequence by increasing the number of support vectors , while in [9] , linear SVM classifiers trained at different resolutions were used for rejection and a reduced set of principle component analysis ( PCA )-based features were used with the nonlinear SVM at the classification stage in order to reduce computation time .
165
-Thirdly , Haar-wavelet features used for all stages are informative [22] and evaluated extremely fast due to the introduction of the integral image .
-Haar wavelet features used for all stages are informative [22] and can be evaluated extremely quickly due to the introduction of the integral image .
166
However , turning the later layers into robustly classifying a smaller number of remaining patterns , it requires a lot more , e.g. , 5 ,660 , weak classifiers , thus making the training task much more complicated .
However , enabling the later layers to robustly classify a smaller number of remaining patterns requires many more weak classifiers ( around 5 ,660 ) , thus making the training task much more complicated .
167
Furthermore , it is unnecessary to re-evaluate these features because they have been previously evaluated .
Furthermore , these features do not need to be re-evaluated because they have already been evaluated .
168
Small number of bins might not well approximate the real distribution while large number of bins might cause over-fitting , increase computation time and waste storage space .
A small number of bins might not accurately approximate the real distribution , while a large number of bins might cause over-fitting , increase computation time , and waste storage space .
169
Actually , our system can benefit from this approach when building the rejection stage and thus also reduce the training time much more .
However , our system can benefit from this approach when building the rejection stage and can thus reduce the training time even further .
170
This is done by taking advantages of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers are extremely fast in computation .
This is done by taking advantage of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers enable extremely fast computation .
171
With this flexible classifier , the moving step size can be increased up to 12 pixels that reduce dramatically number of analyzed patterns .
With this flexible classifier , the moving step size can be increased up to 12 pixels to dramatically reduce the number of analyzed patterns .
172
Our another experiment has shown that , for similar performance , the AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than that trained on the full feature set .
Another experiment we conducted showed that , for similar performance , an AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than one trained on the full feature set . / / [Do you need a reference here , or is this still talking about the experiments you report in this paper?]
173
Since a 36x36 face sample contains a lot of background outside the 24x24 face region while the classifier is required to be fast and to keep all possible face regions , training parameters are set as follows : the minimum detection rate of \MATH and maximum of false positive rate of \MATH .
Since a 36x36 face sample contains a large proportion of background outside the 24x24 face region and the classifier is required to be fast and to keep all possible face regions , a minimum detection rate of \MATH and a maximum of false positive rate of \MATH were set as the training parameters .
174
The SVM classifier using 25 features run fastest while the SVM classifier using 200 features run slowest .
The SVM classifier using 25 features ran the fastest , while the SVM classifier using 200 features was the slowest .
175
Furthermore , the training time of a single SVM ( which takes several hours ) is much smaller than that of a cascade of AdaBoost classifiers ( which might take everal weeks ) .
Furthermore , the training time of a single SVM ( which takes several hours ) is much shorter than that of a cascade of AdaBoost classifiers ( which might take several weeks ) .
176
This training procedure resulted three SVM classifiers whose the numbers of support vectors are 4 ,725 , 5 ,043 , and 4 ,847 respectively .
This training procedure yielded three SVM classifiers whose numbers of support vectors are 4 ,725 , 5 ,043 , and 4 ,847 .
177
If the first 24x24 layer classifier is added to the cascade of 36x36 classifiers , this combination rejects 85 .91\% of analyzed patterns compared to \MATH of using only the first layer of the single cascade 24x24 classifiers .
When the first 24x24 layer classifier was added to the cascade of 36x36 classifiers , this combination rejected 85 .91\% of analyzed patterns compared to \MATH of using only the first layer of the single cascade of 24x24 classifiers .
178
It is clear that our multi-stage system runs faster than the single cascade of 24x24 AdaBoost classifiers while detection rates are comparable .
It is clear that our multi-stage system ran faster than the single cascade of 24x24 AdaBoost classifiers while achieving comparable detection rates .
179
First , the cascade of 36x36 AdaBoost classifiers rejects a lot of non-face patterns extremely fast while slow SVM classifiers only process a very small number of the remaining patterns .
First , the cascade of 36x36 AdaBoost classifiers rejected many of non-face patterns extremely quickly , while slow SVM classifiers only processed a very small number of the remaining patterns .
180
Second , many images in the MIT+CMU test set contain large portion of background which was mentioned in [9] which said the ratio of non-face to face patterns is about 50 ,000 to 1 .
Second , many images in the MIT+CMU test set contain large portion of background , which [9] mentioned has a ratio of non-face to face patterns of about 50 ,000 to 1 .
181
The key contribution of this paper is to introduce a query-dependent feature to represent this relevancy and an unsupervised method to collect training samples for learning the generic classifier .
The key contribution of this research is to introduce a query-dependent feature to represent this relevance and an unsupervised method of collecting training samples to learn the generic classifier .
182
Most of existing image search engines usually use text information for judging relevancy , resulting low precision performance .
Most existing image-search engines usually use text information to determine relevance , resulting in poor precision .
183
However , content-based image understanding is a challenging and unsolved problem .
However , understanding content-based images remains a challenging and unsolved problem .
184
This approach is different from existing approaches such as \CITE that learn a classifier to recognize the identity of the returned faces .
This approach is different from existing ones \CITE that learn a classifier to recognize the identity of the returned faces .
185
From these faces , We can artificially generate face sets similar to the sets returned by search engines given person names .
We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .
186
Note that the label of faces in our approach is not identity of that face . It is the relevance between the face and the associated query .
Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .
187
This generic classifier is a relevance classifier that learns relevancy between an image and the query .
This generic classifier was a relevance classifier that learned relevance between an image and the query .
188
Meanwhile , to build the generic classifier which is independent with any \textit{'personX'} , each face is represented by the query-dependent feature .
Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .
189
The query-dependent feature is used to encode this relevancy .
The query-dependent feature is used to encode this relevance .
190
In practice , it is difficult to know \MATH because it depends on underlying distribution of the input dataset .
In practice , it is difficult to know \MATH because this depends on the underlying distribution of the input dataset .
191
It is a tedious task and requires human labor cost .
This is a tedious task and involves a human-labor cost .
192
First , by mining video archives , we automatically collect a set of faces of \MATH different persons \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of persons; and
First , by mining video archives , we automatically collect a set of faces of \MATH different people \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of people .
193
Specifically , We use the following heuristics to pick a set of different persons appearing in video archives :
We specifically use the following heuristics to pick a set of different people appearing in video archives :
194
-If there are more than one face appearing in different locations in one frame , they likely belong to different persons .
-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .
195
-If two persons appear in video programs broadcast by different broadcast stations ( e .g . , CNN , MSNBC , and CCTV ) , they are likely different .
-If two people appear in video programs broadcast by different broadcast stations ( e.g. , CNN , MSNBC , and CCTV ) , they are likely to be different .
196
To guarantee selected face tracks representing different persons , for one channel , only face tracks of one shot was picked .
To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .
197
As a result , there are 5 ,126 faces of 19 face tracks picked from the 7 channels corresponding to 19 different persons .
As a result , there were 5 ,126 faces of 19 face tracks selected from the seven channels corresponding to 19 different people .
198
It only knows any two face tracks represent different persons .
It only knew any two face tracks represented different people .
199
Specifically , for each face , 9 facial feature points were detected , and 4 more facial feature points were inferred from these 9 points .
Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .
200
As for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID , the curves show the correlation between the performance and the number of features .
The curves plot the correlation between performance and the number of features for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID .
201
-DistScore is significantly better than that of NNScore .
-DistScore performed significantly better than NNScore .
202
The result clearly shows that our proposed method outperforms the other state of the art methods .
The results clearly demonstrate that our proposed method outperformed the other state-of-the-art methods .
203
Experiments shown that although our method is unsupervised and independent with underlying algorithms of existing search engines but successfully learns visual consistency among returned faces to boosts retrieval performance .
Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .
204
Therefore , users need specialized search systems to nd the formula that is relevant to their requirements .
Moreover , users need specialized search systems to find formulas that are relevant to their needs .
205
Furthermore , these systems do not take into account the semantics of mathematical formulas revealed by surrounding natural language text , like the name of the formula and its variables' descrip-tion .
Furthermore , these systems do not take into account the semantics of mathematical formulas as revealed by the surrounding natural language text , e.g. , the formula�fs name or the description of its variables .
206
This site and some recent works done by Adeel et al. [2] and Yokoi and Aizawa [1] propose similarity search methods based on MathML but these works do not make use of the semantics of the formulas' surrounding text , which is considered to be important information sources .
This site and some recent work done by Adeel et al. [2] and Yokoi and Aizawa [1] employ similarity search methods based on MathML but they do not make use of the semantics of the formulas' surrounding text . //[ ? ? propose is unclear in the sense of a website .]
207
The experimental results have shown how helpful this information provides to the users of mathematical search .
The experimental results showed how helpful this information is to mathematical search users .
208
However , this is only a rst step , some important issues are left for future study .
However , this is only a first step ; many important issues are left for future study .
209
In contrast to existing researches that mainly relied on manually encoded transformation rules , we adopt a Statistical-Machine-Translation-based method to automatically extract translation rules from parallel markup corpora .
In contrast to previous research that mainly relied on manually encoded transformation rules , we use a statistical-machine-translation-based method to automatically extract translation rules from parallel markup corpora .
210
Experimental results on the Wolfram Function Site show that our approach achieves an improvement against the prior rule-based system .
Experimental results on the Wolfram Function Site show that our approach is an improvement over prior rule-based systems . // <Note : It seems that where were two prior systems that were compared . If not , you can go back to using a prior system .> .
211
The aim of this paper is to introduce a method for automatic mathematics semantic enrichment that capable of analyze and disambiguate mathematical terms .
The aim of this paper is to describe a method of automatic semantic enrichment for mathematics that is capable of analyzing and disambiguating mathematical terms .
212
- Second , mathematics knowledge such as symbol 's meanings or structural relations is automatically learned while training , therefor it is not required mathematics experts nor human effort and it is also easier to update the system given more data .
- Second , mathematics knowledge such as a symbol 's meanings or structural relations is automatically learned while training ; therefore , the system requires no human effort or expertise , and it is easier to update with more data .
213
We set up another experiment to confirm the correlation between system performance and training set size and saw that increasing the size of training data actually boost the system performance .
We performed another experiment to assess the correlation between the system 's performance and training set size and found that increasing the size of the training data boosted the system 's performance .
214
We also performed extensive side-by-side comparison with prior work \CITE over a data set from ACL-ARC scientific papers .
We also performed an extensive comparison with prior work \CITE using a data set collected from ACL-ARC scientific papers .
215
Our experimental results show that the proposed approach works well on the mathematics semantic enrichment problem and it excels the previous work by providing significantly less error rate .
Our experimental results show that our approach works well in dealing with the mathematics semantic enrichment problem and it outperforms the previous work by making significantly fewer errors .
216
Our task is inherently domain specific therefore we propose an approach which is based on statistical machine learning methods that automatically extract these rules from a dataset .
Our task is inherently domain-specific ; therefore , we devised an approach based on statistical machine learning for automatically extracting rules from a dataset .
217
To overcome these limitations , we introduced two separated sets of rule : fragment rules and translation rules .
To overcome these limitations , we made two separate rule sets : fragment rules and translation rules .
218
If the sub-trees can not be broken any longer , we extract another rules , which we called " translation rules " , at that point .
Once the sub-trees cannot be broken down further , we start to extract other rules , which we call " translation rules " .
219
Given a mathematical expressions in Presentation MathML markup , the system will generate Content MathML markup of that expression .
Given mathematical expressions in Presentation MathML markup , the system will generate Content MathML markup for each expression .
220
In the first experiment , the data is not compatible with SnuggleTeX since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .
In the first experiment , the data was not compatible with SnuggleTeX since SnuggleTeX uses ASCII MathML but the Wolfram Functions site does not .
221
Table \REF contains the various data statistics .
Table \REF lists the various data statistics .
222
Given a Presentation MathML expression $ e $ , we assume that tree $ A $ is the correct Content MathML tree of expression $ e $ and tree $ B $ is the output using the automatic translation .
Given a Presentation MathML expression $ e $ , we assume that tree $ A $ is the correct Content MathML tree of expression $ e $ and tree $ B $ is the output of the automatic translation .
223
For the data in Wolfram Function site , it appeared that SnuggleTeX is not applicable to this data since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .
It appeared that SnuggleTeX was not applicable to the data from the Wolfram Function site since it uses ASCII MathML but the site does not .
224
Our system archived 24 percent TEDR less than the output using SnuggleTeX .
Our system had a 24 percent lower TEDR in comparison with SnuggleTeX .
225
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expression to a Content MathML expression has the significant improvement over a prior system .
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expressions to Content MathML expressions is a significant improvement over prior systems .
226
Recent research shows a major part of difficult cases in event extraction for the biomedical domain are related to coreference .
Current research has shown that major difficulties in event extraction cases for the biomedical domain are related to coreference .
227
However , the shared task results showed that transferring coreference resolution methods developed for other domains to the biological domain was not straight forward , which is supposed to be caused by the domain differences in coreference phenomena .
However , the shared task results indicated that transferring coreference resolution methods developed for other domains to the biological domain was not straightforward , due to the domain differences in the coreference phenomena .
228
Analysis of the experimental results showed that semantic classification using protein information has contributed to an increase in performance ( 2.3 % on the test data , and 4 .0% on the development data , in F-score ) .
Analysis of the experimental results revealed that semantic classification , using protein information , had contributed to an increase in performance by 2.3 % on the test data , and 4 .0% on the development data , in F-score .
229
Since such information is difficult to be transferred across different domains , we need to continue seeking for methods to exploit and use it in coreference resolution .
Since it is difficult to transfer domain-specific information across different domains , we need to continue to seek methods to exploit and use it in coreference resolution .
230
While named entity recognition ( NER ) and relation or event extraction are regarded as standard tasks of biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is more and more recognized as an important component of IE for a higher performance .
While named entity recognition ( NER ) and relation / event extraction are regarded as standard tasks for biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is being recognized more and more as an important component of IE to achieve a higher performance .
231
Without coreference resolution , the performance of IE is often substantially limited due to an abundance of coreference relations in natural language text , i.e. , information pieces written in text with involvement of a coreference relation are hard to be captured [ 9 , 14 ] .
Without coreference resolution , oftentimes , the IE performance issubstantially limited , due to the abundance of coreference relations in natural language text ; information pieces written in text with the involvement of a coreference relation are hard to be captured [ 9 , 14 ] .
232
Without knowing this coreference relation , it becomes hard to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is localization of p65 ( out of nucleus ) according to the framework of BioNLP-ST .
Without knowing this coreference relation , it becomes difficult to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is a localization of p65 ( out of nucleus ) , according to the framework of BioNLP-ST .
233
A detailed analysis on the _nal submissions of the COREF task participants was reported in the organizer 's papers [ 15 , 31 ] , which is summarized in table 2 .
A detailed analysis on the _nal submission of the COREF task participants was reported in the organizer 's papers [ 15 , 31 ] , and is summarized in table 2 .
234
The analysis results in also showed that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type are 27 .5% F-score and 10 .1 F-score respectively , which are far less than that for relative pronoun ( the RELAT type ) 66 .2 % F-score .
An analysis of the results indicated that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type was 27 .5% F-score and 10 .1 F-score , respectively ; the scores were much lower than the F-score for relative pronouns ( the RELAT type ) , which yielded a 66 .2 % F-score .
235
In practice , for definite noun phrase type of anaphors , this is done using a list of possible head words of protein references , and for pronouns , their context words are used .
In practice , for definite noun phrase type of anaphors , this is accomplished , by using a list of possible head-words of protein references ; for pronouns , their context words are used .
236
In theory , all expressions in the set of markables can become antecedent candidates , however too much candidates makes it difficult to achieve correct antecedent prediction .
In theory , all expressions in the set of markables can become antecedent candidates ; however , too many candidates makes it difficult to achieve correct antecedent prediction .
237
More details about the implementation of the main components of our system shown in Figure 2 are presented below .
More details concerning the implementation of the main components of our system shown in Figure 2 are presented below .
238
Thus , the likelihood that two expressions are semantically compatible is definitely beneficial for antecedent prediction , besides syntactic information .
Therefore , the likelihood that two expressions are semantically compatible , is definitely beneficial for antecedent prediction , besides syntactic information .
239
Focusing on specific entity types , i.e. Protein type , helps us to find a proper method for determining the likelihood , and how to encode the likelihood in coreference resolution .
Focusing on specific entity types , i.e. , Protein type , enables us to find a proper method for determining the likelihood , and method for encoding the likelihood in coreference resolution .
240
In other words , their antecedents do not necessarily exist in the textual context ; in particular in biomedical scientific papers , many definite noun phrases do not have antecedents since the referred concepts can be anything understood by experts in the domain .
In other words , their antecedents do not necessarily exist in the textual context ; in particular , in biomedical scientific papers , many definite noun phrases do not have antecedents , since the referenced concepts can include any concept that is understood by subject matter experts in the domain .
241
Breaking down the system performance by types of anaphors gives us an insight into what have been solved by our methods , and what needs more improvement effort .
Breaking down the system performance by the different types of anaphors provides us with insight into what has been accomplished / solved by our methods , and also provides us with improvement opportunities .
242
However , it should be noted that our antecedent prediction for the RELAT type is based completely on the output of Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See section Discussions ) .
However , it should be noted that our antecedent prediction for the RELAT type is based solely on the output of the Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See Discussions section ) .
243
The analysis results are given in section Discussions .
The analyses of the results are provided in the section entitled Discussions .
244
This is an encouraging sign to seek for a systematic method to exploit and include such contextual information in coreference resolution .
This gain is a good indication for seeking a systematic method to develop and include such contextual information in coreference resolution .
245
Our work has confirmed again that domain knowledge is indispensable for coreference resolution .
Our current work has reconfirmed that domain knowledge is indispensable for coreference resolution .
246
This subproblem is often thought as an easy task in coreference resolution systems , however , indeed it is an important subtask which strongly affects the performance of coreference system .
This sub-problem is often regarded as an easy task in coreference resolution systems ; however , in actuality , it is an important subtask , which strongly affects the performance of coreference system .
247
This is also a concern of Vietnamese Treebank ( VTB ) , the first and the only publicly available syntactically annotated corpus so far for the Vietnamese language .
This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language .
248
This performance is far lower than the state-of-the-art performance reported for Berkeley Parser on English Penn Treebank , 90 .3% in F-score ( Petrov et al ., 2006 ) .
This score is far lower than the state-of-the-art performance reported for the Berkeley Parser on the English Penn Treebank , which reported 90 .3% in F-score ( Petrov et al ., 2006 ) .
249
Establishing a gold standard for Vietnamese word segmentation faces some diffcuties coming from the characteristics of the language .
Because of the discussed characteristics of the language , there are challenges in establishing a gold standard for Vietnamese word segmentation .
250
The disagreement is not only because of the different functions of blank spaces as mentioned above , but also because Vietnamese is not an inflectional language like English or Japanese , where morphological forms can be useful clues for word segmentation .
The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation .
251
In this paper , a brief introduction of the Vietnamese treebank VTB and its annotation scheme are given in Section 2 .
In this paper , a brief introduction of the Vietnamese Treebank VTB and its annotation scheme are provided in Section 2 .
252
The rest , which can be considered as the most diffcult and controversial cases of word segmentation , were used to create different versions of the VTB corpus representing different word segmentation criteria .
The rest , which can be considered as the most difficult and controversial instances of word segmentation , were used to create different versions of the VTB corpus , representing different word segmentation criteria .
253
Each unit in Table 1 goes with several example words of which English translations are given in parentheses .
Each unit in Table 1 goes with several example words ; English translations are provided in parentheses .
254
Besides , we added a translation for each token when possible , so that the readers unfamiliar with Vietnamese can have an intuitive idea of how the compound words are formed .
Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed .
255
In this section , we analyzed the VTB corpus to know whether the diffculties in Vietnamese word segmentation affected the quality of VTB annotations .
In this section , we analyzed the VTB corpus to determine whether the difficulties in Vietnamese word segmentation affected the quality of VTB annotations .
256
Most of the diffcult cases of word segmentation lie in two-token variations , occupying the majority of variations ( 92 .9% ) .
Most of the diffcult cases of word segmentation occur in two-token variations , occupying the majority of variations ( 92 .9% ) .
257
The precision of our method is high enough so that so we can use the extracted variations to study the insights of word segmentation problem .
The precision for our method is high , so we can use the extracted variations to provide insights on the word segmentation problem .
258
We further analyzed the 2-gram variations to know what types of 2-grams were most confusing to annotators .
We further analyzed the 2-gram variations to understand what types of 2-grams were most confusing for annotators .
259
The analysis results showed that compound nouns , compound verbs , and compound adjectives are the top diffcult cases of word segmentation .
The analysis revealed that compound nouns , compound verbs , and compound adjectives are the most difficult cases of word segmentation .
260
One of the reasons why the compound words are sometimes splitted , is because the tokens in those compound words have their own meanings , which seem to contribute to the whole meaning of the compounds .
One of the reasons why the compound words are sometimes split , is because the tokens in those compound words have their own meanings , which seem to contribute to the overall meaning of the compounds .
261
This can be seen through the examples given in Table 4 , where the meanings of tokens are given with a subscript .
This can be seen through the examples provided in Table 4 , where the meanings of tokens are given with a subscript .
262
This problem seems to have caused a lot of trouble for the annotators of VTB .
This scenario has proven to be problematic for the annotators of VTB .
263
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these diffcult cases , to see whether the solution is fruitful for applications of the corpus .
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these cases , in order to see whether the solution is successful for applications of the corpus .
264
Checking structural inconsistency of these special characters including percentage% , hyphen - , and so on , we found quite a significant amount of inconsistent annotations .
By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations .
265
It does matter higher-levels of annotation such as POS tagging because we may need one or two different POS tags for different ways of annotation .
Higher-levels of annotation such as POS tagging is significant , because we may need one or two different POS tags for the different methods of annotation .
266
Among them , the best model VAR_SPLIT achieved 36 .91 BLEU score , which is 0 .55 higher than ORG .
Among them , the best-performing model , VAR_SPLIT achieved 36 .91 BLEU score , which is 0 .55 higher than ORG .
267
In TC results , all six augmented models have higher results than ORG .
In TC results , all six augmented models achieved higher results than ORG .
268
In general , the augmented models are better than the ORG .
In general , the augmented models performed better than the ORG .
269
In this paper , we have shown a quantitative analysis of the diffculties in word segmentation , through the detection of problematic cases in the Vietnamese treebank .
In this paper , we have provided a quantitative analysis of the difficulties in word segmentation , through the detection of problematic cases in the Vietnamese Treebank .
270
Although there are several approaches proposed to cope with this problem , their extremely high computational cost limits their scalability on largescale video datasets that may contain millions faces of hundreds characters .
Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .
271
The experimental results demonstrate that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
The experimental results show that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
272
News videos play an important role in our sources of information nowadays because of their rich and important contents .
News videos play an important role as a source of information nowadays because of their rich and relevant contents .
273
Accordingly , it creates an urgent demand for retrieving useful information in such news video datasets .
Accordingly , this creates an urgent demand to retrieve useful information from such news video datasets .
274
Since most of the news is related to human , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
275
On the other hand , efficiency is also an issue of such a face retrieval system beside its accuracy since scales of available datasets are getting larger rapidly , for instance , exceeding thousands hours of videos with millions faces of hundreds character .
Besides accuracy , efficiency is also an issue in such a face retrieval system because the scales of available datasets are rapidly getting larger , for instance , exceeding thousands of hours of videos with millions of faces of hundreds of characters .
276
And , the second step is matching the extracted ones with a given query to return a rank list .
, The second step is matching the extracted appearances with a given query so as to return a rank list .
277
While conventional approaches consider single face images as the basic units for extracting and matching \CITE , recently proposed approaches sifted towards sets of face images called face-tracks .
Whereas conventional approaches consider single face images as the basic units in extracting and matching \CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .
278
Since each face-track is a set of face images , matching face-tracks essentially can be thought of as a problem of matching image sets .
Because each face track is a set of face images , matching face tracks can essentially be thought of as a problem of matching image sets .
279
In these works , image set has been modeled in different way , such as distributions \CITE , subspaces \CITE , convex geometric region in feature space \CITE , or more general manifolds \CITE .
Using these approaches , the image set has been modeled in different ways , including as distributions \CITE , subspaces \CITE , a convex geometric region in a feature space \CITE , or more general manifolds \CITE .
280
The basic idea is to employ a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
The basic idea is to use a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
281
Our first dataset is from 370 hours TRECVID news videos which contains 405,887 detected faces belonging to 41 individuals .
Our first dataset is from 370 hours of TRECVID news videos and contains 405,887 detected faces belonging to 41 individuals .
282
Limitations of this approach includes the expensive computational cost for constructing and clustering high dimensional representation feature vectors; and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure no group contains faces of multiple characters and groups are not over-fragmented.
The limitations of this approach include its high computational cost for constructing and clustering high-dimensional representation feature vectors and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure that no group contains faces of multiple characters and that groups are not over-fragmented.
283
Although using tracking is an efficient solution , it may return poor tracking results since trackers are very sensitive to illumination changes and partial occlusions .
Although using tracking is an efficient solution , it may return poor tracking results because trackers are very sensitive to illumination changes and partial occlusions .
284
However , one character has only around 2.15 videos .
However , each character has only around 2.15 videos .
285
Such a small number of samples for each character is not sufficient for stably evaluating a face matching or recognition approach , which is an important part of a face retrieval system .
Such a small number of samples for each character is not sufficient to stably evaluate a face-matching or recognition approach , which is an important part of a face retrieval system .
286
Because of all above mentioned reasons , we prepare new datasets for evaluating the approaches.
In view of all the above-mentioned considerations , we prepare new datasets for evaluating the approaches.
287
One extracted face-track contains multiple face images of one individual character , varied under different viewpoints , illumination conditions , and expressions within a shot .
Each extracted face track contains multiple face images of one individual character , varied under different viewpoints , illumination conditions , and expressions within a shot .
288
A single face image in a face-track is represented by a feature vector .
Each single face image in a face track is represented by a feature vector .
289
Since the retrieved set is huge , our approach targets an extremely efficient face-track matching strategy while maintaining competitive performance with state-ofthe-art approaches.
Because the retrieved set is huge , our approach targets an extremely efficient face-track matching strategy while maintaining a competitive performance with state-of-the-art approaches.
290
has demonstrated its efficiency and robustness on drama videos \CITE , directly applying the approach to news videos results poor performances due to following issues.
has shown its efficiency and robustness with drama videos \CITE , directly applying the approach to news videos results in poor performance due to the following issues.
291
Since the KLT tracker uses intensity variance for computing the image motion to find the correspondence between points in different frames , it is unreliable when there is a sudden and significant change in illumination .
Because the KLT tracker uses intensity variance for computing the image motion to find the correspondence between points in different frames , it is unreliable when there is a sudden and significant change in illumination .
292
But , they become meaningless to determine the connection between faces.
However , they become meaningless in determining the connection between faces.
293
Based on above observed limitations of the approach in \CITE on news videos , we integrate techniques to bypass these liminations in our proposed approach for face-track extraction on news videos.
Based on the observed limitations of the approach in \CITE when applied to news videos , we integrate techniques to bypass these restrictions in our proposed approach to face-track extraction in news videos.
294
Because such illumination changes are very common and they mostly appear together with important character in a news , a solution to this problem is vital .
Because such illumination changes are very common and mostly occur simultaneously with important characters in a news video , finding a solution to this problem is vital .
295
And , faces appeared in those frames are less informative for recognition since most of the facial identity characteristics are loss due to overlighting .
and that the , faces that appear in those frames are less informative for recognition because most of the facial identity characteristics are lost due to over-lighting .
296
Since such points are likely tracked incorrectly , eliminating them prevent us from transferring tracking errors to latter frames .
Because such points are likely tracked incorrectly , eliminating them prevents us from transferring tracking errors to latter frames .
297
By doing that , our tracking results through a long sequence of frames become more accurate and reliable .
By doing so , our tracking results over a long sequence of frames become more accurate and reliable .
298
, Although these existing approaches achive high accuracy on benchmark datasets , their expensive computational costs limits their practical applications on large-scale datasets .
However , although these approaches have shown high accuracy in benchmark datasets , their high computational costs limit their practical applications in large-scale datasets .
299
By doing that , the require computational cost can be reduced while a sufficient amount of information is kept for improving accuracy .
In doing so , the required computational cost can be reduced while keeping the amount of information sufficient to improve accuracy .
300
Let denote mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} are two mean faces of two face-track A and B , respectively , with N imposes the number of dimension of the feature space .
Let mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} denote the mean faces of face tracks A and B , respectively , with N representing the number of dimensions of the feature space .
301
Besides that , since k-Faces averages multiple faces for a representative face of a face-track , the effects of noisy or outliers faces on estimating the similarity of face-tracks will be substantially reduced.
Besides , because k-Faces averages multiple faces for the representative face of a face track , the effects of noisy or outlier faces on estimating the similarity of face tracks will be substantially reduced.
302
A face-track of one character appearing in a video shot is annotated by indexes of the frames which the first face and the last face of that character occur .
Each face track of a character appearing in a video shot is annotated by indexes of the frames in which the first face and the last face of that character occur .
303
In this experiment , we directly compare our approach with one proposed by Everingham et al .
In this experiment , we directly compare our approach with that proposed by Everingham et al .
304
However , we observe that fully occlusion is rarely happened in news video since characters reported in the news are recorded with care , especially with important and well-known character .
Nevertheless , we observe that full occlusion rarely happens in news video because the characters featured in the news are recorded with care , especially the important and well-known ones .
305
This is a special property of news videos .
This is a special characteristic of news videos .
306
However , a trade-o_ between completely obtaining 6% remaining face-tracks and an overly expensive computational cost should be considered with care.
However , the trade-off between obtaining the 6% remaining face tracks and incurring an overly high computational cost should be considered with care.
307
Since our approach extract face-tracks in each video shot , shot boundaries for videos are required .
Because our approach extracts face tracks in each video shot , the shot boundaries of videos are required .
308
This dataset is observed from NHKNews7 channel in 11 years .
This dataset consists of observations from the NHK News 7 program over 11 years .
309
In the Table 4 , we compare our datasets with some public benchmark datasets .
Table 4 shows a , comparison between our datasets and some public benchmark datasets .
310
Compared to Youtube Faces dataset , although ours have less number of character ( or subjects ) , we provide much more face-tracks ( or video shots ) per character , .
Compared to the YouTube Faces dataset , , we provide much more face tracks ( or video shots ) per character , although our datasets have smaller numbers of characters ( or subjects ) .
311
We compare k-Faces with several approaches , including approaches based on pair-wise distances , MSM \CITE and CMSM \CITE.
We compared k-Faces with several approaches , including those based on pair-wise distances , MSM \CITE and CMSM \CITE.
312
By doing that , the subspaces are expected to be better separatable .
In doing so , the subspaces are expected to be more separable .
313
Besides evaluating k-Faces with different values of k as well as different types of distance ( e.g. , Euclidean , L1 , cosine ) , we try another criterion to select k representative faces in a face-track .
Besides evaluating k-Faces with different values of k and different types of distance ( e.g. , Euclidean , L1 , and cosine ) , we try another criterion for selecting k representative faces in a face track .
314
In the original way , we proposed to select these faces by partitioning the face-track following temporal order and selecting the middle face of each partition .
In the original way , we proposed selecting these faces by partitioning the face track according to the temporal order and choosing the middle face of each partition .
315
In particular , for each dataset , each face-track is alternatively picked out as a query facetrack , while the remaining face-tracks are used as the retrieved database .
In particular , in each dataset , a face track is alternatively picked out as a query face track , while the remaining face tracks are used as the retrieved database .
316
Then , the MAP of the evaluated approach can be computed as following: \MATH
Then , the MAP of the evaluated approach can be computed as follows: \MATH
317
The gap of MAPs between two datasets can be explained by following reasons .
The difference in the MAPs between the two datasets can be explained by following reasons .
318
Thus , besides facial variations caused by enviromental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) in each face-track , face-tracks of a character themself also contain biological variation of the character during time .
Thus , besides facial variations in each face track caused by the environmental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) , the face tracks of the character themselves also reflect the biological variations of the character over time; .
319
Thus , discriminative power of the computed similarity score is reduced , compared to one computed by pair:min .
Thus , the discriminative power of the computed similarity score is reduced , compared to that computed by pair:min .
320
It causes the gap of MAPs between pair:min and pair:min .
This causes the difference in MAPs between pair:min and pair:min .
321
Since the average length of face-tracks on NHKNews7 is longer ( i.e. , each face-track contains more sample faces of a character ) , there is more chance that two face-tracks of the same character contain identical faces.
Because the average length of face tracks on NHKNews7 is longer ( i.e. , each face track contains more sample faces of a character ) , there is a greater chance that two face tracks of the same character contain identical faces.
322
Since k-Faces.KMeans always use all faces in a facetrack for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
Because k-Faces.KMeans always uses all the faces in a face track for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
323
Meanwhile , its disadvantage is the expensive computational cost to perform clustering faces on a high dimensional feature space ( i.e. , 1937 dimensions ) .
However , its disadvantage is the high computational cost of clustering faces on a high-dimensional feature space ( i.e. , 1,937 dimensions ) .
324
Since keep increasing k does not help to obtain imporant accuracy improvement but expensive computational cost , we select k = 20 to investigate the trade-off between accuracy and computational costs of k-Faces approaches compared to others .
Because further increasing k does not help improve accuracy but increases the computational cost , we select k = 20 for investigating the trade-off between the accuracy and computational cost of k-Faces approaches in comparison to others .
325
One may concern that why MSM perform poorly on Trecvid dataset , but it is superior to our k-Faces.Temporal on NHKNews7 .
One may question why MSM performed poorly in the Trecvid dataset , but was superior to k-Faces.Temporal in NHKNews7 .
326
As expected , the results in this experiment demonstrate that our proposed approach is extremely efficient while archiving comparable performance with state-of-the-art approaches�f.
As expected , the results of this experiment show that our proposed approach is extremely efficient while achieving comparable performance with state-of-the-art approaches .
327
Based on that , we introduce techniques and solutions to bypass the problems for robust face-track extraction .
Based on these , we introduce techniques and solutions to overcome these problems to achieve robust face-track extraction .
328
Given an initial query image , Recommend-Me automatically introduces its recommendations to users .
Given an initial query image , Recommend-Me automatically shows its recommendations to users .
329
We introduce an efficient approach for Recommend-Me to deal with quantifying occurences of multiple candidate items over images of the database .
We describe an efficient approach for Recommend-Me to deal with quantifying occurrences of multiple candidate items in the images of the database .
330
The reason is because relevant items are not in the database .
The reason is that relevant items are not in the database .
331
The expected scheme can be described as follows ( see Figure \REF for an example ) .
The envisioned scheme can be described as follows ( see Figure \REF for an example ) .
332
Items with higher assigned numbers will be more recommended .
Items with larger assigned numbers will be more recommended .
333
By providing such recommendations , Recommend-Me supports users to :
By providing such recommendations , Recommend-Me helps users to :
334
No extra information or knowledge is required for input but an initial query image and a retrieved database .
No extra information or knowledge is required for an input besides an initial query image and a database . //<" and the name of the database " ? ? Or " and the location and name of the database " ? ?>
335
Based on these insights , we make an yet another efficiency boost by formulating the problem as an optimization problem which can be solved by applying a branh-and-bound algorithm .
Based on this insight , we can boost efficiency yet again by formulating the problem as an optimization problem that can be solved by applying a branch-and-bound algorithm .
336
One of the most related works to Recommend-Me for query suggestion is proposed by Zha et al in \CITE .
One of the most related studies to ours is that of Zha et al in \CITE .
337
Meanwhile , Recommend-Me supports users to select queries based on the existence of their relevant items in the retrieved database .
Meanwhile , Recommend-Me helps users to select queries based on the existence of relevant items in the retrieved database .
338
Finally , ours and ESS , ESR do not share common approach to construct bounding quality function and to compute bounding values over the sets .
Finally , ESS , ESR and Recommend-Me differ in that they have different approaches to constructing a bounding quality function and to computing bounding values over the sets .
339
In this step , we perform our proposed approach , explained in Section 3 , to find top \MATH ( an expected number of returned region pairs ) of such pairs in the pool .
In this step , we use the approach explained in Section 3 to find the top \MATH ( the expected number of returned region pairs ) of such pairs in the pool .
340
Using those numbers , we rank all groups and then introduce them to users as our recommendations .
Using those numbers , we rank all groups and show them as recommendations to users .
341
In this section , we introduce our proposed approach for efficiently finding top \MATH similar region pairs in the pool of all possible region pairs .
In this section , we describe our approach for efficiently finding the top \MATH similar region pairs in the pool of all possible region pairs .
342
So , many portions of the parameter space can be eliminated if their upper bound values imply that they cannot contain the maximum .
Thus , many portions of the parameter space can be eliminated if their upper bound values imply that they cannot contain the maximum .
343
Given \MATH indicates the set of node pairs formed by paring nodes in \MATH with nodes in \MATH , we have : \MATH
Letting \MATH indicate the set of node pairs formed by pairing nodes in \MATH with nodes in \MATH , we get : \MATH . //<the rewrite is a guess .>
344
So , if \MATH and \MATH are roots of \MATH and \MATH respectively , \MATH will exactly be the entire search space \MATH .
Thus , if \MATH and \MATH are roots of \MATH and \MATH respectively , \MATH will be exactly the entire search space \MATH .
345
And , the normalization terms , which indicate the minimum number of visual words inside any member region of \MATH , \MATH , are computed once by using integral image technique .
Moreover , the normalization terms , which indicate the minimum number of visual words inside any member region of \MATH , \MATH , are computed once by using the integral image technique .
346
Inspired by \CITE , we form the algorithm in best-first manner .
Inspired by \CITE , we devised the algorithm to work in a best-first manner . //<The original describes the way you decided to write the algorithm . In contrast , the rewrite describes the way the algorithm works .>
347
To obtain more than one region pair , we simply continue the loop in the Algorithm 1 until the expected number of region pairs \MATH have been reached .
To obtain more than one region pair , we simply repeat the loop in Algorithm 1 until the expected number of region pairs \MATH is reached .
348
And , all non-leaf nodes will be taken into account as candidate item regions via their attachments .
Moreover , all non-leaf nodes will be taken into account as candidate item regions via their attachments .
349
Given multiple hierarchical structures returned from the first stage , we use their root nodes as initial elements to construct an yet another hierarchical structure over them by divisive clustering .
If multiple hierarchical structures are returned by the first stage , we use their root nodes as the initial elements to construct an yet another hierarchical structure over them by divisive clustering . //<the rewrite is a guess .>
350
We call such recommendations as hit recommendations . Thus , a good recommendation system should accurately provide such hit recommendation to users .
We call such recommendations" hit recommendations " , and a good recommendation system should accurately provide them to users .
351
Based on those insights , we evaluate Recommend-Me system using two evaluation metrics : precision on introducing recommendation and rank of the first hit recommendation on the list .
Based on these insights , we evaluated the Recommend-Me system using two evaluation metrics : precision in pesenting recommendations and rank of the first hit recommendation on the list .
352
We apply an approach used in Pascal VOC challenge to clarify whether a recommendation is a hit recommendation .
We used an approach from the Pascal VOC challenge to clarify whether a recommendation is a hit recommendation or not .
353
Because our target is not to improve search techniques but to facilitate query selection procedure , search performance simply relies on standard techniques if users take an recommendation as a search query .
Because our target is not to improve search techniques but to facilitate query selection , the search simply relies on standard techniques if users use an recommendation as a search query .
354
Given the set of regions in the initial query image , we build a graph in which two regions are connected if they highly overlap each other ( we use the approach of Pascal VOC with tighter threshold , 0 .8 ) .
Given the set of regions in the initial query image , we built a graph in which two regions were connected if they nearly overlapped each other ( we use the approach of Pascal VOC with a tighter threshold , 0 .8 ) .
355
This is because the branch-and-bound algorithm has to visit more parts of the total search space in order to find extra local optimals .
This is because the branch-and-bound algorithm has to visit more parts of the total search space in order to find extra local opitimals .
356
Its superiority is important for practical applications .
This advantage will be important for practical applications .
357
In this paper , we introduced a new system , named Recommend-Me , for visual query suggestion .
We described oursystem , named Recommend-Me , for making visual query suggestions .
358
Given an initial query image and a retrieved database , Recommend-Me introduces recommendations that imposes which and how frequent items in the initial query image appear in the database .
Given an initial query image and a retrieved database , Recommend-Me gives recommendations that impose conditions on which and how frequent items in the initial query image appear in the database .
359
Such recommendations support users to select search query , to rapidly refine the initial query image or to explore the database .
Such recommendations help users to select the search query , to rapidly refine the initial query image or to explore the database .
360
To the best of our knowledge , Recommend-Me is the first attempt toward its targeted suggestion scheme .
To the best of our knowledge , Recommend-Me is the first attempt at developing a targeted suggestion scheme .
