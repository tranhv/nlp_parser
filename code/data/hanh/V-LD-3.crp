0
<document>
<document>
1
<title>
<title>
2
Unsupervised Face Annotation by Mining the Web
Unsupervised Face Annotation by Mining the Web
3
</title>
</title>
4
<abstract>
<abstract>
5
Searching for images of people is an essential task for image and video search engines .
Searching for images of people is an essential task for image and video search engines .
6
However , current search engines have limited capabilities for this task since they rely on text associated with images and video , and such text is likely to return many irrelevant results .
However , current search engines have limited capabilities for this task since they rely on text associated with images and video , and such text is likely to return many irrelevant results .
7
We propose a method to retrieve relevant faces for one person by learning the visual consistency among results retrieved from text-correlation-based search engines .
We propose a method for retrieving relevant faces of one person by learning the visual consistency among results retrieved from text-correlation-based search engines .
8
The method consists of two steps .
The method consists of two steps .
9
In the first step , each candidate face obtained from a text-based search engine is ranked by a score that measures the distribution of visual similarities among the faces .
In the first step , each candidate face obtained from a text-based search engine is ranked with a score that measures the distribution of visual similarities among the faces .
10
Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list .
Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list , respectively .
11
The second step improves this ranking by treating this problem as a classification problem in which input faces are classified as 'person-$X$' or 'non-person-$X$' ; and the faces are re-ranked according to their relevant score inferred from the classifier 's probability output .
The second step improves this ranking by treating this problem as a classification problem in which input faces are classified as 'person-$X$' or 'non-person-$X$' ; and the faces are re-ranked according to their relevant score inferred from the classifier 's probability output .
12
To train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers trained using different subsets .
To train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers trained using different subsets .
13
These training subsets are extracted and labeled automatically from the rank list produced from the classifier trained from the previous step .
These training subsets are extracted and labeled automatically from the rank list produced from the classifier trained from the previous step .
14
In this way , the accuracy of the ranked list increases after a number of iterations .
In this way , the accuracy of the ranked list increases after a number of iterations .
15
Experimental results on various face sets retrieved from captions of news photos show that the retrieval performance improved after each iteration , with the final performance being higher than those of the existing algorithms .
Experimental results on various face sets retrieved from captions of news photos show that the retrieval performance improved after each iteration , with the final performance being higher than those of the existing algorithms .
16
</abstract>
</abstract>
17
<section label= " Introduction " >
<section label= " Introduction " >
18
<p>
<p>
19
With the rapid growth of digital technology , large image and video databases have become more available than ever to users .
With the rapid growth of digital technology , large image and video databases have become more available than ever to users .
20
This trend has shown the need for effective and efficient tools for indexing and retrieving based on visual content .
This trend has shown the need for effective and efficient tools for indexing and retrieving visual content .
21
A typical application is searching for a specific person by providing his or her name .
A typical application is searching for a specific person by providing his or her name .
22
Most current search engines use the text associated with images and video as significant clues for returning results .
Most current search engines use the text associated with images and video as significant clues for returning results .
23
However , other un-queried faces and names may appear with the queried ones ( as shown in Figure xx ) , and this significantly lowers retrieval performance .
However , other un-queried faces and names may appear with the queried ones ( Figure xx ) , and this significantly lowers the retrieval performance .
24
One way to improve the retrieval performance is to take into account visual information present in the retrieved faces .
One way to improve the retrieval performance is to take into account visual information present in the retrieved faces .
25
This task is challenging for the following reasons :
This task is challenging for the following reasons :
26
</p>
</p>
27
<p>
<p>
28
-Large variations in facial appearance due to pose changes , illumination conditions , occlusions and facial expressions make face recognition difficult even with state-of-the-art techniques\CITE ( see an example in Figure xx ) .
-Large variations in facial appearance due to pose changes , illumination conditions , occlusions , and facial expressions make face recognition difficult even with state-of-the-art techniques\CITE ( see example in Figure xx ) .
29
</p>
</p>
30
<p>
<p>
31
-The fact that the retrieved face set consists of faces of several people with no labels makes supervised and unsupervised learning methods inapplicable .
-The fact that the retrieved face set consists of faces of several people with no labels makes supervised and unsupervised learning methods inapplicable .
32
</p>
</p>
33
<p>
<p>
34
We propose a method to solve the above problem .
We propose a method for solving the above problem .
35
The main idea is to assume that there is visual consistency among the results returned from text-based search engines ; and then learn this visual consistency through an interactive process .
The main idea is the assumption that there is visual consistency among the results returned from text-based search engines and this visual consistency is then learned through an interactive process .
36
This method consists of two stages .
This method consists of two stages .
37
In the first stage , we explore the local density of faces to identify potential candidates for relevant faces and irrelevant faces .
In the first stage , we explore the local density of faces to identify potential candidates for relevant faces and irrelevant faces .
38
This stage reflects the fact that the facial images of the queried person tend to form dense clusters , whereas irrelevant facial images are sparse since they look different from each other .
This stage reflects the fact that the facial images of the queried person tend to form dense clusters , whereas irrelevant facial images are sparse since they look different from each other .
39
For each face , we define a score to measure the density of its neighbor set .
For each face , we define a score to measure the density of its neighbor set .
40
This score is used to form a ranked list , in which faces having high density scores are considered relevant and are put at the top of the list .
This score is used to form a ranked list , in which faces with high-density scores are considered relevant and are put at the top .
41
</p>
</p>
42
<p>
<p>
43
The above ranking method is weak since dense clusters have no guarantee of containing relevant faces .
The above ranking method is weak since dense clusters have no guarantee of containing relevant faces .
44
Therefore , a second stage is necessary to improve this ranked list .
Therefore , a second stage is necessary to improve this ranked list .
45
We model this problem as a classification problem in which input faces are classified as person-\MATH ( the queried person ) or non-person-\MATH ( the un-queried person ) .
We model this problem as a classification problem in which input faces are classified as person-\MATH ( the queried person ) or non-person-\MATH ( the un-queried person ) .
46
The faces are ranked according to a relevancy score that is inferred from the classifier 's probability output .
The faces are ranked according to a relevancy score that is inferred from the classifier 's probability output .
47
Since annotation data is not available , the rank list from the previous step is used to assign labels for a subset of faces .
Since annotation data is not available , the rank list from the previous step is used to assign labels for a subset of faces .
48
This subset is then used to train a classifier using supervised methods such as support vector machine ( SVM ) .
This subset is then used to train a classifier using supervised methods such as a support vector machine ( SVM ) .
49
The trained classifier is used to re-rank faces in the original input set .
The trained classifier is used to re-rank faces in the original input set .
50
This step is repeated a number of times to get the final ranked list .
This step is repeated a number of times to get the final ranked list .
51
Since automatically assigning labels from the ranked list is not reliable , the trained classifiers are weak .
Since automatically assigning labels from the ranked list is not reliable , the trained classifiers are weak .
52
To get the final strong classifier , we use the idea of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
To obtain the final strong classifier , we use the [idea / concept?] of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
53
The learned classifier can be further used for recognizing new facial images of the queried person .
The learned classifier can be further used for recognizing new facial images of the queried person .
54
</p>
</p>
55
<p>
<p>
56
The second stage improves the ranked list and recognition performance for the following reasons :
The second stage improves the ranked list and recognition performance for the following reasons :
57
</p>
</p>
58
<p>
<p>
59
-Supervised learning methods , such as SVM , provide a strong theoretical background for finding the optimal decision boundary even with noisy data .
-Supervised learning methods , such as an SVM , provide a strong theoretical background for finding the optimal decision boundary even with noisy data .
60
Furthermore , recent studies \CITE suggest that SVM classifiers provide probability outputs that are suitable for ranking .
Furthermore , recent studies \CITE suggest that SVM classifiers provide probability outputs that are suitable for ranking .
61
</p>
</p>
62
<p>
<p>
63
-The bagging framework helps to leverage noises in the unsupervised labeling process .
-The bagging framework helps to leverage noises in the unsupervised labeling process .
64
</p>
</p>
65
<p>
<p>
66
Our contribution is two-fold :
Our contribution is two-fold :
67
</p>
</p>
68
<p>
<p>
69
-We propose a general framework to boost the face retrieval performance of text-based search engines by visual consistency learning .
-We propose a general framework to boost the face retrieval performance of text-based search engines by visual consistency learning .
70
</p>
</p>
71
<p>
<p>
72
The framework seamlessly integrates data mining techniques such as supervised learning , and unsupervised learning based on bagging .
The framework seamlessly integrates data mining techniques such as supervised learning and unsupervised learning based on bagging .
73
</p>
</p>
74
<p>
<p>
75
-Our framework requires only a few parameters and works stably .
-Our framework requires only a few parameters and works stably .
76
</p>
</p>
77
<p>
<p>
78
We demonstrate its feasibility of a practical web mining application .
We demonstrate its feasibility with a practical web mining application .
79
</p>
</p>
80
<p>
<p>
81
A comprehensive evaluation on a large face dataset of many people was carried out and it confirmed that our approach is promising .
A comprehensive evaluation on a large face dataset of many people was carried out and confirmed that our approach is promising .
82
</p>
</p>
83
</section>
</section>
84
<section label= " Related Work " >
<section label= " Related Work " >
85
<p>
<p>
86
There are several approaches for re-ranking and learning models from web images .
There are several approaches for re-ranking and learning models from web images .
87
Their underlying assumption is that text-based search engines return a large fraction of relevant images .
Their underlying assumption is that text-based search engines return a large fraction of relevant images .
88
The challenge is how to model what is common in the relevant images .
The challenge is how to model what is common in the relevant images .
89
One approach is to model this problem in a probabilistic framework in which the returned images are used to learn the parameters of the model .
One approach is to model this problem in a probabilistic framework in which the returned images are used to learn the parameters of the model .
90
For examples , as described in \CITE , [Reference numbers generally should not be grammatically part of the sentence .
For examples , as described by Fergus et al. \CITE , [Reference numbers generally should not be grammatically part of the sentence .
91
It is better to use the authors�f names .]objects retrieved by an image search engine are re-ranked by extending the constellation model .
It is better to use the authors�f names .] objects retrieved using an image search engine are re-ranked by extending the constellation model .
92
Another proposal , described in \CITE , uses a non-parametric graphical model and an interactive framework to simultaneously learn object class models and collect object class datasets .
Another proposal , described in \CITE , uses a non-parametric graphical model and an interactive framework to simultaneously learn object class models and collect object class datasets .
93
The main contribution of these approaches are probabilistic models that can be learned with a small number of training images .
The main contribution of these approaches is probabilistic models that can be learned with a small number of training images .
94
However , these models are complicated , since they require several hundred parameters for learning , and they are susceptible to over-fitting .
However , these models are complicated since they require several hundred parameters for learning and are susceptible to over-fitting .
95
Furthermore , to obtain robust models , a small amount of supervision is required to select seed images .
Furthermore , to obtain robust models , a small amount of supervision is required to select seed images .
96
</p>
</p>
97
<p>
<p>
98
Another study \CITE proposed a clustering-based method for associating names and faces in news photos .
Another study \CITE proposed a clustering-based method for associating names and faces in news photos .
99
To solve the problem of ambiguity between several names and one face , a modified \MATH-means clustering process was used in which faces are assigned to the closest cluster ( each cluster corresponding to one name ) after a number of iterations .
To solve the problem of ambiguity between several names and one face , a modified \MATH-means clustering process was used in which faces are assigned to the closest cluster ( each cluster corresponding to one name ) after a number of iterations .
100
Although the result was impressive , it is not easy to apply it to our problem since it is based on a strong assumption that requires a perfect alignment in the case that the news photo only has one face and its caption only has one name .
Although the result was impressive , it is not easy to apply it to our problem since it is based on a strong assumption that requires a perfect alignment when a news photo only has one face and its caption only has one name .
101
Furthermore , a large number of irrelevant faces ( more than 12\% ) have to be manually eliminated before clustering .
Furthermore , a large number of irrelevant faces ( more than 12\% ) have to be manually eliminated before clustering .
102
</p>
</p>
103
<p>
<p>
104
A graph-based approach was proposed by \CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
A graph-based approach was proposed by Ozkan and Duygu \CITE , in which a graph is formed from faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
105
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and therefore can be solved by taking an available solution .[It might be unclear as to what " available solution " you are talking about .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and can therefore , be solved by taking an available solution . //It might be unclear as to what " available solution " you are talking about . You might want to give more detail here .
106
You might want to give more detail here .] Although , experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of relevant faces of the queried person and it is easy to extend for the ranking problem .
Although experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of the relevant faces of the queried person and it is easy to extend for the ranking problem .
107
Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to the curse of dimensionality .
Furthermore , choosing an optimal threshold to convert the initial graph into a binary one is difficult and rather ad hoc due to dimensionality .
108
</p>
</p>
109
<p>
<p>
110
The good point of the methods \CITE is they are fully unsupervised .
An advantage of these methods \CITE is they are fully unsupervised .
111
However , the bad point is no model is learned to predict new images of the same category .
However , a disadvantage is that no model is learned for predicting new images of the same category .
112
Furthermore , they perform hard categorization on input images that is [It is not clear if " hard categorization " is inapplicable or if the " input images " are inapplicable .]in applicable for re-ranking .
Furthermore , they are used for performing hard categorization on input images that are inapplicable for re-ranking . //It is not clear if " hard categorization " is inapplicable or if the " input images " are inapplicable .
113
The balance of recall and precision was not addressed .
The balance of recall and precision was not addressed .
114
Typically , these approaches tend to ignore the recall to obtain high precision .
Typically , these approaches tend to ignore the recall to obtain high precision .
115
This leads the number of collected images is reduced .
This leads to the reduction in the number of collected images .
116
</p>
</p>
117
<p>
<p>
118
Our approach combines a number of advances over the existing approaches .
Our approach combines a number of advances over the existing approaches .
119
Specifically , we learn a model for each query from the returned images for purposes such as re-ranking and predicting new images .
Specifically , we learn a model for each query from the returned images for purposes such as re-ranking and predicting new images .
120
However , different from the methods in \cite{xx} , we used an unsupervised method to select training samples automaticallyCITE .
However , we used an unsupervised method to select training samples automatically , which is different from the methods proposed by Fergus et al. and Li et al. \CITE .
121
This unsupervised method is different from the one in \CITE in its way of modeling the distribution of relevant images .
This unsupervised method is different from the one by Ozkan and Dugyu \CITE in the modeling of the distribution of relevant images .
122
We use density-based estimation rather than the densest graph .
We use density-based estimation rather than the densest graph .
123
</p>
</p>
124
</section>
</section>
125
<section label= " Proposed Framework " >
<section label= " Proposed Framework " >
126
<p>
<p>
127
Given a set of images returned by any text-based search engine for a queried person ( e.g. 'George Bush' ) , we perform a ranking process and learning of person |\MATH 's model as follows :
Given a set of images returned by any text-based search engine for a queried person ( e.g. 'George Bush' ) , we perform a ranking process and learning of person |\MATH 's model as follows :
128
</p>
</p>
129
<p>
<p>
130
-Step 1 : Detect faces and eye positions , and then perform face normalizations .
-Step 1 : Detect faces and eye positions , and then perform face normalizations .
131
</p>
</p>
132
<p>
<p>
133
-Step 2 : Compute an eigenface space and project the input faces into this subspace .
-Step 2 : Compute an eigenface space and project the input faces into this subspace .
134
</p>
</p>
135
<p>
<p>
136
-Step 3 : Estimate the ranked list of these faces by Rank-By-Local-Density-Score .
-Step 3 : Estimate the ranked list of these faces by rank-by-local-density score .
137
</p>
</p>
138
<p>
<p>
139
-Step 4 : Improve this ranked list by Rank-By-Bagging-ProbSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet . You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
-Step 4 : Improve this ranked list using rank-by-bagging-probSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet. You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
140
</p>
</p>
141
<p>
<p>
142
Steps 1 and 2 are typical for any face processing system , and they are described in section \REF .
Steps 1 and 2 are typical for any face processing system , and they are described in section \REF .
143
The algorithms used in Step 3 and Step 4 are described in section \REF and section \REF .
The algorithms used in Steps 3 and 4 are described in section \REF and section \REF , respectively .
144
Figure \REF illustrates the proposed framework .
Figure \REF illustrates the proposed framework .
145
</p>
</p>
146
</section>
</section>
147
<section label= " Ranking by Local Density Score " >
<section label= " Ranking by Local Density Score " >
148
<p>
<p>
149
Among the faces retrieved by the text-based search engines for a query of person-\MATH , as shown in Figure \REF , relevant faces usually look similar and can form the largest cluster .
Among the faces retrieved by text-based search engines for a query of person-\MATH , as shown in Figure \REF , relevant faces usually look similar and forms the largest cluster .
150
One approach to re-rank these faces is to do clustering based on visual similarity .
One approach of re-ranking these faces is to cluster based on visual similarity .
151
However , to get ideal clustering result is impossible , since these faces are high dimensional data and the clusters are in different shapes , sizes and densities .
However , to obtain ideal clustering results is impossible since these faces are high dimensional data and the clusters are in different shapes , sizes , and densities .
152
Instead , in \cite{xx} , a graph based approach was proposed CITEin which the nodes are faces and edge weights are the similarities between two faces .
Instead , a graph-based approach was proposed by Ozkan and Dugyu \CITE in which the nodes are faces and edge weights are the similarities between two faces .
153
With the observation that the nodes ( faces ) of the queried person are similar to each other and different from other nodes in the graph , the densest component of the full graph ? the set of highly connected nodes in the graph ? will correspond to the face of the queried person .
With the observation that the nodes ( faces ) of the queried person are similar to each other and different from other nodes in the graph , the densest component of the full graph ? the set of highly connected nodes in the graph ? will correspond to the face of the queried person .
154
The main drawback of this approach is it needs a threshold to convert the initial weighted graph to a binary graph .
The main drawback of this approach is it needs a threshold to convert the initial weighted graph to a binary graph .
155
Choosing this threshold in high dimensional spaces is difficult since different persons might have different optimal thresholds .
Choosing this threshold in high dimensional spaces is difficult since different persons might have different optimal thresholds .
156
</p>
</p>
157
<p>
<p>
158
We use the idea of density-based clustering described in \CITE to solve this problem .
We use the idea of density-based clustering described by Ester et al. and Breunig et al. \CITE to solve this problem . //idea / concept?
159
Specifically , we define local density score ( LDS ) of a point \MATH( i.e. a face ) as the average distance to its k-nearest neighbors :
Specifically , we define the local density score ( LDS ) of a point \MATH( i.e. a face ) as the average distance to its k-nearest neighbors .
160
</p>
</p>
161
<p>
<p>
162
where \MATH is the set of \MATH - neighbors of \MATH , and \MATH is the similarity between \MATH and \MATH .
where \MATH is the set of \MATH - neighbors of \MATH , and \MATH is the similarity between \MATH and \MATH .
163
</p>
</p>
164
<p>
<p>
165
Since faces are represented in high dimensional feature space , and face clusters might have different sizes , shapes and densities ; we do not use directly the Euclidean distance between two points in this feature space for \MATH .
Since faces are represented in high dimensional feature space , and face clusters might have different sizes , shapes , and densities , we do not directly use the Euclidean distance between two points in this feature space for \MATH .
166
Instead , we use another similarity measure defined by the number of shared neighbors between two points .
Instead , we use another similarity measure defined by the number of shared neighbors between two points .
167
The efficiency of this similarity measure for density-based clustering methods was described . //There is no period here , so it is not clear if there should be a period or there should be more to this sentence that is not here . If the sentence does end here , you might want to go into more detail about who or what " described " this .]
The efficiency of this similarity measure for density-based clustering methods was described . //There is no period here , so it is not clear if there should be a period or there should be more to this sentence that is not here . If the sentence does end here , you might want to go into more detail about who or what " described " this .
168
</p>
</p>
169
<p>
<p>
170
A high value of \MATH indicates a strong association between \MATH and its neighbors .
A high value of \MATH indicates a strong association between \MATH and its neighbors .
171
Therefore , we can use this local density score to rank faces .
Therefore , we can use this local density score to rank faces .
172
Faces with higher scores are considered to be potential candidates that are relevant to person-\MATH , while faces with lower scores are considered as outliers and thus are potential candidates for non-person-\MATH .
Faces with higher scores are considered to be potential candidates that are relevant to person-\MATH , while faces with lower scores are considered as outliers and thus are potential candidates for non-person-\MATH .
173
</p>
</p>
174
<p>
<p>
175
Algorithm 1 : Rank-By-Local-Density-Score Step 1 : For each face p , compute LDS( p , k ) , where k is the number of neighbors of p and is the input of the ranking process .
Algorithm 1 : Rank-By-Local-Density-Score Step 1 : For each face p , compute LDS( p , k ) , where k is the number of neighbors of p and is the input of the ranking process .
176
Step 2 : Rank these faces using LDS( p , k ) ( The higher the more relevant ) .
Step 2 : Rank these faces using LDS( p , k ) ( The higher the score the more relevant ) .
177
</p>
</p>
178
</section>
</section>
179
<section label= " Ranking by Bagging of SVM Classifiers " >
<section label= " Ranking by Bagging of SVM Classifiers " >
180
<p>
<p>
181
One limitation of the local density score based ranking is it could not handle the case that faces of another person have strong association in \MATH-neighbor set ( for example , many duplicates ) .
One limitation of the local density score based ranking is it cannot handle faces of another person strongly associated in the \MATH-neighbor set ( for example , many duplicates ) .
182
Therefore , another step is proposed to handle this case .
Therefore , another step is proposed for handling this case .
183
As a result , we have a model that can be used for both re-ranking current faces and predicting new incoming faces .
As a result , we have a model that can be used for both re-ranking current faces and predicting new incoming faces .
184
</p>
</p>
185
<p>
<p>
186
</p>
</p>
187
The main idea is to use a probabilistic model to measure the relevancy of a face to person-\MATH , \MATH .
The main idea is to use a probabilistic model to measure the relevancy of a face to person-\MATH , \MATH .
188
Since the labels are not available for training , we use the input rank list found from the previous step to extract a subset of faces lying at the top and bottom of the ranked list to form the training set .
Since the labels are not available for training , we use the input rank list found from the previous step to extract a subset of faces lying at the top and bottom of the ranked list to form the training set .
189
After that , we use SVM with probabilistic output \CITE implemented in LibSVM \CITE to learn the person-\MATH model .
After that , we use an SVM with probabilistic output \CITE implemented in LibSVM \CITE to learn the person-\MATH model .
190
This model is applied to faces of the original set and the output probabilistic scores are used to re-rank these faces .
This model is applied to faces of the original set , and the output probabilistic scores are used to re-rank these faces .
191
Since it is not guaranteed that faces lying at two ends of the input rank list correctly correspond to the faces of person-\MATH and faces of non person-\MATH , we adopt the idea of bagging framework \CITE in which randomly selecting subsets to train weak classifiers , and then combining these classifiers help reduce the risk of using noisy training sets .
Since it is not guaranteed that faces lying at two ends of the input rank list correctly correspond to the faces of person-\MATH and faces of non person-\MATH , we adopt the [idea / concept?] of a bagging framework \CITE in which randomly selecting subsets to train weak classifiers , and then combining these classifiers help reduce the risk of using noisy training sets .
192
</p>
</p>
193
<p>
<p>
194
The details of Rank-By-Bagging-ProbSVM-InnerLoop method , improving an input rank list by combining weak classifiers trained from subsets annotated by that rank list are described in Algorithm 2 .
The details of the Rank-By-Bagging-ProbSVM-InnerLoop method , improving an input rank list by combining weak classifiers trained from subsets annotated by that rank list , are described in Algorithm 2 .
195
</p>
</p>
196
</section>
</section>
197
<section label= " Algorithm 2 : Rank-By-Bagging-ProbSVM-InnerLoop " >
<section label= " Algorithm 2 : Rank-By-Bagging-ProbSVM-InnerLoop " >
198
<p>
<p>
199
Step 1 : Train a weak classifier hi .
Step 1 : Train a weak classifier , hi .
200
</p>
</p>
201
<p>
<p>
202
Step 1 .1 : Select a set Spos including p% top ranked faces and then randomly select a subset S?pos from Spos .
Step 1 .1 : Select a set Spos including p% of top ranked faces and then randomly select a subset S?pos from Spos .
203
</p>
</p>
204
<p>
<p>
205
Label faces in S?pos as positive samples .
Label faces in S?pos as positive samples .
206
</p>
</p>
207
<p>
<p>
208
Step 1 .2 : Select a set Sneg including p% bottom ranked faces and then randomly select a subset S? neg from Sneg .
Step 1 .2 : Select a set Sneg including p% of bottom ranked faces and then randomly select a subset S? neg from Sneg .
209
</p>
</p>
210
<p>
<p>
211
Label faces in S? neg as negative samples .
Label faces in S? neg as negative samples .
212
</p>
</p>
213
<p>
<p>
214
Step 1 .3 : Use S?pos and S? neg to train a weak classifier hj using LibSVM [8] with probability outputs .
Step 1 .3 : Use S?pos and S? neg to train a weak classifier , hj , using LibSVM [8] with probability outputs .
215
</p>
</p>
216
<p>
<p>
217
Step 2 : Compute ensemble classifier Hi = Pij=1 hj .
Step 2 : Compute ensemble classifier Hi = Pij=1 hj .
218
</p>
</p>
219
<p>
<p>
220
Step 3 : Apply Hi to the original face set and form the rank list Ranki by using the output probabilistic scores .
Step 3 : Apply Hi to the original face set and form the rank list , Ranki , using the output probabilistic scores .
221
</p>
</p>
222
<p>
<p>
223
Step 4 : Repeat steps from Step 1 to Step 3 until Dist2RankList( Ranki?1 ,Ranki ) <= ? .
Step 4 : Repeat steps 1 to 3 until Dist2RankList( Ranki?1 ,Ranki ) <= ? .
224
</p>
</p>
225
<p>
<p>
226
Step 5 : Return Hi = Pij=1 hj .
Step 5 : Return Hi = Pij=1 hj .
227
</p>
</p>
228
</section>
</section>
229
<section label= " Algorithm 3 : Rank-By-Bagging-ProbSVM-OuterLoop " >
<section label= " Algorithm 3 : Rank-By-Bagging-ProbSVM-OuterLoop " >
230
<p>
<p>
231
Step 1 : Rankcur = Rank-By-Bagging-ProbSVM-InnerLoop( Rankprev ) .
Step 1 : Rankcur = Rank-By-Bagging-ProbSVM-InnerLoop ( Rankprev ) .
232
</p>
</p>
233
<p>
<p>
234
Step 2 : dist = Dist2RankList( Rankprev ,Rankcur ) .
Step 2 : dist = Dist2RankList ( Rankprev ,Rankcur ) .
235
</p>
</p>
236
<p>
<p>
237
Step 3 : Rankfinal = Rankcur .
Step 3 : Rankfinal = Rankcur .
238
</p>
</p>
239
<p>
<p>
240
Step 4 : Rankprev = Rankcur .
Step 4 : Rankprev = Rankcur .
241
</p>
</p>
242
<p>
<p>
243
Step 5 : Repeat steps from Step 1 to Step 4 until dist <= ? .
Step 5 : Repeat steps 1 to 4 until dist <= ? .
244
</p>
</p>
245
<p>
<p>
246
Step 5 : Return Rankfinal .
Step 6 : Return Rankfinal .
247
</p>
</p>
248
<p>
<p>
249
Given an input ranked list , Rank-By-Bagging-ProbSVM-InnerLoop is used to improve this rank list .
Given an input ranked list , Rank-By-Bagging-ProbSVM-InnerLoop is used to improve this list .
250
We repeat the process a number of times whereby the ranked list output from the previous step is used as the input ranked list of the next step .
We repeat the process a number of times whereby the ranked list output from the previous step is used as the input ranked list of the next step .
251
In this way , the iterations significantly improve the final ranked list .
In this way , the iterations significantly improve the final ranked list .
252
The details are described in Algorithm 3 .
The details are described in Algorithm 3 .
253
</p>
</p>
254
<p>
<p>
255
To determine the number of iterations of Rank-By-Bagging-ProbSVM-InnerLoop and Rank-By-Bagging-ProbSVM-OuterLoop , we use the \MATH distance \CITE , which is a metric that counts the number of pairwise disagreements between two lists .
To determine the number of iterations of Rank-By-Bagging-ProbSVM-InnerLoop and Rank-By-Bagging-ProbSVM-OuterLoop , we use the \MATH distance \CITE , which is a metric that counts the number of pairwise disagreements between two lists .
256
The larger the distance , the more dissimilar the two lists are .
The larger the distance , the more dissimilar the two lists are .
257
The \MATH distance between two list \MATH and \MATH is defined as follows :
The \MATH distance between two lists , \MATH and \MATH , is defined as follows :
258
</p>
</p>
259
<p>
<p>
260
Since the maximum value of \MATH is \MATH where \MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :
Since the maximum value of \MATH is \MATH , where \MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :
261
</p>
</p>
262
<p>
<p>
263
Using this measure for checking when the loops stop means that if the ranked list does not change significantly after a number of iterations , it is reasonable to stop .
Using this measure for checking when the loops stop means that if the ranked list does not change significantly after a number of iterations , it is reasonable to stop .
264
</p>
</p>
265
</section>
</section>
266
<section label= " Experiments " >
<section label= " Experiments " >
267
<subsection label= " Dataset " >
<subsection label= " Dataset " >
268
<p>
<p>
269
We used the dataset described in \CITE for our experiments .
We used the dataset described by Berg et al. \CITE for our experiments .
270
This dataset consists of approximately half a million news [pictures / photos?] and captions from Yahoo News collected over a period of roughly two years .
This dataset consists of approximately half a million news pictures and captions from Yahoo News collected over a period of roughly two years .
271
This dataset is better than datasets collected from image search engines such as Google that usually limit the total number of returned images to 1 ,000 .
This dataset is better than datasets collected from image search engines such as Google that usually limit the total number of returned images to 1 ,000 .
272
Furthermore , it has annotations that are valuable for evaluation of methods .
Furthermore , it has annotations that are valuable for evaluation of methods .
273
Note that these annotations are used for evaluation purpose only .
Note that these annotations are used for evaluation purpose only .
274
Our method is fully unsupervised , so it assumes the annotations are not available at running time .
Our method is fully unsupervised , so it assumes the annotations are not available at running time .
275
</p>
</p>
276
<p>
<p>
277
Only frontal faces were considered since current frontal face detection systems \CITE can work in real time and have accuracies exceeding 95\% .
Only the front of faces were considered since current frontal face detection systems \CITE work in real time and have accuracies exceeding 95\% .
278
44 ,773 faces were detected and normalized to the size of 86\MATH86 pixels .
44 ,773 faces were detected and normalized to 86\MATH86 pixels .
279
</p>
</p>
280
<p>
<p>
281
We selected fifteen government leaders , including George W. Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , and Abdullah Gul ( Turkey ) , and other key individuals , such as John Paul II ( the Former Pope ) and Hans Blix ( UN ) , because their images frequently appear in the dataset \CITE .
We selected fifteen government leaders , including George W. Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals , such as John Paul II ( the Former Pope ) and Hans Blix ( UN ) , because their images frequently appear in the dataset \CITE .
282
The variations in each person 's name were collected .
Variations in each person 's name were collected .
283
For example , George W. Bush , President Bush , U.S. President , etc. , all refer to the current U.S. president .
For example , George W. Bush , President Bush , U.S. President , etc. , all refer to the current U.S. president .
284
</p>
</p>
285
<p>
<p>
286
We performed simple string search in captions to check whether a caption contains one of these names .
We performed simple string search in captions to check whether a caption contained one of these names .
287
The faces extracted from the corresponding image associated with this caption were returned .
The faces extracted from the corresponding image associated with this caption were returned .
288
The faces retrieved from the different name queries were merged into one set and used as input for ranking .
The faces retrieved from the different name queries were merged into one set and used as input for ranking .
289
</p>
</p>
290
<p>
<p>
291
Figure \REF shows the distribution of retrieved faces from this method and the corresponding number of relevant faces for these fifteen individuals .
Figure \REF shows the distribution of retrieved faces from this method and the corresponding number of relevant faces for these fifteen individuals .
292
In total , 5 ,603 faces were retrieved in which 3 ,374 faces were relevant .
In total , 5 ,603 faces were retrieved in which 3 ,374 faces were relevant .
293
On average , the accuracy was 60 .22\% .
On average , the accuracy was 60 .22\% .
294
</p>
</p>
295
</subsection>
</subsection>
296
<subsection label= " Face Processing " >
<subsection label= " Face Processing " >
297
<p>
<p>
298
We used an eye detector to detect the positions of the eyes in the detected faces .
We used an eye detector to detect the positions of the eyes of the detected faces .
299
The eye detector , built with the same approach as in \CITE , had an accuracy of more than 95\% .
The eye detector , built with the same approach as that of Viola and jones \CITE , had an accuracy of more than 95\% .
300
If the eye positions were not detected , predefined eye locations were assigned .
If the eye positions were not detected , predefined eye locations were assigned .
301
The eye positions were used to align faces to a predefined canonical pose .
The eye positions were used to align faces to a predefined canonical pose .
302
</p>
</p>
303
<p>
<p>
304
To compensate for illumination effects , the subtraction of the bestfit brightness plane followed by histogram equalization was applied .
To compensate for illumination effects , the subtraction of the best-fit brightness plane followed by histogram equalization was applied .
305
This normalization process is shown in Figure \REF .
This normalization process is shown in Figure \REF .
306
</p>
</p>
307
<p>
<p>
308
We then used principle component analysis \CITE to reduce the number of dimensions of the feature vector for face representation .
We then used principle component analysis \CITE to reduce the number of dimensions of the feature vector for face representation .
309
Eigenfaces were computed from the original face set returned by the text-based query method .
Eigenfaces were computed from the original face set returned using the text-based query method .
310
The number of eigenfaces used to form the eigen space was selected so that 97\% of the total energy was retained \CITE .
The number of eigenfaces used to form the eigen space was selected so that 97\% of the total energy was retained \CITE . //It is not clear what you mean by " energy " in this context . This is the first time you mention this term . You might want to specify what kind of energy you are talking about .
311
The number of dimensions of these feature spaces ranged from 80 to 500 .
The number of dimensions of these feature spaces ranged from 80 to 500 .
312
</p>
</p>
313
</subsection>
</subsection>
314
<subsection label= " Evaluation Criteria " >
<subsection label= " Evaluation Criteria " >
315
<p>
<p>
316
We evaluated the retrieval performance with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
We evaluated the retrieval performance with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
317
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as follows :
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as follows :
318
</p>
</p>
319
<p>
<p>
320
Precision and recall only evaluate the quality of an unordered set of retrieved faces .
Precision and recall are only used to evaluate the quality of an unordered set of retrieved faces .
321
To evaluate ranked lists in which both recall and precision are taken into account , the average precision is usually used .
To evaluate ranked lists in which both recall and precision are taken into account , average precision is usually used .
322
The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
323
</p>
</p>
324
<p>
<p>
325
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
326
</p>
</p>
327
<p>
<p>
328
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries
329
</p>
</p>
330
</subsection>
</subsection>
331
<subsection label= " Parameters " >
<subsection label= " Parameters " >
332
<p>
<p>
333
The parameters of our method include :
The parameters of our method include :
334
</p>
</p>
335
<p>
<p>
336
-\MATH : the fraction of faces lying at the top and bottom of the ranked list that are used to form a positive set \MATH and negative set \MATH for training weak classifiers in Rank-By-Bagging-ProbSVM-InnerLoop .
-\MATH : the fraction of faces at the top and bottom of the ranked list that are used to form a positive set \MATH and negative set \MATH for training weak classifiers in Rank-By-Bagging-ProbSVM-InnerLoop .
337
</p>
</p>
338
<p>
<p>
339
We empirically selected \MATH ( i .e 40\% samples of the rank list were used ) since larger \MATH will increase the number of incorrect labels and smaller \MATH will cause over-fitting .
We empirically selected \MATH ( i .e 40\% samples of the rank list were used ) since a larger \MATH will increase the number of incorrect labels , and a smaller \MATH will cause over-fitting .
340
</p>
</p>
341
<p>
<p>
342
In addition , \MATH consists of \MATH samples that are selected randomly with replacement from \MATH .
In addition , \MATH consists of \MATH samples that are selected randomly with replacement from \MATH .
343
This sampling strategy is adopted from the bagging framework \CITE .
This sampling strategy is adopted from the bagging framework \CITE .
344
The same setting was used for \MATH .
The same setting was used for \MATH .
345
</p>
</p>
346
<p>
<p>
347
-\MATH : the maximum Kendall tau distance \MATH between two rank lists \MATH and \MATH .
-\MATH : the maximum Kendall tau distance \MATH between two rank lists \MATH and \MATH .
348
</p>
</p>
349
<p>
<p>
350
This value is used to determine when the inner loop and the outer loop are stopped .
This value is used to determine when the inner loop and the outer loop stop .
351
We set \MATH for balance between accuracy and processing time .
We set \MATH for balancing between accuracy and processing time .
352
Note that smaller \MATH requires more number of iterations making the system 's speed slower .
Note that a smaller \MATH requires more iterations , making the system 's speed slower .
353
</p>
</p>
354
<p>
<p>
355
-\MATH : the kernel type is used for SVM .
-\MATH : the kernel type is used for the SVM .
356
The default is linear kernel that is defined as : \MATH .
The default is a linear kernel that is defined as : \MATH .
357
</p>
</p>
358
<p>
<p>
359
We have tested other kernel types such as RBF or polynomial , the performance did not change so much .
We have tested other kernel types , such as RBF or polynomial , but the performance did not change much .
360
Therefore , we used the linear kernel for simplicity .
Therefore , we used the linear kernel for simplicity .
361
</p>
</p>
362
</subsection>
</subsection>
363
</section>
</section>
364
<section label= " Results " >
<section label= " Results " >
365
<subsection label= " Performance Comparision with Existing Approaches " >
<subsection label= " Performance Comparison with Existing Approaches " >
366
<p>
<p>
367
We performed a comparison between our proposed method with other existing approaches .
We performed a comparison between our proposed method with other ones .
368
Text Based Baseline ( TBL ) : Once faces corresponding with images whose captions contain the query name are returned , they are ranked by the time order .
Text Based Baseline ( TBL ) : Once faces corresponding with images whose captions contain the query name are returned , they are ranked in time order .
369
This is very naive method in which no prior knowledge between names and faces is used .
This is a rather naive method in which no prior knowledge between names and faces is used .
370
</p>
</p>
371
<p>
<p>
372
Distance-Based Outlier ( DBO ) : We adopted the idea of distance-based outliers detection for ranking \CITE .
Distance-Based Outlier ( DBO ) : We adopted the idea of distance-based outlier detection for ranking \CITE .
373
Given a threshold \MATH , for each point \MATH , we counted the number of points \MATH so that \MATH , where \MATH is the Euclidean distance between \MATH and \MATH in the feature space mentioned in section \REF .
Given a threshold \MATH , for each point \MATH , we counted the number of points \MATH so that \MATH , where \MATH is the Euclidean distance between \MATH and \MATH in the feature space mentioned in section \REF .
374
This number then was used as the score to rank faces .
This number was then used as the score to rank faces .
375
We selected a range of \MATH values for experiments : \MATH .}
We selected a range of \MATH values for experiments : \MATH .}
376
</p>
</p>
377
<p>
<p>
378
Densest Sub-Graph based Method ( DSG ) : We re-implemented the densest sub-graph based method \CITE for ranking .
Densest Sub-Graph based Method ( DSG ) : We re-implemented the densest sub-graph based method \CITE for ranking .
379
Once the densest subgraph was found after an edge elimination process , we counted the number of surviving edge of each node ( i .e face ) and used this number as the score for ranking .
Once the densest subgraph was found after an edge elimination process , we counted the number of surviving edges of each node ( i .e face ) and used this number as the ranking score .
380
To form the graph , the Euclidean distance \MATH was used to assign the weight for the edge linked between node $p$ and node \MATH .
To form the graph , the Euclidean distance \MATH was used to assign the weight for the edge linked between node $p$ and node \MATH .
381
DSG require a threshold \MATH to convert the weighted graph to the binary graph before searching for the densest subgraph .
DSG requires a threshold \MATH to convert the weighted graph to the binary graph before searching for the densest subgraph .
382
We selected a range of \MATH values that are the same as the values used in DBO : \MATH .
We selected a range of \MATH values that are the same as the values used in DBO : \MATH .
383
</p>
</p>
384
<p>
<p>
385
Local Density Score ( LDS ) : This is the first stage of our proposed method .
Local Density Score ( LDS ) : This is the first stage of our proposed method .
386
It requires the input value \MATH to compute the local density score .
It requires the input value \MATH to compute the local density score .
387
Since we do not know the number of returned faces from text based search engines , we used another input value \MATH defined as the fraction of neighbors and estimated \MATH by the formula : \MATH , where \MATH is the number of returned faces .
Since we do not know the number of returned faces from text-based search engines , we used another input value \MATH , defined as the fraction of neighbors , and estimated \MATH by the formula : \MATH , where \MATH is the number of returned faces .
388
We used a range of $fraction$ values for experiments : \MATH .
We used a range of $fraction$ values for experiments : \MATH .
389
In the case of large number of returned faces , we set \MATH to the maximum value of 200 : \MATH .
For a large number of returned faces , we set \MATH to the maximum value of 200 : \MATH .
390
</p>
</p>
391
<p>
<p>
392
Unsupervised Ensemble Learning Using Local Density Score ( UEL-LDS ) : This is a combination of ranking by local density scores and then the ranked list is used for training classifier [Singular or plural?]to boost the rank list .
Unsupervised Ensemble Learning Using Local Density Score ( UEL-LDS ) : This is a combination of ranking by local density scores , and the ranked list is used for training a classifier to boost the rank list .
393
</p>
</p>
394
<p>
<p>
395
Supervised Learning ( SVM-SUP ) : We randomly selected a portion \MATH of the data with annotations to train the classifier ; and then used this classifier to re-rank remaining faces .
Supervised Learning ( SVM-SUP ) : We randomly selected a portion \MATH of the data with annotations to train the classifier ; and then used this classifier to re-rank the remaining faces .
396
This process was repeated five times and the average performance was reported .
This process was repeated five times and the average performance was reported .
397
We used a range of portion \MATH values for experiments : \MATH .
We used a range of portion \MATH values for experiments : \MATH .
398
</p>
</p>
399
<p>
<p>
400
Figure \REF shows a performance comparison of these methods .
Figure \REF shows a performance comparison of these methods .
401
</p>
</p>
402
<p>
<p>
403
Our proposed methods ( LDS and UEL-LDS ) outperform other unsupervised methods such as TBL , DBO and DSG .
Our proposed methods ( LDS and UEL-LDS ) outperform other unsupervised methods such as TBL , DBO , and DSG .
404
Furthermore , the performance of methods DBO and DSG are sensitive to the distance threshold ; while the performance of our proposed method is less sensitive .
Furthermore , the DBO and DSG methods are sensitive to the distance threshold , while the performance of our proposed method is less sensitive .
405
It confirms that the similarity measure using shared nearest neighbors is relieable for estimation of the local density score .
It confirms that the similarity measure using shared nearest neighbors is reliable for estimation of the local density score .
406
The performance of UEL-LDS is slightly better than LDS since the training sets labeled automatically from the ranked list are noisy .
The performance of UEL-LDS is slightly better than LDS since the training sets labeled automatically from the ranked list are noisy .
407
However , UEL-LDS improves the performance significantly even when the performance of LDS is poor .
However , UEL-LDS improves significantly even when the performance of LDS is poor .
408
These performances are worse than that of SVM-SUP using a small number of labeled samples .
These performances are worse than that of SVM-SUP using a small number of labeled samples .
409
</p>
</p>
410
<p>
<p>
411
Figure \REF shows an examples of top 50 faces ranked by the methods TBL , DBO , DSG and LDS .
Figure \REF shows an examples of the top 50 faces ranked using the TBL , DBO , DSG , and LDS methods .
412
The performance of DBO is poor since a low threshold is used .
The performance of DBO is poor since a low threshold is used .
413
This makes irrelevant faces that are near duplicates ( row 2 and row 3 in Figure \REF( b ) ) ranked higher than relevant faces .
This ranks irrelevant faces that are near duplicates ( rows 2 and 3 in Figure \REF( b ) ) higher than relevant faces .
414
This explains the same situation with DSG .
This explains the same situation with DSG .
415
</p>
</p>
416
</subsection>
</subsection>
417
<subsection label= " Performance of Ensemble Classifiers " >
<subsection label= " Performance of Ensemble Classifiers " >
418
<p>
<p>
419
In Figure \REF , we show the performance of five single classifiers and that of five ensemble classifiers .
In Figure \REF , we show the performance of five single classifiers and that of five ensemble classifiers .
420
The ensemble classifier \MATH is formed by combination of single classifiers from \MATH to \MATH .
The ensemble classifier \MATH is formed by combining single classifiers from \MATH to \MATH .
421
It clearly indicates that the ensemble classifier is more stable that single weak classifiers .
It clearly indicates that the ensemble classifier is more stable than single weak classifiers . //You use both plural and singular forms of " classifier " here , so it is a bit confusing if you are talking about a single classifier or more than one . I suggest you use the same form throughout if applicable .]
422
</p>
</p>
423
</subsection>
</subsection>
424
<section label= " New Face Annotation " >
<section label= " New Face Annotation " >
425
<p>
<p>
426
We conducted another experiment to show the effectiveness of our approach in which learned models can be used to annotate new faces of other databases .
We conducted another experiment to show the effectiveness of our approach in which learned models are used to annotate new faces of other databases .
427
For each name in the list , we used it as the query to obtain top 500 images from Google Image Search Engine .
We used each name in the list as a query to obtain the top 500 images from the Google Image Search Engine ( GoogleSE ) .
428
Next , these images were processed as the steps described in section \REF : extracting faces , detecting eyes and doing normalization .
Next , these images were processed using the steps described in section \REF : extracting faces , detecting eyes , and doing normalization .
429
We projected these faces to the PCA subspace trained for that name and used the learned model to re-rank faces .
We projected these faces to the PCA subspace trained for that name and used the learned model to re-rank faces .
430
</p>
</p>
431
<p>
<p>
432
There were 4 ,103 faces ( including false positives - non-faces were detected as faces ) detected from 7 ,500 returned images .
There were 4 ,103 faces ( including false positives - non-faces detected as faces ) detected from 7 ,500 returned images .
433
We manually labeled these faces and there were 2 ,342 relevant faces .
We manually labeled these faces and there were 2 ,342 relevant faces .
434
On average , the accuracy of the Google Search Engine ( GoogleSE ) is 57 .08\% .
On average , the accuracy of the GoogleSE is 57 .08\% .
435
</p>
</p>
436
<p>
<p>
437
In Table \REF , we compare the performance of the methods .
In Table \REF , we compare the performance of the methods .
438
The performance of UEL-LDS was obtained by running the best system , which is shown as the peak of UEL-LDS curve in Figure \REF .
The performance of UEL-LDS was obtained by running the best system , which is shown as the peak of the UEL-LDS curve in Figure \REF .
439
The performances of SVM-SUP-05 and SVM-SUP-10 correspond to the supervised systems ( cf . section \REF ) that used \MATH of the data set respectively .
The performances of SVM-SUP-05 and SVM-SUP-10 correspond to the supervised systems ( cf . section \REF ) that used \MATH of the data set , respectively .
440
We evaluated the performance by calculating the precision at top 20 returned faces , which is popular for image search engines ; and recall and precision on all detected faces of the test set .
We evaluated the performance by calculating the precision of the top 20 returned faces , which is common for image search engines and recall and precision on all detected faces of the test set .
441
UEL-LDS achieved comparable performance to the supervised methods and outperformed the baseline GoogleSE .
UEL-LDS achieved comparable performance to the supervised methods and outperformed the baseline GoogleSE .
442
The precision at top 20 of SVM-SUP-05 is poorer than that of UEL-LDS is due to small number of training samples .
The precision of the top 20 of SVM-SUP-05 is poorer than that of UEL-LDS due to the small number of training samples .
443
</p>
</p>
444
<p>
<p>
445
Figure \REF shows top 20 faces ranked by these two methods .
Figure \REF shows top 20 faces ranked using these two methods .
446
<p>
<p>
447
</subsectiom>
</subsectiom>
448
</section>
</section>
449
<section label= " Discussion " >
<section label= " Discussion " >
450
<p>
<p>
451
Our approach works fairly well for well known people , where the main assumption that text-based search engines return a large fraction of relevant images is satisfied .
Our approach works fairly well for well known people , where the main assumption that text-based search engines return a large fraction of relevant images is satisfied .
452
Figure \REF shows an example where this assumption is broken .
Figure \REF shows an example where this assumption is broken .
453
Consequently , as shown in Figure \REF , the model learned by this set obtained poor performance in recognizing new faces returned by GoogleSE .
Consequently , as shown in Figure \REF , the model learned by this set performed poorly in recognizing new faces returned by GoogleSE .
454
Our approach solely relies on the above assumption , therefore it is not affected by the ranking of text-based search engines .
Our approach solely relies on the above assumption ; therefore , it is not affected by the ranking of text-based search engines .
455
</p>
</p>
456
<p>
<p>
457
The iteration of bagging SVM classifiers does not guarantee a significant improvement in performance .
The iteration of bagging SVM classifiers does not guarantee a significant improvement in performance .
458
Our future work is to study how to improve the quality of the training sets used in this iteration .
The aim of our future work is to study how to improve the quality of the training sets used in this iteration .
459
</p>
</p>
460
</section>
</section>
461
<section label= " Conclusion " >
<section label= " Conclusion " >
462
<p>
<p>
463
We presented a method for ranking faces retrieved using text-based correlation methods in searches for a specific person .
We presented a method for ranking faces retrieved using text-based correlation methods in searches for a specific person .
464
This method learns the visual consistency among the faces in a two-stage process .
This method learns the visual consistency among faces in a two-stage process .
465
In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely relevant or irrelevant faces .
In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely to be relevant or irrelevant faces , respectively .
466
In the second stage , a bagging framework is used to combine weak classifiers trained on subsets labeled from the ranked list into a strong classifier .
In the second stage , a bagging framework is used to combine weak classifiers trained on subsets labeled from the ranked list into a strong classifier .
467
This strong classifier is then applied to the original set to re-rank faces on the basis of the output probabilistic scores .
This strong classifier is then applied to the original set to re-rank faces on the basis of the output probabilistic scores .
468
Experiments on various face sets showed the effectiveness of this method .
Experiments on various face sets showed the effectiveness of this method .
469
Our approach is beneficial in the case multiple faces residing in the returned image as shown in Figure \REF .
Our approach is beneficial when there are several faces in a returned image , as shown in Figure \REF .
470
</p>
</p>
471
</section>
</section>
472
</document>
</document>
