While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .
In this paper , we investigate the effects of word segmentation methods on SMT , by comparing the evaluation results of the translation outputs , while varying word segmentation methods .
Additionally , meta-evaluations of evaluation metrics are also provided to investigate the validity of the metrics .
The experimental results confirmed that supervised morphological analyzers were competitive with , and performed considerably better than an unsupervised analyzer and a heuristic segmentation method .
However , a character-based segmentation achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on the corpus sizes and domains .
In conclusion , we discuss the problem of the comparability of evaluation metrics , and consider ways of improving word segmentation more than popular supervised morphological analyzers .
Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .
In order to process such languages , we need to tokenize each sentence .
This process is called word segmentation .
Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .
Thus , we investigate how Japanese word segmentation affects SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .
The word segmentation methods include both standard Japanese morphological analyzers and several heuristic methods .
In addition , we examine an unsupervised morphological analyzer , and its results .
We focus on the meta-evaluation of the current evaluation metrics and determine whether the metrics are consistent or not , when we vary word segmentation methods .
Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on the outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .
They acquired a 2 .3 score improvement in comparing the worst to best combinations .
Wang et al.
( 2010 ) suggested a new short unit word segmentation standard in Chinese , which defines a more frequent string subset as a word .
For instance , one word “全球化 globalization” was separated into two words “全球 global” and “化 -lization” .
By this standard , they obtained 1 .0 BLEU score improvement within Chinese-Japanese SMT .
However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .
In addition , comparing morphological analyzers is necessary , because different analyzers produce different outputs to SMT .
We therefore conduct several translation tasks between English and Japanese .
We measure the qualities of Japanese morphological analyzers and compare them with other word segmentation methods .
We also investigate consistencies of evaluation metrics by comparing the results .
This work aims at empirically comparing representative word segmentation methods in terms of the SMT quality .
The following experiments are designed in order to answer these questions :
- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affects the SMT evaluation metrics , depending on the corpus sizes and domains .
- Whether or not SMT evaluation metrics provide a consistent measure , while varying word segmentation methods .
We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .
As shown in Table 1 , the following word segmentation methods output delimiters ( “|” represents a delimiter ) for a given input character sequence .
The most popular method for Japanese word segmentation is to apply a morphological analyzer to obtain morpheme-based segmentation .
It is ; however , unclear as to which analyzer works better for the SMT task .
Therefore , we use four representative morphological analyzers and compare them :
- KyTea predicts word segmentation delimiters by pointwise prediction ( Neubig et al. , 2011 ) , using linear Support Vector Machine or logistic regression .
- MeCab regards word segmentation as a sequence labeling problem .
It uses Conditional Random Field for learning ( Kudo et al. , 2004 ) .
- JUMAN also regards word segmentation as a sequence labeling problem , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .
The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98% for news texts .
On the other hand , the unsupervised method , latticelm , achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news texts , while the method does not have any answers for word definitions .
Therefore , it is not possible to compare its result with the supervised results , even though it is fair to compare it from the SMT contribution point-of-view .
Furthermore , their policies concerning word segmentation definitions vary significantly .
While MeCab can change its definitions by external lexicons , and JUMAN has its own internal standard , KyTea is based on the short unit standard of the Balanced Corpus of Contemporary Written Japanese , which is considered to have one of the shortest definitions of Japanese words .
For example , if we are given a string , “見れば( if someone sees )” , MeCab separates it into two words , “見れ | ば” and JUMAN retains the same string , but KyTea outputs it as three words , “見 | れ | ば” where every character is a word .
For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .
In our experiments , we further investigate such morphological analysis accuracies and word definition problems .
Chang et al.
( 2008 ) suggested that word segmentation consistency and granularity can be important factors for SMT .
Therefore , we introduce two heuristic methods for Japanese word segmentation .
One method is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .
CAT places a word boundary when character categories change .
Character categories in Japanese include : Kanji ( Chinese character ) , Hiragana , Katakana , Latin alphabet , numeral digit , multi-byte alphabet , multi-byte digit , and other tokens .
The CHAR method considers every Unicode character as a word , as proposed by Xu et al.
( 2004 ) .
We use two news corpora : Reuters corpora ( REUTERS ) and Japanese-English News Article Alignment Data ( JENAAD ) ( Utiyama and Isahara , 2003 ) .
Another corpus we use in the experiments is a Wikipedia corpus : the Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .
From these corpora , we prepared three data sets , as explained below .
For REUTERS , we used all 56 ,282 sentences .
Then , we split the data into three parts : the first 1 ,000 as the test , the next 500 as the development , and the remaining 55 ,282 as the training data .
For this data , we combined the JENAAD and REUTERS news corpora to acquire one news corpus .
We used all 56 ,282 and 150 ,000 sentences , respectively .
For each corpus , we divided the sentences into the first 1 ,000 for testing , the next 500 for development , and the remaining for training .
In total , we gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively .
Since the WIKIPEDIA corpus is a multi-category XML dataset , we sorted them by the DOCID in ascending order , and by the document categories : LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .
Next , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .
Then , sentence pairs that include a character “|” in English or Japanese were removed , because it caused a problem with Moses .
Finally , we obtained 477 ,012 sentence pairs in total .
In order to adjust the balance of the domains , we sampled the data twice : First , we extracted the first line for every 477 lines .
Then , we merged the remaining 476 ,012 lines , and from this extract , we extracted the first line for every 952 lines .
Finally , we obtained 1 ,000 test , 500 development , and 475 ,512 training data .
We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .
Currently , the most popular way to evaluate SMT is to use word-based evaluation metrics , such as BLEU and RIBES .
However , these word-based evaluation metrics have a problem on independency of word segmentation evaluations .
If we do not have segmented reference and test data , we cannot evaluate the outputs by word-based evaluation metrics .
For example , for English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .
On the other hand , for Japanese-English translations , we must tokenize test data to evaluate the outputs .
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .
It is also possible to detokenize SMT outputs first , and then tokenize them by the shared word segmentation .
However , our preliminary experiments indicated that the results obtained with this method were not independent from word segmentation of the training data .
The best results were obtained when we used the same word segmentation as the training data .
Hence , if we keep our word-based evaluations , this problem remains .
In order to manage this issue , we used one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .
As this method evaluates the character-level information , outputs are not required to be segmented , and it is free from word segmentation variations .
We have conducted English and Japanese machine translation in both directions , following the steps below :
1 .Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .
2 .Run Japanese word segmentation methods and a normalization script , which was introduced by the NTCIR-9 PATMT task .
3 .Tokenize and lowercase English texts by Moses' tokenizer and lowercase scripts .
4 .Create language models from target languages' training data , with SRILM 1 .5 .12 .
5 .Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .
6 .Decode source test data with Moses ( 2010-08-13 ) .
7 .Compute evaluation scores of the outputs .
We used Enju 2 .4 .2 ( Miyao and Tsujii , 2005 ) and Head Finalization ( Isozaki et al. , 2010b ) to preprocess English data .
This method enabled more accurate translations within English-Japanese translations than with the conventional settings .
We have applied the following Head Finalization rules from ( Su-doh et al. , 2011 ) :
- Reverse each phrase 's word orders when the phrase does not end with a head .
- Exclude coordination from reversing
- Convert plural nouns to singular forms
- Remove articles “a” , “an” , and “the”
- Insert pseudo-particles _va0 , _va1 , and _va2 .
For the pseudo-particles , we use the following insertion rules ( arg1 and arg2 are swapped when the head verb 's voice is passive ) :
- Add _va0 after the arg1 entry of the sentence head verb
- Add _va1 after arg1 entries of other verbs
- Add _va2 after arg2 entries of all verbs
Table 2 and Table 3 show the English-Japanese and Japanese-English evaluation results .
The best scores in each evaluation metrics are highlighted for each data set .
All evaluation metrics have been used in both directions between English and Japanese , to measure the consistency and sufficiency of the metrics in the language pair .
In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .
Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost the same , while small changes have been introduced , due to statistical errors and the differences in the methods in how to treat space characters .
We found that the three supervised morphological analyzers : KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .
For instance , on REUTERS in Table 2 , BLEU scores ranged from 27 .88 to 29 .53 , while for latticelm , the score was 15 .28 and for CAT , the score was 22 .10 .
The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .
These two results were the worst , in all of the settings .
The results of CHAR were counterintuitive and are yet to be discussed .
The results were better than the results for the supervised morphological analyzers in BLEU .
It was almost competitive in RIBES and BLUE in Characters .
CHAR achieved the best score in BLEU on REUTERS ( 38 .42 ) , but the second-best was KyTea ( 29 .53 ) .
For BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .
For Japanese-English translations , the evaluation scores were generally lower than for English-Japanese translations .
This is because Japanese-English translations are conducted without Head-Finalization .
Again , the supervised morphological analyzers KyTea , MeCab , and JUMAN were competitive .
All supervised analyzers performed better than the unsupervised and the both heuristic methods .
Conversely , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT performed competitively with the supervised analyzers in RIBES .
latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .
In this case , CHAR was not competitive to the supervised analyzers in total .
The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .
We found that the results of the supervised morphological analyzers were better in both English-Japanese and Japanese-English experiments .
Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .
This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions , in most cases .
On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT performed much poorer than the supervised morphological analyzers .
The experiments demonstrated an unexpected result for CHAR .
It excelled with English-Japanese translations , but not with Japanese-English translations .
We consider the possible reasons for this result in the following list :
- The Head Finalization of English-Japanese lead better phrase alignments .
- Since CHAR treats a character as a word , the best combination of its phrase alignments were the best suited for the SMT decoding .
On the other hand , we observed the following issues from our error analysis :
- Uncommon named entities were almost wrongly translated .
( For example , チェコ Czech was produced instead of チェコスロバキア Czechoslovakia . )
- Long sentences were not translated as well as other word segmentation outputs .
The reasons for the CHAR results are yet to be analyzed in detail .
However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .
We are planning to conduct further investigations in the future .
The current evaluation metrics that we pursued in this paper were insufficient to discuss the relative advantages and disadvantages of word segmentation in detail , since the scores that were produced were inconsistent , as explained below :
- There were many contradictory figures among evaluation metrics .
There was a case that BLEU was high , while other metrics were low .
Moreover , there is also a case in which RIBES and BLEU in Characters were incompatible with each other .
For example , on WIKIPEDIA in Table 2 , while CHAR was the highest , and performed better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .
- If we compare every column in a row , there were tendencies that the best and the worst corpora were the same for every evaluation metrics .
In Table 2 , REUTERS was the best and WIKIPEDIA was the worst in terms of BLEU , but also JENAAD+REUTERS was the best and WIKIPEDIA was the worst in terms of RIBES .
- Even when we compare every row in a column , there were no tendencies .
For instance , in terms of BLEU in Characters , CHAR , JUMAN , and MeCab achieved the best scores in Table 3 .
This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
In summary , we found that the representative morphological analyzers were competitive and much better than both the unsupervised analyzer and one of our heuristic methods .
Nevertheless , a heuristic word segmentation method CHAR achieved relatively good word-based BLEU scores and competitive character-based BLEU results , compared to the supervised analyzers .
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , the data was insufficient for discussing the relative advantages and disadvantages of word segmentation , with accuracy .
We have also suggested that it is possible to implement more optimized word segmentation on SMT .
On Contribution of Syntactic Dependencies to Word Sense Disambiguation
Traditionally , many researchers have addressed word sense disambiguation ( WSD ) as an independent classification problem for each word .
However , the problem with these approaches , is that they disregard the interdependencies of word senses , and that it is limited in its applicability to the word senses for which training instances are served .**[<-This sentence is a bit confusing]
In this paper , we propose a supervised WSD model based on the syntactic dependencies of word senses .
In particular , we assume that strong dependencies between the sense of a syntactic head and those of its dependents exist .
We describe these dependencies on the tree-structured conditional random fields ( T-CRFs ) , and obtain the most appropriate assignment of senses optimized over the sentence .
Furthermore , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can also work for words that do not appear in the training data ; these combined features help relieve the data sparseness problem .
In experiments , we display the appropriateness of considering the sense dependencies , as well as the advantage of [having ? Using ?] the combination of fine- and coarse-grained tag sets .
The performance of our model is shown to be comparable to those of state-of-the-art WSD systems .
We also present an in-depth analysis of the effectiveness of the sense dependency features by using intuitive examples .
Word sense disambiguation ( WSD ) is one of the most fundamental problems in computational linguistics .
The task of WSD is to resolve the inherent polysemy of words by determining the appropriate sense( s ) for each polysemous word in a given text .
It is considered to be an intermediate , but necessary step for many NLP applications , including machine translation and information extraction , which[what does " which " refer to ? Machine translation ? Information extraction , or both ? Clarify] require the knowledge of word senses to perform better .
One major obstacle for large-scale and precise WSD is solving the data sparseness problem caused by the fine-grained nature of sense distinction .
In recent years in order to resolve this problem , several semi-supervised approaches have been explored .
Some researchers have addressed the scarcity of the training data directly , and have explored the methods to obtain more tagged instances , by co-training and self-training .
Other researchers have employed useful global information , such as the domain information extracted from unannotated corpora .
Although the use of global information has succeeded in dramatically increasing the performance of WSD , there is much room left to examine the effectiveness of local or syntactic information .
One such information yet to be explored , is the interdependency of word senses .
Although the use of local and syntactic information has been common in WSD , traditional approaches to WSD are based on the individual classification framework for each word ; each word 's sense is treated independently , regardless of any interdependencies or cooccurrences of word senses .
In turn , the resulting sense assignment may not be semantically consistent over the sentence .
To solve this problem is of great interest from both a practical and theoretical viewpoint .
In this thesis , we present a WSD model that naturally handles all content words in a sentence .
We focus on using the interdependency of word senses , so that we can directly address the issue of semantic ambiguity in a whole sentence that arose from the interaction of each word 's sense ambiguity . **[ <- this part is confusing .]
Specifically , we assume that are strong sense dependencies between a syntactic head , and its dependents in the dependency tree , rather than between neighboring words of a sentence .
We confirm the validity of this assumption , by showing the superiority of the tree-structured models over the linear-chain models .
Furthermore , we combine these sense dependency features with various coarse-grained sense tag sets .
This is to relieve the data sparseness problem caused by the explosion in the number of features , which is roughly squared by the combination of two word senses .
The combined features also enable our model to work , even for words that do not appear in the training data , which traditional individual classifiers cannot handle .
As a machine learning method , we adopt the tree-structured conditional random fields ( T-CRFs ) .
We solve WSD as a labeling problem to a sentence described as a dependency tree , where the vertices correspond to the words , and the edges correspond to the sense dependencies .
In this model , the intensities of the sense dependencies are described as the weights of edge features .
T-CRFs also enable us to incorporate various sense tag sets all together into a simple framework .
In our experiments , three interesting results are found : the interdependency of word senses contribute to the improvement of WSD models , the combined features with coarse-grained sense tags work effectively , and the tree-structured model outperforms the linear-chain model .
These results are confirmed on three data sets ( the SemCor corpus and the Senseval-2 and -3 English all-words task test sets ) , and on two sense inventories ( WordNet synsets and supersenses ) .
Our final model is shown to perform comparably to state-of-the-art WSD systems .
The rest of the paper is organized as follows : In Section 2 , we describe background topics related to WSD .
In Section 3 , we describe current problems of WSD , and related works .
In Section 4 , we describe our model with intuitive examples , and we describe the machine learning method that we use .
In Section 5 , 6 , and 7 , we present our experimental setup , the results , and an in-depth analysis on the contribution of the sense dependency features .
Finally , in Section 8 , we present our concluding remarks .
The WordNet is a broad-coverage machine-readable dictionary ( MRD ) for English , containing about 150 ,000 words .
WordNet also serves as an ontology of various kinds of meta data , relations among words and senses , and a well-organized hierarchical classification of word senses that are defined .
In this paper , we always refer to the WordNet version 2 .0 unless otherwise noted .
The statistical information of the WordNet 2 .0 is shown in Table 1 and 2 .
As shown in Figure 1 , in WordNet , nouns and verbs are organized into hierarchical structures with IS-A ( hypernym-hyponym ) relationships among words , .
Nouns have a far deeper structure than verbs do , while that the structure of verbs is transversely developed .
All nouns and verbs , with the exception of some top-level concepts , are classified into primitive groups called supersenses , which we will describe later .
Figure 1 shows the WordNet hierarchical structure for the first sense ( financial bank ) of a noun bank , where each line indicates a synset with the list of words headed by its supersense label ; an arrow denotes that the two synsets are in an IS-A relation .
The synset {group#1 , grouping#1} is a broad semantic category that governs the supersense group noun .group .
The lower synsets {social group#1} , {organization#1 , organisation#3} , and {institution#1 , establishment#2} are the more specific synsets , which in this paper we call the first , second , and third general synsets .
Note that since the organizations of adjectives and adverbs are very different from those of nouns and verbs , we use this hierarchical information for only nouns and verbs .
A supersense is a coarse-grained semantic category , with which each noun or verb synset in WordNet is associated .
Noun and verb synsets are associated with 26 and 15 categories , respectively .
The coarse-grained sets of sense labels are easily recognizable , and enable us to build a high-performance and robust tagger with small training data .
Hence , we can expect them to act as a good smoothing feature for WSD , which would make up for the problem of the sparseness of features , commonly associated with finer-grained senses .
The effectiveness of using supersenses for WSD has recently been shown by several researchers ( e.g. , , and ) .
The lists of supersenses are shown below .
- Noun supersense : act , animal , artifact , attribute , body , cognition , communication , event , feeling , food , group , location , motive , object , quantity , phenomenon , plant , possession , process , person , relation , shape , state , substance , time , Tops
- Verb supersense : body , change , cognition , communication , competition , consumption , contact , creation , emotion , perception , possession , social , stative , weather
Since data sparsity has been a significant issue in WSD , the sense frequency information is necessary in achieving a good performance .
In this section , we introduce two kinds of sense frequency information .
A sense ranking is the ranking of a sense of a word in the WordNet .
Since senses of a word are ordered according to frequency , the sense ranking acts as a useful feature that offers a preference for frequent senses .
It is also important as a back-off feature , which enables our model to output the first ( most frequent ) sense when no other features are active for that word .
The first sense classifier is known as a strong baseline in WSD , which can even be considered as a good alternative to WSD .
In our experiment , our first sense classifier achieved the accuracies 65 .3% for the Senseval-2 English all-words task data , and 63 .4% for the Senseval-3 English all-words task data .
Since the sense ranking in the WordNet is based on the word frequency in the SemCor , this baseline performs far better on the SemCor : 75 .9% for the brown1 section and 74 .3% for the brown2 section .
Alternatively , we can consider incorporating the first sense of each word as a feature .
Instead of uniformly predicting the distribution of sense frequencies according to their sense ranking , it can capture the conditional probability of each sense over the first sense .
When sufficient training data is available for every sense this method is considered to be a good feature that reflects the sense frequency information .
For such a reason , we use this first sense feature instead of the ranking feature , for the supersense-based evaluation .
For the unsupervised WSD , the use of sense dependencies has been a common method .
introduces an unsupervised graph-based algorithm , and shows a significant improvement over the sequence labeling model over the individual label assignment .
built a model based on various word semantic similarity measures , and graph centrality algorithms , which also used the graph structure that incorporates the word-sense dependencies .
Thus , the effectiveness of sense dependencies for unsupervised WSD has been shown by several researches .
On the contrary , the traditional approach to supervised WSD is to solve an independent classification problem for each word .
This approach has been developed along with research based on the lexical sample task in the Sensevals .
However , as described in Section 1 , this approach cannot handle the interdependencies among word senses , and may output a semantically inconsistent assignment of senses .
Recently , with the growing interest in the all-words task , a few supervised WSD systems have incorporated the sense dependencies .
SenseLearner and SuperSenseLearner incorporate sequential sense dependencies into the supervised WSD frameworks .
They no longer treat each word sense individually , assuming the sense dependencies between adjacent words .
also took a sequential tagging approach for the disambiguation of WordNet supersenses . [<-This sentence is a bit confusing]
The dependencies that they considered , however , are rather simple ones between the adjacent words , and between either WordNet synsets or supersenses .
Note additionally , that they do not mention the means or the quality of contribution in improving supervised WSD .
The exponential family model proposed by , captures the occurrences and co-occurrences of words and senses in a joint probability distribution .
Although they focused on the use of the co-occurrences of word senses rather than that of dependencies , they clarified the contribution of sense co-occurrences to the supervised WSD .
In this context , it is of interest to note whether the sense dependencies on a syntactic structure , rather than on a linear chain , works effectively or not .**[why ?]
To the extent of our knowledge , there exists no model that considers the interdependencies of word senses on a syntactic tree .
Furthermore , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not been explicitly examined thus far .
These questions are clarified by our research .
In Section 1 , we presented one of the most significant issues in WSD - the data sparsity problem .
This problem may even be magnified , when taking into consideration the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .
In order to relieve this problem , we use the hierarchical information in the WordNet , including the superordinate words and supersenses , as described in Section 2 .1 and 2 .2 .
The use of hierarchical information has been motivated by several different researches .
For example , a WSD system by , ranked second in the Senseval-3 , consists of two models : the first model applied to words seen in the training data , and the second model performs a generalized disambiguation process for words unseen in the data , by using the hierarchical information in the WordNet .
The fine granularity of the WordNet synsets is not just a major obstacle in achieving a high-performance WSD , but is sometimes too fine-grained even for a human to disambiguate .
This is reflected in the low inter-annotator agreement of sense tagging ( typically around 70% ) , which implies that WSD models would be unlikely to perform better than the accuracy achieved .
Also , this fine-grained nature is reported to be inappropriate for many NLP applications .
For example , reported that coarse-grained sense distinctions are sufficient for several NLP applications .
In particular , the use of the supersenses has recently been investigated by , and has received much attention in the WSD field .
In this case , the inter-annotator agreements have reached nearly90% .
For this reason , we use the WordNet supersenses , as well as the synsets as our sense inventory .
In Section 3 , we described two problems in the WSD field .
One problem is the independent classification of each word 's sense , regardless of the sense dependencies among words .
The other problem is the scarcity of the training data that arose from the fine granularity of the sense distinction .
We address these problems by combining two methods .
The first [method ?] is the use of the syntactic dependencies of word senses on a dependency tree .
In particular , we assume that there are strong dependencies of word senses between a head and its dependents in the dependency tree , rather than between neighboring words in the sentence .
Even though some models so far have considered the dependencies between adjacent words , no one has focused on the syntactic dependencies of word senses .
Thus , to the extent of our knowledge , our model is the first WSD model that incorporates the sense dependencies based on a syntactic tree .
The second [method ?] combines various coarse-grained sense tag sets with the WordNet synsets .
This enables our model to work for unseen words in the training data , and is expected to relieve the data sparseness problem .
In our experiment , these tag sets are used in two ways .
One way directly uses them as the sense inventory , instead of as a finer sense inventory .
In our supersense-based model , we use the supersenses as the sense inventory , and each word sense is disambiguated at the granularity level of this tag set .
This method serves us many more training instances for each coarser sense , while we can no longer distinguish finer senses .
The other way uses them**[<-define " them " ] in combination with finer sense tag sets .
In our synset-based model , three coarse-grained label sets are incorporated in combination with the fine-grained WordNet synsets .
Although sense disambiguation is still based on the finer senses , the coarser sense tags will help the discrimination of the finer senses , thereby serving generalized information for each fine-grained sense .
This approach has been taken in several hierarchical WSD methods , but has never been combined with the sense dependencies in a way that have used them .
The process of WSD is summarized below .
At the beginning , we parse target sentences with a dependency parser , and compact the outputted trees in order to capture informative dependencies among words , as described in Section 4 .3 .
Then , the WSD task is regarded as a labeling task on the tree structures .
By using T-CRFs , we can model this as the maximization of the probability of word sense trees , given the scores for vertices and edges .
In the training phase , all vertex features and edge features are extracted using the gold-standard senses , and the weight vectors are optimized over the training data .
Finally , in the testing phase , all possible combinations of senses are evaluated for each sentence , and the most probable sense assignment is selected by evaluating the equation 3 .
Conditional Random Fields ( CRFs ) are graph-based probabilistic discriminative models proposed by .
CRFs are state-of-the-art methods for sequence labeling problems in many NLP tasks .
CRFs construct a conditional model / MATH from a set of paired observations and label sequences .
The conditional probability of a label sequence / MATH conditioned on a data sequence / MATH is given by / MATH	 , where / MATH and / MATH are the feature vectors for an edge and a vertex , / MATH and / MATH are the weight vectors , / MATH and / MATH are the set of components of / MATH associated with an edge / MATH and a vertex / MATH , and / MATH is the partition function that constrains the sum of all the probabilities to be 1 .
Tree-structured CRFs ( T-CRFs ) are different from widely used linear-chain CRFs , in that the random variables are organized in a tree structure ( acyclic graph ) .
Hence , we can consider them relevant in modeling the syntactic dependencies of word senses , which cannot be represented by linear structures .
In this model , the optimal label assignment / MATH for an observation sequence / MATH is then calculated by / MATH , where / MATH denotes a vertex corresponding to a word , while / MATH denotes the vertex corresponding to its parent in the dependency tree .
If we interpret / MATH as the vertex associated with the preceding word in a sentence , it delineates into a linear-chain CRF .
Although T-CRFs are relatively new models , they have already been applied to several NLP tasks , such as semantic role labeling and semantic annotation , proving to be useful in modeling the semantic structure of a text .
Our model is the first application of T-CRFs to WSD .
In this section , we introduce the method of building graph structures on which CRFs are constructed .
First , we describe how to construct a tree used in the tree-structured model .
Let us consider the synset-level disambiguation of the following sentence .
( i ) - The man destroys confidence in banks .
In the beginning , we parse this sentence with Sagae and Tsujii 's dependency parser , which outputs parsed trees in the CoNLL-X dependency format .
The left-hand side of Figure 2 shows the parsed tree for Sentence ( i ) , where each child-parent edge denotes a directed dependency of words , and the labels on the edges denote the dependency types .
This dependency tree describes dependencies among all words in a sentence , including content words and function words .
However , some of these dependencies are not informative for our WSD task , because our task does not focus on the disambiguation function words .
For example , on the right-hand side of Figure 2 , the dependencies among confidence-in-bank are split into the two dependencies confidence-in and in-bank ; hence our model cannot capture the direct dependency between confidence and bank .
One way to resolve this problem is to use higher-order ( semi-Markov ) dependencies , but this may drastically increase the computational cost .
Thus , for the synset-based model , we convert the outputted dependency tree into a tree of content words , as exemplified on the right-hand side of Figure 2 .
In this process , the function words are removed from the tree , and their parent and child vertices are directly connected with the dependency labels of the uppermost edge in the original tree .
Then , on the right-hand side of Figure 2 , the dependency between confidence and bank is now described as a direct edge .
By the compaction of the trees , therefore , our model can capture more useful dependencies among word senses .
For the supersense-based model , we further convert the tree into a tree of nouns and verbs , because supersenses are defined for only these two parts of speech .
The inclusion of removed words and dependency relation labels are performed in the same manner as in the synset-based model ; the tree on the right hand side of Figure 2 in this case remains unchanged , because the sentence does not contain any adjectives nor adverbs .
For the linear-chain models , parsing a sentence is unnecessary .
At first , we connect every adjacent words with an edge , and build a linear chain .
Next , as the same reason as for the tree-structured case , we remove those words that we do not need to disambiguate from the graph , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .
Thus , the process of the tree compaction[ ? ?] is performed in the same manner , as described in Figure 3 .
In this section , let us present an intuitive illustration of how our model works .
Here , we focus on three words : destroy , confidence , and bank in Sentence ( I ) . For simplicity , we consider only two major senses for each word as described in Table 3 , so that the number of possible sense assignments is / MATH .
After an appropriate compaction of the dependency tree , relationships among destroy , confidence , and bank , are represented as direct connections .
Now , our objective is to determine the correct assignment of senses to these words , given the trained weight vector for features .
We conduct this by evaluating the scores for all possible assignment of senses .
Let us start from the dependency between confidence and bank .
The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) , but is unrelated to a natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .
Because bank does not have a " person " meaning , the weight of confidence( n )#2-bank( n )#1 is expected to be higher than for other possible sense bigrams .
A similar argument can be made for the dependency between destroy and confidence .
We can assume that destroy( v )#1 is usually associated with real objects , whereas destroy( v )#2 can take either a real entity or an abstract thing as its direct object .
Given confidence does not have an " object " meaning , the weights of destroy( v )#2-confidence( n )#1 and destroy( v )#2-confidence( n )#2 would be the largest [largest what ?] among others .
Finally , given all scores for these sense dependencies , we can evaluate the overall score for the sentence , and see / MATHdestroy( v )#2 , confidence( n )#2 , bank( n )#1 / MATH is the most probable assignment of senses .
Practically , specific bigrams of synsets such as confidence( n )#2-bank( n )#1 and destroy( v )#2-confidence( n )#2 may not appear in the training data .
In this case , sense bigrams combined with coarser sense labels work effectively .
For example , if there are synset bigrams such as destroy( v )#2-affection( n )#1 in the training data , the model can still perform the disambiguation process properly by considering a generalized synset-supersense bigram destroy( v )#2-noun .feeling .
The detailed description of sense bigrams are provided in Section 4 .7 .
Using the information in the WordNet , we make use of four sense labels for each word : a synset / MATH , two general synsets / MATH and / MATH , and a supersense / MATH , which we introduced in Section 2 .
These labels represent word senses at various levels , and are to be combined with the vertex and edge features .
We hereafter distinguish each sense label by putting one of the prefixes WS , G1 , G2 , and SS , as in WS :bank#1 and SS :noun .group .
The examples of these sense labels are shown in Table 4 .
In our model , we combine the synset and supersense labels with the vertex features , and all four sense labels with the edge features .
We denote the set of sense labels for vertex features by / MATH , and the one for edge features by / MATH .
Each of these sense labels is combined with the contextual information in the vertex features , whereas all possible combinations of two sense labels comprise the edge features .
We implement as vertex features a set of typical contextual features widely used in many supervised WSD models .
Most of these features are those used by with the exception of the syntactic features .
In order to see the efficiency of sense dependency features , we include as vertex features the word forms , lemmas , and parts of speech of both the parent and the child words in the dependency tree .
These features provide the syntactic information of the parent and child words , but are not semantically disambiguated .
Therefore , if the sense bigram features work effectively over these features , it clearly shows that there exist instances that cannot be disambiguated without considering the interdependency of word senses .
The list of vertex features also includes the information of both the preceding and following words , which in the linear-chain model plays the same role as the parent and child information in the tree-structured model .
Below is the list of contextual information used for the vertex features in the synset-based model .
We refer to these features as / MATH .
- Word form ( WF ) : word form as it appears in a text .
- Global context ( GC ) : bag-of-words within a 60-word window .
- Local PoS ( LP ) : / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , and / MATH , where / MATH in / MATH denotes the relative position to the target word .
- Local context ( LC ) : / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , / MATH , and / MATH , where / MATH denotes the word at the relative position / MATH , and / MATH the n-gram from the relative position / MATH to / MATH .
- Syntactic context ( SC ) : word forms , lemmas , and parts of speech of the parent and child words .
Using this contextual information , and the set of vertex labels / MATH , we construct a set of features on a vertex / MATH by / MATH .
Additionally , we include the sense ranking feature ( see Section 2 .3 for detail ) .
Note that this feature is not combined with any sense label nor with any contextual information .
For the supersense-based model , we use vertex features based on , which include some features from the named entity recognition literature , including the word shape features , along with the standard feature set for WSD .
As the sense frequency information , we use the first sense feature .
Unlike in the synset-based model , we do not incorporate the syntactic information of the parent and child words , since it has not been reported to improve the performance .
We design a set of edge features that represents the inter-word sense dependencies .
For each edge , we define the sense bigram features / MATH .
Moreover , in addition to these simple bigrams , we define two kinds of combined bigrams : the sense bigrams with dependency relation labels ( e.g. WS :confidence#2-( NMOD )-WS :bank#1 ) , and the sense bigrams with removed words in between ( e.g. WS :confidence#2-in-WS :bank#1 ) .
Consequently , the number of features for each edge is / MATH .
In this section , we introduce corpora that we have used for the evaluation .
SemCor is a corpus in which all content words are annotated with the WordNet synsets , and consists of balanced 352 files from the Brown Corpus .
It is divided into three parts : brown1 , brown2 , and brownv sections .
In brown1 and brown2 , all content words ( nouns , verbs , adjectives , and adverbs ) are semantically annotated , while in brownv only verbs are annotated .
Also , we use two data sets from the Senseval ( International Workshop on Evaluating Word Sense Disambiguation Systems ) exercises : the Senseval-2 English all-words task test set , consisting of three articles from the Wall Street Journal , and the Senseval-3 English all-words task test set , consisting of two articles from the Wall Street Journal and a fiction excerpt from the unannotated portion of the Brown corpus .
As the data sets for evaluation , we use the brown1 and brown2 sections ( denoted as SEM ) of SemCor , and the Senseval-2 and -3 all-words task test sets ( denoted as SE2 and SE3 , respectively ) .
We use the converted versions annotated with WordNet 2 .0 synsets .
These data sets are different from the originals because multi-word expressions are already segmented .
However , our model cannot output any answers to multi-word expressions that have no directly corresponding WordNet synsets , because we treat expression as one unit in the process of WSD .
For example , the multi-word expression tear-filled is treated as one instance , but are untagged with any WordNet synsets in the converted corpus , while in the original corpus it[define " it " ] is tagged with two WordNet synsets for tear and filled .
For this reason , we exclude such instances beforehand , and evaluate our models focused on expressions that have corresponding synsets in the WordNet .
The resulting statistics of the data sets are shown in Table 5 .
The evaluation of our model is performed by splitting these corpora into training , development , and test sets .
At first , all files in SEM are sorted according to their file names and distributed into five data sets in order ( denoted as SEM-A , SEM-B , SEM-C , SEM-D , and SEM-E ) , so that each set has almost the same distribution of domains .
Furthermore , each of these five data sets is again split into two sets : SEM-A1 , SEM-A2 , / MATH , SEM-E1 , and SEM-E2 , also according to the order of file names .
Our evaluation is based on a 5-fold cross validation scheme .
In the training phase , four sets ( e.g. SEM-A , SEM-B , SEM-C , SEM-D ) in the SEM are used for training .
Next , for the evaluation on SemCor , one half of the rest ( e.g. SEM-E1 ) is used for development and the other half ( e.g. SEM-E2 ) is used for evaluation .
For the evaluation on the Senseval data sets , all instances of the rest ( e.g. SEM-E ) are used for development , and one of the Senseval data sets ( SE2 or SE3 ) is used for evaluation .
Lastly , for the comparison with state-of-the-art models , our model is trained on the whole set of SEM , and SE2 and SE3 are used for development and evaluation respectively .
All sentences are parsed by the Sagae and Tsujii 's dependency parser , and the T-CRF model is trained by using Amis .
During the development phase , we tune the Gaussian parameter / MATH for the / MATH regularization term .
As the evaluation measure , we use the standard recall measure , which is equivalent to the precision as we output answers to all instances . **[This section is a bit monotonous]
The synset-based evaluation is performed based on the WordNet synsets .
We evaluate the outputs of our system for all instances that are semantically tagged in the data sets .
Each target word is either a noun , verb , adjective , or adverb .
For the supersense-based evaluation , we follow most of the experimental setup in .
As noted , in the WordNet , the labeling of supersensesis semantically inconsistent , and top level synsets are tagged as the supersense noun .Tops[ ? ?] rather than the specific supersense they govern .
For example , nouns such as peach and plum are tagged as noun .plant but their hypernym plant itself belongs to noun .Tops .
For this reason , we adopted the modification of noun supersenses in the same way as , substituting noun .Tops labels with more specific supersense labels when possible , and left some general nouns with noun .TopsoteNouns , which are left with noun .Tops are : entity , thing , anything , something , nothing , object , living thing , organism , benthos , heterotroph , life , and biont . .
The evaluation is based on these modified labels .
We ignore the adjective and adverb instances in the evaluation .**[This section is a bit confusing . Maybe break up the longer sentences to clarify]
Table 6 is the list of models that we use for the evaluation , where FS and SR correspond to the first sense and sense ranking features respectively , and non-dependency denotes models that do not incorporate sense dependency features ( i.e.
only the vertex features ) .
In this section , we focus on the contribution of the sense dependencies .
Table 7 shows the comparisons between the tree-structured models with sense dependencies ( dependency models ) and the models without sense dependencies ( non-dependency models ) .
Each figure displays the mean recall ( equivalent to the precisions ) averaged over the five trials of the cross validation , the " Diff . " rows show the differences between the dependency models and the non-dependency models , and / MATH and / MATH denote the statistical significance of / MATH and / MATH respectively .
From Table 7 , it can be seen that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .
These improvements seem insignificant in figures ; however , considering that for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model by only 0 .37% on SEM , the further improvement of 0 .21% is substantial , because it indicates that our dependency model could handle 57% more instances over the first sense baseline .
Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed worse than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness [of ...] regardless of the existence of the sense frequency information .
These results suggest that for the synset-based model , in which most synsets do not have enough instances in the training data , the combination with sense-frequency information is necessary in order to avoid the data sparseness problem .
Similarly , Table 8 shows the comparisons between the linear-chain dependency models and the non-dependency models .
In the supersense-based evaluation , although the differences are slightly smaller than in the tree-structured models , we confirmed that the sense dependencies with the first sense features work effectively , with the overall improvements of 0 .29% , 0 .20% , and 0 .30% for the three data sets .
However , without the first sense features , no statistically significant improvement nor deterioration is observed .
In the synset-based evaluation , the overall trend is almost same as in the tree-structured case .
Nonetheless , by the incorporation of the sense dependencies , the improvements with the sense ranking features was even less , and the deteriorations without them[define " them " ] were even more than in the tree-structured case .
These results seem to suggest that the sense dependencies on the tree structures are more robust than those on the linear chains .
In this section , let us focus on the difference between the tree-structured models and the linear-chain models .
In the results shown in Table 9 , although some of the differences are marginal , we can see that the tree-structured models outperformed the linear-chain models , focusing on the statistically significant differences .**[<-This is a confusing sentence]
Thus , although both the dependency trees and the linear chains capture useful dependencies of word senses , the dependencies on the tree structures capture more important information .
Table 10 shows the contributions of the coarse-grained labels .
Whereas Tree-WS-SR and Tree-WS use all four sense labels for the edge features ( / MATH ) , Tree-WS-SR' and Tree-WS' only use the synset labels ( / MATH ) . Thus , we can see the contribution of the coarse-grained sense labels .
Although the improvements are marginal , we can see that the coarse-grained sense labels consistently did improve the performance on all the data sets , relieving the data sparseness problem .
Since synset-based models can directly be used as supersense taggers by a simple conversion of senses , we compared the performance of the synset-based model with that of the supersense-based model .
Interestingly , when evaluated at the supersense level , the synset-based models considerably outperformed the supersense-based models , with an overall improvements of 0 .69% with the sense frequency information and 1 .41% without it , as shown in Table fcomp-ws-ss-tree .
Thus , even though the granularity of the supersenses is sufficient for many NLP tasks , they are too coarse-grained to capture enough information for WSD models ; therefore , for the supersense-based disambiguation , we can improve the performance by considering finer-grained senses .
Table 12 shows the comparison of our model with the state-of-the-art WSD systems .
The evaluation here is performed with the Senseval official scorer .
Our model Tree-WS-SR outperformed the two best systems in the Senseval-3 ( Gambl and SenseLearner ) , but lagged behind PNNL by 1 .6% .
However , taking into consideration that all systems in Table 12 except for Simil-Prime utilize other sense-annotated corpora in addition to SemCor , such as the Senseval data sets or example sentences in the WordNet , and that our model cannot handle multi-word expressions that do not exist in the WordNet as noted in Section 5 .1 , we can conclude that the performance of our T-CRF model is comparable to that of state-of-the-art WSD systems . **[This is a long sentence- shorten .]
Table 13 shows the list of the 15 largest-weighted sense dependency features in the tree-structured , synset-based model ( Tree-WS ) .
The list includes features associated with verb-noun relations ( e.g. SS :verb .consumption-SS :noun .food ) and noun-noun relations ( e.g. SS :noun .communication-SS :noun .communication ) , which we will describe in detail with several examples .
Hereinafter , / MATH denotes / MATH in Equation 3 , and / MATH denotes the exponential of / MATH .
We call a feature either with a positive lambda or with an alpha larger than 1 as an excitatory feature , and those features with either with a negative lambda or an alpha smaller than 1 as an inhibitory feature .
Also , Table 14 shows the 15 largest-weighted sense dependency features in the linear-chain , synset-based model .
When compared to the outputs of the tree-structured model , we can see that the linear-chain model captures more successive noun-noun dependencies , while the tree-structured model captures more adjective-noun and verb-object dependencies .
Thus , although the difference of the recalls is small , we can assume that the sense dependency features in the tree-structured model , and those in the linear-chain model have different contributions to the results .
The simultaneous use of both is of an interest from practical and semantical perspectives ; however , since it makes our model no longer a tree , the implementation is not straightforward .
Hence , this is left as one of our future works .
In this section , we present an instance-based analyses based on the first 100 instances for which the answer of the dependency model Tree-WS-SR differs from that of the non-dependency model NoDep-WS-SR in the first trial on SemCor .
We extracted only the largest-weighted edge feature for each instance , assuming that this feature had the largest contribution to the result .
These instances consist of 54 positive instances , for which Tree-WS-SR output the correct answer while NoDep-WS-SR did not , and 46 negative instances , for which Tree-WS-SR did not output the correct answer while NoDep-WS-SR did .
Table 15 and 16 show the count of each edge type for these instances .
For both positive and negative instances , the verb-noun dependencies are the dominant dependencies , corresponding to 48% of all the instances .
One noteworthy point is that more number of noun-noun dependencies are found in the positive instances than in the negative instances , further suggesting that noun-noun dependencies are particularly likely to capture useful dependencies and contribute to positive instances .
First of all , let us present two instances in which the verb-noun dependencies worked effectively .
The first sentence is :
From this earth , then , while it was still virgin God took dust and fashioned the man , the beginning of humanity .
Surprisingly , the verb take has as many as 42 senses in the WordNet .
But fortunately , the first six senses belong to different supersenses , and our dependency model succeeded in outputting the correct sense take#4 ( SS :verb .contact , take physically ) by making use of the strong dependency SS :verb .contact-SS :noun .substance ( / MATH ) , given dust#1 belongs to noun .substance .
The second instance is also a positive instance from the SEM-A data set .
For a serious young man who plays golf with a serious intensity , Palmer has such an inherent sense of humor that it relieves the strain and keeps his nerves from jangling like banjo strings .
Here , has is an ambiguous verb that has 19 senses in the WordNet .
The correct sense here is have( v )#2 ( SS :verb .stative , have as a feature ) .
Given sense of humor#1 belongs to the supersense noun .attribute , the correct sense was output by the strong verb-object dependency G1 :have( v )#2-( OBJ )-SS :noun .attribute ( / MATH ) .
While this verb-object dependency had a large excitatory weight , the corresponding verb-subject dependency had an inhibitory weight ( G1 :have( v )#2-( SBJ )-SS :noun .attribute ( / MATH ) ) , which indicates that the dependency relationlabel also contributed to the result .
Note also that this long dependency cannot be described by linear-chain models .
Next , let us show a typical negative example , where a verb-subject dependency worked inappropriately .
The repeated efforts in Christian history to describe death as altogether the consequence of human sin show that these two aspects of death cannot be separated .
The correct sense for show here is show#2 ( verb .cognition , establish the validity ) , but the model output show#3 ( verb .communication , prove evidence for ) affected by the long dependency WS :testify( v )#2-( SBJ )-SS :noun .act ( / MATH ) between efforts and show .
This subject information seems to be inadequate for the disambiguation of show .
Next we focus on the noun-noun dependencies .
The first example is a negative instance .
Philadelphia permitted him to seek a better connection after he had refused to reconsider his decision to end his career as a player .
The noun career has two meanings : the particular occupation for which you are trained ( career#1 ) and the general progression of your working or professional life ( career#2 ) .
From the phrase career as a player , we can assume that the correct sense of career can be either of two senses , with the possibility that there is a preference for career#2 , as captured by the largest-weighted dependency WS :career%1%2-( NMOD )-SS :noun .person ( / MATH ) between career and player .
Although there was originally the preference for the correct sense career#1 by the sense frequency features , the noun-noun dependency thus contributed to the wrong answer career#2 .
The determining clue for this instance seems to be the verb-object dependency end-career , which was not captured by our model .
Among the ten positive instances of the noun-noun dependencies , four instances were contributed by the noun-of-noun dependencies .
Since dependencies of this type were not observed in the negative instances , they seem to particularly contribute to the positive instances .
Let us consider the following example .
The embarrassment of these theories over the naturalness of death is an illustration of the thesis that death cannot be only a punishment , for some termination seems necessary in a life that is lived within the natural order of time and change .
Although the correct sense time#5 ( noun .Tops , the continuum of experience in which events pass from the future through the present to the past ) is not a frequent sense , our model correctly output the correct sense by using the dependency SS :noun .object-of-WS :time%1%5 ( / MATH ) , given natural order#1 belongs to the supersense noun .object .
Through our result , we observed that the noun-noun dependencies in coordination relations work remarkably well .
In the following sentence , three words nails , levels , and T squares are in a coordination relation .
He also bought a huge square of pegboard for hanging up his tools , and lumber for his workbench , sandpaper and glue and assorted nails , levels and T squares and plumb lines and several gadgets that he had no idea how touse or what they were for .
Here , the correct sense for nail is nail#2 ( noun .artifact , a thin pointed piece of metal ) , and that for level is level#5 ( noun .artifact , indicator of the horizontal ) .
The relatively low frequency of these senses prevent our model from outputting the correct senses in an ordinal way .
However , the dependency model could capture the fact that two words in a coordination relation are quite likely to belong to the same semantic group ( SS :noun .artifact-( COORD )-SS :noun .artifact ( / MATH ) ) , and hence succeeded in the correct disambiguation of these three words .
More generally , we have observed that the coordination features for an edge that connects the same supersense all have positive weights .
In this paper , we proposed a novel approach for the all-words WSD , focusing on the use of syntactic dependencies of word senses , and investigated the contribution of these dependencies to WSD .
Our proposals were twofold : to apply tree-structured CRFs to the dependency trees , and to use the combined bigrams of fine- and coarse-grained senses as edge features .
In our experiments , the sense dependency features were shown to work effectively for WSD , with a 0 .29% , 0 .64% , and 0 .30% improvement of recalls for SemCor , Senseval-2 , and Senseval-3 data sets , respectively .
Despite the small improvements in overall figures , these improvements indeed correspond to 25%-57% improvements over the first sense baseline .
The dependency tree structures were shown to be appropriate in modeling the dependencies of word senses , because the results of the tree-structured models outperformed the [results of ?] linear-chain models .
In the analysis section , we presented an in-depth analysis of the observed instances , and observed that the noun-noun dependencies particularly contribute to the positive instances .
In addition , the combination of coarse-grained tag sets with the sense dependency features were proved to be effective .
However , our experiments showed that even when combined with the coarse-grained tag sets , the sense dependency features do not improve the performance , unless combined with proper sense frequency information . This is due to the data sparseness problem .
The supersense-based WSD models , on the contrary , exhibited the robustness [of ...] regardless of the existence of the sense frequency information , while they are defeated by the synset-based models in recalls .
These results show the importance of fine-grained and coarse-grained sense information , and show that the combination of both enables us to build a more precise and robust WSD system .
The performance of our tree-structured model was comparable to that of the state-of-the-art WSD systems .
Although our model was based on a simple framework , and was trained only on the SemCor corpus , the results that we gained were promising . They suggested that our model still has a great potential for improvement .
Our next interest is to combine our framework with the recently-developed semi-supervised frameworks .
The combination of the local and syntactic dependencies with the global information is expected to further the WSD research .
Exploiting Motion Patterns for Action Recognition with Depth Sequences
Dense trajectory-based approaches have been used to recognize actions in 2D video because they can capture most discriminative motion patterns . 
However , there are not many studies related to exploiting discriminative motion patterns in depth video . 
In this study , we extend the dense trajectory-based approach to depth video and show its effectiveness at recognizing actions . 
In particular , dense trajectories are extracted from depth video . 
These 2D videos are formed from views which capture the discriminative motion patterns similar to observing actions from different directions . 
We evaluate this approach to action recognition on the benchmark MSR Action 3D , MSR Gesture 3D and 3D Action Pairs datasets . 
The results of the evaluation indicate that our approach is effective at recognizing actions in depth video and outperforms other state-of-the-art approaches .
The recognition of actions depicted in videos is an active topic of research in the computer vision field </CITE> , and it has a diverse range of applications in areas like surveillance , video retrieval , human-computer interaction , and smart environments . 
As a result of the diversity and complexity of actions , as well as the complicated nature of most environments  ( e.g. , background clutter and illumination variation ) , action recognition is still a challenging problem . 
Recent approaches can be divided into three major categories : silhouette-based </CITE> , salient point-based </CITE> and trajectory-based </CITE> . 
All of these approaches basically try to capture motion information appearing in videos , since it is crucial information for presenting actions . 
Recent studies </CITE> have shown that exploiting discriminative motion patterns is a successful means of action recognition .
Most of the existing studies deal with video sequences captured by traditional 2D cameras .
Although , there have been many improvements to the motion pattern-based approach for 2D video </CITE> , the above - mentioned challenges  ( e.g. background clutter and illumination variation ) are still difficult to meet . 
Meanwhile , the advent of RGB-D cameras , e.g. , the Kinect camera , has made it easier to capture depth maps together with RGB images in real time . 
The depth maps enrich the information available as cues , such as the shape of body and its motions . 
In addition , the depth information in these maps is less sensitive to the problems affecting RGB information . 
Because of these advantages , recent research has concentrated on exploiting depth maps for action recognition </CITE> . 
However , to the best of our knowledge , none have succeeded in developing a discriminative motion pattern-based approach , the state-of-the-art for 2D video , for depth video . 
In this paper , we investigate this approach for depth sequences .
The key idea of the motion pattern-based approach is to capture discriminative trajectories in video .
Therefore , to effectively exploit this approach on depth video , it is necessary to extract trajectories from depth video . 
To do that , a straightforward method is to consider the depth value as an intensity value , extract trajectories from the 2D video , and apply standard motion analysis techniques for 2D data .
Unfortunately , this method will expose the inherent limitations of the 2D trajectory-based approaches and will result in confused motion patterns that cannot be distinguished by observing 2D videos from one view . 
For example , a forward punch and hammer may be confused if we view them from the front because they contain ``lift arm up'' and ``stretch out'' movements that are indistinguishable when viewed from the front . 
However , if we properly use the depth information in the motion pattern analysis , it is possible to extract discriminative motion patterns which are inaccessible from one fixed view but discernible from different views .
To deal with such cases , we try to get more information on such actions from various directions .
Information from different views can provide clearer cues to discriminate such actions . 
To collect such information from depth video , we propose a method that virtually projects depth maps onto multiple view images , as shown in Figure </fig> . 
2D videos can be easily obtained from the projections . 
Motion features are then calculated to generate corresponding projection representations . 
Finally , a depth video representation is formed by fusing the projection representations .
In our experiments , we use a dense trajectory-based approach </CITE> that exploits discriminative motion patterns; this is the state-of-the-art approach for recognizing actions depicted in 2D videos . 
To evaluate the effectiveness of the proposed method , we conducted experiments on the MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
The results show that our method outperforms other state-of-the-art methods at recognizing actions using depth data . 
Our key contributions are as follows    : (1) we propose an effective method to exploit trajectories in depth video , and (2) we perform comprehensive experiments on a challenging benchmark dataset and indicate that our method is the best of the state-of-the-art depth-based methods .
After a brief review of the related work in Section , our action recognition framework is presented in Section . 
The proposed method is described in Section . 
Section describes the experimental settings and results . 
We discuss the results in section . 
A summary of our work is given in Section .
There are three popular approaches to action recognition in 2D video    : silhouette-based , salient point-based and trajectory-based . 
The silhouette-based approach , as described in </CITE> , is powerful since it encodes a great deal of information in a sequence of images . 
However , it is sensitive to pose changes , noise , and occlusions . 
In addition , it depends on the accuracy of the localization , background subtraction , and tracking when extracting the region of interest . 
Another approach , based on salient points , generates a compact video representation and can deal with background clutter , occlusions , and scale changes . 
This approach has been shown to be effective in several studies </CITE> . 
However , in the case of recognizing complicated motions , the salient point-based approach has to deal with several challenges due to the lack of relationships among the salient points . 
Recent studies </CITE> have used the trajectory-based approach to capture motion patterns in video . 
Although motion patterns are very complicated , structured motion can be easily perceived by humans , as has been shown by Johansson </CITE> .
The most recent methods of exploiting depth information can be categorized into two major types . 
The first one is to adapt 2D techniques to depth data . 
The second one is to devise 3D techniques for directly exploiting depth data .
Regarding the first direction , X.Yang et al. </CITE> propose Depth Motion Maps ( DMMs ) to capture global activities in depth sequences . 
DMMs are generated by stacking the motion energy of depth maps projected on three orthogonal Cartesian planes . 
A Histogram of Oriented Gradients  ( HOG ) </CITE> is computed from the DMMs to represent an action video . 
The approach can accumulate more silhouette information from the projections . 
However , silhouette extraction is not a trivial task due to problems such as occlusion and poor data quality . 
Another approach , proposed by L. Xia and J.K. Aggarwal </CITE> , is to use filtering to extract spatio-temporal interest points from depth videos  ( DSTIPs ) . 
It is an extension of the work of Dollar et al. </CITE> to depth data . 
First , 2D and 1D filters ( e.g. Gaussian and Gabor filters ) are respectively applied to the spatial dimensions and temporal dimension of the depth video . 
A correction function is used to suppress points that are depth noise . 
The points with the largest responses resulting from this filtering are selected as the DSTIPs for each video . 
In addition , a depth cuboid similarity feature ( DCSF ) is used to describe a 3D cuboid around the DSTIPs with supporting size to be adaptable to the depth . 
That study demonstrated the effectiveness of using 2D techniques on depth data . 
Nevertheless , the authors did not overcome the inherent limitations of the 2D techniques . 
That is , this approach still faces the challenges mentioned in the introduction .
Regarding the second direction , </CITE> uses a bag of 3D points to characterize a set of salient postures .
The 3D points are extracted from the contours of planar projections of the 3D depth map . 
About 1% of the 3D points are sampled to calculate a feature . 
Unlike </CITE> , the methods described in </CITE> use occupancy patterns to represent features in action videos .
A.W. Vieira et al. </CITE> proposed a new feature descriptor , called Space-Time Occupancy Patterns ( STOP ) . 
This descriptor is formed from the sparse cells of a 4D space-time grid dividing up a sequence of depth maps .
The points inside the sparse cells are typically on the silhouette or on the moving parts of an object . 
J. Wang et al. </CITE> created semi-local features , called Random Occupancy Pattern ( ROP ) features , from randomly sampled 4D sub-volumes of different sizes and at different locations . 
The random sampling is performed according to a weighted scheme in order to effectively explore the large dense sampling space . 
The authors also used sparse coding to robustly encode these features . 
J. Wang et al. </CITE> designed features , named Local Occupancy Patterns ( LOPs ) , to describe the local ``depth appearance'' of each joint of the body . 
LOP features are computed on the basis of a 3D point cloud around a particular joint . 
Moreover , they concatenate the LOP features with skeleton information-based features and apply a Short Fourier Transform to obtain Fourier Temporal Pyramid features at each joint . 
The Fourier features are utilized in a novel actionlet ensemble model to represent each action in the video .
Recently , Oreifej and Liu </CITE> presented a new descriptor for depth maps , named Histogram of Oriented 4D Surface Normals ( HON4D ) . 
To construct the HON4D ,  4D normal vectors are first computed from the depth sequence . 
Next , these 4D normal vectors are distributed into spatio-temporal cells . 
To quantize the 4D normal vectors , the 4D space is quantized by using the vertices of a regular polychoron . 
The quantization is refined by additional projectors to make the 4D normal vectors in each cell denser and more discriminative . 
Afterwards , the HON4D features in the cells are concatenated to make depth video depicting actions . 
A number of studies </CITE> have shown the effectiveness of approaches that directly exploit 3D data for human action recognition . 
However , constraints , such as segmentation of the human body in </CITE> , or the location of the body in </CITE> , have somewhat limited their applicability .
Inspired by the results of Shotton et al. </CITE> and L. Xia et al. </CITE> , some studies </CITE> have developed skeleton-based methods from sequences of depth maps .
 </CITE> proposed an EigenJoints-based action recognition system using a Naive-Bayes-Nearest-Neighbor classifier . 
The system is able to capture the characteristics of posture , motion and offset of the frames . 
In addition , non-quantization of descriptors and distance computations have proved to be effective for action recognition . 
J. Luo et al. </CITE> proposed a new discriminative dictionary learning algorithm ( DL-GSGC ) to better represent 3D joint features . 
This algorithm incorporates both group sparsity and geometric constraints . 
In addition , to keep temporal information , a temporal pyramid matching method can be used on each sequence of depth maps . 
Most of the skeleton information-based approaches have state-of-the-art performance on benchmark datasets . 
However , their dependence on skeleton information is a detriment when such information is not available or incorrect .
Different from the previous studies , we take a dense trajectory-based approach to action recognition . 
We do not require the human body to be segmented , unlike the methods in </CITE> . 
Moreover , unlike the methods in </CITE> , no skeleton has to be extracted . 
We investigate the benefit of generating 2D transformed videos from depth data , as mentioned in </CITE> . 
Moreover , we leverage the trajectory feature to represent actions in video . 
To the best of our knowledge , no study has previously proposed to adapt the dense trajectory-based approach to human action recognition in depth video . 
We evaluated the recognition accuracy of our method on depth video using the dense trajectories proposed by H. Wang et al. </CITE> .
Here , we present a unified action recognition framework for depth data. 
We extract discriminative motion patterns from multiple views and apply a bag-of-words ( BoW ) model to them to compute feature vectors for the feature fusion scheme . 
The motivation behind using the bag-of-words model is that it can handle a variable number of motion patterns produced by arbitrary movements from various subjects . 
The fused feature vectors computed from the bag-of-words model are then input to the classifiers in the training and testing phases . 
The following steps are concise descriptions of the processes in our framework .
Projection : In this step , the problem is to find an action representation that effectively captures discriminative motion patterns . 
Currently , capturing motion patterns in 3D data has had less success than in 2D data . 
Therefore , in this step , we try to represent each 3D action as a combination of 2D actions . 
To do that , M depth maps are projected onto N view planes to obtain corresponding 2D action videos .
After the projection , each 2D motion video is abstracted by using several local motion patterns .
Feature Extraction : Here , we use a trajectory-based approach to capture discriminative motion patterns in the 2D videos .
With this approach , we can avoid problems related to segmentation of the human body as well as skeleton extraction . 
Trajectory-aligned descriptors are then calculated on the extracted trajectories in order to build N ``bags of motion patterns'' corresponding to N views .
Clustering : The clustering step converts the ``bag of motion patterns'' from the dataset into a ``bag of quantized motion patterns'' . 
A quantized motion pattern can be considered to be representative of several similar motion patterns . 
A standard clustering method ( e.g. k-means ) can be applied to all the motion patterns . 
Quantized motion patterns are then defined as the centers of the learned clusters . 
The number of clusters is the size of the ``bag of quantized motion patterns'' .
Quantization and Fusion : To represent an action with captured motion patterns , we map each “ motion pattern “ to a certain `` quantized motion pattern '' through the matching process . 
Afterwards , a histogram of the quantized motion patterns is generated to represent the action in the corresponding view . 
After that , the histograms generated from all views are concatenated to form a larger feature vector as input to the classifiers . 
Since each individual feature vector has the same meaning , the feature fusion can guarantee the effectiveness of the action representation .
Training and Testing : After the final feature representations are generated , we separate them into two histogram databases for the training and testing phases . 
We use a machine learning method such as Support Vector Machine ( SVM ) to make the classification . 
In practice , we use the precomputed-kernel technique with the histogram intersection kernel for this process . 
In addition , we employ the one-vs-all strategy for multi-class classification .
We compared our trajectory-based approach with state-of-the-art methods for recognizing human actions with depth data . 
Our approach does not use skeleton extraction , which is an important part of some methods , such as </CITE> . 
In fact , extracting a skeleton exactly is still an unsolved problem because of challenges such as cluttered backgrounds , poor hardware quality , and camera motion .
As mentioned in section </Ref> , we decompose each 3D action into a set of 2D actions and leverage the trajectory-based approach to capture the discriminative motion patterns . 
In this section , we describe our method to obtain 2D videos from various views . 
In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. 
As mentioned in section </CITE> , we decompose each 3D action into a set of 2D actions and leverage the trajectory-based approach to capture the discriminative motion patterns . 
In this section , we describe our method to obtain 2D videos from various views . 
In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. </CITE> , which has state-of-the-art performance in action recognition . 
Related aspects including dense sampling , tracking , and feature descriptors are also referred to .
Point </Eq> is the projection of point </Eq> along the view direction </Eq> onto the view plane </Eq> , which has state-of-the-art performance in action recognition . 
Our method to obtain discriminative motion patterns for recognizing human actions in depth video is as follows . 
First , 2D motion videos are formed from a sequence of depth maps , as illustrated in Figure </fig> . 
In this step , to obtain a 2D motion video from a view </Eq> , corresponding to a view plane </Eq> , in each depth map </Eq> , each point </Eq> is projected to </Eq> on the view plane </Eq> ( see Figure </Fig> ) as follows    :
where , The intensity value </Eq> at the projected point </Eq> is calculated as    :
Thus , given a set of points </Eq> , we have a projection </Eq> . 
Therefore , a set of the projections obtained from a given sequence of M depth maps for the view direction </Eq> is formed into a corresponding 2D motion video </Eq> . 
Each 2D video can be regarded as a 2D motion representation of the corresponding action in the depth video .
In particular , we use three 2D motion representations of action in three view directions    : the front , side , and top in 3D space , corresponding to three view planes    : </Eq> , </Eq> and </Eq> . 
The corresponding projections in these view directions are    :
The corresponding intensity values of the three projections are    :
Essentially , there are points we can observe from a certain view , but not from other views . 
Indeed , considering the example shown in Figure </Fig> , two points </Eq> have the corresponding projections </Eq> along the view direction </Eq>    :
However , if we observe </Eq> and </Eq> along the view direction </Eq> , their projection is only </Eq>  :
In such cases , we cannot observe points ( i.e. , point </Eq> ) hidden by others ( i.e. , point </Eq> ) along a certain view direction ( i.e. , view direction </Eq> ) . 
Therefore , the corresponding intensity value </Eq> at the projected point </Eq> is  :
Here , </Eq> are parameters of the plane </Eq> , and </Eq> is the coordinate of </Eq> .
Points </Eq> and </Eq> are respectively the projections of </Eq> and </Eq> along the view direction </Eq> onto the view plane </Eq> .  
</Eq> is the projection of </Eq> and </Eq> along the view direction </Eq> onto the view plane </Eq> . 
In this case , the intensity value of </Eq> is calculated using the distance from </Eq> to </Eq> .
Equations are obviously distance equations between a point and a plane . 
Therefore , equation can be represented as  :
Here , </Eq> is the distance between the point </Eq> and the plane </Eq> .
In addition , Figure </Fig> shows that is greater than . 
The evaluation leads to an important conclusion in the form of the following equation  :
More generally , if there are N points on a line parallel to the view direction </Eq> , the intensity value at the projected point on plane </Eq> is  :
Trajectories provide a compact representation of motion information in video . 
Trajectories from intensity videos can be used for multimedia event detection ( MED ) , video mining , action classification , and so on . 
Trajectory extraction crucially depends on the sampling and tracking processes . 
Some methods , such as </CITE> , use KLT tracker </CITE> or matched SIFT descriptors </CITE> between consecutive frames to obtain feature trajectories . 
Recently , the dense trajectory-based motion feature proposed by </CITE> has achieved high levels of performances on MED systems , including the segment-based system </CITE> on TRECVID MED 2010 and 2011 , AXES </CITE> , and BBNVISER </CITE> on TRECVID MED 2012 . 
The sampling is performed at multiple scales with a factor of </Eq> . 
Tracking is then performed to form trajectories . 
At each scale , in frame t , each point </Eq> is tracked to point </Eq> in the next frame </Eq> by using  : where </Eq> denotes the dense optical flow field , </Eq> is the kernel of median filtering , and </Eq> is the rounded position of </Eq> . 
The algorithm presented in </CITE> uses dense optical flows . 
To avoid drifting , it sets a suitable trajectory length of 15 frames . 
It also removes trajectories with sudden changes .
Once the trajectories have been extracted , two kinds of descriptor , i.e. , a trajectory shape descriptor and a trajectory-aligned descriptor , can be used . 
In our experiments , we only used trajectory-aligned descriptors , including the HOG </CITE> , the Histogram of Optical Flow ( HOF ) </CITE> , and the Motion Boundary Histogram ( MBH ) </CITE> . 
HOG captures local appearance information , while HOF and MBH encode local motion patterns . 
The descriptors are computed within a space-time volume </Eq> spatial pixels and </Eq> temporal frames ) around the trajectory . 
This volume is divided into a 3D grid ( spatially into </Eq> grid and temporally into </Eq> segments ) . 
The default settings of these parameters are </Eq> = 32 pixels , </Eq> = 15 frames , </Eq> = 2 , and </Eq> = 3 .
According to the authors of </CITE> , all three descriptors are capable of recognizing actions in intensity video . 
The experimental settings for these descriptors are based on an empirical study </CITE> . 
We also conducted experiments on all the three descriptors to evaluate their effectiveness on depth video .
This section presents the experimental results of our approach for the MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
Our aim is to clear up the following issues  : 
(1) Recognition accuracy in case of single view; 
(2) The role of compensating information from multiple views; 
(3) Comparison with state-of-the-art approaches . 
We will concentrate on the MSR Action 3D dataset when explaining the experimental results and show only the final results for the MSR Gesture 3D dataset and 3D Action Pairs dataset . 
All experimental results are reported for the settings described in section . 
Unlike the state-of-the-art approaches , our reported results are for concatenating action representations from combinations of three views  : front , side and top . 
All the results are compared in terms of recognition accuracy . 
The best performance is highlighted in bold .
Here , we detail the parameters used in each step of our 3D action recognition framework .
Projection : Our aim in this step is to select the best views to capture the most discriminative motion patterns . 
We make projections from front , side , and top views  : these views have been demonstrated to be effective in other studies </CITE> . 
The projections are then used to make three 2D motion videos .
Feature Extraction : We use an application available online to extract trajectory - aligned descriptors ( i.e. , MBH , HOG and HOF ) for each 2D motion video . 
The experimental results reported in section are for the MBH descriptor . 
The HOG and HOF descriptors are described in section .
Clustering : The purpose of this step is to learn a visual vocabulary or codebook . 
We create three codebooks ( i.e. three bags of quantized motion patterns ) corresponding to the front , side , and top views . 
In addition , to ensure a unified framework on all benchmark datasets , we cluster the extracted motion features around 2000 codewords ( i.e. 2000 quantized motion patterns ) for each codebook . 
The k-means algorithm with the Euclidean distance is used in this step .
Quantization and Fusion : Two popular strategies for quantizing extracted features are hard-assignment and soft-assignment . 
To guarantee the efficiency of our framework , we use the hard-assignment strategy to quantize the dense trajectory motion features extracted in step Feature Extraction . 
With this strategy , each feature vector can be assigned to a codeword or be rejected as an outlier by using the Euclidean distance . 
The results of the quantization step are 2D motion representations corresponding to the selected views .
After that , these 2D motion representations are concatenated to form the final motion representation for the corresponding depth video . 
The final representations are then separated into two histogram databases for the training and testing phases .
Training and Testing : Most of the recent methods using SVM classifiers refer to the libSVM library </CITE> published online . 
In our framework , we use the libSVM library with the histogram intersection kernel  :
The one-vs-all strategy is used for classifiers in both training and testing . 
The predicted value of each action is defined as the maximum score obtained from all the classifiers . 
This score indicates whether a human action is confused with another or not .
This dataset </CITE> contains 20 actions , as shown in Figure </Fig> . 
Actions were performed two or three times by ten subjects in the context of game console interaction .
In total , there were 567 sequences of depth maps . 
The depth maps were shot at a frame rate of 15 fps . 
The size of the depth map was </Eq> to ensure processing efficiency .
The three action subsets used in the experiments .
In order to conduct a fair comparison , we used the same experimental settings as in </CITE> . 
In these settings , the dataset is divided into three action subsets . 
Each subset has eight actions . 
The AS1 and AS2 subsets are such that grouped actions have similar movements . 
The AS3 subset groups complex actions together . 
For instance , the hammer action seems to be confused with the forward punch action in AS1 , and the hand catch and side boxing actions are similar movements in AS2 . 
For each subset , we selected half of the subjects for training and the rest for testing ( i.e. , a cross subjects test ) .
In this part , we evaluate the dense trajectory-based approach on single views . 
A straightforward view is the front . 
A simple way to obtain an action representation of the front view from the depth video is to consider the depth value as an intensity value . 
Table </tab> shows three confusion matrices corresponding to evaluations on three action subsets of the MSR Action 3D dataset . 
The results reported in the table </tab> indicate that AS1 , AS2 subsets contain many confusable actions , e.g. , hammer (a03) and forward punch (a05) in AS1 , or side-boxing (a12) and hand catch (a04) in AS2 , as is mentioned in the dataset description . 
The main cause is the similar movements of the actions in the same view direction . 
That is why we need compensating motion information from other views ( e.g. , the side view and/or top view ) .
This section presents our experiments on action recognition using information from multiple views . 
The action representations in a depth sequence are fused by concatenating the feature vectors computed from the corresponding views .
We report the experimental results on three action subsets and the average of the three subsets . 
Figure </Fig> compares intensity representations from the front , side and top and their fused representation .
The average recognition accuracy of the fusion ( 96.67%) is better than those of the individual intensity representations on the three action subsets . 
These results demonstrate the effectiveness of leveraging depth information from multiple views .
The results in Figure </Fig> show the role of views in our approach . 
They confirm that action representations from the front view achieve the best accuracy . 
Obviously , a front view representation is an indispensable component of a combination . 
Therefore , in what follows , all view combinations that we describe will include a front view , i.e. , front and side or front and top . 
Figure </Fig> shows the performance of the view combinations . 
Interestingly , t the front and top combination ( 96.95\% ) beats combining all three views ( 96.67\% ) as well as front and side combination ( 93.94\% ) in terms of average accuracy . 
In addition , the results in this figure </Fig> indicate two interesting points .
Firstly , compensating information from various views can cause unexpected risks , due to erroneous information from certain views . 
Indeed , consider two actions  : high arm wave and two hand wave . 
Although both contain the ``wave arm'' movement , we can easily distinguish them from the number of performed movements apparent in the front and top views . 
However , if we observe the two actions from the side view , half of the body is hidden ( see Figure </Fig> ) . 
In this case , it becomes easy to confuse movements of the two actions , and merging information from the side view into the front and top view combination actually causes the performance of the recognition system to decrease .
Secondly , the experimental results indicate a good way to decrease computational cost but still ensure convincing performance . 
Looking at figure </Fig> , we can see that the performances of two combinations , i.e. , ( front & top ) and ( front & side & top ) , are comparable . 
In some cases , such as in action subsets 2 , 3 , and average , the front and top combination has better performance . 
Obviously , if we eliminate unnecessary views , we can improve the efficiency of our system but still achieve competitive results .
These points confirm that information from multiple views is better than information from only single views . 
In addition , they suggest that there is an optimal way of combining views . 
These are promising results for building an effective and efficient recognition system .
Table </tab> compares the results of our approach with those of the state-of-the-art approaches on three action subsets of the MSR Action 3D dataset ( see Table </tab> ) . 
The compared approaches use various feature representations , such as silhouette features </CITE> , skeletal joint features like </CITE> , local occupancy patterns </CITE> , normal orientation features </CITE> , and cuboid similarity features </CITE> . 
For the same setting ( i.e. , a cross subjects test ) , the table </tab> indicates that our approach achieves the highest accuracy .
The Gesture3D dataset </CITE> is a hand gesture dataset of depth sequences captured by a depth camera . 
This dataset contains a set of 12 gestures defined by American Sign Language ( ASL ) ( see Figure </Fig> ) . 
In this dataset , ten subjects performed each gesture two or three times . 
In total , the dataset contains 333 depth sequences . 
The main challenge here is self-occlusion . 
We used the experimental settings in </CITE> ( i.e. , the leave-one-subject-out cross-validation ) to evaluate our approach . 
The results are in Table </tab> , from which it is clear that our approach outperformed the previous approaches .
The 3D Action Pairs dataset </CITE> is a new type of action dataset . 
The dataset contains pairs of actions , such that within each pair , the motion and shape cues are similar , but their correlations vary . 
It is useful for evaluating how well the approaches capture the prominent cues jointly in depth sequences . 
There are six pairs of actions ( see figure </Fig> ) . 
Each action was performed three times by ten subjects . 
Actions from the first five subjects were used for testing , the rest for training .
We compared our method with the HON4D approach </CITE> , which so far has had the best performance on this dataset . 
Table </tab> summarizes the results , and Table </tab> shows the confusion matrices . 
It is clear that our approach significantly outperformed the state-of-the-art approach , which suffered from confusion within the action pairs .
According to </CITE> , MBH is the best feature descriptor for dense trajectories on intensity videos .
Therefore , in the previous experiments , we only used the MBH descriptor to represent the motion information . 
Due to the difference between the depth data and intensity data , we conducted other experiments to investigate the impact of feature descriptors by replacing MBH with HOG and HOF .
Table </tab> lists the average recognition accuracies of the approach using different descriptors from three views ( i.e. front , side , and top ) . 
The experimental results verify that the MBH descriptor is still the best trajectory-aligned descriptor on the experimental datasets . 
However , HOG and HOF do give competitive performance .
In addition , extracting HOG or HOF is less expensive than extracting MBH . 
These advantages are promising for building effective and efficient systems .
We proposed a novel approach to effectively exploit discriminative motion patterns for human action recognition using depth sequences . 
The motion patterns based on trajectories jointly encode local motion and appearance cues .
Compensating information from different view directions is used to deal with actions that can be confused due to their having similar movements . 
We conducted an analysis of the role of single views in merging information with the aim of obtaining the best combination of views . 
In addition , we extensively evaluated our approach on three challenging benchmark datasets and found that it significantly outperformed state-of-the-art methods .
Our motion pattern-based approach with compensating information from separate motion representations shows promising results . 
Our study also suggests the importance of discriminative motion patterns for recognizing human actions in depth sequences . 
Therefore , depth-based motion trajectories can be beneficial for action recognition systems using depth cameras . 
This is an interesting idea for us to pursue in our future work .
Utilizing State - of - the - art Parsers to Diagnose Problems in Treebank Annotation for a Less Resourced Language
The recent success of statistical parsing methods has made treebanks become important resources for building good parsers . 
However , constructing high - quality annotated treebanks is a challenging task . 
We utilized two publicly available parsers , Berkeley and MST parsers , for feedback on improving the quality of part - of - speech tagging for the Vietnamese Treebank . 
Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties with Vietnamese parsing that required further improvements to existing parsing technologies .
Treebanks , i .e . , corpora annotated with syntactic structures , have become more and more important for language processing . 
The Vietnamese Treebank ( VTB ) has been built as part of the national project for ``Vietnamese language and speech processing ( VLSP )'' to strengthen automatic processing of the Vietnamese language \cite . 
However , when we trained the Berkeley parser \cite in our preliminary experiment with VTB and evaluated it using the corpus , the parser only achieved an F - score of 72 .1\%  . 
This percentage was far lower than the state - of - the - art performance reported for the Berkeley parser on the English Penn Treebank of 90 .2\% \cite . 
There are two possible reasons for this . 
First , the quality of VTB was not good enough to construct a good parser that included the quality of the annotation scheme , the annotation guidelines , and the annotation process . 
Second , parsing Vietnamese is a difficult problem on its own , and we need to seek new solutions to this .
Nguyen et al . \cite proposed methods of improving the annotations of word segmentation ( WS ) for VTB . 
They also evaluated different word segmentation criteria in two applications , i .e . , machine translation and text classification . 
This paper focuses on improving the quality of parts - of - speech ( POS ) annotations by using state - of - the - art parsers to provide feedback for this process .
The difficulties with Vietnamese POS tagging have been recognized by many researchers \cite . 
There is little consensus as to the methodology for classifying words . 
Polysemous words , i .e . , words with the same surface form but having different meanings and grammar functions , are very popular in the Vietnamese language . 
For example , the word \textviet can be a noun that means \emph , or an adjective that means \emph depending on the context . 
This characteristic makes it difficult to tag POSs for Vietnamese , either manually or automatically .
The rest of this paper is organized as follows : a brief introduction to VTB and its annotation schemes are provided in Section \ref . 
Then , previous work is summarized in Section \ref . 
Section \ref describes our methods of detecting and correcting inconsistencies in POSs in the VTB corpus . 
Evaluations of these methods are described in Section \ref . 
Finally , Section \ref explains our evaluations of the Berkeley parser and maximum spanning tree ( MST ) parser on different versions of the VTB corpus , which were created by using detected inconsistencies . 
These results from evaluations are considered to be a way of measuring the effect of automatically detected and corrected inconsistencies . 
We could observe difficulties with Vietnamese that affected the quality of parsers by analyzing the results from parsing .
Our experiences in using state - of - the - art parsers for treebank annotation , which are presented in this paper , should not only benefit the Vietnamese language , but also other languages with similar characteristics .
The VTB corpus contains 10 .433 sentences ( 274 .266 tokens ) , semi - manually annotated with three layers of WS , POS tagging , and bracketing . 
The first annotation is produced for each annotation layer by using automatic tools . Then , the annotators revise these data . 
The WS and POS annotation schemes were introduced by Nguyen et al . \cite . 
This section briefly introduces POS tag sets and a bracketing annotation scheme .
VTB specifies the 18 different POS tags summarized in Table \ref \cite . 
Each unit in this table goes with several example words . 
English translations of these words are included in braces . 
However , as we could not find any appropriate English translations for some words , these empty translations have been denoted by asterisks ( * ) .
The VTB corpus is annotated with three syntactic tag types : constituency tags , functional tags , and null - element tags . 
There are 18 constituency tags in VTB . The functional tags are used to enrich information for syntactic trees , such as where functional tag ``SUB'' is combined with constituency tag ``NP'' , which is presented as ``NP - SUB'' to indicate this noun phrase is a subject . 
There are 17 functional tags in VTB . The head word of a phrase is annotated with functional tag ``H'' .
The phrase structures of Vietnamese include three positions : \emph , \emph , and \emph \cite . 
The head word of the phrase is in the <head> position . 
The words that are in the <pre - head> and <post - head> positions are modifiers of the head word .
There are special types of nouns in Vietnamese that we have called Nc - nouns in this paper . 
Nc - nouns can be classifier nouns ( Nc ) , or common nouns ( N ) depending on their modifiers . 
For example , the Nc - noun \emph is a classifier noun if its modifier is the word \textviet , which means a specific fish , similar to \emph in English ) . 
However , the Nc - noun \emph is a common noun if its modifier is the word \textviet , which means \emph in English ) . 
We found that Nc - nouns always appeared in the head positions of noun phrases by investigating the VTB corpus . 
There is currently little consensus as to what the methodology is for annotating Nc - nouns \cite .
Nguyen et al . \shortcite described methods of detecting and correcting WS inconsistencies in the VTB corpus . 
These methods focused on two types of WS inconsistencies in variation and structure , which are defined below .
\emph are sequences of tokens that have more than one way of being segmented in the corpus .
\emph occur when different sequences have similar structures , and thus should be split in the same way , but are segmented in different ways in the corpus . 
Nguyen et al . \shortcite pointed out three typical cases of structural inconsistencies that were analyzed as classifier nouns ( Nc ) , affixes ( S ) , and special characters .
Nguyen et al .\shortcite analyzed N - gram sequences and phrase structures to detect WS inconsistencies . 
Then , the detected WS inconsistencies were classified into several patterns of inconsistencies , parts of which were manually fixed to improve the quality of the corpus . 
The rest were used to create different versions of the VTB corpus . 
These data sets were evaluated on automatic WS and its applications to text classification and English - Vietnamese statistical machine translations to find appropriate criteria for automatic WS and its applications .
Their experiments revealed that the VAR\_FREQ data set achieved excellent results in these applications . 
The VAR\_FREQ data set was the original VTB corpus with manually corrected structural inconsistencies in special characters and selected segmentations with higher frequencies in all detected variations . 
Therefore , we used the VAR\_FREQ data set in our experiments .
We propose two kinds of methods of detecting and correcting inconsistencies . 
They correspond to two different types of POS inconsistencies that we call multi - POS inconsistencies ( MI ) and Nc inconsistencies ( NcI ) , which are defined as follows .
\emph are words that are not Nc - nouns and have more than one POS tag at each position in each phrase category .
\emph are sequences of Nc - nouns and modifiers , in which Nc - nouns have more than one way of being annotated as POSs in the VTB corpus .
We separated the inconsistencies with POSs into these two types of inconsistencies because Nc - nouns are special types of words in Vietnamese . 
The methods of detecting and correcting Nc inconsistencies were language - specific methods developed based on the characteristics of Vietnamese . 
However , as the methods for MI are rather general , they can be applied to other languages .
\emph
Our main problem was to distinguish multi - POS inconsistencies from polysemous words , since polysemous words should not be considered inconsistent for annotations . 
Our method was based on the position of words in phrases and phrase categories . 
This idea resulted from the observation that polysemous words have many POS tags; however , each word usually has only one true POS tag at each position in each phrase category . 
For example , when a phrase category is a verb phrase , the word \emph in the pre - head position of the verb phrase \emph should be a modal , but the word \emph in the head position should be a verb . 
Further , the word \emph in the head position of a noun phrase \emph should be a noun , but the word \emph in the head position of the verb phrase \emph should be a verb . 
This may be more frequent in Vietnamese because it is not an inflectional language i .e . , the word form does not change according to tenses , word categories ( e .g . , nouns , verbs , and adjectives ) , or number ( singular and plural ) .
The method involved three steps . 
First , we extracted words in the same position for each phrase category . 
Second , we counted the number of different POS tags of each word . 
Words that had more than one POS tag were determined to be multi - POS inconsistencies . 
For example , in the following two preposition phrases , \textviet , the words \textviet appear at the head positions of both phrases , but they are annotated with different POS tags , i .e . , preposition ( E ) and conjunction ( C ) . 
Therefore , they are multi - POS inconsistencies according to our method .
It should be noted that this method was applied to words that were direct children of a phrase . Embedded phrases , such as \textviet , were considered separately .
A multi - POS inconsistency detected with the MI\_DM method is denoted by \emph , where \emph is a POS tag of word \emph , \emph is the frequency of POS tag \emph and AC involves applying the condition of \emph . 
Our method of correcting the POS tag for POS inconsistency \emph involves two steps . 
First , we select the POS tag with the highest frequency of all POS tags of \emph ( \emph ) . Second , we replace POS tags \emph of all instances \emph satisfying condition \emph with POS tag \emph . 
The AC of word \emph for multi - POS inconsistencies is its phrase category and position in the phrase .
For example , \textviet is a multi - POS inconsistency in the pre - head position of a noun phrase . 
The frequency of POS tag ``L'' is 27 and the frequency of POS tag ``P'' is two . 
Therefore , ``L'' is the POS tag that was selected by the MI\_CM method . 
We replace all POS tags \emph of instances \textviet in the pre - head positions of noun phrases with POS tag ``L'' .
As mentioned in Section \ref , an Nc - noun can be annotated with POS tag ``Nc'' or ``N'' depending on the modifier that follows that Nc - noun . 
Analyzing the VTB corpus revealed that Nc - nouns had two characteristics . 
First , an Nc - noun that is followed by the same word is usually annotated with the same POS tag . Second , an Nc - noun that is followed by a phrase or nothing at each occurrence is annotated with the same POS tag . 
Based on these two cases , we propose two methods of detecting Nc inconsistencies , which we have called NcI\_DM1 and NcI\_DM2 . They are described below .
\emph : We counted Nc - nouns in VTB that had two or more ways of being annotated as POSs , satisfying the condition that Nc - nouns are followed by a phrase or nothing . 
For example , the Nc - noun \emph in \emph is followed by nothing or it is followed by a prepositional phrase as in \textviet .
\emph : We counted two - gram sequences beginning with an Nc - noun in VTB that had two or more ways of being annotated as POSs of the Nc - noun , satisfying the conditions that two tokens were all in the same phrase and they all had the same depth in a phrase . 
For example , the Nc - noun \emph in the two - gram \textviet was sometimes annotated ``Nc'' , and sometimes annotated ``N'' in VTB; in addition , as \emph and \textviet in the structure \textviet were in the same phrase and had the same depth , \emph was an Nc inconsistency .
We denoted Nc inconsistencies with \emph similarly to multi - POS inconsistencies . 
We also replaced the POS tag of Nc - nouns with the highest frequency tag . 
The only differences were the conditions of application that varied according to the previous two cases of Nc inconsistencies .
For Nc inconsistencies detected by the NcI\_DM1 method , AC is defined as follows : \emph is an Nc - noun that is followed by nothing or a phrase .
For Nc inconsistencies detected by the NcI\_DM2 method , AC is defined as follows : \emph a Nc - noun that must be followed by a word , \emph .
We detected and corrected multi - POS inconsistencies and Nc inconsistencies based on the two data sets of ORG and VAR\_FREQ . 
The ORG data set was the original VTB corpus and VAR\_FREQ was the original corpus with modifications to WS annotation . 
This setting was made similar to that used by Nguyen et al . \shortcite to enable comparison .
There are a total of 128 ,871 phrases in the VTB corpus . 
The top five types of phrases are noun phrases ( NPs ) ( representing 49 .6\% of the total number of phrases ) , verb phrases ( VPs ) , prepositional phrases ( PP ) , adjectival phrases ( ADJPs ) , and quantity phrases ( QP ) , representing 99 .1\% of the total number of phrases in the VTB corpus . 
We analyzed the VTB corpus based on these five types of phrases .
Tables \ref and \ref summarize the overall statistics for multi - POS inconsistencies and Nc inconsistencies for each phrase category . 
The second and third columns in these tables indicate the numbers of inconsistencies and their instances that were detected in the ORG data set . 
The fourth and fifth columns indicate the numbers of inconsistencies and their instances that were detected in the VAR\_FREQ data set . The rows in Table \ref indicate the number of Nc inconsistencies and the number of instances detected with the NcI\_DM1 and NcI\_DM2 methods .
According to Table \ref , most of the multi - POS inconsistencies occurred in noun phrases , representing more than 72\% of the total number of multi - POS inconsistencies . 
All Nc inconsistencies in Table \ref are also in noun phrases . 
There are two possible reasons for this . 
First , noun phrases represent the majority of phrases in VTB ( represent 49 .6\% of the total number of phrases in the VTB corpus ) . 
Second , nouns are sub - divided into many other types ( common nouns ( N ) , classifier nouns ( Nc ) , proper nouns ( Np ) , and unit nouns ( Nu ) ) ( mentioned in Section \ref , which may confuse annotators in annotating POS tags for nouns . 
In addition , the high number of Nc inconsistencies in Table \ref indicate that it is difficult to distinguish between Nc and other types of nouns . 
Therefore , we need to have clearer annotation guidelines for this .
We estimated the accuracy with which our methods detected and corrected inconsistencies in POS tagging by manually inspecting inconsistent annotations . 
We manually inspected the two data sets of ORG\_EVAL and ORG\_POS\_EVAL . 
We randomly selected 100 sentences in ORG\_EVAL , which contained instances of POS inconsistencies in the ORG data set . 
ORG\_EVAL contained 459 instances of 157 POS inconsistencies . 
ORG\_POS\_EVAL was the ORG\_EVAL data set with corrections made to multi - POS inconsistencies and Nc inconsistencies with our methods of correction above .
We manually checked POS inconsistencies and found that 153 cases out of 157 POS inconsistencies ( 97 .5\% ) were actual inconsistencies . 
There were four cases that our method detected as multi - POS inconsistencies , but they were actually ambiguities in Vietnamese POS tagging . 
They were polysemous words whose meanings and POS tags depended on surrounding words , but did not depend on their positions in phrases . 
For example , the word \textviet in the post - head positions of the verb phrases VP1 and VP2 below , can be a noun that means\emph in English , or it can be an adjective that means \emph , depending on the preceding verb .
We analyzed the detected POS inconsistencies to find the reasons for inconsistent POS annotations . 
We classified the detected POS inconsistencies according to pairs of POS tags . 
There were a total of 85 patterns of pairs of POS tags . Table \ref lists the top five confusing patterns ( PoPOS ) , their counts of inconsistencies ( counts ) , and examples . 
It also seemed to be extremely confusing for the annotators to distinguish types of nouns ( Nc and N , and N and Np ) and distinguish nouns from other types of words ( such as verbs , adjectives , and pronouns ) .
We investigated POS inconsistencies and the annotation guidelines \cite to find why common nouns were sometimes tagged as classifier nouns and vice versa , and verbs were sometimes tagged as common nouns and vice versa . 
We found that these POS inconsistencies belonged to polysemous words that were difficult to tag .
The difficulties with tagging polysemous words were due to four main reasons : 
( 1 ) The POS of a polysemous word changes according to the function of that polysemous word in each phrase category or changes according to the meaning of surrounding words . 
Although polysemous words are annotated with different POS tags , they do not change their word form . 
( 2 ) The way polysemous words are tagged according to their context is not completely clear in the POS tagging guidelines . 
( 3 ) Annotators referred to a dictionary that had been built as part of the VLSP project \cite ( VLSP dictionary ) to annotate the VTB corpus . 
However , this dictionary lacked various words and did not cover all contexts for the words . 
For example , \textviet in Vietnamese is an adjective when it is the head word of an adjectival phrase , but \textviet is an adverb when it is the modifier of a quantifier noun ( such as \textviet . 
However , the VLSP dictionary only considered \textviet to be an adjective ( \textviet ) . 
No cases where \textviet was an adverb were mentioned in this dictionary . 
( 4 ) There are several overlapping but conflicting instructions across the annotation guidelines for different layers of the treebank . 
For example , the combinations of affixes and words they modify to create compound words are clear in the WS guidelines , but POS tagging guidelines treat affixes as words and they are annotated as POS tags ``S'' . 
For words modifying quantifier nouns , such as \textviet , the POS tagging guidelines treat them as adjectives , but the bracketing guidelines treat them as adverbs . 
Therefore , our method detected multi - POS inconsistencies as \textviet , \emph at the pre - head positions of noun phrases . 
Since the frequencies of the adjective tags were greater than those of adverb tags ( fA > fR ) , these words were automatically assigned to adjective POS tags ( A ) according to our method of correction . 
These were POS inconsistencies that our method of correction could not be applied to , because the frequency of incorrect POS tags was higher than that of actual POS tags .
We carried out experiments to evaluate two popular parsers , a syntactic parser and a dependency parser , on different versions of the VTB corpus . 
Some of these data sets were made the same as the data settings for WS in Nguyen et al . \shortcite . 
The other data sets contained changes in POS annotations following our methods of correcting inconsistencies presented in Section \ref . 
We could observe how the problems with WS and POS tagging influenced the quality of Vietnamese parsing by analyzing the parsing results .
Data . Nine configurations of the VTB corpus were created as follows :
ORG : The original VTB corpus .
BASE , STRUCT\_AFFIX , STRUCT\_NC , VAR\_SPLIT , VAR\_COMB , and VAR\_FREQ correspond to different settings for WS described in Nguyen et al .\shortcite .
ORG\_POS : The ORG data set with corrections for multi - POS inconsistencies and Nc inconsistencies by using the methods in Section \ref and \ref .
VAR\_FREQ\_POS : The VAR\_FREQ data set with corrections for multi - POS inconsistencies and Nc inconsistencies by using the methods in Section \ref and \ref .
Each of the nine data sets was randomly split into two subsets for training and testing our parser models . 
The training set contained 9 ,443 sentences , and the testing set contained 1 ,000 sentences .
Tools
We used the Berkeley parser \cite to evaluate the syntactic parser on VTB . 
This parser has been used in experiments in English , German , and Chinese and achieved an F1 of 90 .2\% on the English Penn Treebank .
We used the conversion tool built by Johansson et al . \shortcite to convert VTB into dependency trees .
We used the MST parser to evaluate the dependency of parsing on VTB . 
This parser was evaluated on the English Penn Treebank \cite and 13 other languages \cite . 
Its accuracy achieved 90 .7\% on the English Penn Treebank .
We made use of the bracket scoring program EVALB , which was built by Sekine et al . ( 1997 ) , to evaluate the performance of the Berkeley parser . 
As an evaluation tool was included in the MST parser tool , we used it to evaluate the MST parser .
The bracketing F - measures of the Berkeley parser on nine configurations of the VTB corpus are listed in Table \ref . 
The dependency of the MST parser on the accuracies of nine configurations of the VTB corpus are summarized in Table \ref . 
These results indicate that the quality of the treebank strongly affected the quality of the parsers .
According to Table \ref , all modifications to WS inconsistencies improved the performance of the Berkeley parser except for STRUCT\_NC and VAR\_SPLIT . 
More importantly , the ORG\_POS model achieved better results than the ORG model , and the VAR\_FREQ\_POS model achieved better results than the VAR\_FREQ model , which indicates that the modifications to POS inconsistencies improved the performance of the Berkeley parser . 
The VAR\_FREQ\_POS model scored 1 .11 points higher than ORG , which is a significant improvement .
The dependency of the MST parser on accuracies in Table \ref indicates that all modifications to POS inconsistencies improved the performance of the MST parser . 
All modifications to WS inconsistencies also improved the performance of the MST parser except for STRUCT\_NC . 
The VAR\_FREQ\_POS model scored 7 .36 points higher than ORG , which is a significant improvement .
The results for the Berkeley parser and MST parser trained on the POS - modified versions of VTB were better than those trained on the original VTB corpus , but they were still much lower than the performance of the same parsers on the English language . 
We analyzed error based on the output data of the best parsing results ( VAR\_FREQ\_POS ) for the Berkeley parser , and found that the unmatched annotations between gold and test data were caused by ambiguous POS sequences in the VTB corpus .
An ambiguous POS sequence is a sequence of POS tags that has two or more constituency tags . 
For example , there are the verb phrases \emph and the adjectival phrase \emph in the training data of VAR\_FREQ\_POS . 
As these two phrases have the same POS sequence \emph , \emph is an ambiguous POS sequence , and VP and ADJP are confusing constituency tags ( CCTs ) . 
We found 42 ,373 occurrences of 213 ambiguous POS sequences ( representing 37 .02\% of all phrases ) in the training data of VAR\_FREQ\_POS . 
We also found 1 ,065 occurrences of 13 ambiguous POS sequences in the parsing results for VAR\_FREQ\_POS . 
Some examples of ambiguous POS sequences , their CCTs , and the number of occurrences of each CCT in the training data of VAR\_FREQ\_POS are listed in Table \ref .
We classified the detected ambiguous POS sequences according to pairs of different CCTs to find the reasons for ambiguity in each pair . 
There were a total of 42 pairs of CCTs , whose top three pairs , along with their counts of types of ambiguous POS sequences , and examples of ambiguous POS sequences are listed in Table \ref . 
We extracted different POS tags at each position of each phrase category for each pair of CCTs , based on the ambiguous POS sequences . 
For example , the third row in Table \ref has ``R A V'' and ``A V N'' , which are two ambiguous POS sequences that were sometimes annotated as VP and sometimes annotated as ADJP . 
The different POS tags that were extracted from the pre - head positions of VPs based on these two POS sequences were ``R , A'' and ``R'' was the POS tag that was extracted from the pre - head positions of ADJPs based on these two POS sequences . 
These POS tags are important clues to finding reasons for ambiguities in POS sequences .
Table \ref summarizes the extracted POS tags at pre - head positions for the top three pairs of CCTs . 
For example , the POS tags in row NP - VP and column 1 are in the pre - head positions of NPs and the POS tags in row NP - VP and column 2 are in the pre - head positions of VP . 
By comparing these results with the structures of the pre - head positions of phrase categories in the VTB bracketing guidelines \cite , we found many cases that were not annotated according to instructions in the VTB bracketing guidelines , such as those according to Table \ref , where an adjective ( A ) is in the pre - head position of VP , but according to the VTB bracketing guidelines , the structure of the pre - head position of VB only includes an adverb ( R ) .
We investigated cases that had not been annotated according to the guidelines , and found two possible reasons that caused ambiguous POS sequences . 
First , although our methods improved the quality of the VTB corpus , some errors in POS annotations remained in the VTB corpus . 
These errors in POS annotations were cases to which our methods could not be applied ( mentioned in Section \ref ) . 
Second , there were ambiguities in POS sequences caused by Vietnamese characteristics , such as the adjectival phrase \textviet and the noun phrase \emph that had the same POS sequence of ``R N A'' .
Therefore , POS annotation errors need to be eliminated from the VTB corpus to further improve its quality and that of the Vietnamese parser . 
We not only need to eliminate overlapping and conflicting instructions , which were mentioned in Section \ref , from the guidelines , but we also have to complete annotation instructions for cases that have not been treated ( or not been clearly treated ) in the guidelines . 
We may also need to improve POS tag set because adverbs modifying adjectives , nouns , and verbs are all presently tagged as ``R'' , which caused ambiguous POS sequences , such as the ambiguous POS sequence ``R N A'' mentioned above . 
If we use different POS tags for the adverb \textviet , which modifies the adjective \textviet , and the adverb \textviet , which modifies the noun \textviet , we can eliminate ambiguous POS sequences in these cases .
We proposed several methods of improving the quality of the VTB corpus . 
Our manual evaluation revealed that our methods improved the quality of the VTB corpus by 6 .5\% with correct POS tags . 
Analysis of inconsistencies and the annotation guidelines suggested that : 
( 1 ) better instructions should be added to the VTB guidelines to help annotators to distinguish difficult POS tags , 
( 2 ) overlapping and conflicting instructions should be eliminated from the VTB guidelines , 
and ( 3 ) annotations that referred to dictionaries should be avoided .
To the best of our knowledge , this paper is the first report on evaluating state - of - the - art parsers used on the Vietnamese language . 
The results obtained from evaluating these two parsers were used as feedback to improve the quality of treebank annotations . 
We also thoroughly analyzed the parsing output , which revealed challenging issues in treebank annotations and in the Vietnamese parsing problem itself .
Overcoming Spatial Verification Failure 
An object-centric boosting technique for visual instance search that is more precise than existing ones is proposed . 
A hybrid method is applied to solve the problem of lacking confidence when using standard spatial verification methods . 
We classify pairs of verified visual words into three categories — discriminative , weak relevant , and context - inferred — on the basis of the relation of these words to the object proposal location . 
In this work , we use a deformable parts model ( DPM ) detector to demonstrate the proposed technique . 
Three corresponding weighting functions are also proposed to compute the final similarity score between query topic and shot video . 
We evaluate our method using TRECVID Instance Search data sets and find that it performs better than conventional methods on many kinds of query objects , with improvements of about 29.9% and 22.3% compared to standard BOW and the spatial verification model , respectively .
The objective of this work is to address the problem of instance search or object retrieval in video databases that include hundreds of thousands of shots made up of millions of frames . 
The term instance search ( INS ) is defined formally by TRECVID \cite as “finding video segments of a certain specific object , place , or person after being given visual examples from a video collection” . 
In practice , INS has many applications , including archive video searching , law enforcement , brand-logo protection , personal video organization , and surveillance .
Most of the conventional approaches follow the original bag-of-visual-word ( BOW ) model first introduced by Sivic in a case involving video retrieval \cite . 
This model relies on the key assumption that two similar images will share a significant amount of local patches that can be matched against each other . 
Using sparse feature detectors ( e.g. , DoG \cite , Hessian affine \cite , MSER \cite ) is very efficient in terms of finding regions of richly textured objects such as buildings , paintings , and advertising . 
As reported \cite , \cite , \cite , the mean average precision ( mAP ) evaluated on standard benchmarks such as Oxford buildings and Paris buildings is approaching 90 percent . 
Therefore , searching this kind of object is to some extent a solved problem .
For small and fairly textured objects , however , conventional feature detectors cannot collect enough local information , thus invalidating the BOW's assumption . 
Therefore , the standard BOW and its improvements , including spatial re-ranking \cite and query expansion \cite , are poised to encounter a lot of difficulties stemming from noisy backgrounds and complex capturing conditions . 
After years of exploration in TRECVID competition , most INS systems currently in use \cite are based on the BOW model . 
However , these have been reported to be very low in mAP due to the many different types of query object , so there are still many challenges that remain to be addressed . 
In this paper , we aim at improving the accuracy of an instance search system that focuses on many different realistic objects .
Several have already been successfully applied , including RootSIFT feature \cite , large vocabulary \cite , soft assignment \cite , multiple detectors and features combination at late fusion[x] query-adaptive asymmetrical dissimilarities \cite , the topology model for spatial verification \cite , and weak geometric consistency ( WGC ) with hamming embedding ( HE ) \cite . 
In addition to its importance as a technique to improve accuracy , spatial consistency checking is a step for query expansion to improve the recall of the system . 
However , this approach is not very efficient for searching different types of query . 
There are three main reasons for the failure of spatial verification : ( i ) the absence of features due to viewpoint and lighting condition changes , ( ii ) the existence of confus objects that share similar parts with the query object . 
These lead to a lack of high confidence visual words for re-ranking . 
Figure \ref gives some examples of a scenario in which database frames get a high similarity score after spatial checking due to reasons ( ii ) and ( iii ) above , namely , the existence of many similar objects that share the same part with the query object and a noisy background .
The typical way of addressing the above issue is to perform sampling in a dense grid on various scales . 
in an image is fully covered by local patches that are aggregated to a single vector and used for spatial verification . 
Much research has recently shifted from local feature detection to the use of denser techniques , some of the applications of which include dense feature on image classification \cite , fine-grained classification \cite , and action recognition in videos \cite . 
The dense method , however , requires a special treatment for application to large - scale retrieval since it extracts many more patches than sparse ones , so it is not applicable for large - scale data if taking a per-feature level storage approach . 
Using a dense feature at the post - processing stage instead of at the early stage is more reasonable .
Here , an object detector based on a dense feature such as HOG to boost the confidence of a visual word . 
Whichever words satisfy both BOW and the detection models are used to boost the similarity score at the end . 
We use the deformable parts model \cite to demonstrate our proposed method since it is currently one of the best algorithms for object detection . 
This detector could easily be replaced by other detectors without changing the structure of the system .
Contribution : In this paper , we propose a novel re - ranking method that exploits object proposal to boost the confidence of visual words . 
The performance of the proposed approach is significantly better than that of other re-ranking methods ( geometric consistency checking , multi-features late fusion technique ) thanks to the following contributions :
To the best of our knowledge , this is the first time that .
Our new re - ranking method combines two complementary models : BOW and object detector using dense features .
the proposed detector in our scheme small , texture-less , and occluded objects . 
In contrast , conventional BOW is suitable for big and richly textured objects . 
In the experimental section , we will demonstrate how these characteristics complement each other through use of a simple late fusion technique .
We propose a new that takes into account the location information of a candidate object . 
The shared visual words on a database frame are classified into four categories on the basis of the relationship between word location and bounding box of DPM algorithm . 
Each class of visual word will contribute to the final score with a boosting function . 
Unlike idf as a global weighting , our confidence function can be applied for a specific query object .
The rest of this paper is organized as follows . 
Related work is discussed in Section \ref . 
The details of our framework are presented in Section \ref . 
Sections \ref and \ref explain why we chose to combine DPM with BOW and propose a location - based fusion technique for re- ranking . 
Section \ref presents our experimental results using two datasets ( INS2013 and INS2014 ) . 
We conclude the paper with a brief summary in Section \ref .
BOW is an unstructured model that assumes all visual words are independent in a high dimensional vector . 
Many researches have tackled . 
Spatial verification is one of the most effective approaches to improve the accuracy of retrieval systems . 
It is also a prerequisite for other advanced methods such as query expansion . 
Spatial re-ranking , which checks the geometric consistency in a short list of about 200–1000 results , the BOW model . 
Another effective model , first applied by J. Philbin \cite , uses RANSAC to exploit the local shape of an affine covariant region for rigid affine consistency checking . 
Hough pyramid matching for spatial re-ranking a hierarchical structure to group matches , thus resulting in an algorithm that is only linear in the number of putative correspondences \cite . 
An elastic spatial checking technique has been proposed to emphasize the topology layouts of matching points \cite .
Another approach is spatial ranking , which incorporates spatial information at the original ranking stage .
Jegou et al. \cite use a Hough-like voting scheme in the space of similarity transformation between query and database image . 
However , this is a weak geometric consistency checking . 
Cao et al. proposed using spatial-bag-of-features the spatial ordering of visual words under various linear and circular projections \cite , while Shen et al. proposed transforming query ROI by predefined scales \cite . 
However , this latter method is much more computationally expensive than other systems such as BOW or WGC .
None of the above methods consider or provide a systematic solution to handle the failure of spatial re-ranking caused by a low number of features or by the existence of confus objects that have as a query object . 
To the best of our knowledge , this is the first time that the spatial verification problem has been solved .
The deformable parts model ( DPM ) is a state-of-the-art algorithm in object detection . 
The original version of DPM is very slow , making it difficult to apply on large-scale data . 
However , due to research on improving the accuracy and speed of DPM \cite , and seems like it could be effective for detection on a large-scale dataset . 
Moreover , if we combine part filters with a root filter , DPM could also function as an implicit spatial check method for our proposed approach .
All systems implemented in the experiment section are based upon the following settings . 
Keyframes are extracted from raw video at the rate of 5 frames per second . 
For feature extraction , we use a Hessian affine detector \cite and a SIFT descriptor \cite . 
To improve the performance of the retrieval system , RootSIFT \cite post-processing is applied , with no additional storage memory or computational cost . 
A large vocabulary with 1 million visual words is trained using the approximate K-means ( AKM ) algorithm \cite . 
To reduce quantization errors without adding storage memory , we use hard-assignment on database keyframes and soft-assignment on query images with three nearest neighbors . 
All frames of a shot are aggregated into one high-dimensional histogram vector using average pooling .
Each query image of a topic is independently compared to all shots using asymmetrical metrics \cite .
The final relevance score between query topic and shot are computed using the average fusion of all ranking lists returned from each query example retrieval .
For simplicity of notation , here we consider only the set of query examples and keyframes of a shot in the video dataset . 
Other shots are processed similarly . 
Let </Eq> be the vector BOW of the k-th query image and the j-th frame of the video shot , respectively , where </Eq> is the size of the codebook . 
To build an inverted index with a compact representation , we use average pooling :
where </Eq> is the number of keyframes a shot . 
The similarity score of a shot with given query examples is computed by 
where </Eq> is the number of query examples and </Eq> is the asymmetrical similarity score . 
The top </Eq> retrieved shots based on the </Eq> similarity score are then used for the re-ranking .
In order to verify the confidence of shared visual words between query and database frames , we propose using an object detector with denser features . 
that DPM is a state-of-the-art algorithm with a parts-based representation model . 
A histogram of gradient features ( HOG ) and latent-SVM were originally designed to handle changing lighting conditions and variations in the appearance of an object .
DPM takes advantage of shape information : The BOW model with a SIFT descriptor ( or its extension , rootSIFT ) and the DPM model with HOG features are both based on the idea of computing the histogram of a gradient vector at a group pixel . 
The difference is that BOW uses all of the features independently while the DPM algorithm groups local blocks together to represent a meaningful part . 
Moreover , the feature detector used in standard BOW retrieval systems is very sparse and sensitive to lighting condition or viewpoint , while the HOG feature of DPM always covers an object in its entirety . 
Figure \ref shows a comparison of the Hessian affine feature used in BOW and the HOG feature used in DPM . 
We can clearly see that the distribution of Hessian affine features is not uniform across regions within the same image . 
In the case of a low contrast image , the number of features is very sparse or even completely absent . 
In contrast , the HOG feature of a part filter can always be described under all conditions , including cases of low contrast .
BOW and DPM are two complementary methods : In fact , there is no one method that is superior for all cases . 
For query objects that have dense local features , the BOW model is effective if accompanied with a spatial re-ranking method . 
However , in the case of a small and textureless object , when the lighting condition changes over time , the extracted features are sparser , which leads to spatial verification failure . 
In contrast , the DPM algorithm can describe any object under any lighting condition and is more effective when it comes to taking advantage of shape information . 
However , DPM algorithms are generally suitable for detecting generic objects that have discriminative curves to show that they are significantly different from other ones , and this characteristic results in failure in objects with too much texture .
To demonstrate of these two approaches , we use a simple late fusion technique that combines two scores using an formula . 
This method will be compared to standard spatial re-ranking in the experiment section . 
Let </Eq> be a model trained from all query examples and 100 random images crawled from Google Images with "things" as the . 
The similarity score of the query topic and in the database is computed as where </Eq> is a keyframe of a video shot . 
The final late fusion similarity of the BOW and DPM models is computed using the following average normalized formula :	
where </Eq> are the z-score normalized values of </Eq> on the top K of the rank list , respectively :
Note that this is just a simple average fusion technique on normalized scores , and there is still room to improve on fusion formula xx . 
The values of </Eq> and </Eq> can be estimated adaptively using more information about the number of visual words and ROI area given by topic examples .
In the late fusion approach , we simply combined the score values of the two methods without using any other information such as shared word or bounding box detected by DPM algorithm . 
In this section , we propose an integrated approach to effectively utilize this information . 
Figure \ref displays all the cases that may occur with the BOW shared words and the candidate's bounding box . 
The ROI rectangle in the left image is the location of a query object when put in the search engine and the object proposal region ( OPR ) rectangle in the right image is the DPM bounding box . 
Each arrow starts from the visual word of a query image and ends at the visual word of a video frame the database . 
All pairs of matching points are divided into the following categories :
Type 1 : Outlier shared words , which are incorrect pairings of matched points represented by red arrows . 
These pairs of points will be removed by the RANSAC algorithm , and the remaining pairs are inlier pairs of matching points that fall into one of the next three types .
Type 2 : Discriminative points , which are represented by green arrows connecting words in the foreground to words inside the DPM bounding box . 
These points satisfy both the BOW model and the DPM algorithm and thus contribute to the accurate identification of the query object . 
We choose a monotonically increasing function of the number of discriminative points </Eq> .
Type 3 : Weakly relevant points , which are represented by blue arrows connecting a point in the foreground to a point outside the DPM bounding box . 
These points indicate an inconsistency between the BOW model and DPM , but they still contain valuable information , so we also use a monotonically increasing function of the number of weakly relevant points </Eq> .
This function does not increase as fast as function </Eq> .
Type 4 : Context information points , which are represented by black arrows connecting outside ( background ) points of a query image to points outside the bounding box . 
Although the main query object to be searched in the foreground , the features in the background sometimes provide useful information for detecting objects . 
For example , the logo of a car company is often located next to a car . 
Context information can contribute to the final result , albeit slightly , so the weighting function of context points </Eq> should be used , although these do not increase as fast as functions </Eq> , </Eq> .
are incorrect cases and thus removed by the RANSAC algorithm ; we do not consider them in the final score formula .
Let </Eq> be a set of points in an image that represents the location of the k-th given query example .
where </Eq> and </Eq> are spatially verified points belonging to the Hessian affine features extracted from the query image and video frame that have the same word ID , respectively .
Our goal is not to find the optimal weighting function but rather to prove that our new method of calculating the score associated with selecting a suitable weighting function can improve accuracy significantly compared with . 
The weighting functions must satisfy the following properties :
Property 1 . for all </Eq> . 
The greater the number of verified shared words , the higher the confidence is .
Property 2 . </Eq> for all </Eq>
This feature aims and the average score of the component models and </Eq> when the number of shared visual words of each type equals 0 . 
Therefore , in special cases , if n = 0 , we are .
Property 3 . </Eq> for all </Eq> . 
This feature : distinctive points , weak points , and distinctive point bringing context information .
For ease of testing , we assume that these functions are in the same class of polynomial function . 
In this paper , we propose three weighting functions :
The final score of the proposed method becomes :
where </Eq> .
In this section , we discuss the series of experiments we performed to evaluate the proposed re-ranking method using BOW and an object detector model , where DPM was chosen to demonstrate our idea . 
First , in order to demonstrate the complementary characteristics of BOW and DPM , its average late fusion . 
Second , we compare our location-based fusion approach with other state-of-the-art techniques : practical spatial re-ranking \cite , Hamming embedding and weak geometric consistency \cite , and multi-features fusion[x] and topology checking \cite . 
Third , we examine the impact of value top </Eq> and the contributions of each weighting function to the performance of the system as a whole . 
To determine the performance of the proposed method with multiple types of query , we use TRECVID Instance Search ( INS ) datasets for evaluation . 
Specifically , we used the TRECVID INS [xx] benchmarks from 2013 and 2014 , which were released in the evaluation campaign organized by NIST , referred to here as INS2013 and INS2014 , respectively . 
Both have the same collection of test videos with a master shot reference . 
The dataset contains approximately 244 video files extracted from the BBC EastEnders program with a total of 300 GB in storage and a duration of 464 hours . 
Each query topic of INS2013 and INS2014 consists of several query images and corresponding masks that delimit an object , place , or person entity in an example video and locate for each topic up to 1000 of the shots most likely to contain a recognizable instance of the entity . 
We should point out that INS datasets are quite challenging due to containing a variety of query types including small to big objects and rich texture to texture-less objects .
Evaluation protocol . 
The ground truth files for each query are created manually and provided by the TRECVID organization . 
To evaluate the performance of each system , we use the mean average precision as a standard score .
First , to demonstrate the complementary characteristics of BOW and DPM , we evaluate the performance of the late fusion technique . 
The top 10 ,000 shots retrieved from the standard BOW model are used for re-ranking using the DPM object detector . 
from all query example to find the location of the candidate object and its corresponding score value , the similarity value for re-ranking . 
The score representing one shot is the highest one of all keyframes ( Eq.3 ) , BOW+DPM : average late fusion on normalized scores of BOW and DPM .
Figure \ref shows that , although BOW is a state-of-the-art model for image retrieval , in some cases , DPM gives a better result : for example , 9108 , 9109 , 9114 , 9118 compared to 9124 , 9125 , 9128 , respectively . 
DPM+BOW gives a performance approximately on par with the higher one between DPM and BOW for most query topics . 
In some cases , e.g. , 9102 , 9103 , 9123 , it is also significantly better than the two baseline methods . 
Therefore , the overall MAP of DPB+BOW is 28.21% , which outperforms both BOW and DPM ( 25.01% and 21.23% , respectively ; see Table \ref ) . 
In this section , we compare the proposed method with other state-of-the-art systems that have been reported on INS2013 and INS2014 . 
The winner of the TRECVID INS 2013 competition was the multi-features system , which combines six pairs of feature detectors and descriptors using asymmetrical similarity and late fusion technique [x] .
Hamming embedding and weak geometric consistency ( HE+WCG ) dominant scale and orientation but geometric checking . 
The topology checking ( TC ) approach uses Delaunay triangulation to improve the quality of visual matching , but it cannot overcome the problem spatial verification mentioned in this paper . 
Practical spatial re-ranking ( PSC ) applies RANSAC for a representative frame to avoid having to verify all images of a shot or query topic .
Table \ref shows a comparison of the results of the proposed method and the state-of-the-art methods . 
As stated above , performance is evaluated two benchmarks . 
As shown in the table , the proposed approach consistently outperforms the other methods , even though we use only one pair of feature detector and descriptor . 
Our late fusion technique BOW+DPM also delivers an approximate accuracy compared to the multi-features approach with a simple average computation . 
There is still room for improvement by adaptively combining the BOW and DPM models . 
Although the multi-features and other spatial re-ranking methods improve performance very much , they cannot resolve the failure of spatial verification . 
The results also show that our method is effective not only for small and texture-less objects but also for big and richly textured ones .
Effect of parameter </Eq> . 
Here , we briefly examine the effect of value </Eq> on the final mAP when taking top </Eq> shots as input at the re-ranking stage . 
Figure \ref shows that when </Eq> , the accuracy of the proposed algorithm seem to be saturated with no more improvement . 
In contrast , when </Eq> , there are still many positive shots in the bottom of the rank list returned the standard BOW model . 
Even in the case of the least value </Eq> , the mAP of the proposed method is </Eq> and </Eq> ( for INS2013 and INS2014 , respectively ) . 
Contribution of each component in late fusion formula . 
Here , weighting functions by constant 1 to evaluate the effect of each component on the system performance . 
We compare the BOW methods with our final method using the configurations in Table \ref , where the base score refers to the average normalized score of BOW and DPM</Eq> .
Table \ref shows that removing any component of the re-ranking formula decreases the accuracy significantly . 
As a result , in the final formula . 
Without this component , the performance drops significantly . 
In addition to satisfying the three properties mentioned in Section \ref , </Eq> should not be rapidly increasing functions because . 
Therefore , a square function for </Eq> . 
Similarly , the next most important components are functions </Eq> . 
We can see that verifying the pair matching shared word using the RANSAC algorithm also helps to significantly improve the accuracy . 
This was shown in configuration Ours\_w/oRANSAC , where the same formulae xx are applied without any spatial verification step .
In this paper , we addressed the failure of spatial verification in instance search/object retrieval systems . 
A new hybrid method that takes advantage of the complementary characteristics of the BOW and DPM models has been introduced to overcome this challenge . 
We classify pairs of verified visual words into three types based on the relationship between the location of visual words and the region . 
Each of these types corresponds to a weighting function that will boost the similarity score at the time of re-ranking . 
Experimental results demonstrate the superiority of our method compared to other state-of-the-art approaches .
Our future work will involve exploring adaptive late fusion techniques for the base score that combine the normalized BOW and DPM similarity scores . 
Our scheme enables us to replace the DPM with other object detectors without changing the structure of the system .
A Minimum-Delay Routing Tree Algorithm for Access-Point Communications in Wireless Mesh Networks
As a scalable , flexible access network to the Internet , we have studied the design optimization and the implementation of the Wireless Internet-access Mesh NETwork ( WIMNET ) .
WIMNET is composed of multiple access points ( APs ) that are connected with each other by wireless communications . 
At least one AP performs as the gateway ( GW ) to the Internet .
Any host in the network can access to the Internet through this GW , after associating its neighbor AP and communicating with multihop wireless links between APs . 
The delay along the routing path degrades the performance of WIMNET . 
To avoid the bottleneck of communications by minimizing the maximum delay , we formulate the routing tree problem for AP communications , and prove the NP-completeness of its decision version .
Then , we propose the greedy heuristic algorithm of sequentially selecting one link that minimizes the delay of the predicted tree containing the link . 
The effectiveness of our algorithm is verified through extensive simulations in instances with 25 APs using the WIMNET simulator .
Nowadays , the Internet has become widely used in our daily lives with the rapid development of inexpensive small communication devices and high-speed communication technology . 
A variety of information , data , and services have actually been provided through the Internet . 
As a result , a high-speed , flexible , and inexpensive Internet access has been strongly desired by those who use wireless communication devices at mobile hosts such as personal computers ( PCs ) and personal digital assistances ( PDAs ) . 
In order to meet these demands , wireless local area networks ( WLANs ) have been widely studied and deployed as the access network to the Internet . 
WLANs have now become available in many private and public spaces including offices , schools , homes , airports , stations , and shopping malls .
An emergency technology called the wireless mesh network is considered attractive and praise by academics and industries as a flexible and inexpensice large-scale WLAN </CITE> . 
It is composed of multiple wireless routers distributed in the network field , so that it can expand the wireless coverage area by a single router that is often confined to a small space . 
In the wireless mesh network , data communications between routers are offered by multihop wireless communications , in addition to those between routers and hosts .
Among several variations of the wireless mesh network , our study has focused on the network for the Internet access , called the Wireless Internet-access Mesh NETwork ( WIMNET ) for convenience . 
WIMNET uses only access points ( APs ) as wireless routers , and realizes wireless communications between APs mainly on the MAC layer using the wireless distribution system ( WDS ) </CITE> . 
Figure 1 illustrates the overview of WIMNET . 
At least one AP in WIMNET acts as a gateway ( GW ) to the Internet .
In WIMNET , each AP takes two different roles for wireless communications , specifically a wireless hub for its associated hosts and a wireless bridge for relaying packets between APs .
To reduce the radio interference between these two communications , we use different protocols with different channels ( radio frequencies ) for them . 
Actually , we assign the IEEE 802.11b/g protocol with 2.4GHz for the former role , which is usually available at mobile hosts , and the IEEE 802.11a protocol with 5GHz for the latter one . 
For each protocol , several non-interfered frequency channels are available . 
Using this feature , an AP can use multiple channels by equipped with multiple NICs ( Network Interface Cards ) to increase the bandwidth </CITE> . 
For the optimal design of WIMNET , we have studied related combinatorial optimization problems and their algorithms . 
In </CITE> , we defined the AP allocation problem with its heuristic algorithms . 
In </CITE> , we studied the gateway AP selection problem . 
In </CITE> , we studied the NIC and channel assignment problem .
As the size of WIMNET is expanded , two factors may determine its performance of communications . 
The first factor is the increase of the transmission delay at wireless links between GWs and their adjacent APs . 
In WIMNET , all the packets to/from user hosts associated with APs other than GWs must be handled by a GW to access the Internet .
Thus , wireless communications around GWs can be very crowded , and one link between GW and its adjacent AP may become the bottleneck of the whole communication in WIMNET . 
The second factor is the propagation delay due to multihop communications between an AP and GW .
The increase of the hop count ( the number of hops ) in the multihop communication directly increases the delay , because packets are transmitted in a bucket - brigade manner . 
It can also increase the transmission delay because more links need to be activated .
Therefore , the proper routing for communications between APs to minimize both delays is very important to improve the performance of the large-scale WIMNET .
The routing paths for AP communications in WIMNET with a single GW becomes a spanning tree rooted at GW of the graph representing the AP network topology , because every AP must be connected with GW for the Internet access . 
In this paper , using this graph representation , we formulate the minimum-delay routing tree problem to find optimal routing paths for AP communications in WIMNET . 
We prove the NPcompleteness of the decision version via a reduction from the NP-complete bin packing problem </CITE> . 
Then , we propose the greedy heuristic algorithm for this problem .
The precise estimations of both delays are essential for the proper routing . 
The propagation delay to an AP can be easily estimated by the summation of the link delays along the path between this AP and GW . 
However , it is hard to estimate the transmission delay of a link during the tree construction in the greedy algorithm . 
Although this estimation requires the sum of traffics at the link that are transmitted from all the APs along the same path as described later , this path can be known after the routing tree is completed .
Therefore , before selecting one link to construct a routing tree , our algorithm predicts the remaining part of the tree using the Dijkstra shortest path algorithm with link delays , and estimates the transmission delay using the traffics along this predicted path . 
The effectiveness of our approach is evaluated through simulations using the WIMNET simulator </CITE> that has been developed by our group .
The rest of this paper is organized as follows : Section II defines the minimum-delay routing tree problem in WIMNET .
Section IV proves the NP-completeness of its decision version .
Section IV presents our greedy heuristic algorithm . 
Section V shows evaluation results of our algorithm by simulations .
Section VI notes some related works . 
Section VII provides the conclusion and some future works of this paper .
In the minimum-delay routing tree problem for AP communications in WIMNET , the AP network topology , G = ( V , E ) , is given as the input with one GW to the Internet . 
A vertex in V represents an AP and an edge in E represents a wireless link between two APs . 
Every vertex in V and every edge in E are associated with non-negative weights . 
The weight hi for the ith AP ( =APi ) represents the expected number of hosts associated with this AP . 
The weight sij for the link from APi to AP j ( =linkij ) represents the bandwidth or the transmission speed . 
They are estimated or observed beforehand to design WIMNET properly . 
Then , the traffic through each link in E is estimated from vertex weights , assuming that every host communicates the same amount of traffic to/from the Internet .
The objective of this problem is to minimize the two delays described in Section I . 
The propagation delay Di prop for APi is given by the summation of link delays along the route Pi from GW to APi . 
Because the link delay is inversely proportional to the bandwidth , Di prop is given by : </Eq> ( 1 )
The transmission delay Di trans for the link between GW and its adjacent AP ( let APi ) is given by the transmission time of all the traffics at this bottleneck link . 
Dtrans i is given by : </Eq> ( 2 ) where AP g represents GW , and </Eq> represents the traffic of link </Eq> , which will be discussed in II-C .
Then , the cost function E is defined to represent the minimization of the largest propagation delay and the largest transmission delay in the routing tree . 
E is given by : </Eq> ( 3 ) where </Eq> and </Eq> represent constant coefficients , the function </Eq> returns the maximum value in the set S , and Ng represents the set of APs adjacent to GW . 
Note that </Eq> = 200 and </Eq> = 1 are used in our simulations .
We formulate this minimum-delay routing tree problem as a combinatorial optimization problem .
Input : G = ( V , E ) : the AP network topology with </Eq> : a gateway to the Internet , hi : the expected number of hosts associated with APi for </Eq> , </Eq> : the bandwidth of </Eq> for </Eq> and </Eq> , TRs : the average sending traffic from a host , and TRr : the average receiving traffic to a host .
Output : T : the routing tree .
Constraint : T must include every AP in V ( T must be a spanning tree rooted at GW ) .
Objective : to minimize the cost function E in ( 3 ) .
The traffic of a link is given by the summation of the traffics to/from the APs along the path located on the opposite side of this link from GW in the routing tree T , because these traffics must be transmitted through this link to access the Internet in this multihop network .
1 ) Initialization of the traffic of </Eq> : </Eq> .
2 ) Calculation of the traffic of </Eq> : </Eq> and </Eq> if </Eq> is included in Pk .
Figure 2 shows that the traffic of the link from GW to its adjacent AP is calculated by the summation of the traffics to the APs along the same path in the routing tree , because they must be transmitted by this link . tk ( k = 1 , 2 , 3 , 4 ) represents the traffic to APk .
In this section , the NP-completeness of the decision version of the minimum-delay routing tree problem is proved via a reduction from the NP-complete bin packing problem </CITE> .
The decision version of the minimum-delay routing tree problem , Min-Tree , is defined as follows :
Instance : The same inputs as the minimum-delay routing tree problem in II-B and an additional constant E0 .
Question : Is there a routing tree to satisfy E ? E0 ?
The bin packing problem , Bin-packing , is defined as the following decision problem :
Instance : a set of M cups with water of volume </Eq> for cup i , and L bins with a constant volume B .
Question : Is there a way of pouring the water of all the cups into bins without spilling , such that the whole water of any cup is poured into the same bin ?
Clearly , Min-Tree belongs to the class NP . 
Then , we show that an arbitrary instance of Bin-packing can be transformed into the following Min-Tree instance :
Input : G = ( V , E ) : GW ( =AP0 ) is connected with the K APs ( =APi for </Eq> ) , where the K APs are connected with all of the remaining M APs ( =APi for </Eq> ) , the M APs are connected with each other , and N = 1 + K + M , as shown in Figure 3 , </Eq> for </Eq> and </Eq> for </Eq> for </Eq> and </Eq> .
In this Min-Tree instance , the cost function E in ( 3 ) is equal to the maximum traffic among the links incident to GW . 
The traffic of such a link is given by the summation of the traffics from the APs along the route to GW in the tree . 
Thus , each of K routes in the tree is equivalent to one bin packing of cups . 
This proves the NP-completeness of Min-Tree .
In this section , we propose a greedy heuristic algorithm for the minimum-delay routing tree problem for AP communications in WIMNET .
In WIMNET , all the traffics of the hosts for the Internet access must pass through GW . 
When the hosts are associated with APs other than GW , their traffics must be transmitted by one of the links between GW and its adjacent APs . 
Because the number of such links is usually small , the minimization of the transmission delay of one link among them may increase delays of the other links . 
To deal with this problem , our algorithm actually seeks the minimization of the difference between the maximum transmission delay and the minimum one . 
This modified cost function is given by : </Eq> ( 4 ) where min </Eq> returns the minimum value in the set S .
Our algorithm greedily constructs the routing tree T = ( VT  , ET  ) by repeating the selection of one link that minimizes the modified cost function Em if it is added into T , until every AP is included in the tree .
To estimate the transmission delay in Em accurately , the complete routing tree is predicted by expanding the current partial tree using the Dijkstra shortest path algorithm in terms of the link delay . 
Here , we note that the ?-term in Em is calculated from the current tree T with the selected link . 
In the procedure , the routing tree T = ( VT  , ET  ) is first initialized with GW , and then , T is sequentially constructed by selecting the best link of minimizing the cost function of the predicted tree one by one , until V T = V  .
1 ) Calculate the delay at linkij by </Eq> .
2 ) Initialize the routing tree T by V T=GW and </Eq> .
3 ) Construct T by repeating the selection of the link satisfying the following two conditions , until every AP in V is included in T : it connects an </Eq> and an </Eq> , and when it is included in T , Em with the tree predicted in IV-C becomes minimum .
The complete routing tree is predicted by expanding the current partial tree T by selecting the links that connect the remaining APs using the Dijkstra shortest path algorithm .
1 ) Initialize the set of the unselected APs , A , by A = V  , and the decision variables , AP weight labels ( propagation delay ) and AP route labels ( previous APs in paths from GW ) , in the Dijkstra algorithm using the current partial tree T as follows :
a ) for AP weight labels : set the propagation delay in 1 for any </Eq> , and set </Eq> for the remaining APs .
b ) for AP route labels : set the previous AP along the route from GW in T for any AP in T , and set </Eq> for the remaining APs .
2 ) Select one AP ( let APk ) in A that has the smallest weight label .
3 ) Remove APk from A , and terminate the procedure if </Eq> .
4 ) Update the both labels of the APs adjacent to </Eq> .
In this section , we discuss the performance improvement by our approach via network simulations using the WIMNET simulator .
The WIMNET simulator simulates the least functions for wireless communications of hosts and APs required to evaluate throughputs and delays , because this simulator has bee developed for evaluations of a large-scale WIMNET with reasonable CPU time on a conventional PC . 
Thus , a sequence of functions such as host movements , communication request arrivals , and wireless link activations , are synchronized by a single global clock called a time slot . 
Within an integral multiple of time slots , a host or an AP can complete the one frame transmission and the acknowledgement reception . 
In this paper , the duration time of one time slot is set 0.2ms .
From our past experimental results , the maximum bandwidth of links between APs is set 30Mbps , and that between an AP and hosts is 20Mbps . 
Thus , the former link is completed with two slots , and the latter is with three slots , assuming every frame size is 1 , 500bytes . 
When two or more links within their wireless ranges are activated at the same time slot , only one link , randomly selected , is successfully activated , and the others are inserted into waiting queues to avoid collisions , assuming DCF and RTS/CTS functions . 
The packets for each request are routed along the path in the routing tree T . 
Only the connection-less communication is implemented in the WIMNET simulator , where the retransmission is not considered .
First , we verify our approach using rather artificial simple instances with fixed traffic patterns .
1 ) Simulated Instances : In the simulated instances , 5 ? 5 ( N = 25 ) APs are regularly placed with the same interval in the field . 
Each AP can communicate with its four neighbor APs , where any link has the same maximum bandwidth . 
The center AP is selected as the gateway to the Internet . 
Two types of biased fixed traffic patterns , or host distributions , in the field shown in Figures 4 and 5 are considered . 
A white circle in both figures represents an AP associated with 1 host . 
A black circle represents an AP with 8 hosts in Figure 4 and with 10 hosts in Figure 5 . 
We have selected them so that the total number of hosts in each instance becomes about 100 for the unification of loads . 
These simple but extreme cases are used to clarify the superiority of our combined delay approach to single delay approaches .
For each traffic pattern , we first applied our routing tree algorithm in Section IV , and then , the channel configuration algorithm in </CITE> to optimally assign the traditional NICs to Aps and the channels to the communication links used in the routing tree .
Then , we executed the WIMNET simulator using the network configuration given by these algorithms .
Before starting each simulation run , every host has 125 packets sending to the gateway , and the gateway has 1 ,000 packets sending to each host , where each packet has a single frame size .
When all of the packets are received by their destinations , one simulation run is terminated .
For the network performance , the throughput is calculated by the total received packet size divided by the simulation time .
2 ) Simulation Results : In simulations , in addition to our algorithm ( proposal ) , the algorithm considering only the propagation delay ( comp1 ) , which has often been used in routing , and the algorithm considering only the transmission delay ( comp2 ) , are executed for comparisons .
Figure 6 and 7 illustrate the routing trees obtained by three algorithms for traffic patterns 1 and 2 , respectively . 
For both patterns , proposal and comp1 find trees with the shortest path for any AP , whereas comp2 does not find such a tree for traffic pattern 2 .
Figures 8 and 9 show changes of throughputs when the number of additional NICs at Aps increases .
When every AP is assigned only one NIC , the throughout is similar among them .
The reason is that the bandwidth of the single channel shared among the links around GW becomes the bottleneck of whole communications in WIMNET , where the gateway must handle every traffic from/to the Internet .
However , the throughput is improved by utilizing multiple channels even when a small number of additional NICs are used .
This improvement becomes best for proposal , where the routing tree gives the shortest path for any AP , and disperses traffics more equally among the bottleneck links .
Then , we investigate the effectiveness of our approach with three different gateway positions in Figure 10 under random traffic patterns .
In these simulations , the number of associated hosts for each AP is randomly selected between 1 and 7 hosts for each of 50 runs where the total number of hosts in the field is always 100 .
Figure 11 shows changes of their average throughputs with the increase of additional NICs for three gateway positions .
When 13 Aps are added , the throughputs become saturated .
This maximum throughput is about 120Mbps for the center GW , 90Mbps for the edge-center GW , and 60Mbps for the corner GW .
These results are coincident with their rough estimations that can be given by sums of adjacent Aps .
This fact justifies our routing algorithm for these random instances .
Several works have been reported for routings of point-to-point communications in wireless mesh networks .
Unfortunately , the realization of this synchronous wireless mesh network is very hard , and the superiority of the performance is actually not clear to the conventional asynchronous one including this paper .
Besides , it assumes that every AP has the same number of associated hosts and assigned NICs , which is different from out more practical assumption in this paper .
ETT represents the average transmission time that can be calculated from the loss rate and the bandwidth of the link .
WCETT is given by the summation of ETT along the route and the maximum of the summation of ETT for the links using the same channel .
This paper has presented the formulation of the minimum-delay routing problem for access-point communications in the wireless Internet-access mesh network ( WIMNET ) , and proved the NP-completeness of its decision version .
Then , it has proposed the greedy heuristic algorithm with repetitions of complete tree predictions .
The effectiveness of our approach is verified through simulations using the WIMNET simulator .
The significant performance improvements with a small number of additional NICs are observed using proper routing trees found by our algorithm .
In future works , our algorithm will be evaluated under non-uniform link bandwidths and/or dynamic traffic changes in simulations .
Besides , to justify our approach in the real world , the performance of WIMNET will be investigated after implementing real APs with multiple NICs .
Quality Measurement for Transmitted Audio Data Using Distribution of Sub-band Signals
Recently , the remarkable progress of network technology has increased the requirement for transmission of high quality multimedia data . 
By the trend , it has been issued to investigate an efficient methodology for quality measurement of transmitted multimedia data . 
In this paper , we propose a new audio quality measurement technique to substitute for a typical quality measurement tool , RMSE (Root Mean Squared Error) .
The proposed method modifies the variance of sub-band signals to perform the estimation of audio quality at the transmitter  , the receiver is able to estimate the quality distortion of transmitted audio data by calculating the distance between the variance and the reference value representing the characteristics of sub-band signals , so called EVE (Estimated Variance Error) . 
The proposed is not a reference technique , so it does not require the original data to measure the audio quality . 
On the Gaussian noise channel with several standard deviations , we prove that the proposed scheme has good performance , and it is a novel alternative to RMSE .
With the rapid growth of the Internet , most consumers have requested for service providers to transmit the multimedia data with high quality . 
It encourages the researchers to develop the compression technology such as MPEG ( Motion Pictures Experts Group ) and JPEG ( Joint Photographic Experts Group ) / JPEG-2000 , the network environment to guarantee the QoS ( Quality of Services ) , and so on .
Although the wellconstructed network environment is most important to satisfy the demands of consumers , it is costly since it is primarily based on the physical layer of OSI ( Open Systems Interconnection ) seven layers .
For that reason , recently , it has become a new issue to research schemes for quality measurement of multimedia data on the application layer .
It is a good methodology to solve the cost problem .
Several quality measurement techniques of multimedia data have introduced by </CITE> . 
R .Reibman </CITE> proposed the quality monitoring techniques of video . 
Compared with other studies , he introduced an especial method .
The proposed algorithm on </CITE> measures the quality distortion from the video bit-stream instead of the samples .
Through the simulations , he claimed that his proposed algorithm using bit error is related to the real quality of video .
In </CITE> , they estimate the quality distortion of audios and still images by using fragile watermarking . 
The method embeds watermark information into frequency components , which can be obtained by DCT ( Discrete Cosine Transform ) or DWT ( Discrete Wavelet Transform ) .
The measurement tool estimates the quality distortion from correct or incorrect extracted watermark bits . 
Even if the fragile watermark is broken proportionally to channel error ratio , the embedded watermark could be easily distorted by such unintentional errors as compressions .
Moreover it is difficult to decide a feature to embed watermark , which is sensitive to quality distortion .
In this paper , we propose a new audio quality measurement technique .
It uses the distribution of sub-band signals , which is obtained by DWT .
At the transmitter , we alter the variance of sub-band signals into the constant value .
The receiver monitors the audio quality by calculating the variance error from the constant value .
We denominate it as EVE ( Estimated Variance Error ) .
In general , since the variance moves according to the changing of signals , we could estimate the quality distortion by checking the distance of variance .
And , modifying the specific sub-band signals can decrease the distortion caused by the process of transmission since modifying the variance of whole sub-band signals is a waste of resource .
Besides , it allows us to select the optimal sub-band , which is most sensitive to noise .
Note that a quality measurement system should be able to estimate the quantity of noise from the transmitted multimedia data .
Moreover , the proposed is no reference quality measurement technique ; that is , it does not require the original audio data on the receiver .
In section II , we describe our quality measurement technique using the distribution of sub-band signals in more detail .
Section III shows the simulation results .
To demonstrate the efficiency of the proposed method , we compare the result with RMSE ( Root Mean Square Error ) on the Gaussian noise channel .
Finally , we conclude this paper in section IV .
In transmitting the multimedia data through wired / wireless network , the quality is almost dependent on channel noise .
Especially , if the characteristic of channel is represented by a kind of additive noise , RMSE ( Root Mean Squared Error ) can be used as a good quality measurement tool .
RMSE can be redefined by the difference of variance between the original data and the transmitted data .
It is proved in the appendix .
In this paper , we propose a quality measurement technique for transmitted audio data using the variance of sub-band signals , which is obtained by DWT ( Discrete Wavelet Transform ) .
Fig . 1 shows the proposed quality measurement system .
The method modifies the variance of sub-band signals into constant value , or reference value and estimates the quality distortion from the distance between the variance of transmitted audio data and the constant value .
The reason of employing the sub-band signals for modifying the variance is that altering the whole frequency band signals has an influence on the distortion of audio quality .
Fig . 2 shows the main idea of the proposed scheme .
For the sake of simplicity , let’s assume that the sub-band signals is a random variable , X , with a uniform distribution as shown in Fig . 2-(a) .
The distribution is modified according to the reference value .
For bigger reference values , the sub-band signals are modified to make their distribution concentrate on the mean value as shown in Fig . 2-(b) .
On the other hand , for smaller reference values , the distribution becomes more distant from the mean value as shown in Fig . 2-(c) . 
The variance of random variable X with uniform distribution , </Eq> , is as follows .
That is , if we know the characteristics of signals , the reference value can be determined as constant value .
The quality measurement is quite easily performed , as the quality distortion can be estimated just by calculating the distance of variance from the reference value .
It means that the method can measure the audio quality without the original data .
It is greatly correlated with RMSE . 
Therefore , we designate the estimated RMSE as EVE ( Estimated Variance Error )
To measure the quality of the transmission audio , we modify the variance of sub-band signals of original data into reference values .
The original signal is decomposed into several sub-band signals by using DWT .
Fig . 3 shows the procedure of modifying the variance of original audio data into reference values .
Let’s consider only the simple system decomposed into two channels .
Firstly , the original signal </Eq> where , N is the number of audio samples) is divided into M small frames , </Eq> , where , M is the number of frame) .
The signal of each frame is converted onto normalized range [ -1 , 1 ] .
The normalized j-th sample of i-th frame , </Eq> , is obtained by </Eq>
Where , max is the maximum value and min is the minimum value that an audio sample can have .
And then , each frame is decomposed into low frequency and high frequency band signals by analysis filter bank , HL(z) and HH(z) .
To alter the variance of sub-band signals , one specific sub-band signals are selected .
Note that , it is more reasonable that the high frequency band is selected since its signals are more sensitive than the low frequency band signals to noise . 
If the sub-band signals are divided into smaller frames , the distortion caused by modifying the variance can be decreased .
However , some modified sub-band signals that belong to certain frame can invade other neighbor frames .
For that reason , the sub-band signals are modified by an exponential function , sign </Eq> .
The exponential function can cope with interference between altered sub-band signals .
Fig . 4 shows that that the exponential function is able to prevent by exponential function in terms of k the modified sub-band signals from getting over the range [- 1 , 1] .
If the modified sub-band signals are denoted by random variable Y  , the variance of Y  , </Eq> , is obtained by </Eq> (3)
It means that we can alter the variance by controlling k .
The variation of variance according to k is shown in Fig . 5 .
The selected sub-band signals are modified by the exponential function , and the variance become the reference value , r , where , r could be determined by the characteristics of signals .
For example , as shown in section II , the case of uniform distribution has the reference value of 1/3 . 
Now , the variance of specific the sub-band signals in every frame is equal to the reference value .
The whole sub-band signals including some part of the modified sub-band signals are transformed into the modified signals , </Eq> , passing through reconstruction filter bank , GL(z) and GH(z) .
Finally , the reconstructed audio signal , </Eq> , is denormalized onto original range and it is transmitted to the receiver .
The denormalized j-th sample of ith frame , </Eq> , is obtained by </Eq> (4)
Fig . 6 shows the proposed audio quality measurement process .
It is quite simple since the quality of transmitted audio signal can be estimated just by checking the difference between the variance of the sub-band signals and the reference value , r .
Similar to the process of transmission , audio signal passing the noise channel </Eq> is divided into M small frames , </Eq> .
The signal of each frame is converted onto the normalized range [-1 , 1] .
Then , each frame signal is decomposed into sub-band signals by using the same analysis filter bank as used in the transmitter .
In these sub-band signals , the quality of transmitted audio signal is determined as the estimated variance error , so called EVE .
The EVE of i-th frame , </Eq> , is calculated by </Eq> (5)
Note that the proposed audio quality measurement system does not require the original data .
The simulations are carried out on mono pop , rock , and classic music with 16-bits/sample and sampling rate of 44.1 KHz , respectively .
For sub-band decomposition , the packet 5/3 - tap biorthogonal perfect reconstruction filter bank </CITE> is applied recursively to low and high frequency band signals .
Here , the original audio signal is decomposed into eight sub-bands on three multi-resolution levels .
The filter bank can be implemented by a fast operation algorithm called lifting </CITE> .
The frequency ranges of each sub-band are listed in table 1 .
One frame consists of 4.644 sec ( 204,800 sam-ples ) in the time domain .
To evaluate the proposed quality measurement method , we perform the simulation comparing with RMSE , E , by using cross correlation .
Where </Eq> denotes the cross correlation of i-th frame .
The simulation is carried out on the Gaussian noise channel , which has the mean of zero , and the standard deviation from 100 to 1,000 .
As a high frequency band is more sensitive than a low frequency band to noise ,
we experimentally select the 5th-band as the optimal sub-band to modify the variance .
The maximum and minimum values for converting onto the normalized range is respectively determined as 32,767 and -32,768 , since the range of audio signals quantized by 16-bits is from 32,767 to -32 ,768 .
As mentioned in section II , if the characteristics of signals could be known , we can calculate the reference value r .
In general , because the high frequency band has Laplacian distribution , the variance of normalized sub-band signals is close to zero ( That is , the distribution is concentrated on the mean value ) .
For that reason , we determine r as 0.0001 .
Table 2 shows the audio quality after altering the variance of the subband signals .
Since the proposed method uses one specific frequency resource , the modified audio data has little quality distortion .
The simulation results after Gaussian noise addition is shown in Fig . 7 , in terms of several standard deviations .
The quality measurement is performed frame by frame .
To compare with RMSE , we indicate the real range , not the normalized range ( Actually , the quality is represented from zero to one in the proposed audio quality measurement system ) .
The circle dot denotes the result of RMSE , the squared dot is EVE , and the triangle dot is in case of no Gaussian noise .
EVE shows the modality , which is proportional to RMSE .
It means , like RMSE , EVE could estimate the quality distortion according to the strength of the error .
As shown in Fig . 8 , the proposed system is high correlated with RMSE and table 3 represents the cross correlation between RMSE and EVE .
Moreover , the proposed method is not a reference technique , i.e. , it does not require the original data for audio quality measurement .
In this paper , we proposed a new quality measurement technique for transmitted audio data by calculating the variance of intentionally modified sub-band signal .
Through the simulations , we proved that the proposed enables to estimate the quality distortion , not requiring the original data at the receiver .
As a results , the proposed is a good alternative to the traditional quality measurement tool , RMSE .
Using SOM based Graph Clustering for Extracting Main Ideas from Documents
In this paper , we would like to present a graph clustering system for grouping the similar documents and extracting the main ideas in documents . 
To cluster the documents , we need a model for representing the documents . 
The traditional approaches used a word set based model or a vector based model for representing the documents . 
These models discard the important structural information of documents such as word position , the semantic relations of words in document …
Recently , some research works using the graph for representing the documents have been appeared . 
We use the graph to becreated by analyzing the co-occurrence and position of two words in a section of document . 
After representing the documents by using graph , we used self organizing map ( SOM ) with two dimensional output layer for grouping the graphs . 
One of the advantages of SOM is to cluster the data without specifying the number of clusters . 
Besides , two-dimensional SOM output layer can be put on the computer display and it can help to access the similar documents on the computer display . 
We use the graph distance based on the maximum common sub-graph ( mcs ) which is discovered by maximal frequent sub-graph algorithm and the updated operation of neurons on SOM ouput layer based on the weighted means graphs and the genetic algorithm .
In this paper , we would like to present the research results of a system for clustering documents and extracting their main ideas . 
To cluster documents , we need a model for representing the documents . 
In previous approaches , a word - set - based model or a vector - based model were used for representing the documents . 
These models , however , discard the important information such as position and the semantic relation of words .
Recent studies use graphs as an alternative , and we apply this model to representing documents </CITE> .
Prevous approaches to text clustering rely on word - set - based or vector - based models to represent documents . 
The SOM neural network clustering methodology , 
developed by Kohonen since 1980 , is utilized to cluster the graphs </CITE> . 
SOM is superior in its capability of clustering data without having to define the number of clusters . 
This capability is very important and better than the traditional clustering algorithms such as the kmeans . 
Besides , SOM can put the documents on a document map and help to access the content of similar texts . 
We study the method for calculating the distance between two graphs based on the maximal common subgraph , 
which is determined by the maximal frequent subgraph 
discovery algorithm with 100% support . 
The method of adjustment of the weighted graph on the nodes of SOM output layer is based on the weighted means graph and genetic algorithm . 
The remainder of this paper is organized as follows : 
2 ) Using graph model for representing the documents 
3 ) SOM neural network
4 ) Using SOM for clustering graphs and extracting main ideas from documents 
5 ) Experiment and discussions 
and 6 ) Conclusion and future work
We introduce two approaches of using graph for document representation . 
The first approach , developed by Tomita et al.</CITE> 
uses subject graphs for representing documents . 
In this approach , the graph is created by the following steps :
− Extracting the frequent terms in the document .
− Calculating the significance from the co-occurrence frequency of the two terms in a unit of document such as sentence , paragraph … .
If the co-occurrence frequency of two terms is greater than a threshold , we create an edge to connect these terms .
An example of the subject graph for document representation is shown in figure 1 .
The second one is the approach developed by Adam Schenker and Mark Last </CITE> . 
In this approach , each term appearing in the document , becomes a node , except for stop words such as “ a ” , “ the ” , “ of ”  , “ and ” , and “ or ” which contain little information . 
This is accomplished by labeling each node with the term it represents . 
Each word is represented by only a single node even when the word appears more than once in the text . 
Thus each node in the graph represents a unique word . 
For example , this is labeled with an unique term .
In the graph-based model , the authors create the graph by representing each word , except stop words, in the document as one unique node in the graph , regardless of how many times it appears in the text . The direction from one node to another corresponds to the relative positions of the words , and is illustrated by a directed edge starting from the node representing the preceding word . 
The edges are then labeled according to the section that contains the words . These sections include title , link or the clickable hyperlinks , and the remaining  called text . Nodes corresponding infrequent words are excluded from the graph .
An example of directed graph representation is given in figure 2 . 
The oval indicates nodes and their corresponding term labels , the edges are labeled according the title ( TI ) , link ( L ) or text ( TX ) . 
We also use a semantic edge called TS ( text similarity ) for connecting two similar terms . 
For example , " board " and " plank " both means " a piece of wood " . 
We used Wordnet for measuring word similarity </CITE> .
The second approach is superior to the first . 
Suppose that we have two sentences : 
" Cats eat mice " and " Mice eats cats " . 
The first approach fails to distinguish the meanings of these sentences beacuse it ignores the positions of the words ,
while according to the second approach ,
the word positions matter and are expressed in the representation model .
We used the second approach for representing the document in our proposed system .
SOM has been developed by T Kohonen </CITE> . 
After the learning phase , SOM will be used to create a mapping between the high dimension objects of training set with a smaller dimension clusters . 
In our system , we use the 2D SOM , because it is suitable for placing 2D SOM output layer on the computer display .
In SOM neural network structure , there is a weight expressing the measure of a link between the input and the output . 
The learning process will adjust these weights based on the training data set . 
The result of this learning process will create the clusters of similar documents in the nodes of SOM output layer . 
The learning pattern will belong to the cluster with minimum distance to this cluster . 
The traditional learning algorithm of SOM neural network is listed as follows :
Step 1 : Randomly initialize the weight of SOM output layer
Initialize N c( t ) ( the radius of neighboring area ) and set time t=1
Step 2 : Present an input vector v( t ) and normalize the input vector v( t )
Calculate the Euclidean distance ( dE  ) from input vector v( t ) to all weight vectors of all nodes in SOM output layer and choose the neuron with the minimum distance from input vector v( t ) to the weight vector ( winner ) as follows :
Where i , j is the valid index which is established base on the size of SOM output layer .
Step 3 : Update the weight of nodes in the neighboring area of winner ( ic , jc ) by using the following formula :
Where γ is a constant determining the learning rate 0 ≤ γ ≤ 1 and </Eq>
Step 4 : Update t = t + 1 , present next input vector and go back to step 2 until satisfying the convergence criteria or exceeding the maximum number of iterations .
A graph G = ( V ,E ) , consists of a set of vertices </Eq> , and a set of edges </Eq> . 
Let Lv and LE be the set of vertex and edge labels , respectively , and let V  : </Eq> be the labeling functions that assign labels to each vertex and edge . 
The size of a graph G , denoted </Eq> is the cardinality of the edge set ( i.e. , </Eq> ) . 
A graph G1 = ( V1 ,E1 ) is a sub-graph of another graph G2 = ( V2 ,E2 ) , denoted </Eq> , if there exists a 1-1 mapping </Eq> , such that </Eq> implies </Eq> . 
Further , f preserves vertex labels , i.e. , </Eq> , and preserves edge labels , i.e. ,</Eq> . 
f is also called a sub-graph isomorphism from G1 to G2 . 
If </Eq> , we also say that G2 is a super-graph of G1 . 
Note also that two graphs G1 and G2 are isomorphic </Eq> . 
Let D be a set of graphs , then we write </Eq> . 
G is said to be a maximal common sub-graph of </Eq> , and </Eq> , such that </Eq> . 
Let D be a database ( a set ) of graphs . 
The support of a graph G in D denoted </Eq> is ratio of number of graphs of D containing graph G and </Eq> . 
Graph G is called frequent if </Eq> , where minsup is a user-specified minimum support threshold . 
Let </Eq> be the set of frequent graphs of D for a given support minsup . 
A graph </Eq> is said to be maximal frequent graph if there exists no graph </Eq> such that </Eq> </CITE> .
The input data of a SOM is a set of graphs representing the documents . 
After the SOM is being trained , the input graphs will be grouped into nodes on the SOM output layer </CITE> .
Each neuron on the SOM layer output is a weighted graph . 
This graph is initialized based on the input value of SOM neural network .
C . Distance between two graphs H Bunke </CITE> proposed a formula for calculating the distance between two graphs . 
Given graph G1 and G2 , the distance of two graphs G1 and G2 denoted as d( G1 ,G2 ) is calculated as follows :
Where “ mcs ” is the maximal common graph and </Eq> is the size of graph G ( number of vertices and edges of graph G ) .
Consider figure 3.a and 3.b as follows :
Figure ( 3.a ) is graph G1 , and figure ( 3.b ) is graph G2  , figure ( 3.c ) is the maximal common graph of graph G1 and G2 . 
The distance of graph G1 and G2 is : </Eq>
We compute mcs(  ) by using SPIN – an algorithm for mining the maximal frequent sub-graph in graph mining </CITE> .
SPIN takes two graphs as input and mines for maximal frequent sub-graphs with 100% support . 
The maximal frequent sub-graph with maximal size is used to compute the size of the maximal common sub-graph in formula ( 2 ) .
In the training process of SOM neural network , we need to adjust the weight of neurons laying in the neighboring area of the winning neurons . 
When using the SOM neural network for clustering the graph , each node in the SOM layer output is a graph , and we call this kind of graph as weighted graph . 
To adjust the weighted graph , H. Bunke </CITE> used the weighted means graph of a pair of weighted graphs . 
Given two graph G1 and G2 , the graph G denotes weighted means graph of weighted graph G1 and graph G2 if there is a number </CITE> such as </Eq> , we have  :
And </Eq> ( 4 )
From formulas ( 3 ) and ( 4 ) , we have </Eq>
To adjust the weight of nodes of SOM output layer , we use formula ( 1 ) . 
We can write formula ( 1 ) as follows : </Eq> ( 5 ) and </Eq> ( 6 )
If we replace G1 by x , G by ynew and G2 by yold , the operator  ” - ” by the distance between two graphs , the formula ( 5 ) and ( 6 ) will be formula ( 7 ) and ( 8 ) as follows : </Eq> ( 7 ) and </Eq> ( 8 )
If </Eq> , the formulas ( 7 ) and ( 8 ) will be formula ( 3 ) , ( 4 ) and graph G is the weighted means graph of graph G1 and graph G2 </CITE> .
From formulas ( 7 ) and ( 8 ) , we have formula ( 9 ) as follows : </Eq>
We used the genetic algorithm </CITE> for seeking the weighted means graph of two weight graphs G1 and G2 with the following steps  :
1 ) Overview of the genetic algorithm Genetic Algorithms ( GA ) are programs that simulate the logic of Darwinian selection . 
GA is a algorithm which makes it easy to search a large search space . 
The general algorithm is as follows  :
Generate initial population .
Assign fitness function to all individuals of population 
Generation = 1 
REPEAT Select individuals from population of current generation
Create new off-springs with crossover operation
Create new off-springs with mutation operation
Compute new fitness for all individuals
Delete all the unfit individuals to give space to new off-springs
Check if best solution is found
generation = generation + 1
UNTIL best solution is found or generation >= MaxLoop
The Maxloop is user defined constant and determine the maximal generation pf genetic algorithm .
2 ) Initilalizing the population
The graph is represented by an adjacency matrix , the vertex set of graphs is </Eq> . 
The chromosome is a set of graphs candidates of weighted means graphs ( set of matrices ) . 
Set of initial graphs are initialized randomly based of graph G1 and G2 .
Crossover operation in this case will be the crossover of two matrices . 
The details of crossover operation are as follows :
a ) Before crossover : </Eq>
b ) After crossover : </Eq>
3 ) Mutation operation
− Select random positions i ,j in matrix representing the selected chromosome .
− Determine the random value of 0 or 1 ( add or delete the edge of graph )
− Assign this random value to aij and aji
4 ) Fitness function
Maximize the following function :
The less value of this function gives the more fitness value of chromosome .
5 ) GA parameters
We used the following GA parameters : GA population size .
Main ideas of documents are the sentences containing as many words determined by the order of occurrence on the weighted graphs . 
Main idea is created based on the weighted graph representing a group of similar documents . 
A typical weighted graph of SOM output layer is shown in figure 4 .
The Precision , Recall and F-measure are used for evaluating the result of clustering . 
The clustering to be created by human experts is called manual clustering . 
We compare the clustering result of documents to be created by our system with the result of manual clustering .
Consider a set of n documents , Let m be the number of clusters to be created from n documents by manual clustering .
Methods Let k be the number of clusters in the clustering result to be created by our system . 
In the evaluation process , we have m ≤ k . 
To evaluate the system , we use Precision , Recall and Fmeasure and calculate them by two methods .
Let </Eq> in figure 5 , cluster mi to be created by manual clustering is A∪B; this cluster contains a+b documents . The cluster ki to be created by our system is A∪C; this cluster contains a + c documents .
Two above clusters have an intersection A which contains common documents of two clusters .
The Precision is a measure of the number of documents that match those in the manual clustering against the number of documents in the system cluster . 
If P=1 then all documents in cluster ki are in cluster mi </Eq>
The Recall between two cluster mi and cluster ki denoted as R ( recall ) and is calculated by formula ( 11 ) . 
If R =1 then all documents in cluster mi are in cluster ki </Eq>( 11 )
The Precision and Recall can be combined to F-Measure .
The F-Measure is calculated by formula ( 12 ) : </Eq> ( 12 )
High value of α gives more influence to recall while low value gives it to precision . 
A common value of α in formula ( 12 ) is 0.5 . 
This formula can be written as : </Eq> ( 13 )
Brew C. </CITE> propose a method to evaluate the clustering result as follows . 
For each cluster in the clustering result to be created by system , we calculate the F-measure comparing with all clusters to be created by manual method . 
Then we select the maximal value of F-measure for this cluster ( each column in table 1 and table 2 ) .
The process is repeated for the remain system clusters . 
High value of the total of F-measure gives the accuracy of clustering system .
The test set contains 500 scientific documents belonging to 5 different topics such as database , data mining , computer network , web programming , and artificial intelligence . 
Each topic has 100 documents . 
The size of SOM output layer is 8x8; The repeated cycle of training algorithm is 5 ,000; The cycle of adjusting the radius of neighboring area is 50 . 
The directed graph is used for representing the documents .
1 ) Method of clustering the vectors
Result : The method of manual clustering has 5 clusters , each of which has 100 documents . 
In this experiment , we used the vector model for representing the documents . 
The number of result clusters is 8 . 
We comparing the results created by vector clustering and those created by manual clustering . 
We use formulas </CITE> for calculating the Precision , Recall , F-measure .
Table 2 provides the results of calculation .
The sum of maximal value of F-measure for vector clustering is 0.32 + 0.34 + 0.54 + 0.43 + 0.43 = 2.06
2 ) Method of clustering the graphs
In this experiment , we use the graph model for representing the documents . 
The number of result clusters is 6 . 
We comparing the results created by graph clustering and those created by manual clustering . 
We use formulas </CITE> for calculating the Precision , Recall , F-measure . 
Table 2 provides the results of calculation .
Discussion : We test our proposed solution with many data sets and calculate the sum of max value of F-measure , we hold that the sum of max of F-Measure for graph cluster is higher PC Man than the sum of max of F-measure for vector clustering . 
This result encourages us to continue developing the method of using the graph for representing the documents .
The sum of maximal value of F-measure for graph clustering is 0.54+0.32+0.68+0.56+0.54=2.64
Comparing to the vector - based clustering algorithm , our proposed algorithm still remains the core parts of the SOM learning algorithm . 
Our proposed algorithm only changes the formula of calculating the distance between the input patterns and the weighted graphs and the adjustment way of weighted graphs .
The time complexity of our proposed algorithm focuses on the time complexity of calculation the distance between two graphs and the calculation of weighted means graph based on the genetic algorithm . 
Figure 6 provides the chart to compare the average processing time of two algorithms with test set containing 100 , 150 , 200 , 250 , 300 , 350 ,400 , 450 , 500 documents . 
The computer to be used for testing is PC Pentium 4 , 3GB with 500 M byte RAM .
The value in this chart in figure 6 holds that our proposed algorithm has average processing time estimated 1.7 times higher than the vector based clustering algorithm . 
However , as we mentioned above the accuracy of our proposed methods is higher than the vector based clustering graph . 
Moreover , it also open a new way to improve the quality of document clustering by using the SOM neural network .
In this paper , we present the result of building a graph - based clustering system by using the SOM neural networks . 
We use the graph model for representing the documents . 
The graph model can represent the structural information of documents such as the semantic relation of words , the position of words in documents , and concepts implicitly present in documents . 
After clustering the document , on the SOM output layer are the weighted graph . 
These weighted graph contain the words which help to choose the main ideas form a of documents . 
We choose the maximal common sub-graph to calculate the distance between graphs . 
The approach of maximal frequent sub-graph with 100% support is used to find the maximal common sub-graph . 
To adjust the weighted graphs on the nodes of the SOM output layer , we use the weighted means graph concept and genetic algorithms . 
We test our proposed methods on the corpus of Vietnamese articles and analyze the results . 
We continue to do the research in calculating the distance between graphs and the way to reduce the time complexity of our proposed algorithm . 
We also study the capability of using the conceptual graph for representing documents and enhancing the richness of document representation model .
Acknowledgment
The authors would like to express our thanks to Ministry of Science and Technology for financial support to Fundamental research project numbered 202806 and the valuable comments of reviewers .
Two-stage Incremental Working Set Selection for Fast Support Vector Training on Large Datasets
We propose iSVM - an incremental algorithm that achieves high speed in training support vector machines ( SVMs ) on large datasets . 
In the common decomposition framework , iSVM starts with a minimum working set ( WS ) , and then iteratively selects one training example to update the WS in each optimization loop .
iSVM employs a two-stage strategy in processing the training data . 
In the first stage , the most prominent vector among randomly sampled data is added to the WS . 
This stage results in an approximate SVM solution . 
The second stage uses temporal solutions to scan through the whole training data once again to find the remaining support vectors ( SVs ) . 
We show that iSVM is especially efficient for training SVMs on applications where data size is much larger than number of SVs . 
On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples , iSVM takes less than one hour to train an SVM with 94% testing accuracy , compared to seven hours with LibSVM – one of the state-of-the-art SVM implementations . 
We also provide analysis and experimental comparisons between iSVM and the related algorithms .
In recent years support vector machine ( SVM ) \CITE has been successfully applied in various machine learning applications .
However , scalability still remains one of biggest challenges for SVM in particular and kernel-based methods in general . 
It is due to the fact that training an SVM requires solving a quadratic programming ( QP ) problem in which , for the worst case , the complexity becomes </Eq> for time and </Eq> for memory requirement , where l is the number of training examples \CITE .
There have been a number of approaches to scalability problem of SVM training . 
Among them , decomposition is the most widely implemented method in various SVM software and libraries , e.g. LibSVM \CITE , SVM light \CITE , CoreSVM \CITE , HeroSVM \CITE , and SimpleSVM \CITE . 
The main idea of decomposition algorithms is to divide training data into two sets : an active working set ( WS ) whose coefficients can be updated , and an inactive set whose coefficients are temporally fixed \CITE . 
The extreme case of this decomposition approach is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that does optimization on a set of only two examples . 
For each optimization loop , SMO scans through the whole training data to find a good pair of vectors , and then updates coefficients of the two selected vectors analytically . 
In order to find a good pair in the next iteration , it is required to update the violation of optimality criteria of all training vectors . 
It is very expensive for applications where the number of updates ( training data ) is huge .
In this paper , we introduce a two-stage incremental WS selection method in training SVMs . 
The proposed algorithm starts from finding an initial SVM solution on a minimum WS ( two vectors from opposite classes in a two-class classification task ) . 
It then iteratively selects one training vector to update the WS . 
The selection of new vectors is divided into two stages . 
In the first stage , only the most prominent vector among a fixed number of sampling data is added to the WS while all others remain in the training data . 
From the second stage , all training examples are checked once again . 
For both stages , each optimality violated vector is used to update the WS and find a new SVM solution . 
We show that with this two-stage WS selection strategy , the proposed iSVM has a linear time complexity in number of training examples and cubic in number of SVs . 
Experiments on large benchmark datasets show that iSVM is very fast when working on applications where number of SVs is much smaller than number of training data . 
On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples , iSVM takes less than one hour to train a SVM with 94% testing accuracy .
With LibSVM \CITE – one of the state-of-the-art SMO implementations , it takes seven hours .
The rest of this paper is organized as follows . 
In Section II , we briefly describe SVM training problems and decomposition algorithms for solving them . 
We introduce iSVM and its complexity analysis in section III . 
In section IV , we discuss the relations between iSVM and related SVM training algorithms . 
Experiments for evaluating iSVM and comparison with other algorithms are reported in section V . 
Section VI is for conclusion .
In support vector learning \CITE , we are given a set of l training examples </Eq> with labels </Eq> . 
The main task of training an SVM is to solve the following optimization problem :
( 1 ) where </Eq> is a kernel function calculating dot product between two vector </Eq> and </Eq> in some feature space ; C is a parameter penalizing each "  noisy "  example in the given training data . 
The optimal coefficients </Eq> will form a decision function :
( 2 ) Problem ( 1 ) involves l variables </Eq> and </Eq> parameters </Eq> . 
The </Eq> number of parameters </Eq> quickly exceeds memory capacity of a normal computer when the number of training examples gets larger than , say , 100 ,000 .
This over demanding in memory requirement causes the main difficulty in training SVMs .
The main idea of decomposition algorithms , e.g. \CITE , is to break down the QP problem ( 1 ) of size l into a series of much smaller QPs , and iteratively perform optimization on these sub-problems . 
In each iteration , decomposition algorithms divide l training examples into two categories : a set of active vectors W corresponding coefficients </Eq> can be updated , and a set of inactive vectors corresponding coefficients are temporally fixed . 
Active vectors are updated by some optimization method to minimize objective function L on W .
After that fixed vectors are checked and used for updating the working set W . Optimization loop will stop when all optimality conditions are satisfied . 
The extreme case of the decomposition method is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that optimizes a set of only two vectors . 
The power of SMO resides in the fact that updating scheme could be done analytically . Call </Eq> and </Eq> be chosen vectors , then the best new values of </Eq> and </Eq> in terms of reducing best the objective function L in ( 1 ) are ( ignoring the box constraint </Eq> ) :
Input : Training data </Eq> Initialize a feasible solution 
Set iteration t = 0
While StoppingCondition is not satisfied
Select a pair of vectors </Eq>
Update </Eq> analytically
Update violation states </Eq>
Set t = t + 1
Endwhile
Output : Coefficients ( 3 ) </Eq> . 
This updating scheme leads to the reduction of objective function L an amount of ( 4 )
Based on this reduction rate , different heuristics have been proposed to select the best pair of vectors </Eq> with a reasonable cost </Eq> .
The analytical solution property of SMO makes it become a core optimizer for many SVM implementations . 
In TABLE I we describe main procedures in an SMO implementation .
The most expensive procedure of the SMO is updating violation of optimality criteria </Eq> of all training vectors in step 5 . 
This calculation is used to select the best pair of vectors in the next iteration , and it is required for every training example .
Step 5 becomes very expensive when the number of updates ( or training data ) is huge . Moreover , as only SVs ( training vectors </Eq> with corresponding coefficients </Eq> will contribute to form the final decision function ( 2 ) , step 5 of SMO can be very inefficient when many training examples are not SVs . 
To improve the efficiency , shrinking technique \CITE can be applied to remove non-support vectors . 
However , there has been no way to determine whether a training example is a SV or not from the beginning .
In this section , we introduce an incremental strategy for selecting SVs . 
The main idea is temporal SVM solutions can be used to determine good candidates of SVs , and optimization process is performed only on a small set of selected candidates . 
The following subsections describe in detail the main steps of our proposed algorithm iSVM in TABLE II .
Input : Training data </Eq>
Select the first working set </Eq> . 
Find the first temporal solution </Eq> . 
Set </Eq>
Set iteration t = 0
While StoppingCondition is not satisfied
Select one vector </Eq> in T
Update </Eq>
Find new solution </Eq> on </Eq>
Set t = t + 1
Endwhile
Output : Coefficients </Eq>
As SMO is used for optimization on the selected working set </Eq> in step 0 and step 5 , a minimum set of two vectors are selected to build the first working set </Eq> . For a two-class classification task , this is simply selecting any two training vectors from two opposite classes ( in our implementation they are first vectors belonging to the positive and negative classes in the given training data ) . 
Compared with previously proposed methods , this initialization step is simpler . 
In \CITE the authors suggested to select randomly a set of p training instances , where p is a training parameter . 
In \CITE , two closest vectors were recommended to form the first working set ( for the best reduction rate in ( 4 ) ) . 
However , our preliminary experiments indicated that the result of iSVM does not depend much on this initialization scheme .
There have been different schemes proposed to update the WS in the common decomposition frame work . 
In fact , this is a distinctive step for each algorithm . 
Different updating strategies will produce different results in terms of convergence speed and final solution . 
In iSVM we propose a two-stage process for expanding and updating the WS . 
In the first stage , iSVM tries to find a good approximation of SVM solution as quickly as possible . 
It then scans through the whole training data once again to examine all the remaining vectors one-by-one .
Potential SVs will be used to update the WS and find a new and better SVM .
1 ) Re-sampling Selection : In support vector learning , if we know in advance which training example will be SV , we can remove all non-support vectors without changing the optimal solution . 
An effective non-support vector removal strategy will guarantee an efficient algorithm . 
We do this by a twostage data processing procedure .
In the first stage , step 3 of iSVM examines only a small number training examples and selects the most prominent vector to add to the WS . 
The selection is based on violation of optimality criteria of a training vector with respect to a temporal solution found in previous iteration . 
At iteration t , </Eq> is selected based on the following criterion : ( 6 ) is the temporal solution </Eq> at iteration t . 
The selection heuristic ( 5 ) is exactly the maximal violating heuristic that has been used by SMO \CITE and other early decomposition implementations .
The difference is that SMO updates all </Eq> and then scans through all of them to select the best pair . 
iSVM uses temporal solutions St to examine a fixed number of training examples . 
Difference in complexity between the two strategies will be analyzed in more detail in subsection D .
In the first stage , only the most prominent ( in terms of optimality violation ) vector is removed from T while all other vectors remain in the training data . 
They can be re-examined in the next iterations and/or in the second phase . 
There are two reasons for the re-examination . 
Firstly , only the best vector is added to the WS , not every temporally violated vector . 
Secondly , temporal solutions are still not close enough to the optimal solution ( as only a small number of vectors are examined ) . 
Removing training examples from the very beginning might result in the mistaken removal of the true SVs .
The first stage will finish when we are sure to some extent that </Eq> is a good approximation . 
In iSVM , we use the following heuristics for ending the first stage : i ) None of N randomly selected vectors violates optimality criteria , or ii ) Size of </Eq> is bigger than a predefined number ( 1 ,000 in our experiments ) , or iii ) All training vectors are examined once in average .
The first condition has been used in various situations , including kernel matrix approximation \CITE and CoreSVM \CITE . 
This heuristic is based on the fact that with a sample size of N = 59 , we still can catch one among 5% most violating vectors \CITE . 
The second and third conditions mean that the temporal solution will be considered stable and close enough to the optimal solution when a big number of training examples are examined .
After a good approximation has been achieved , we can switch to the second stage to remove non-support vectors without affecting much the final solution .
2 ) Final Scanning : Based on the assumption that phase one produces a good approximate solution , phase two examines all the training examples remaining in T one-by-one . 
If vector </Eq> violates optimality criteria with respect to temporal solution </Eq> , then it is immediately used to update </Eq> to form a new solution . 
Otherwise , it is removed from training data T .
The algorithm will stop when all training examples in T are examined .
The SMO is used to minimize the objective function L on the selected set of vectors </Eq> . 
It is very efficient because </Eq> is usually much smaller than the whole training data . 
Moreover , SMO can start optimization on Wt from </Eq> - the optimal solution on </Eq> in previous iteration . 
The difference is only about one newly added vector . 
This makes SMO converge very fast .
In this section we analyze the computational complexity of the proposed iSVM algorithm . 
At each iteration t , step 3 takes time </Eq> to examine one training example . 
In the first phase , each training example is examined at most once ( when the stopping condition iii ) is applied ) . 
The second phase scans the training data once more . 
Thus , each training example is examined at most twice . 
Totally step 3 takes time </Eq> .
Theoretically solving a QP problem of size </Eq> in step 3 takes time </Eq> . 
However , solution </Eq> at iteration t – 1 is used as an initial point , then step 5 requires only time </Eq> per iteration ( in fact , it can be done in </Eq> by an efficient updating procedure \CITE ) . 
Totally , step 5 takes time </Eq> .
From the second phase , the shrinking technique is applied ( a non support vector in </Eq> is replaced by a new vector found by step 3 ) , then size of the final working set approximates the number of final support vectors . 
In total , the time complexity of iSVM is </Eq> , which is linear with number of training examples l and cubic with number of support vectors .
Time complexity of the SMO algorithm described in TABLE I is </Eq> where </Eq> is the number of SMO iterations . 
In iSVM , SMO is used to solve the QP on a small set of selective training examples . 
From the complexity analysis above , we can see that iSVM has an advantage over the traditional SMO implementation when the number of the SVs is much smaller than the data size .
iSVM belongs to the decomposition family of SVM training algorithms . 
In this section , we discuss properties of iSVM and its relation to conventional methods .
In the initialization step , iSVM selects any two vectors from opposite classes ( for a two-class classification task ) .
Based on calculation ( 2 ) , a closer pair of vectors will produce a better reduction in objective function L . 
However , our preliminary experiments indicate that final solutions are not affected much by this initialization scheme which has been used by CoreSVM \CITE or SimpleSVM \CITE . 
Other initialization strategies include selecting randomly a set of p vectors \CITE , or using all training data from the beginning \CITE .
For updating the working set , several algorithms share the same way of adding only one vector to the WS in each optimization loop , e.g. SimpleSVM \CITE , CoreSVM \CITE .
Different selection criteria have been proposed , including optimality violation \CITE , probabilistic estimation \CITE . 
iSVM differs from others in its two-stage strategy . 
In the first stage , temporal solutions are not good enough to justify which training example is surely an SV or not , so iSVM re examines them in the second phase . 
Note that the working set grows from a minimum size ; thus , the first phase runs very fast because the size of the working set </Eq> is small and only the best among N examined vectors is added to the working set . 
It is not clearly described in \CITE and \CITE that training examples are re-examined or not . 
If they are removed from T too early from beginning , then it is highly likely many good training examples ( or SVs ) might be removed . 
In contrast , if they remain in T all the time and re-examined many times , then the computation is not efficient .
Comparing with other approximation methods , iSVM uses a rather simple stopping condition . 
In our point of view , CoreSVM uses a looser condition : none of N sampled training data violates the optimality condition with respect to temporal solution at iteration </Eq> . 
In the experiment section , we will show that using this stopping condition will lead to trained SVMs with smaller number of SVs , faster training time , but bigger variation in predictive performance .
In this section , we describe our experiments to evaluate iSVM and comparisons with other SVM training algorithms .
We select four datasets from different domains : web page categorization from UCI machine learning repository ( " Web " ) , text-decoding used in IJCNN 2001 conference competition ( " IJCNN"  ) , extended USPS hand written digit recognition data for discriminating '0' and '1' ( " zero-one"  ) , and KDD-CUP 1999 network intrusion detection datasets used in the KDD 1999 conference competition ( " KDD-CUP99"  ) .
All of the datasets summarized in TABLE III have nearly or more than 50 ,000 training examples . 
All of our experiments were conducted on a PC Windows machine with a 3GHz CPU and 2GB RAM memory .
In the first experiment , we compare training performance of iSVM with LibSVM \CITE – one of the best SMO implementation , CoreSVM \CITE – a recent proposed algorithm that has achieved a remarkable performance on the KDD-CUP 1999 dataset . 
Comparison criteria include training time , number of support vector , and testing accuracy . 
Parameters were chosen for achieving good accuracy on testing data : Gaussian kernel </Eq> with </Eq> for " Web " , </Eq> for " IJCNN " , </Eq> for " USPS zero-one " , </Eq> for " KDD-CUP"  . 
As both iSVM and CoreSVM \CITE use probabilistic trick to speedup in step 3 training results are random variables .
We conduct this experiment ten times and estimate statistics of these variables ( for CoreSVM we randomly mix the original data ten times and run training program on these mixed data ) .
From experimental data reported in TABLE IV we can see that iSVM runs much faster than LibSVM on the " zero-one " and " KDD-CUP99 " where the number of SVs is much smaller than number of training data ( 0.45% and 0.01% correspondingly ) . 
This shows the advantage of the two-stage incremental WS selection strategy . 
On the KDD-CUP 1999 data , CoreSVM has an incredible training time : two seconds in average for training on a nearly five millions training examples . 
In the next experiment , we investigate and analyze more to show that iSVM with an early stopping condition can also achieve this training time performance . 
However , the cost is the huge variation of trained machines .
In the second experiment , we try iSVM with different stopping conditions on the KDD-CUP 1999 dataset . 
The first one is right after the first stage finished , or when there is no vector out of 59 randomly selected training data violates the temporal optimality conditions . 
Other stopping conditions are based on the total number of examined examples ( by the second stage ) : 10% , 20% , 40% and 80% of the whole data .
This experiment is also conducted ten times to have estimation of training times and testing accuracies . 
As we can see in Figure 2 , with an early stopping condition , iSVM runs faster but produces SVMs with higher variation in predictive accuracy ) .
Especially , the first stage finishes after a very small number of iterations ( 32 in average ) . 
It means that the trained machines are determined by a maximum of only 1900 training examples , corresponding to 0.04% of the whole data . 
This number explains why iSVM takes only 2.8 seconds to train on the KDD-CUP 1999 data with nearly five million records .
From our point of view , the same phenomenon happens for CoreSVM . 
The huge variation of the trained machine indicates that i ) is a too loose stopping condition for the KDD-CUP 1999 data case . 
We draw variations in predictive accuracy and average training time of iSVM and CoreSVM in Figure 2 for comparison .
Note that there has been different approaches tackling the KDD-CUP 1999 problem . 
In TABLE V we describe performances produced by different methods , including data random sampling , active SVM learning \CITE , clustering-based \CITE , CoreSVM \CITE , LibSVM \CITE , and iSVM . 
iSVM achieves superior generalization performance and faster than SMO , random selection , cluster-based , and active learning .
We have introduced a new incremental algorithm for training SVMs . 
iSVM differs from other methods in its twostage strategy to process training data . The first phase aims at finding a good approximate SVM solution as quickly as possible . 
The second phase uses temporal solutions to find out the remaining SVs . 
The analysis and the experimental result indicate that iSVM has advantage over conventional SMO implementation on applications where number of training examples is much larger than number of SVs . 
Training SVMs with large number of SVs is our research issue in the future .
Requirements Engineering in a Model-based Methodology for Embedded Automotive Software
This paper examines the requirements engineering in a model-based methodology for embedded automotive software .
The methodology relies on two standard languages : EAST-ADL for automotive architecture description and SysML for systems modeling . 
The requirements engineering in the methodology describes phases of elicitation , modeling , traceability , verification and validation . 
It is illustrated by applying in a case study of the knock controller – a part of the engine management system .
Modern car is now equipped with more and more functionalities dependent on embedded electronics , ranging from powertrain and chassis control to body comfort and infotainment . 
These functionalities are distributed over a networked Electronic Control Units ( ECU ) . 
The size and complexity of software for these embedded electronics increase rapidly with its cost rising from 10% of the overall cost in 1970 to 40% in 2010 . 
Actually , 90% of innovations in the automotive industry concern embedded electronics and 80% among them are software \CITE .
A big challenge in developing automotive software concerns its quality . 
Automotive systems are safety-critical systems whose failures may cause severe damage or loss , so software errors led directly to car recalls . 
According to the report \CITE , one-third of the recalls in recent years are caused by software errors . 
More effort is needed on software’s verification and testing .
Another challenge concerns the reduction the development time . 
The automotive market is shared by manufacturers , suppliers and tool vendors , and all need shorten processes which favor the exchangeability among them and the reuse of software in different product lines . 
They also need to follow requirements along the development , from the specification to the design and the code , to anticipate and communicate its changes throughout the team .
A new paradigm software development is formed in this context to face these challenges . 
In Europe , automotive actors cooperated to establish mutual communication standardized for software development .
The recent result of this corporation is EAST-ADL \CITE
an architecture description language dedicated to automotive software . 
It provides a means to describe the functionality of a vehicle , from high level requirements to implementation details . 
It focuses on structural aspects , leaving behavioral aspects for existing tools . EASTADL is based on Unified Modeling Language 2 \CITE but has automotive-specific constructs and semantics in order to make system models unambiguous , consistent and exchangeable .
Model-based development ( MBD ) is a preferred approach for automotive software because it improves the specification , design , and implementation phases . 
MBD benefits from the Systems Modeling Language ( SysML ) \CITE , another recently defined by Object Management Group ( OMG ) . 
SysML is used to model the requirements and physical parameters of automotive systems . 
SysML has also the capacities of facilitating the design and verification .
MeMVaTEx research project \CITE develops a modelbased methodology that emphasizes the requirements validation and traceability . 
The methodology investigates two languages : EAST-ADL for automotive architecture description and SysML for system modeling . 
The methodology describes a step-by-step process with appropriate tools supporting each step . 
It aims to give a seamless solution for industrial use . 
An automotive case study shows that the engine knock controller – a part of the Engine Management System ( EMS ) - is used to illustrated the methodology .
This paper shows the requirements engineering in the methodology . 
It describes phases of elicitation , modeling and traceability , verification and validation , and accompanied tools .
The methodology concerns other aspects such as safety , real-time , variability , or model transformation that will not be addressed here . 
Our related works \CITE can be found on the website .
The remainder of the paper is organized as follows : Section II and Section III discuss how EASTADL and SysML are used architecture description and system modeling .
Section IV describes the knock phenomenon and controller . 
Section V presents the requirements engineering in three main phases : elicitation , modeling and traceability , verification and validation . 
We describe the framework used for engineering tools and with using these tools . 
Section VI concludes the paper .
EAST-ADL \CITE stands for Electronic Architecture and Software Tools-Architecture Description Language . 
The language is defined in the Embedded Electronic Architecture ( EEA ) project , one of many project initiated by Information Technology for European Advancement ( ITEA ) . 
Important car manufactures , suppliers , tool vendors , and research institutes in Europe take part in this project to give birth to EAST-ADL . 
This language is intended to support the development of automotive embedded software by capturing all the related engineering information , including software , hardware , and its environment .
EAST-ADL reflects different views and details of the architecture and is structured in five abstraction layers as illustrated in Figure 1 .
These layers are :
- Vehicle level ( VL ) describes electronic features from the driver’s point of view .
- Analysis level ( AL ) gives abstract functional definition of features in system context .
- Design level gives ( DL ) detailed functional definition of software including elementary decomposition .
- Implementation level ( IL ) describes reusable code and system configuration for hardware deployment .
- Operational level ( OL ) supports final binary software deployment .
EAST-ADL has just been revised in the ATESST project \CITE .
Version 2 of EAST-ADL now links directly to AUTomotive Open System Architecture ( AUTOSAR ) , another initiative from the automotive industry which standardizes software architecture and interfaces for ECUs . 
Essentially , AUTOSAR’s scope concerns the last two Implementation and Operational levels of the EAST-ADL . 
The project ATESST3 tries to the harmonize EAST-ADL 2 and AUTOSAR with summaries can be found in \CITE .
Since its adoption in 1997 by Object Management Group ( OMG ) to the last version called UML 2 \CITE in 2007 , UML is successfully used by software engineers for modeling their software . 
Web applications and banking transactions benefits particularly from UML . 
However , UML lacks important elements to be used by system engineers to modeling their systems , such as a means for modeling requirements , physical constraints among components , or internal transactions between subsystems . 
Many specific profiles were invented , giving partly solutions for some problems .
System Modeling Language ( SysML ) is an OMG standard , developed to fill the semantic gap between systems , software , and other engineering disciplines . 
By definition , OMG SysML enables system engineers in different domains to analyze , specify , design , and verify their complex systems , enhancing systems quality .
Technically , SysML reuses a subset of UML 2 , adding new diagrams and modifying others . 
It includes diagrams that can be used to specify system requirements , behavior , structure , and parametric relationships , known as the four pillars of SysML . 
Of the four pillars , only requirements and parametric diagrams are entirely new . 
Figure 2 illustrates the complete SysML diagrams . 
More descriptions and applications of SysML diagrams can be found in \CITE .
The project MeMVaTEx pays particular attention to the requirement diagram . 
This diagram represents text requirements in the model . 
A requirement may have links to other requirements or to modeling actefacts via a set of four new stereotyped dependencies ( see Figure 3 ) .
- " derive " indicates the derivation of requirement from other requirements .
- " refine " indicates that an element is a refinement of a textual requirement .
- " satisfy " shows the satisfaction of requirement by design .
- " verify " shows the link from a test case to the requirement it verifies .
With these new stereotypes , engineers can keep track of any requirement from the the phase of specification in terms of how it is broken into sub-requirements and which design blocks satisfy requirement or which parts of code are concerned . 
With thousands of requirements may change during the development of an ordinary automotive project \CITE , the new capacity of SysML helps maintaining requirements traceability .
New SysML stereotypes introduce the link between requirements and test cases that are crucial for requirements verification and validation because under strict regulations such as IEC 61508 \CITE and the upcoming ISO 26262 \CITE , each requirement must be tested by test cases .
A test case is a general mechanism to represent any of the standard vertification methods for inspection , analysis , demonstration , or testing . 
SysML can represent test cases and link them to their corresponding requirements or use cases . 
A test case model , sus as Interaction , Slate Machine , Sequence , and Activity Diagram , can be either operational or behavioral .
These new capacities of SysML will be detailed through the case study in Section V .
In a four-stroke gasoline engine , air and vaporized fuel are drawn in in the first stroke ( intake ) . 
In the second stroke , fuel vapor and air are compressed and ignited ( compression ) .
Fuel combusts and piston is pushed downwards in the third stroke ( combustion ) and exhaust is driven out in the last stroke ( exhaust ) . 
The cycle can be seen in the left of the Figure 4 .
In practice , ignition usually occurs before the end of the second stroke in order to maximize power fuel economy and minimize exhaust emission . 
When the temperature and pressure of the unburned air/fuel mixture exceeds a critical level , a second auto-ignition occurs as shown in the right picture in Figure 4 . 
The two-flame crossing produces a shock wave with rapid increase in cylinder pressure . 
The impulse caused by the shock wave excites a resonance in the cylinder at a characteristic frequency .
Damages to piston , ring , and exhaust valves could ensue if sustained heavy knock occurs .
An appropriate anti-knock control , represented in Figure 5 , is applied to each cylinder at every engine cycle at any predetermined speed . 
A knock control system consists of noise sensors that monitor the noise level , and a controller that computers corrections based on the feeback from noise sensors during the in-cylinder combustion process . 
Spectral analysis techniques are used to allow the engine controller \CITE to detect knocks and advance or retard the ignition .
We use the V-model in the Figure 6 to illustrate the requirements engineering phase by phase . 
It begins with the requirements elicitation from the specification . 
Then requirements are represented in models from architecture level to design level down to the code . 
Verification and validation ( V&V ) are present along the requirements engineering , showing V&V activities in each phase and for each requirement .
This phase consists of a list of requirements that can be exploited during the next phases . 
System engineers , safety experts , and time are needed to build a complete and consistent list of requirements . 
Most of the project’s failure is due to insufficient attentions in this phase , as reported in \CITE .
In the MeMVaTEx project , requirements are classified by EAST-ADL levels . 
At each level , requirements are numbered and structured in functional ( F ) and non-functional ( NF ) . 
Nonfunctional requirements are classified by categories such as performance ( P ) , safety ( S ) and availability ( A ) . 
Note that the respects of regulation in automotive domains introduce safety requirements at each level , resulting more complexity in the design and test . 
It also led us to extend SysML Requirement stereotype to a particular MeMVaTEx Requirements stereotype
Table I gives examples of requirements of the knock controller . 
Requirements are actually stored in tabular applications such as Word or Excel .
The SysML Requirement stereotype as defined by the standard contains a description Text , an identifier ( Id ) and links to other requirements , design elements , and test cases for each requirement ( see Figure 7 , left ) . 
When taking into accounts other aspects of analysis , verification , and validation , this definition is not detailed enough . 
In order to better support the requirements engineering , we have interest in extending this SysML Requirement stereotype by adding new fields .
These fields are described in detail in \CITE . 
We call the new stereotype MeMVaTEx Requirement ( see Figure 7 , right ) .
This phase consists of selecting requirements from an upper level and links them to one or many requirements from the lower levels using one of the four stereotypes defined above . 
Doing that correctly guarantees the bidirectional traceability from requirements to design and code . 
We show an example of requirement modeling from the Vehicle level to design level in Figure 8 . 
Requirements are classified by EAST-ADL 2 levels .
In the diagram , the requirements traceability from Vehicle Level to Design Level is shown : AL-F-12 is a functional requirement at the Analysis Level . 
It is derived from the requirement VL-F-9 at Vehicle Level , and then refined to DLF-7 at Design Level . 
The three requirements are respectively satisfied by KnockCorrection , EngineControl , and ThresholdCalculation blocks 
The basic design of KnockFunction block is sketched in a Block Definition Diagram in the Figure 9 . 
It show blocks involved and its item flows . 
Each block in the KnockFunction can be detailed by using Internal Diagram Block ( IBD ) .
The V&V is an important phase in software development . 
V&V activities concern two aspects :
- Verification of the realization , i.e. " Did we build the product right? " 
It is the analysis of the works that have been done , including general document analysis , code inspection and review , unit and integration testing .
- Validation of the application , i.e. " Did we build the right product? "
This is a test phase whose objective is to show that intended services are fulfilled . This test phase is realized on the product .
In the MeMVaTEx project , it is needed that V&V activities must link to and test each requirement as requested by safety regulations . 
For example , the test case represented by an activity diagram in Figure 10 is intended to test such requirement as . 
In this case , the internal structure of the capturing block and the way it works may be known to the tester . 
This kind of test is called white-box testing .
There are also functional requirements such as the AL-F-10 .
In this case , the tester may have no knowledge of the internal structure of the knock control block . 
This kind of test is known as black-box testing . 
Input data are sent to the structure and the output are compared to the expected output to giving the verdict . 
This test in Figure 11 .
Tools are listed in reference to the V-model . 
On the left branch , requirements management ( RM ) tools like Telelogic DOORS \CITE , IBM RequisitePro \CITE , or TNI Reqtify \CITE are used in projects with a large number of requirements . 
They have the capacities managing tracing requirement , and support the team’s corporation .
Major modeling tools such as Telelogic Rhapsody \CITE , IBM Rational \CITE , or ARTiSAN Studio \CITE support UML/SysML and have the capacities to import specific profiles . It can also export models into an interchangeable format .
Simulink is a prime tool at the implementation level .
Simulink gives the most details descriptions of a functional block . 
Simulink and its tool suite can generate code and test cases , and verify the design by simulation .
On the right branch , the validation can be reinforced by running software on simulator ( software-in-the-loop , SIL ) or by injecting code on dedicated hardware then running simulation ( hardware-in-the-loop , HIL ) \CITE . 
Finally , a prototype testing validates the product . 
The validation is an enterprise and proprietary solution .
In the MeMVaTEx project , there are about many hundreds of requirements for the knock controller . 
It can be managed using only Office applications like Word and Excel . 
The use of RM such as Reqtify or DOORS is planned for future use when the number of requirements is big enough .
Modeling is done using ARTiSAN Studio . 
For this particular purpose , we need the EAST-ADL profile and MARTE \CITE , an-other UML profile for real-time modeling . 
These profiles are imported into ARTiSAN Studio . 
ARTiSAN can be connected to RM tools to import requirements and export traceability or requirement tables as seen in Figure 13 .
At the implementation level , ARTiSAN introduces an integration with Simulink models that will give systems engineers the ability to define and simulate function block diagrams in Simulink and export them into a SysML model in ARTiSAN for ongoing development and maintenance .
For now , test cases are generated manually and the use of many validation tools is under consideration .
The actual context in developing software for embedded electronics raises challenges of managing the complexity of software while still guaranteeing the quality and productivity .
The automotive industry introduces many standards as a base from which automotive actors will compete on implementing software using proper processes and methodologies . 
This paper presents requirements engineering in a model-based methodology proposed by the MeMVaTEx project .  
In order to facilitate the requirements validation and traceability . 
The methodology is structured by EAST-ADL abstraction levels and benefits from the SysML systems modeling . 
By conducting a case study , we demonstrate the engineering of requirements through different phases and suggest tools for each phase .
Software using these standards is planned to be implemented in real cars from 2010 \CITE . 
Processes and tools for software may change or emerge by then . 
MeMVaTEx methodology is not definitive but open for changes and evolution before a seamless solution is reached .
﻿<document>
Incorporating Statistical Background Model and Joint Probabilistic Data Association Filter into Motorcycle Tracking
Multi - target tracking is an attractive research field due to its widespread application areas and challenges .
Every point tracking method includes two mechanisms : object detection and data association .
This paper is a combination between a statistical background modeling method for foreground object detection and Joint Probabilistic Data Association filter ( JPDAF ) in the context of motorcycle tracking .
A major limitation of JPDAF is its inability to adapt to changes in the number of targets , but in this work , it is modified so that we can successfully apply JPDAF with known number of targets at each time instant .
The experimental system works well with the number of targets less than 10 / frame and be able to self-evolve with gradual and " once-off " background changes .
Motion understanding is an essential function of human vision . 
Consequently , object tracking takes the crucial role in computer vision .
Multi - target tracking has widespread applications in both military ( air defense , air traffic control , ocean surveillance ) and civilian areas (  for automatical surveillance demands in public or secret places ) , especially when human labour becomes more and more expensive .
Object tracking , in general , is a challenging problem .
Its complexities arise due to the following factors : loss of information caused by projection from 3D to 2D space , complex object motions , complex object shapes , partial and full object occlusions , scene illumination changes , and realtime processing requirements .
There are three main categories of object tracking </CITE> : point tracking , kernel tracking , and silhouette tracking .
While kernel and silhouette tracking concern object shapes , point tracking considers an object as a
point and just focuses on its position and motion , which can be represented by state vector .
Filtering is a class of methods that is suited for solving the dynamic state estimation problems of point tracking .
In multi-target tracking , we have a task of finding a correspondence between the current targets and measurements , named data association .
Data association is a complicated problem especially in the presence of occlusions , misdetections , entries , and exits of objects .
There are many statistical techniques for data association </CITE> , among them , Joint Probabilistic Data Association ( JPDA) is the method that aims to find a correspondence between measurements and objects at the current time step based on enumerating all possible associations and computing the association probabilities . It is a widely used technique for data association (  </CITE> , </CITE> ) .
However , to have a good JPDA filter ( JPDAF ) , it is required to have accurate measurements . That means we need to have good object detection results .
Every tracking method requires an object detection mechanism . There are those which just need the detection at the first time objects appear , while the others need it in every frame . Point tracking belongs to this type .
One effective way for foreground object detection is to give an accurate background model .
Recently , L . Li et al proposed a foreground object detection method by statistical modeling of complex backgrounds </CITE> .
This work used a Bayesian framework for incorporating three types of features , i.e. , spatial and temporal features into a representation of complex background containing both stationary and nonstationary objects .
With the statistics of background features , the method is able to represent the appearances of both static and dynamic background pixels and self - evolve to gradual as well as sudden " once - off " background changes .
Taking advantage of the excellent object detection results from this method , this paper employs JPDAF for vehicle tracking in the motorcycle lane .
A major limitation of JPDAF is its inability to adapt to changes in the number of targets , because it is confused between a measurement originated from a new object appearance and a false alarm .
However , in the context of motorcycle surveillance , we has proposed a strategy to detect new objects entering and objects leaving the observation area , so that we can successfully apply JPDAF with known number of targets at each time instant . 
The experimental system has good results with the number of targets less than 10 / frame , including results detecting and tracking the wrong-wayed motorcycles .
Motorcycle tracking in particular and traffic tracking in generral is an interesting but challenging application .
Its main difficulties can be enumerated as the severely occlusions when traffic density is high ( especially in rush hours ) , the shadows of big vehicles , and the real-time processing demand of a traffic surveillance system .
This paper is the next step ( after </CITE> ) to find the most satisfying approach for automatical traffic surveillance in big cities of Vietnam .
The rest of this paper is organized as Section II is the main ideas for statistical modeling of complex background proposed in </CITE> . Section III reviews the background of JPDAF , a complete algorithm and experimented results on simulated data of JPDAF are presented at the end of this section , section IV is the combination of statistical background model and the modified JPDAF so that they can be applied in the motorcycle tracking situation . The experimental results of this combination are submitted in section V , and section VI presents our conclusion .
Let </Eq> be a pixel in a video frame at time </Eq> with its Decartes co-ordinate , </Eq> be the feature vector extracted at </Eq> .
Then using the Bayes formula , we can determine the probability that </Eq> belongs to a background given </Eq> as follow : </Eq> ( 1 ) where </Eq> implies that </Eq> belongs to the background .
Similarly , the probability that </Eq> belongs to a foreground object given </Eq> is : </Eq> ( 2 ) where </Eq> refers that </Eq> is a foreground point .
According to Bayesian decision rule , </Eq> will be classified as a background point if : </Eq> ( 3 )
Undergoing some transformations , ( 3 ) is equivalent to : </Eq> ( 4 ) where </Eq> is the probability that </Eq> is classified as the background , </Eq> is the probability that </Eq> is observed at </Eq> , and </Eq> is the probability that </Eq> is observed when </Eq> has already been classified as the background .
Thus , we can use </Eq> , </Eq> and </Eq> , which will be modeled and estimated based on statistics in subsection B and C , to judge whether a point comes from a background or a foreground .
To estimate </Eq> , </Eq> and </Eq> , we need a data structure to take into account the statistical information relevant to feature vector </Eq> at </Eq> over a sequence of frames .
Each feature type at </Eq> has a table of statistics defined as : </Eq> ( 5 ) where </Eq> grasps the </Eq> at time </Eq> based on the classification results at </Eq> up to time </Eq> , and </Eq> takes note the statistics of the </Eq> feature vectors which have the highest frequencies at </Eq> , each </Eq> contains : </Eq> ( 6 ) where </Eq> is the dimension of </Eq> .
In table </Eq> , the </Eq> are kept sorting in descending order with respect to </Eq> , the frequence of </Eq> .
Then , the first </Eq> members in </Eq> will be used to estimate </Eq> , </Eq> and </Eq> in subsection C .
Another important issue in background modeling is feature selection .
Three types of features (namely spectral, spatial and temporal features) are combined for complex background modeling. 
1 ) Feature selection for static background pixels : due to the constancy in shape and appearance of a pixel coming from a static background object , spectral and spatial features , in this case being its color and gradient , are exploited .
Let </Eq> be the color vector and </Eq> be the gradient vector of a pixel </Eq> , then we respectively need two tables </Eq> and </Eq> to make them learned .
Because two types of features are used , the decision rule in ( 4 ) must be modified with the assumption that color and gradient vectors are independent : </Eq> ( 7 )
With color and gradient features , we need a quantization measure that is less sensitive to illumination changes , so a normalized distance measure based on the inner product of two vectors is adopted </CITE> : </Eq> ( 8 ) where </Eq> , </Eq> and </Eq> are identified with each other if </Eq> .
2 ) Feature selection for dynamic background pixels : as for a dynamic background object , its motion is usually in a small range ( so that it is still referred as a background ) and has a period, for example, waving tree branches and their shadows .
Hence , the color co-occurrence feature is used to take advantage of these properties .
Let </Eq> and </Eq> be the color features at time </Eq> and </Eq> at pixel </Eq> , then the color co-occurrence vector at time </Eq> and pixel </Eq> is
defined as </Eq> .
In this case , another distance measure is used : </Eq> ( 9 )
So far , we have already had a data structure for statistics , now for the procedure of feature learning .
There are two kinds of background changes , so we will have different learning strategy for each one .
1 ) Gradual background changes : once we have the classification result at pixel </Eq> ( subsection D ) and time </Eq> , its statistics at the next time instant will be updated as follows : </Eq> ( 10 ) where </Eq> , </Eq> is the learning rate .
If </Eq> is classified as background point at time </Eq> , then </Eq> , else </Eq> .
If the input feature vector </Eq> is identified with </Eq> then </Eq> , otherwise , </Eq> .
Besides , if there is no </Eq> in table </Eq> identified with </Eq> , the last component in </Eq> will be replaced by new one : </Eq> ( 11 )
2 ) " Once - off " background changes : an " once - off " background change occurs when there is a suddenly change in illumination , or when a moving foreground object stops and becomes a background instance, or when a background becomes a foreground suddenly .
When this happens , we have : </Eq> ( 12 )
Or </Eq> ( 13 ) where M is a high percentage threshold ( 80 % ~ 90 % ) .
Thus , ( 13 ) can be considered as a condition to check if an " once - off " background change is happening .
In that case , the statistics of the foreground should be turned to background statistics : </Eq> ( 14 ) for </Eq> .
This learning process is also proved that </Eq> will converge to 1 as long as the background features are observed frequently </CITE> .
1 ) Change detection : In order to have a proper feature selection as mentioned in C , we need to know whether a pixel is static or dynamic .
Therefore , color - based background differencing and interframe differencing methods are applied to detect changes .
Background differencing calculates the difference between background </Eq> and input frame , while interframe differencing performs the same work on consecutive frames .
Let </Eq> and </Eq> be the background difference and interframe difference respectively .
If </Eq> and </Eq> , pixel </Eq> is referred to nonchange background point .
If </Eq> , </Eq> is classified as dynamic point , then color co - occurrence features are used for background / foreground classification , otherwise , </Eq> is a static point , so color and gradient features are used in the next step .
2 ) Background / Foreground classification : Let </Eq> be the input feature at pixel </Eq> and time </Eq> .
The probabilities are estimated as follow : </Eq> ( 15 ) where </Eq> , </Eq> is the set of </Eq> that are identified with </Eq> : </Eq> ( 16 )
If there is no </Eq> identified with </Eq> , </Eq> and </Eq> .
As saying above , if </Eq> is a static pixel , we will have </Eq> and </Eq> , thus , </Eq> and </Eq> are used as their tables of statistics .
After calculating the probabilities as ( 15 ) , ( 7 ) is used to classified </Eq> as background or foreground .
Note that in this case : </Eq> .
Similarly , if </Eq> is a dynamic pixel , </Eq> and ( 4 ) is used as the classification criterion .
3 ) Foreground object segmentation : after finishing the background / foreground classification for all pixels , an " oil spreading " algorithm is applied to find connected regions of foreground pixels .
Then some Heuristic technologies are used to separate objects sticking to each other due to shades .
To make the background differencing in the change detection step more accurate , the background image should be regularly updated .
Let </Eq> and </Eq> be the background and input frame at </Eq> and time </Eq> .
If </Eq> is referred to a nonchange background point , the background at </Eq> is updated as : </Eq> ( 17 ) where </Eq> .
Otherwise , if </Eq> classified as a background point ( static or dynamic ) , the background at </Eq> is replaced by the new one : </Eq> ( 18 )
Figure 1 presents the complete algorithm of foreground object detection .
Let </Eq> be the number of objects at time </Eq> , and </Eq> be the number of measurements received .
The set of objects and measurements at time t can be respectively denoted as : </Eq> ( 19 ) </Eq> ( 20 )
Let </Eq> , </Eq> denote the joint association event between objects and measurements , where </Eq> is the particular event which assigns measurement </Eq> to object </Eq> .
The joint association event probability is : </Eq> ( 21 ) where </Eq> is the sequence of measurements up to time </Eq> , </Eq> is the normalization constant .
The first term </Eq> is the likelihood function of the measurements , given by : </Eq> ( 22 ) where </Eq> is the number of false alarms , </Eq> is the probability of number of false alarms , which is usually Poisson distributed , </Eq> is the likelihood that measurement </Eq> is originated from target </Eq> .
The second term </Eq> is the prior probability of a joint association event , given by : </Eq> ( 23 ) where </Eq> is the probability of detection of an object with the assumption that target detection occurs independently over time with known probability .
Thus , the probability of a joint association event is : </Eq> ( 24 )
The state estimation of object </Eq> is : </Eq> ( 25 )
Let the association probability for a particular association between measurement </Eq> and object </Eq> be defined by : </Eq> ( 26 )
Hence , ( 25 ) becomes : </Eq> ( 27 ) where </Eq> is the state estimation from Kalman filter </CITE> with the assumption on association between measurement </Eq> and object </Eq> .
Note that : </Eq> , and in fact , it is difficult to propose a model for exactly estimating </Eq> in ( 26 ) as the theory , so we want to normalize </Eq> so that </Eq> before using in ( 27 ) . 
Hence : </Eq> ( 28 ) and ( 27 ) becomes : </Eq> ( 29 )
Figure 2 below is the complete algorithm of JPDAF at each time instant </Eq> .
Figure 3 is the experimental results of JPDAF performed on simulated data of 8 targets in 100 time steps .
The left one of each image pair is the simulated data and the right one is the estimated track of each target .
Targets ’ positions are initialized in the area of [ 0 . . 500 ] x [ 0 . .50 ] , false alarms are taken randomly in the area of [ 0 . . 200 ] x [ 0 . .200 ] , </Eq> = 0 . 98 , </Eq> .
Statistical background model is applied to detect moving objects in the motorcycle lane with the parameters in Table 1 .
The color and gradient vectors are obtained by quantizing their domains to 256 resolution levels , while for color cooccurrence vectors , the number of quantized levels is 32 , </Eq> = 0 . 005 is used for the distance measure in ( 8 ) while </Eq> = 2 is used for ( 9 ) .
Using the measurements achieved from the detection stage , JPDAF performs data association between the current measurements and targets .
At each time </Eq> , based on the accuracy of detection results , we can propose a strategy to detect new objects entering the observation area .
If : </Eq> so that </Eq> ( 30 ) where </Eq> and </Eq> are respectively the Decartes coordinates of object </Eq> at time </Eq> and measurement </Eq> at time </Eq> is a small positive number .
Then </Eq> is considered as a measurement originated from a new object .
That means if a measurement is not " too close " with any target at the last time instant , it is implied that a new target has occurred .
Besides , if an object is at the end of the observation area and it is not a new object or it is misdetected more than 3 time instant , it will be removed .
To increase the accuracy of JPDAF , beside the spatial distance , the information of color histogram should be incorporated into the likelihood </Eq> in ( 22 ) .
Hence , Bhattachayya distance is employed to calculate the " distance " between the reference color model </Eq> and the candidate color model </Eq> of each target , ( details in </CITE> ) : </Eq> ( 31 ) where reference color model of a target is chosen as its last state and the candidate color model is its current measurement .
Moreover , for increasing the accuracy , the reference and candidate model are divided into two sub - regions ( Figure 4 ) , then the color likelihood of a candidate model is produced : </Eq> ( 32 )
Let </Eq> be the spatial distance likelihood </Eq> which attained by Kalman filter between measurement </Eq> and target </Eq> </CITE> , then the likelihood </Eq> in ( 24 ) is defined as : </Eq> ( 33 ) where </Eq> because spatial distance information has a higher priority than color in this context .
In our application , we chose </Eq> .
The below is some results of object detection ( Figure 5 ( a ) ) . The left image of each pair is the input frame and the right one is the detection result .
The experimental sequences are taken from the motorcycle lane in a cloudy weather and the illumination changes are easily seen , but the detection algorithm still works very well .
The background is learned rapidly . Figure 5 ( b ) is a learned background after 60 frames . Together with the statistics of background features , the results of background / foreground classification step is very accurate , and there are almost no misclassified background point .
But the difficulty is in the segmentation step , when the object density at the end of the observation area is high , many occlusions usually occurs and the segmentation step will usually make mistakes ( Figure 6 ) .
Figure 5 ( c ) is an example of " once - off " background change during which there was a motorbike stopping close to the pavement for a while and it became background soon after that .
Table II is the quantitative results of object detection .
The system was tested on ten sequences which has the object density < 10 objects / frame , each sequence has an average length of 10 seconds and uses the first 30 frames ( 1 second ) for initial background learning ( " + 30 " in Length column ) .
The precision rate = 100 % demonstrated that there is no background object which is classified as foreground , and the mistake percentages in the recall rate is caused by the incorrect segmentation .
The results of JPDAF depends on the object detection results : if objects are correctly detected , the tracking algorithm will works very well .
In general , this system works well with a reasonable number of targets / frame ( < 10 targets / frame) .
With the strategy for detecting objects entering and exiting the observation area , the JPDAF can also detect and track the motorcycles driven in wrong direction .
Figure 7 shows some tracking results , including of the tracking of wrong - wayed motorcycle ( Figure 7 ( d ) , object 10 ) .
Table III shows the statistics of full correct tracks in the ten sequences above ( the mis - tracked objects in any frame are not counted) .
Since JPDAF is an NP-hard problem ( the number of possible joint association events at each time instant </Eq> is </Eq> , the computation cost of JPDAF is one of its major weak points .
All of these experiments are deployed on a Pentium IV 2 . 4 Ghz , 512 MB RAM , and due to the high cost of object detection and tracking algorithm , the processing rate is 2s / frame with the frame size is 360 x 240 and the sequence rate is 30 frames / s .
This paper is a next step on the way searching an efficient approach for a motorcycle surveillance system after using Particle filter in </CITE> .
Some improvements have been achieved in object detection step which has more accurate results in the whole observation area and the ability to efficiently adapt to illumination changes and " once - off " changes .
However , occlusions have not been strictly handled and the computation cost is one of the major limitations .
In the future , we hope thatmany new multi - target tracking methods will be applied in this context, and the best selection will be produced .
Comparing Diﬀerent Criteria for Vietnamese Word Segmentation
Syntactically annotated corpora have become important resources for natural language processing , due in part to the success of corpus-based methods . 
Since words are often considered as primitive units of language structures , the annotation of word segmentation forms the basis of these corpora . 
This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language . 
Although word segmentation is straight-forward for space-delimited languages like English , this is not the case for languages like Vietnamese for which a standard criterion for word segmentation does not exist . 
This work explores the challenges of Vietnamese word segmentation through the detection and correction of inconsistency for VTB . 
Then , by combining and splitting the inconsistent annotations that were detected , we are able to observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation . 
The analysis and experimental results showed that our methods improved the quality of VTB , which positively affected the performance of its applications .
Treebanks , which are corpora annotated with syntactic structures , have become more and more important for language processing . 
In order to strengthen the automatic processing of the Vietnamese language , the Vietnamese Treebank ( VTB ) has been built as a part of the national project , " Vietnamese language and speech processing ( VLSP ) " ( Nguyen et al. ,  2009b ) .
However , in our preliminary experiment with VTB , when we trained the Berkeley parser  ( Petrov et al. ,  2006 ) and evaluated it by using the corpus , the parser achieved only 65.8% in F-score . 
This score is far lower than the state-of-the-art performance reported for the Berkeley Parser on the English Penn Treebank , which reported 90.3% in F-score ( Petrov et al. ,  2006 ) . 
There are two possible reasons to explain this outcome . 
One reason for this outcome is the quality of VTB , including the quality of the annotation scheme , the annotation guidelines , and the annotation process . 
The second reason is the difficulty of parsing Vietnamese; we need to seek new solutions to address this problem .
VTB is annotated with three layers : word segmentation , POS tagging , and bracketing .
This paper focuses on the word segmentation , since the most basic unit of a treebank are words ( Di Sciullo and Edwin ,  1987 ) , and defining " words " is the first step ( Xia , 2000b ,a; Sornlertlamvanich et al. ,  1997 , 1999 ) . 
For languages like English , defining " words " is almost trivial , because the blank spaces denote word delimiters . 
However , for an isolating language like Vietnamese , for which blank spaces play a role of syllable delimiters , " What are words? " is not a trivial question . 
For example , the sentence " Học sinh học sinh học ( students learn biology ) </CITE> " is composed of three words , " học sinh ( student ) " , " học ( learn ) , " and " sinh học ( biology ) " . 
Word segmentation is expected to break down the sentence at the boundaries of these words , instead of splitting " học sinh ( student ) " and " sinh học ( biology ) . " 
Note that the terminology " word segmentation " also refers to the task of extracting words statistically without concerning a gold-standard for segmentation , as in  ( Ha , 2003; Le et al. ,  2010 ) . 
In such a context , the extracted words are more appropriate for building a dictionary , rather than for corpus-based language processing , which are outside of the scope of this paper . 
Because of the discussed characteristics of the language , there are challenges in establishing a gold standard for Vietnamese word segmentation .
The difficulties in Vietnamese word segmentation have been recognized by many researchers  ( Ha , 2003; Nguyen et al. ,  2004 , 2006; Le et al. ,  2010 ) . 
Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus as to the methodology for segmenting a sentence into words . 
The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation . 
While similar problems also occur with Chinese word segmentation  ( Xia , 2000b ) , Vietnamese word segmentation may be more difficult , because the modern Vietnamese writing system is based on Latin characters , which represent the pronunciation , but not the meaning of words . 
All of these characteristics make it diﬃcult to perform word segmentation for Vietnamese , both manually and automatically , and have thus resulted in different criteria for word segmentation . 
However , so far , there have been few studies on the challenges in word segmentation , and the comparison of different word segmentation criteria .
In this paper , a brief introduction of the Vietnamese Treebank VTB and its annotation scheme are provided in Section 2 . 
Then , we described our methods for the detection and correction of the problematic annotations in the VTB corpus ( Section  4.2 ) . 
We classified the problematic annotations into several patterns of inconsistency , part of which were manually fixed to improve the quality of the corpus . 
The rest , which can be considered as the most difficult and controversial instances of word segmentation , were used to create different versions of the VTB corpus , representing different word segmentation criteria . 
Finally , we evaluated these criteria in automatic word segmentation , and its application in text classification and English-Vietnamese statistical machine translation , in Section 4 .
This study is not only beneficial for the development of computational processing technologies for Vietnamese , a language spoken by over 90 million people , but also for similar languages such as Thai , Laos , and so on . 
This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language , like English , to a language that has not yet been intensively studied .
Word segmentation in VTB aims at establishing a standard for word segmentation in a context of multi-level language processing . 
VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al. , a ) , which can be divided up into three groups : single , compound , and special " words . " 
Single words contain only one token . 
The terminology tokens refers to text spans that are separated from each other by blank spaces . 
Compound words have two or more tokens , and are divided into four types : compound words composed by semantic coordination ( semantic-coordinated compound ) , compound words composed by semantic subordination ( semantic-subordinated compound ) , compound words with an aﬃx , and reduplicated words . 
Special " words " include idioms , locutions , proper names , date times , numbers , symbols , sentence marks , foreign words , or abbreviations . 
The segmentation of these types of words forms a basis for the POS tagging , with 18 different POS tags , as shown in Table 2 ( Nguyen et al. , c ) .
Each unit in Table 1 goes with several example words; English translations are provided in parentheses .
Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed . 
The subscript of a token translation is the index of that token in the compound word . 
However , for some tokens , we could not find any appropriate English translation , so we gave it an empty translation , marked with an asterisk . 
Note that a Vietnamese word or a token in context can have other meanings , in addition to the given translations .
A classifier noun , denoted by the part-of-speech Nc in Table 2 , is a special type of word in Vietnamese .
Classifier nouns are specific to several Southeast Asian languages , like Vietnamese and Thai . 
One of the functions of classifier nouns is to express the definiteness . 
For example , the common noun " bàn " generally means tables , while " cái bàn " means a specific table , similar to the table in English .
In this section , we analyzed the VTB corpus to determine whether the difficulties in Vietnamese word segmentation affected the quality of VTB annotations . 
The analysis revealed several types of inconsistent annotations , which are also problematic for Vietnamese word segmentation . 
Our analysis is based on two types of inconsistencies : variation and structural inconsistency , which are defined below .
Variation inconsistency : is a sequence of tokens , which has more than one way of segmentation in the corpus . 
For example , " con gái/girl " can remain as one word , or be segmented into two words , " con " and " gái " . 
A variation can be an annotation inconsistency , or an ambiguity in Vietnamese . 
While ambiguity cases reflect the difficulty of the language , annotation inconsistencies are usually caused by the confusion in the decision of annotators , which should be eliminated in annotation . 
We use the term variation instance to refer to a single occurrence of a variation .
Structural inconsistency : happens when different sequences have similar structures , and thus should be split in the same way , but are segmented in different ways in the corpus . 
For example , " con gái/girl " and " con trai/boy " have similar structures : a combination of a classifier noun and a common noun , Nc + N , so when " con gái/girl " is split , and " con trai/boy " is not , it is considered as a structural inconsistency of Nc . 
It is likely that structural inconsistency at the word segmentation level complicates the higher levels of processing , including POS tagging and bracketing .
The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in the VTB , following the definition for variation inconsistency , above . 
In detail , we counted N-gram sequences of different lengths in VTB that have two or more ways of word segmentation , satisfying one of the following two conditions :
N tokens are all in the same phrase , and all have the same depth in phrase . 
For example , the 3-gram " nhà tình nghĩa ( house of gratitude ) " in this structure " ( NP ( Nc-H căn ) ( N nhà ) ( A tình nghĩa ) ) , " OR N tokens are all in the same phrase , and some tokens can appear in an embedded phrase , which contains only one word . 
For example , " nhà tình nghĩa " in this structure " ( NP ( Nc-H căn ) ( N nhà ) ( ADJP ( A tình nghĩa ) ) ) , " where the ADJP contains only one word .
Table 3 shows the overall statistics of the variation inconsistency detected by the method described above . 
Most of the diﬃcult cases of word segmentation occur in two-token variations , occupying the majority of variations ( 92.9% ) . 
This ratio of 2-gram variations is much higher than the average ratio of two-token words in Vietnamese , as reported in ( Nguyen et al. ,  2009a ) , which is 80% . 
Variations that have lengths of three and four tokens occupy 6.1% and 1.0% , respectively .
We estimated the precision of our method by randomly selecting 130 2-gram variation instances , extracted from the method described above , and manually checked whether the inconsistencies are true .
We found that 129 cases occupying 99.2% of all extracted 2-grams are true inconsistencies . 
Only one instance of inconsistency was an ambiguous sequence giá c , which is one word when it means price , and two words giá/price c /all in đàu có giá c /all have ( their own ) price . 
The precision for our method is high , so we can use the extracted variations to provide insights on the word segmentation problem .
We further analyzed the 2-gram variations to understand what types of 2-grams were most confusing for annotators . 
The analysis revealed that compound nouns , compound verbs , and compound adjectives are the most difficult cases of word segmentation .
We classified the 2-gram variations according to their POS sequences in case the tokens in the 2-gram are split . 
There are a total of 54 patterns of POS sequences . 
The top 10 confusing patterns , their counts of 2-gram variations , and examples are depicted in Table 4 . 
Table 5 and Table 6 show the POS patterns that are a specific POS tag , appearing at the beginning or ending of the sequence .
Investigating the inconsistent 2-grams extracted , we found that most of them are compound words , according to the VTB guidelines ( Section 2 ) . 
One of the reasons why the compound words are sometimes split , is because the tokens in those compound words have their own meanings , which seem to contribute to the overall meaning of the compounds . 
This can be seen through the examples provided in Table 4 , where the meanings of tokens are given with a subscript . 
This scenario has proven to be problematic for the annotators of VTB .
Furthermore , by observing the POS patterns in Table 5 and Table 6 , we can see the potential for structural inconsistency , particularly for closed-set POS tags . 
Among them , classifier nouns ( Nc ) and affixes ( S ) are two typical cases of structural inconsistency , which will be used in several settings for our experiments . 
The same aﬃx or classifier noun can modify different nouns , so when they are sometimes split and combined in the variations , we can conclude that classifier nouns and affixes involve in-structural inconsistencies . 
In the following section , we present our detection method for structural inconsistency for classifier nouns and affixes .
The detection method for structural inconsistency of classifier nouns and affixes is simple . 
We collected all affixes and classifier nouns in the VTB corpus , and then extracted 2-grams containing these affixes or classifier nouns , which are also structural inconsistencies . 
For example , since " con " is tagged as a classifier noun in VTB , we extracted all 2-grams of " con " including both " con gái/girl " and " con trai/boy " .
Even though the sequence , " con trai " is always split into two words throughout the corpus , it can still be an inconsistency , if we consider similar structures such as " con gái " . 
In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent , if we consider the higher analysis levels , POS tagging .
According to the VTB POS-tagging annotation guidelines ( Nguyen et al. , c ) , classifier nouns should be separated from the words that they modify . 
However , in practice , it is confusing when the classifier noun can be standalone as a single word . 
For example a classifier noun , e.g. , " con " in " con trai ( boy ) " , or " con gái ( girl ) " , can also be a simple word , which means " I ( first person pronoun used by a child when talking to his/her parents ) " , or part of a complex noun " con cái ( children ) " . 
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these cases , in order to see whether the solution is successful for applications of the corpus .
By examining the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like a percentage ( % ) in " 30% " , is split or combined with " 30 " . 
Such inconsistent annotations are manually fixed based on their textual context .
By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations . 
For example , the character , % , in " 30% " is split , but is combined with a number in " 50 % " , which is considered to be a structural inconsistency . 
Note that it can be argued that splitting " N% " into two words or combined in one word is dependent on the blank space in-between N and " % " . 
Higher-levels of annotation such as POS tagging is significant , because we may need one or two different POS tags for the different methods of annotation . 
Therefore , we think that it is better to carefully preprocess text and segment these special characters in a consistent way .
To improve the quality of the VTB corpus , we extracted the problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency . 
Automatically modification is diﬃcult , since we must check the semantics of the special characters in their contexts . 
For example , hyphens in date expressions like " 5-4-1975 " , which refers to the date , "the fifth of April , 1975 ," are combined with the numbers . 
However , when the hyphen indicates " ( from ) to " or " around ... or " , as in " 2-3 giờ sáng " , meaning " around 2 or 3 o’clock in the morning " , we decided to separate it from the surrounding numbers . 
As a result , we have fixed 685 inconsistent annotations of 21 special characters in VTB .
The variation inconsistency and structural inconsistency found in Section 3 can also be seen as representatives of different word segmentation criteria for Vietnamese . 
We organized the inconsistency detected in seven configurations of the original VTB corpus . 
Then , by using these data sets , we could observe the influence of the different word segmentation criteria on three tasks : automatic word segmentation , text classification , and English-Vietnamese statistical machine translation .
Seven data sets corresponding to different segmentation criteria are organized as follows .
ORG  : The original VTB corpus . 
BASE  : The original VTB corpus + Manual modification of special characters done in Section 3.3 . 
VAR_SPLIT  : BASE + split all variations detected in Section  3.1 . 
VAR_COMB  : BASE + combine all variations detected in Section  3.1 . 
VAR_FREQ  : BASE + select the segmentation with higher frequency among all variations detected in Section  3.1 . 
STRUCT_NC  : BASE + combine all classifier nouns detected in Section 3.2 with the words they modify .
STRUCT_AFFIX  : BASE + combine all suﬃxes detected in Section 3.2 with the words they modify . 
These data sets are used in our experiments , as illustrated in Figure 1 . 
The names of the data sets are also used to label our experimental configurations . 
In this section , we briefly describe the task settings and the methods used for word segmentation ( WS ) , text classification ( TC ) , and English-Vietnamese statistical machine translation ( SMT ) .
We used YamCha ( Kudo and Matsumoto ,  2003 ) , a multi-purpose chunking tool , to train our word segmentation models . 
The core of YamCha is the Support Vector Machine ( SVM ) machine learning method , which has been proven to be effective for NLP tasks . 
For the Vietnamese word segmentation problem , each token is labeled with standard B , I , or O labels , corresponding to the beginning , inside , and outside positions , respectively . 
The label of each token is determined based on the lexical features of two preceding words , and the two following words of that token . 
Since the Vietnamese language is not inflectional , we cannot utilize inflection features for word segmentation .
Each of the seven data sets is split into two subsets for training and testing our WS models . 
The training set contains 8443 sentences , and the test set contains 2000 sentences .
Text classification is defined as a task of determining the most suitable topic from the predefined topics , for an input document . 
We implemented a text classification system similar to the system presented in ( Nguyen et al. ,  2012 ) . 
The difference is that we performed the task at the document level , instead of at the sentence level .
The processing of the system is summarized as follows . 
An input document is preprocessed with word segmentation and stop-word removals . 
Then , the document is represented in the form of a vector of weighted words appearing in the document . 
The weight is calculated using standard tf-idf product . 
An SVM-based classifier predicts the most probable topic for the vector , which also is the topic for the input document . 
In our experiment , for comparison of diﬀerent word segmentation criteria in topic classification , we only vary the word segmentation model used for this task , while fixing other configurations .
News articles of five topics : music , stock , entertainment , education , and fashion are used . 
The sizes of the training and test data sets are summarized in Table 8 .
A phrase-based SMT system for English-Vietnamese translation was implemented . 
In this system , we used SRILM  ( Stolcke , 2002 ) to build the language model , GIZA++ ( Och and  Ney , 2003 ) to train the word-aligned model , and Moses ( Holmqvist  et al. ,  2007 ) to train the phrase-based statistical translation model . 
Translation results are evaluated using the BLUE score ( Papineni  et al. ,  2002 ) . 
Both training and test data are word-segmented using the word segmentation models achieved . 
For the experiment , we used the VCL_EVC bilingual corpus , 18000 pairs of sentences for training , and 1000 pairs for testing .
According  to  the  result  in  Table  9 ,  the  VAR_SPLIT  criterion  gives  the  highest  WS performance . 
With the exception of STRUCT_NC , all of the modifications to the original VTB corpus increase the performance of WS . 
However , the word segmentation criterion with higher performance is not necessarily a better criterion , but a criterion should also be judged through applications of word segmentation . 
In both SMT and TC experiments , the BASE model , which is based on the manually-modified inconsistency of special characters , achieved better results than the ORG model . 
In particular , in the TC experiment , the BASE model achieved 0.66 point higher than ORG , which is a significant improvement . 
The results support the conclusion that the quality of the word-segmentation corpus is very important for building NLP applications .
The SMT results show that three out of six augmented models , VAR_SPLIT , VAR_FREQ and BASE , performed better than the ORG configuration . 
Among them , the best-performing model , VAR_SPLIT achieved 36.91 BLEU score , which is 0.55 higher than ORG . 
In TC results , all six augmented models achieved higher results than ORG . 
In general , the augmented models performed better than the ORG . 
Additionally , because our automatic methods for inconsistency detection could not cover all of the types of inconsistencies in word segmentation annotation , further improvement of corpus quality is demanded .
Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining aﬃxes with their head nouns resulted in slightly better results for WS and TC , and did not change the performance of SMT . 
However , the combination of classifier nouns with their head nouns had negative eﬀects on WS and SMT .
Another part of the scope of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining . 
Splitting and combining variations are reflected by VAR_COMB and VAR_SPLIT , while STRUCT_AFFIX and STRUCT_NC represent the combination of affixes or classifier nouns with the words that they modify . 
STRUCT_AFFIX and STRUCT_NC are contrasted with BASE where aﬃxes and classifier nouns remain untouched . 
Comparing VAR_COMB and VAR_SPLIT in both the TC experiment and SMT experiment , we see that the VAR_SPLIT results are better in both cases . 
Since the ratio of combined variations in the ORG corpus is 60.9% , it can be observed that splitting seems to be better than combining for WS , TC and SMT .
In this paper , we have provided a quantitative analysis of the difficulties in word segmentation , through the detection of problematic cases in the Vietnamese Treebank . 
Based on the analysis , we automatically created data that represent the different word segmentation criteria , and evaluated the criteria indirectly through their applications .
Our experimental results showed that manual modification , done for annotation of special characters , and most other word segmentation criteria , significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , in comparison with the use of the original VTB corpus . 
Since the VTB corpus is the first eﬀort in building a treebank for Vietnamese , and is the only corpus that is publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building efficient NLP systems .
Generating short summary videos for rushes is a challenging task due to the difficulty in eliminating redundancy and determining the important objects and events to be placed in the summary .
Redundancy elimination is difficult since repetitive segments , which are takes of the same scene , usually have different lengths and motion patterns .
This makes approaches using one keyframe for a shot representation fail when trying to form a cluster .
In addition , even repetitive segments can be precisely determined , but the summary generated by concatenating together the selected segments still takes longer than the upper limit .
It is questionable to select a sub-segment so that it conveys information of the scene as much as possible .
,We introduce two approaches to solve these problems .
In the first approach , one keyframe is used for representing a shot when forming a cluster; and sub-segments are selected using the motion information for generating the summary .
Meanwhile , in the second approach , all the frames of a given shot are used for clustering; and a simple skimming method is used to select the sub-segments .
The experimental results on the TRECVID 2008 dataset and a comparison between the two approaches are also reported .
With the availability of multimedia databases growing at an exponential rate , users are increasingly requiring assistance in accessing digital video contents .
Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \CITE .
Summary videos can help users more efficiently and effectively browse and navigate through large video archives .
Generating summary videos for BBC rushes \CITE is a challenging task due to the difficulty with redundancy elimination and determining the most important objects and events to be placed in the summary .
Since the length of the summary is limited to 2\% duration of the original video , there is a trade-off between the recall and usability ( e.g. user friendly through smooth presentation , / being easy to understand ) .
High recall , i.e. many objects and events ( called scenes ) included in the summary , usually reduces the number of frames for each scene .
For example , the maximum duration for a summary of a 30 minute video is 36 seconds ( \MATH ) .
If the summary consists of 20 scenes , the average duration for each scene is 1.8 seconds .
For an event such as " `Woman attacks man on bench on left and runs off with large bag .
" ', with this length constraint , it would be difficult to present it in a pleasant tempo and rhythm .
On the other hand , a smooth presentation of these events would consume a large number of frames , which would decrease the recall .
In general , generating summary videos consists of the following steps :
Video segmentation : This step breaks down the original video into segments , such as shots or sub-shots .
Each segment should be aligned so that it is a part of a scene .
Redundancy elimination : This step groups the segments that belong to the same take into clusters .
Only one representative segment is used for the final summary video .
The others are discarded .
Junk elimination : This step removes the color bars , clapboards , and the all black or all white frames that are unnecessary in the final summary video .
Summary generation : This step selects the frames from the representative segments of clusters and concatenates them to form the final summary video .
While the steps for video segmentation and junk elimination are easy to handle , the steps for redundancy elimination and summary generation are difficult .
For example , as for redundancy elimination , the question is how to represent a segment in a feature vector and how to compute the similarity between two segments having different lengths and motion patterns .
In the other case , assuming that we have selected the appropriate segments , the total length of these segments is usually larger than that of the final summary .
The question is how to determine the most important parts of the selected segments so that they convey as much of the information of the scene as possible .
In this paper , we present two approaches for handling these difficult steps .
The first approach represents each segment by using one key-frame and groups similar segments by clustering them on these key-frames .
Then the portion of each segment that has the highest motion is included in the final summary .
Meanwhile , the second approach uses another strategy for redundancy elimination .
Specifically , for each segment , a set of frames are extracted by sampling at a certain time interval ( e.g. 5 frames ) .
The clustering process is performed on the frames of all the segments .
Then , segments that share a large enough number of frames with respect to their size are merged into one cluster .
In order to generate the final summary , with each representative segment , the middle part is selected with a skim rate of 2 frames .
This paper is organized as follows : section \REF introduces the details of the first approach; , while section \REF presents the details of the second approach .
Section \REF describes our experimental results on the TRECVID 2008 dataset .
Finally , section \REF and section \REF conclude the paper .
By definition , all rushes are unedited; therefore they must consist of hard cuts only .
The shot boundary detection algorithm in \CITE is used to determine the shot boundary and to partition the input video into shots .
A local color histogram is extracted by dividing a video frame into \MATH blocks .
The \MATH distance is used to compute the distance between each block of frames \MATH and \MATH .
Next , these values are sorted into ascending order .
The sum of the middle eight of these 16 values is used to define the cut between frames \MATH and \MATH if these values exceed the threshold \MATH .
However , this algorithm cannot distinguish between hard cuts and the motion of large objects .
To overcome this problem , motion-based features are computed for each video frame using the Lucas-Kanade point-based tracking functions provided in the OpenCV toolkit\footnote{http : //opencvlibrary.sourceforge.net / } .
The magnitude is computed from the motion vector for each frame .
Therefore , if the algorithm detected a cut between frames \MATH and \MATH , whose magnitude is larger than the threshold \MATH , these cuts are rejected since they are the motions of large objects .
Finally , short shots of less than 25 frames ( 1 second ) are removed .
The sub-shot segmentation algorithm in \CITE is used to divide shots into smaller units .
The first frame of the shot is chosen as the base frame \MATH and the next frame \MATH for a comparison .
The \MATH distance used to compute the distance of the frame sequence until the sum of the sorted value of the lower eight is larger than the threshold \MATH . //[distance / length?]
The frames from \MATH to \MATH , then , form a sub-shot and frame \MATH is used as the next base frame .
Finally , the short sub-shots of less than 25 frames are removed .
We use the keyframe extraction algorithm proposed in \CITE to extract the representative keyframes from each sub-shot .
In this approach , the cosine distance is used to measure the difference between neighboring frames in each sub-shot .
Keyframes are selected at the midpoints between two consecutive high curvature points where the high curvature points are detected from the curve of the cumulative frame difference .
The characteristics of color bars are vertically averaged , and the color histograms for each block in the same column should be similar .
We used the algorithm proposed in \CITE by using the \MATH distance to compute the histogram differences between any two neighboring blocks in each column .
Next , we sort these values into ascending order .
If the value of the \MATH is smaller than the threshold \MATH , then these sub-shots are defined as a color bar sub-shot .
From the properties of a single color image , the dominant color in its global histogram is large .
If the value of the \MATH of the global color histogram is larger than the threshold \MATH , then these sub-shots are defined as a single color sub-shot .
In rushes videos , there are many types of clapper boards , but the same type of clapper board is often used in the same movie .
There are many types of clapper boards , such as scale , rotation , and illumination changes .
The NDK algorithm , proposed in \CITE , is invariant to image scaling , translation , rotation , illumination changes , and affine or 3D projection .
A set of 80 example keyframes of clapper boards were extracted from the development set and is used as a set of queries .
Next , we extract the keypoints of the keyframes given from section \REF and match them with the query .
If the result of the NDK algorithm returns a match from a keyframe with a query then the sub-shot is defined as a clapper board sub-shot .
The unused keyframes containing story units for the generated video summary are removed .
However , rushes videos containing a repetitive story , such as a retake of scenes , are unedited .
To efficiently create rushes videos , the repetitive contents must be eliminated .
Generally , a group of continuous contents often share some properties .
With this characteristic in mind , a clustering technique can be used to separate the data into groups of similar contents .
Each group , called a cluster , consists of contents that are similar between themselves and dissimilar to the contents of other groups .
GreedyRSC , proposed in \CITE , is used to find clusters at high precision and the number of clusters is automatically determined .
To do clustering on keyframes , three different features , including the mean , variance , and skew , are extracted from the local color histogram .
These values are used to represent the keyframes content and are defined as follows :
Figure \REF shows an example of a clustering result .
So far , we have completely removed the unused contents from rushes videos and reduced any repetition of the story contents .
The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of a summary is 2\% of the original video ) , less repetitive content , and must have as many objects and events as possible .
To reach this objective , only the most important keyframes should be selected to generate a summary video .
To generate a summary , we first compute its maximum duration in seconds \MATH ,
where \MATH is the maximum duration for the summary .
Second , we compute the quota length for each cluster based on the cluster size \MATH .
Third , merge the consecutive sub-shots in each cluster into shots and compute the priority of each shot based on the priority of the shot weighted duration and shot weighted average motion magnitude using the following equation : \MATH</p>
Next , these \MATH values are sorted into descending order and the first shot is selected .
Fourth , sub-shots in the selected shot in descending order are sorted based on the average motion magnitude .
The sub-shots are selected from top to bottom until the quota length for that shot is reached .
Fifth , for each selected sub-shot , 25 frames ( 1 second ) around the middle are extracted to generate the final summary .
This system has some modifications from the system developed for the same task last year \CITE .
Specifically , the original video is broken down into segments , which are shots with a hard cut transition .
These segments are further broken down into fragments so that each fragment represents a portion of a scene .
In order to reduce the computation time , we only extract a subset of the frames from the original video by sampling it at a five frame interval ( i.e. extract frames 0 , 5th , 10th , and so on ) .
For each frame , we use grid color moments with the same configuration as in \CITE for feature representation .
The segment boundary , which is located at the hard cut transition , is determined by using a loose threshold on the Euclidean distance between two consecutive frames .
Meanwhile , the fragment boundary is determined by using a strict threshold to detect any dramatic motion .
Instead of selecting one keyframe to represent one fragment as many other systems do , we use all the frames of each fragment for the redundancy elimination .
We use GreedyRSC \CITE to do the clustering on the set of all the sampled frames extracted from the original video .
The number of clusters is determined automatically using this method .
Frames that belong to the same cluster are assigned the same label .
By this discretization process , we can cast one fragment as one string whose characters are the labels of its frames .
We compute the similarity between two fragments by counting the number of shared characters between two strings and being normalized to the size of each string .
If this value is larger than the threshold , these two segments are merged into one cluster .
We found that this approach is more effective than the approach using one keyframe for one fragment since the more keyframes that are used , the more information is available to make the right decision .
We select junk frames such as color bar frames , and single color ( black or white ) frames to form the reference junk frame set .
To check whether a fragment is junk , we compare the frames of this fragment to the frames of the reference junk frame set .
The similarity between two frames is the Euclidean distance between two grid color moment feature vectors .
We empirically select the thresholds for each type of junk .
If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered junk and all the fragments of the cluster containing the junk fragment are eliminated .
In our system, we only check the fragments that are located at the two ends of the original video for reducing the computation time .
However, by using the clustering result, junk fragments that are not checked against the reference junk frame set are also removed .
For each cluster, we merge adjacent fragments into longer fragments and select the longest fragment as the representative fragment to be included in the final summary .
Since the length of these fragments is still larger than the maximum length of the final summary, we use the following simple strategy to shrink these fragments .
First, we assign a quota, which is the maximum duration, for each fragment by dividing the maximum duration for the summary to the number of clusters .
Second, for each fragment, we extract the portion that is expanded from the central part of the fragment .
This portion covers a duration twice the size of the fragment quota by selecting the frames with a sampling rate of two frames .
Specifically, we select frames \MATH, \MATH, ..., \MATH, \MATH, ..., \MATH, \MATH, where \MATH is the middle frame of the fragment, and \MATH is half the number of frames computed from the quota\MATH and the frame rate ( 25fps ) \MATH :
We have tested our approaches on 40 videos from the TRECVID 2008 test set .
Table \REF presents a comparison between these approaches for the measures used in evaluation of this task \CITE .
The NII-2system achieves a higher recall ( IN ) than the NII-1 system because NII-1 only uses one keyframe for each sub-shot and has a shorter duration ( DU ) for summary videos .
However, NII-1 has better quality .
The summary videos generated by NII-1 have less duplication ( RE ), are presented in a smoother way ( TE ), and are easy to judge for inclusions ( TT ) .
In terms of efficiency, NII-2 is much better .
The clapper board detection process using NDK consumes around half of the processing time of NII-1, but its performance is low due to the large variations in clapper boards in the videos ( see Figure \REF ) .
The comparable performance in the junk elimination of both systems suggests that simpler methods are more favorable .
In addition, by using simple features and sampling frames in the original video, NII-2 significantly increases the processing time ( computed from the time the input video is taken to the time the summary video is picked ) to quasi real-time .
Practical summarization systems usually have a good balance between the fraction of inclusions and user-friendliness .
In Table \REF, we present the performance of such systems .
The 14 systems listed in this table have an IN score that is above the median ( 0.45 ); and other scores, such as RE and TE, are larger than half of the maximum score ( 2.5 ) .
Compared to the other systems listed in this table, our NII-2system is one of the fastest .
Compared to the other systems participating in this task of TRECVID 2008, NII-1 performed better in such measures as DU and TT ( see Figure \REF and Figure \REF; while NII-2 performs well in the IN measure ( see Figure \REF ) .
One of the most difficult steps is redundancy elimination .
The lack of discriminative representation of the segments and robust clustering methods is the main reason \CITE .
Two typical cases that usually happen in clustering results are fragmentation and outliers .
Fragmentation is where samples of one cluster are put into several different clusters .
Outliers are irrelevant and noisy samples in one cluster due to the poor determination of the cluster boundary .
Therefore, it is necessary to develop robust methods for detecting repetitive segments .
Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .
Although the results are not as high as expected, we still believe that this approach is promising .
We have presented two different approaches for generating a short summary for rushes videos .
In the first approach, NII-1, clustering the set of keyframes extracted from the sub-shots helps to eliminate redundancy .
With each representative segment of each cluster, the portion with the highest degree of motion is selected to form the summary .
This approach has a good usability score but is not very good at recall .
In the second approach, NII-2, all the frames of each sub-shot are used to compute the similarity among the sub-shots in the clustering process .
With each representative segment of each cluster, the middle part is selected to form the summary with a skipping rate of two frames .
This approach is good for recall and has a reasonably good usability score .
Compared to other systems participating in the TRECVID 2008 summarization task, NII-2 is among the best systems with a good balance between recall and usability .
Face Retrieval Improvement by the Learning of Visual Consistency
Searching for images of people is one of the essential tasks required by users for image and video search engines .
However , the current search engines have limited capabilities for this task since they usually rely on texts associated with image and video , which are likely to return many irrelevant results .
We propose a method to effectively retrieve relevant faces for one person by learning visual consistency from results retrieved from text correlation based search engines .
This problem is challenging because ( i ) there is no label provided making it difficult to use supervised-based ranking methods .
( ii ) current face recognition techniques are still immature with wild-face databases even with supervised learning methods .
In the proposed method , we treat this problem as a classification problem in which input faces are classified as 'person-X' ( the queried person ) or 'non-person-X' , and the faces are ranked based on their relevant score inferred from the classifier 's probability output .
To train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers , which are trained using different subsets .
These training subsets are extracted and labeled automatically from the rank list produced from the classifier trained from the previous step .
In addition , outlier detection methods are used to produce the rank list for initialization .
Experimental results on various face sets retrieved from the captions of news photos show that the retrieval performance improved after each iteration with the final performance outperforming the baseline algorithms .
With the rapid growth of digital technology , large image and video databases are more available than ever to users .
Therefore , effective and efficient tools are needed for indexing and retrieving based on visual contents .
A typical example for this application is searching for a specific person by providing his or her name .
Usually , most current search engines use the texts associated with images or videos as significant clues for returning results .
However , other un-queried faces and names appear simultaneously and are aligned ( as shown in Figure \REF ) , which significantly lowers retrieval performance .
Therefore , it is necessary to improve the retrieval performance by taking into account the visual information from the retrieved faces .
This problem is challenging due to the following reasons :
-Large variations in face appearance due to pose changes , illumination conditions , occlusions , and facial expressions make face recognition difficult even with state of the art techniques \CITE .
-The fact the retrieved face set consists of faces of several people with no label makes supervised learning methods as well as unsupervised learning methods such as , \MATH -means , inapplicable .
We propose a method to solve the above-mentioned problem .
The main idea is to assume that there is visual consistency among the results returned from current text-based search engines .
This method consists of two stages .
In the first stage , we explore local density of faces to identify potential candidates for relevant faces .
This stage is based on the observation that facial images of the queried person tend to form dense clusters while irrelevant facial images are sparse since they look different from each other .
We use an outlier detection method for this purpose .
The output is a rank list in which faces with larger number of neighbors within a certain distance are considered as relevant and are therefore put at the top of the list . //[What do you mean by �gneighbors�h ? Do you mean the un-queried faces ? ]
Since the above ranking method is based on the number of neighbors , it is sensitive to the specified distance .
A second stage is necessary to improve this rank list .
We model this problem as a classification problem in which input faces are classified as person-X ( the queried person ) or non-person-X ( the irrelevant person ) .
The faces are ranked based on their relevancy score that is inferred from the classifier 's probability output .
Since annotation data is not available , the rank list from the previous step is used to assign labels for a subset of faces .
This subset then is used to train a classifier using supervised methods such as support vector machines ( SVM ) .
The trained classifier is used to re-rank faces in the original input set .
This step is repeated a number of times to get the final rank list .
Since automatically assigning labels from the rank list is not reliable , the trained classifiers are weak .
In order to get the final strong classifier , we use the idea of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
This stage is effective for improving the rank list for the following reasons :
-Supervised learning methods such , as SVMs , provide a strong theoretical background in finding optimal decision boundary even with existence of noisy data .
Furthermore , recent studies suggest that \CITE SVM classifiers provide probability outputs that are suitable for ranking .
-Bagging framework helps to leverage noises in the unsupervised labeling process .
Our contribution is two-fold :
-We propose a general framework to boost the face retrieval performance from results retrieved from text correlation-based search engines by the learning of visual consistency .
It seamlessly integrates current data mining methods such as outlier detection , supervised learning , and unsupervised learning based on bagging for a practical problem . //[What or who is �glearning�h visual consistency ? Are the search engines learning ? ]
Our framework requires few parameters and works stably .
-We demonstrate the feasibility of using tolerance of supervised learning methods when working with noisy datasets combined with ensemble learning to improve the final performance .
There are several more proposed approaches for general object classification than for those for face retrieval .
For example , as described in \CITE , objects are retrieved by an image search engine and then are re-ranked by the learning of visual consistencies from the retrieved objects .
Compared to the problem of face-based recognition , the problem of object classification is easier since classification of different object types such as airplane and non-airplane only needs to handle inter-variations between different categories , while discriminating between person-A and person-B requires handling of both intra-variations and inter-variations of the same category .
Furthermore , in order to work in unsupervised mode , these approaches need a method to collect negative samples ( e.g. non-airplane ) , which are inapplicable to our problem .
A graph-based approach was proposed by \CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph with an available solution . //[Do graphs have solutions ? They just provide information .]
Although , experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of relevant faces of the queried person .
Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to dimensionality .
In another work \CITE , a clustering-based approach was proposed for associating names and faces in news photos .
To solve the problem of ambiguity between several names and one face , a modified \MATH -means clustering process was used in which faces are assigned to the closest cluster ( each cluster corresponding to one name ) after a number of iterations .
Although the result was impressive , it is not easy to apply it to our problem since a large number of irrelevant faces ( more than 12% ) are eliminated manually before performing clustering .
This paper is organized as follows : Section \REF introduces our proposed framework .
Section \REF briefly introduces typical outlier detection methods .
Experiments and results are described in section \REF .
Finally , section \REF concludes the paper .
Given a set of faces returned by any text-based correlation search engine , our method is used to perform a ranking process summarized as follows :
-Step 1 : Detect eye positions , and then perform face normalizations .
-Step 2 : Compute an eigenface space and project the input faces into this subspace .
-Step 3 : Estimate ranks of faces using an outlier detection method mentioned in \REF .
-Step 4 : Train an ensemble classifier \MATH using this rank list by Bag-Rank-SVM .
-Step 5 : Use the classifier \MATH to estimate the probability of faces in the original set .
Rank these faces using their probability scores .
-Step 6 : Repeat steps 4 and 5 $T$ times and return ranked faces produced by the last classifier \MATH to users .
Steps 1 and 2 are typical for any face processing system and described in detail in \REF .
Step 3 used to find initial ranks for faces described in \REF .
We used a simple outlier detection method for this step .
The Bag-Rank-SVM algorithm is described as follows :
-Step 1 : Select a set \MATH including \MATH top ranked faces and then randomly select a subset \MATH from \MATH .
Label faces in \MATH as positive samples .
-Step 2 : Select a set \MATH including \MATH bottom ranked faces and then randomly select a subset \MATH from \MATH .
Label faces in \MATH as negative samples .
-Step 3 : Use \MATH and \MATH to train a weak classifier \MATH using LibSVM \CITE with probability outputs .
-Step 4 : Repeat steps Step 1 to Step 3 \MATH times .
-Step 5 : Return \MATH .
Since it is not guaranteed that the top \MATH and bottom \MATH of faces in the rank list correctly correspond to the faces of the queried person-\MATH and faces of non person-\MATH as shown in Figure \REF , randomly selecting subsets to train weak classifiers , and then combining these classifiers might help reduce the risk of using noisy training sets .
In our framework , outlier detection methods are used to initialize the rank list that is then used to label a subset of samples for training SVM classifiers .
We introduce two common outlier detection methods , distance-based outlier detection ( DBO ) \CITE and local outlier factor-based method ( LOF ) \CITE .
Adapting the definition from Knorr \CITE , given a set of objects \MATH , an object \MATH is considered as an outlier if there are fewer than \MATH neighboring objects in \MATH lying within a distance \MATH .
This outlier detection process is summarized as follows :
-Step 1 : Compute the distance between every pair of data objects .
-Step 2 : For each object , compute \MATH , which is the number of neighboring objects lying within a distance \MATH .
-Step 3 : Rank objects based on their scores \MATH .
In our experiments , the distance between two objects is the Euclidean distance between two faces and is computed in the eigen-subspace ( described in section \REF ) .
Figure \REF shows two examples of good and bad performances using this method for ranking relevant faces .
According to the method described in \CITE , the local outlier factor of an object \MATH is computed by the following steps and then used to rank faces :
-Step 1 : For each data object \MATH compute the \MATH ( the distance to the \MATH nearest neighbor ) and \MATH ( all points in a \MATH sphere ) .
- Step 2 : Compute the reachability distance for each data object \MATH with respect to data object \MATH as : \MATH , where \MATH is the distance from data object \MATH to data object \MATH .
-Step 3 : Compute local reachability density of data object \MATH as inverse of the average reachability distance based on the \MATH ( minimum number of data objects ) of the nearest neighbors to data object \MATH .
-Step 4 : Compute LOF of data object \MATH as the average of the ratios of the local reachability density of data object \MATH and local reachability density of \MATH of nearest neighbors .
We used the dataset described in \CITE for our experiments .
This dataset consisted of approximately half a million news pictures and captions from Yahoo News over a period of roughly two years .
Using a robust face detector , 44 , 773 faces were detected and normalized to the size of 86\MATH86 pixels .
After eliminating faces whose facial features were poorly detected by a rectification process and faces whose associated names were not extracted properly from the corresponding captions , 30 , 281 faces were kept .
Figure \REF shows an example of a news photo and its caption .
We selected sixteen government leaders including George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals such as John Paul II ( the Former Pope ) and Kofi Annan and Hans Blix ( UN ) since their images appeared frequently in the dataset \CITE .
For each person , variations of his name were collected . For example , George W . Bush , President Bush , U . S . President , etc are variations of U . S . President Bush .
We indexed image captions and then used this index to retrieve faces associated with the captions containing names of the queried person .
The faces retrieved from different names of each person were merged into a set used for our ranking process .
Figure \REF shows faces retrieved when searching Mr . Kofi Annan .
Figure \REF shows the distribution of retrieved faces from this method and the corresponding number of relevant faces for these ten individuals .
In total , 3 , 907 faces were retrieved in which 2 , 094 faces were relevant .
On average , the precision was 52.49% . //[precision / accuracy ? ]
We used an eye detector to detect eye positions of detected faces .
These eye positions were used to align faces to a predefined canonical pose .
To compensate for illumination effects , the subtraction of the best-fit brightness plane followed by histogram equalization was applied .
This normalization process is shown in Figure \REF .
We then used principle component analysis \CITE to reduce the number of dimensions of the feature vector for face representation .
Eigenfaces were computed from the original face set returned by the text-based query method .
A number of eigenfaces was selected so that 97% of the total energy was retained \CITE . //[What is that number ? ]
We evaluated the retrieval performance with measures that are commonly used in information retrieval such as precision , recall , and average precision .
Given a queried person , assuming that \MATH is the total number of faces returned , \MATH is the number of relevant faces , \MATH is the number of relevant faces , we calculated recall and precision as follows : //[Nrel and Nhit are exactly the same here . They should be different .]
Precision and recall only evaluate the quality of an unordered set of retrieved faces .
To evaluate ranked lists , average precision is used .
The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0.0 , 0.1 , 0.2 , ... , 1.0 .
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .
We show in Figure \REF the retrieval performance of the outlier detection methods and the baseline method using text correlation .
In the baseline method , faces were sorted by the time the associated news article was published .
It indicated that the DBO-based method outperformed the others .
The baseline method performed the worst .
The LOF-based method tends to be less sensitive when the threshold is changed .
This suggests that the input face sets were quite dense .
We studied the effect of choosing the number of times \MATH appeared in the Bag-Rank-SVM algorithm .
We used DBO as the method for making the initial rank list from which 30 training subsets were generated and used for training SVM classifiers using linear kernels with probability output .
To select one subset , we set \MATH and \MATH which means 20% of the highest ranked faces were used for \MATH and 30% of the lowest ranked faces were used for \MATH .
The subsets \MATH and \MATH were generated by randomly selecting with replacement 70% samples of \MATH and \MATH .[�gWith replacement�h does not make sense here . I am not sure what you want to say .]
Figure \REF shows the performance of single and ensemble classifiers .
It suggests that the performance does not change significantly after five iterations .
In addition , the performance of the ranking process improved when the ensemble classifier was used .
We set the number of iterations for the Bag-Rank-SVM algorithm at five and set the number of iterations of the outer loop $T=30$ to see how much the final performance changes .
As shown in Figure \REF , the performance did not change much after five iterations .
From these experiments , \MATH and \MATH are suitable values for the proposed method .
The performance of different methods shown in Figure \REF indicates that our proposed method outperformed the distance-based outlier detection method and performed comparable to the supervised method using 5% annotation data .
As shown in Figures \REF , \REF , \REF , our proposed method produced better results in terms of average precision in which relevant faces were put at the top of the returned list .
We presented a method for effectively ranking faces retrieved using text-based correlation methods when searching for a specific person .
Using the rank list estimated from the previous steps , we automatically selected a subset of positive and negative samples to train a classifier using SVM with probability outputs . //[Since this is the conclusion , you might want to be more specific on what �gthe previous steps�h are . ]
This classifier was used to rank input faces for the next step .
Since the labels of training sets were still noisy , the classifiers trained from these datasets were weak .
By combining multiple weak classifiers in a bagging framework , we constructed the final strong classifier , which produced good results .
To obtain the initial rank for the first step , we proposed using a common outlier detection method .
Experiments on a large number of persons with thousands of retrieved images showed the effectiveness of the proposed method .
Face detection , tracking , and recognition for broadcast video
Human face processing techniques for broadcast video , including face detection , tracking , and recognition , have long been a topic that has attracted a lot of research interest due to its crucial value in various applications , such as in video structuring , indexing , retrieval , and summarization .
The main reason for this is that the human face provides rich information for people 's appearances , such as for a government leader in a news video , a pitcher in a sports video or a hero in a movie , and is the basis for interpreting facts .
This article describes some state-of-the art techniques for face detection , tracking , and recognition with applications to broadcast video .
Face detection , which is the task of localizing faces in an input image , is a fundamental part of any face processing system .
The extracted faces can then be used for initializing face tracking or automatic face recognition .
An ideal face detector should possess the following characteristics :
- Robustness : it should be capable of handling appearance variations , such as pose changes , size , illuminations , occlusions , complex backgrounds , facial expressions , and low resolutions .
- Quickness : it should be fast in order to perform real-time processing , which is an important factor in processing large video archives .
- Simplicity : the training process should be simple .
For example , the training time is short , the number of parameters is small , and training samples are collected cheaply .
Many approaches have been proposed for building faster and more robust face detectors \CITE .
Among them , those using advanced learning methods , such as neural network , support vector machines and boosting , are the best .
Typically , detecting the faces in an image takes the following steps :
- Window scanning : in order to detect faces at multiple locations and sizes , a fixed window size ( e.g. 24 x 24 pixels ) is used to extract image patterns at every location and scale .
The number of patterns extracted from a 320 x 240 frame image is large , approximately 160 ,000 , in which only a small number of patterns contain a face .
- Feature extraction : given an image pattern , the features are extracted .
The most popular feature type is the Haar wavelet because it is very fast to compute using the integral image \CITE .
Other feature types can be listed including the pixel intensity \CITE , local binary patterns \CITE , and edge orientation histogram \CITE .
- Classification : the extracted features are passed through a classifier that has been previously trained to classify the input pattern associated with these features as a face or a non-face . //[trained / programmed ?]
- Merging overlapping detections : since the classifier is insensitive to small changes in translation and scale , there might be multiple detections around each face .
In order to return a single final detection per face , it is necessary to combine the overlapping detections into a single detection .
Since the vast majority of processed patterns are non-face , the single classifier based systems , such as the neural network \CITE and the support vector machines \CITE , are usually slow .
To overcome this problem , a combination of simple-to-complex classifiers has been proposed \CITE leading to the first real-time robust face detector in the world .
In this structure , fast and simple classifiers are used as filters in the earliest stages to quickly reject a large number of the non-face patterns and then slower but more accurate classifiers are used for classifying the face-like patterns .
In this way , the complexity of classifiers can be adapted to correspond to the increasing difficulty with the input patterns .
Training classifiers usually consist of the following steps :
- Training set preparation : Supervised learning methods require a large number of training samples to obtain accurate classifiers .
The training samples are patterns that must be labeled as face ( positive samples ) or non-face ( negative samples ) in advance .
Face patterns are manually collected from images containing faces and then are scaled to the same size and normalized to a canonical pose in which the eyes , mouth , and nose are aligned .
Then these face patterns can be used to generate other artificial faces by randomly rotating the images ( about their center points ) by up to 10 degrees , scaling them between 90 and 110% , translating them up to half a pixel , and mirroring them to enlarge the number of positive samples \CITE .
The collection of non-face patterns is usually done automatically by scanning through images which contain no faces .
The accurate classifier described in \CITE requires about five thousand original face patterns and hundreds of millions of non-face patterns extracted from 9 ,500 non-face images .
In \CITE a smaller number of training samples can be used to build a robust face detector by using an edge orientation histogram .
- Learning method selection : Basically , in an ideal situation with the proper settings , the advanced learning methods , such as the neural network , support vector machines , and AdaBoost , can perform similarly .
However , in practice , it is difficult to find these proper settings .
Using a neural network requires the design of layers , nodes , etc. , which is complicated .
Therefore , it is preferable to use support vector machines because only two parameters are necessary if a RBF kernel is used and many tools are available .
Another learning method that has been widely used in many object detection systems is AdaBoost and its variants .
The advantage of AdaBoost is it can be used for both selecting features and learning the classifier .
Face tracking is the process of locating a moving face or several of them over a period of time using a camera , as illustrated in Fig. 1 .
A given face is first initialized manually or by a face detector .
The face tracker then analyzes the subsequent video frames and outputs the location of the initialized face within these frames by estimating the motion parameters of the moving face .
This is different from face detection , the outcome of which is the position and scale of one single face in one single frame ; face tracking enables the information acquisition of multiple consecutive faces within consecutive video frames .
More important , these faces have the same identity .
Although frame-based face detection techniques have been successfully demonstrated on real images , the current ability for detecting faces from video is still primitive .
The quality of the detector responses can decrease due to different reasons including occlusions , lighting conditions , and face poses .
Without any additional information , these responses can easily be rejected , even if they indicate the presence of a face .
It is therefore important to incorporate the temporal information in a video sequence to provide more complete video segments displaying the person of interest , which is always named as / already called ? face tracking .
One of the main applications for face tracking is in the person retrieval from broadcast video , for example : " Intelligent fast-forwards�E, where the video jumps to the next scene containing a certain person / actor ; or retrieval of different TV interventions , e.g. interviews , shows , etc. , of a given person in a video or a large collection of TV broadcast videos .
In [5] , the person retrieval system for a feature-length movie video is proposed using straightforward face tracking .
At run time a user outlines a face in a video frame , and the face tracks within the movie are then ranked according to their similarity to the outlined query face in the same way as Google .
Since one face track corresponds to one identity , the workload of intra-shot face matching is greatly reduced , which is not available in frame-based face detection .
In addition , face tracking provides multiple examples of the same character 's appearance to help with inter-shot face matching .
Face tracking is also used in the area of face-name association , the objective of which is to label television or movie footage with the identity of the person present in each frame of the video .
Everingham et al. [8] proposed an automatic face-name association system .
This system uses a face tracker similar to the one in [5] that can extract a few hundred tracks of each particular character in a single shot .
Based on the temporal information obtained from the face tracker , the textual information for TV and the movie footage including the subtitles and transcripts is employed to assign the character 's name to each face track .
For instance , shots containing a particular person can be retrieved by a keyword like " Bush " or " Julia Roberts " instead of the use of an outlined query face as used in [5] .
Besides broadcast video , face tracker also has important applications in the videos used in humanoid robotics , visual surveillance , human-computer interaction ( HCI ) , video conferencing , and face-based biometric person authentication among others .
Choosing a face tracker can be a difficult task because of the variety of face trackers currently available .
The application provider will have to decide which face tracker is best suited to his / her individual needs and , of course , the type of video that he / she wants to use as the target .
Generally speaking , the important issues that should be addressed include speed , robustness , and accuracy .
Can the system run in real time ? Similar to many other processing tools for broadcast video , speed is not the most critical issue because offline processing is permitted in most video structuring and indexing cases .
However , a real-time face tracker will become necessary if a target archive is established from too large a quantity of videos , e.g. 24-hour continuous video recording that needs daily structuring .
On the other hand , the speed of the tracker is critical in most of the application cases for non-broadcast video , e.g. HCI .
It should be noted that there is always a tradeoff between speed and performance-related issues including the robustness and accuracy .
Can the system cope with varying illuminations , facial expressions , scales , poses , camerawork , occlusion , and large head motions ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who is moving from an indoor to an outdoor environment .
Face tracking also tends to fail under large facial deformations of the eyes , nose , mouth , etc. due to the facial expression variation .
Different from non-broadcast video , e.g. video used for HCI , faces appearing in broadcast video vary from large close-up faces to small faces taken by a long-shot .
A smaller face scale always leads to a lower resolution and will reject most face trackers designed by computer vision researchers .
Pose variations , i.e. head rotations including the pitch , roll , and yaw , is another influencing factor , which can cause disappearances of parts of faces .
In some cases , the scale and pose variations might be caused by camerawork changes .
The partial disappearance of a face is also apt to happen due to occlusion by other objects , and motion information may be distracted by an alternate motion .
Moreover , the task of face tracking becomes even more difficult when the head is moving fast relative to the frame rate , so that the tracker fails to �arrive in time�E.
How accurate is the tracking ? The first factor that affects the accuracy might be the false face detections generated when initializing the tracker by a face detector .
This problem is difficult to solve because it has a fixed threshold .
Lowering the threshold of the face detector reduces the number of false rejections , but increases the number of false detections , and vice versa .
The drifting or the long sequence motion problem is another factor that might affect the accuracy .
This problem always happens due to the imperfect motion estimation technique .
A tracker might accumulate motion errors and eventually lose track of a face , for instance , when tracking faces that change from a frontal view to a profile position .
Face tracking can be considered an algorithm that analyzes the video frames and outputs the location of moving faces within the video frame .
For each tracked face , three steps are involved , which are the initialization , tracking , and stopping procedures , as illustrated in Fig. 2 .
Most of the developed methods use a face detector for the initialization of their tracking processes .
An always ignored but existing difficulty with this step lies in the control of the false face detections described above .
Another problem is the difficulty in handling the appearance of new non-frontal faces .
Although there have been literatures on profile or intermediate pose face detectors , this kind of work suffers from the false-detection problem far more than a frontal face detector .
To alleviate these two problems , Chaudhury et al. [1] used two face probability maps instead of a fixed threshold to initialize the face tracker , one for frontal views and one for profiles .
All local maxima in these maps are chosen as the face candidates , the face probabilities of which are propagated throughout the temporal sequence .
Candidates whose probabilities either go to zero or remain low over time are determined as non-face and eliminated .
The information from the two face probability maps is combined to represent an intermediate head pose .
Their experiments showed that the proposed probabilistic detector improved the accuracy more than a traditional face detector and is able to handle the head movement covering a range of �90 degrees out-of-plane rotation ( yaw ) .
After initialization , one should choose what features to track before tracking a face .
The exploitation of color is one of the more common choices in order to be invariant to facial expressions , scale , and pose changes [4 , 9] .
However , color-based face trackers often depend on a learning set dedicated to the type of processed videos and are not guaranteed to be easily expendable to unknown videos with varying illumination conditions or different races .
Also , color is susceptible to occlusion by other head-like objects .
Two other choices are the key-point [5 , 8] and facial features [3 , 6 , 10] , e.g. eyes , nose , mouth , etc. , both of which are more robust to varying illuminations and occlusions .
Although the generality of key-points allows for tracking different kinds of objects , without any face-specific knowledge its discriminant power between the target and clutter might be in peril under tough conditions , e.g. strong background noise .
Facial features enable the tracking of higher-level information from a human face , but are weak in lower video quality .
Most facial-feature-based face trackers [6 , 10] have been tested using only non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .
Note that these different cues described above may be combined .
An appearance-based or featureless tracker matches an observation model of the entire facial appearance with the input image , instead of choosing only a few features to track .
One example of an appearance-based face tracker is [1] , which was introduced above .
Another example was proposed by Li et al. [9] , which uses a multi-view face detector to detect and track faces from different poses .
Besides the face-based observation model , a head model is also included to represent the information of head rear .
It is based on the idea that a head can be considered an object of interest instead of a face , because the face is not always present in the tracking process .
An extended particle filter is proposed to fuse these two interrelated information together so as to handle the occlusion due to out-of-plane head rotation ( yaw ) that is more than �90 degrees .
During the tracking procedure , face tracking systems usually use a motion model that describes how the image of the target might change for different possible motions of the face to track .
Some examples of simple motion models are as follows .
Based on the assumption that a face can be considered a planar object , the corresponding motion model can be a 2D transformation , e.g. affine transformation or homography , of an image of the face , e.g. the initial frame [3 , 6] .
Some researchers view a face as a rigid 3D object , thus the motion model defines its aspect depending on its 3D position and orientation [10] .
However , a face is actually both 3D and deformable .
Some systems try to model faces in this sense , and the image of deformed face can be covered with a mesh , i.e. a sophisticated geometry and texture face model [2 , 7] .
The motion of the face is defined by the position of the nodes of the mesh .
Generally if the quality of the video is high , a more sophisticated motion model is used , and then the face tracker generates a more accurate result .
For instance , a sophisticated geometry and texture model might suffer from false face detections and a level of drifting [less than / that is worse than ?] a simple 2D transformation model .
However , it must be noted that most 3D-based and mesh-based face trackers require a relatively clear appearance , high resolution , and a limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
Both of these requirements are always unavailable in the case of broadcast video .
Therefore , most 3D-based and mesh-based face trackers are only tested by using non-broadcast video , e.g. webcam video [2 , 7 , 10] .
Finally , the stopping procedure is rarely discussed .
This constitutes a major deficiency for the face tracking algorithms that are generally not able to stop a face track in case of tracking errors , i.e. drifting .
Arnaud et al. [3] proposed an approach that uses a general object tracker for face tracking and a stopping criterion based on the addition of an eye tracker to alleviate drifting .
The two positions of the tracked eyes are compared with the tracked face position .
If neither of the eyes is in the face region , it will be determined as drifting and the tracking process will be stopped .
In addition , most mesh-based and top-down trackers are assumed to be able to avoid drifting .
Face tracking has attracted much attention from researchers in communities including multimedia content analysis , computer vision , etc. because of its wide applications .
However , while most of the attempts have been on the face tracking for high-quality videos by computer vision researchers , only a limited number of face trackers are designed for broadcast video .
This is because the current ability of face tracking still depends on a relatively clear appearance , high resolution , and limited pose variation of the face , which are unavailable in broadcast video .
On the other hand , currently proposed face trackers are still evaluated by using different types of videos and different criteria .
A general evaluation criterion , in terms of speed , robustness , and accuracy , is needed for a performance comparison between the face trackers with different purposes .
Unsupervised Face Annotation by Mining the Web
Searching for images of people is an essential task for image and video search engines .
However , current search engines have limited capabilities for this task since they rely on text associated with images and video , and such text is likely to return many irrelevant results .
We propose a method for retrieving relevant faces of one person by learning the visual consistency among results retrieved from text-correlation-based search engines .
The method consists of two steps .
In the first step , each candidate face obtained from a text-based search engine is ranked with a score that measures the distribution of visual similarities among the faces .
Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list , respectively .
The second step improves this ranking by treating this problem as a classification problem in which input faces are classified as 'person-$X$' or 'non-person-$X$' ; and the faces are re-ranked according to their relevant score inferred from the classifier 's probability output .
To train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers trained using different subsets .
These training subsets are extracted and labeled automatically from the rank list produced from the classifier trained from the previous step .
In this way , the accuracy of the ranked list increases after a number of iterations .
Experimental results on various face sets retrieved from captions of news photos show that the retrieval performance improved after each iteration , with the final performance being higher than those of the existing algorithms .
With the rapid growth of digital technology , large image and video databases have become more available than ever to users .
This trend has shown the need for effective and efficient tools for indexing and retrieving visual content .
A typical application is searching for a specific person by providing his or her name .
Most current search engines use the text associated with images and video as significant clues for returning results .
However , other un-queried faces and names may appear with the queried ones ( Figure xx ) , and this significantly lowers the retrieval performance .
One way to improve the retrieval performance is to take into account visual information present in the retrieved faces .
This task is challenging for the following reasons :
-Large variations in facial appearance due to pose changes , illumination conditions , occlusions , and facial expressions make face recognition difficult even with state-of-the-art techniques\CITE ( see example in Figure xx ) .
-The fact that the retrieved face set consists of faces of several people with no labels makes supervised and unsupervised learning methods inapplicable .
We propose a method for solving the above problem .
The main idea is the assumption that there is visual consistency among the results returned from text-based search engines and this visual consistency is then learned through an interactive process .
This method consists of two stages .
In the first stage , we explore the local density of faces to identify potential candidates for relevant faces and irrelevant faces .
This stage reflects the fact that the facial images of the queried person tend to form dense clusters , whereas irrelevant facial images are sparse since they look different from each other .
For each face , we define a score to measure the density of its neighbor set .
This score is used to form a ranked list , in which faces with high-density scores are considered relevant and are put at the top .
The above ranking method is weak since dense clusters have no guarantee of containing relevant faces .
Therefore , a second stage is necessary to improve this ranked list .
We model this problem as a classification problem in which input faces are classified as person-\MATH ( the queried person ) or non-person-\MATH ( the un-queried person ) .
The faces are ranked according to a relevancy score that is inferred from the classifier 's probability output .
Since annotation data is not available , the rank list from the previous step is used to assign labels for a subset of faces .
This subset is then used to train a classifier using supervised methods such as a support vector machine ( SVM ) .
The trained classifier is used to re-rank faces in the original input set .
This step is repeated a number of times to get the final ranked list .
Since automatically assigning labels from the ranked list is not reliable , the trained classifiers are weak .
To obtain the final strong classifier , we use the [idea / concept?] of ensemble learning \CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .
The learned classifier can be further used for recognizing new facial images of the queried person .
The second stage improves the ranked list and recognition performance for the following reasons :
-Supervised learning methods , such as an SVM , provide a strong theoretical background for finding the optimal decision boundary even with noisy data .
Furthermore , recent studies \CITE suggest that SVM classifiers provide probability outputs that are suitable for ranking .
-The bagging framework helps to leverage noises in the unsupervised labeling process .
Our contribution is two-fold :
-We propose a general framework to boost the face retrieval performance of text-based search engines by visual consistency learning .
The framework seamlessly integrates data mining techniques such as supervised learning and unsupervised learning based on bagging .
-Our framework requires only a few parameters and works stably .
We demonstrate its feasibility with a practical web mining application .
A comprehensive evaluation on a large face dataset of many people was carried out and confirmed that our approach is promising .
There are several approaches for re-ranking and learning models from web images .
Their underlying assumption is that text-based search engines return a large fraction of relevant images .
The challenge is how to model what is common in the relevant images .
One approach is to model this problem in a probabilistic framework in which the returned images are used to learn the parameters of the model .
For examples , as described by Fergus et al. \CITE , [Reference numbers generally should not be grammatically part of the sentence .
It is better to use the authors�f names .] objects retrieved using an image search engine are re-ranked by extending the constellation model .
Another proposal , described in \CITE , uses a non-parametric graphical model and an interactive framework to simultaneously learn object class models and collect object class datasets .
The main contribution of these approaches is probabilistic models that can be learned with a small number of training images .
However , these models are complicated since they require several hundred parameters for learning and are susceptible to over-fitting .
Furthermore , to obtain robust models , a small amount of supervision is required to select seed images .
Another study \CITE proposed a clustering-based method for associating names and faces in news photos .
To solve the problem of ambiguity between several names and one face , a modified \MATH-means clustering process was used in which faces are assigned to the closest cluster ( each cluster corresponding to one name ) after a number of iterations .
Although the result was impressive , it is not easy to apply it to our problem since it is based on a strong assumption that requires a perfect alignment when a news photo only has one face and its caption only has one name .
Furthermore , a large number of irrelevant faces ( more than 12\% ) have to be manually eliminated before clustering .
A graph-based approach was proposed by Ozkan and Duygu \CITE , in which a graph is formed from faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and can therefore , be solved by taking an available solution . //It might be unclear as to what " available solution " you are talking about . You might want to give more detail here .
Although experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of the relevant faces of the queried person and it is easy to extend for the ranking problem .
Furthermore , choosing an optimal threshold to convert the initial graph into a binary one is difficult and rather ad hoc due to dimensionality .
An advantage of these methods \CITE is they are fully unsupervised .
However , a disadvantage is that no model is learned for predicting new images of the same category .
Furthermore , they are used for performing hard categorization on input images that are inapplicable for re-ranking . //It is not clear if " hard categorization " is inapplicable or if the " input images " are inapplicable .
The balance of recall and precision was not addressed .
Typically , these approaches tend to ignore the recall to obtain high precision .
This leads to the reduction in the number of collected images .
Our approach combines a number of advances over the existing approaches .
Specifically , we learn a model for each query from the returned images for purposes such as re-ranking and predicting new images .
However , we used an unsupervised method to select training samples automatically , which is different from the methods proposed by Fergus et al. and Li et al. \CITE .
This unsupervised method is different from the one by Ozkan and Dugyu \CITE in the modeling of the distribution of relevant images .
We use density-based estimation rather than the densest graph .
Given a set of images returned by any text-based search engine for a queried person ( e.g. 'George Bush' ) , we perform a ranking process and learning of person |\MATH 's model as follows :
-Step 1 : Detect faces and eye positions , and then perform face normalizations .
-Step 2 : Compute an eigenface space and project the input faces into this subspace .
-Step 3 : Estimate the ranked list of these faces by rank-by-local-density score .
-Step 4 : Improve this ranked list using rank-by-bagging-probSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet. You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
Steps 1 and 2 are typical for any face processing system , and they are described in section \REF .
The algorithms used in Steps 3 and 4 are described in section \REF and section \REF , respectively .
Figure \REF illustrates the proposed framework .
Among the faces retrieved by text-based search engines for a query of person-\MATH , as shown in Figure \REF , relevant faces usually look similar and forms the largest cluster .
One approach of re-ranking these faces is to cluster based on visual similarity .
However , to obtain ideal clustering results is impossible since these faces are high dimensional data and the clusters are in different shapes , sizes , and densities .
Instead , a graph-based approach was proposed by Ozkan and Dugyu \CITE in which the nodes are faces and edge weights are the similarities between two faces .
With the observation that the nodes ( faces ) of the queried person are similar to each other and different from other nodes in the graph , the densest component of the full graph ? the set of highly connected nodes in the graph ? will correspond to the face of the queried person .
The main drawback of this approach is it needs a threshold to convert the initial weighted graph to a binary graph .
Choosing this threshold in high dimensional spaces is difficult since different persons might have different optimal thresholds .
We use the idea of density-based clustering described by Ester et al. and Breunig et al. \CITE to solve this problem . //idea / concept?
Specifically , we define the local density score ( LDS ) of a point \MATH( i.e. a face ) as the average distance to its k-nearest neighbors .
where \MATH is the set of \MATH - neighbors of \MATH , and \MATH is the similarity between \MATH and \MATH .
Since faces are represented in high dimensional feature space , and face clusters might have different sizes , shapes , and densities , we do not directly use the Euclidean distance between two points in this feature space for \MATH .
Instead , we use another similarity measure defined by the number of shared neighbors between two points .
The efficiency of this similarity measure for density-based clustering methods was described . //There is no period here , so it is not clear if there should be a period or there should be more to this sentence that is not here . If the sentence does end here , you might want to go into more detail about who or what " described " this .
A high value of \MATH indicates a strong association between \MATH and its neighbors .
Therefore , we can use this local density score to rank faces .
Faces with higher scores are considered to be potential candidates that are relevant to person-\MATH , while faces with lower scores are considered as outliers and thus are potential candidates for non-person-\MATH .
Algorithm 1 : Rank-By-Local-Density-Score Step 1 : For each face p , compute LDS( p , k ) , where k is the number of neighbors of p and is the input of the ranking process .
Step 2 : Rank these faces using LDS( p , k ) ( The higher the score the more relevant ) .
One limitation of the local density score based ranking is it cannot handle faces of another person strongly associated in the \MATH-neighbor set ( for example , many duplicates ) .
Therefore , another step is proposed for handling this case .
As a result , we have a model that can be used for both re-ranking current faces and predicting new incoming faces .
The main idea is to use a probabilistic model to measure the relevancy of a face to person-\MATH , \MATH .
Since the labels are not available for training , we use the input rank list found from the previous step to extract a subset of faces lying at the top and bottom of the ranked list to form the training set .
After that , we use an SVM with probabilistic output \CITE implemented in LibSVM \CITE to learn the person-\MATH model .
This model is applied to faces of the original set , and the output probabilistic scores are used to re-rank these faces .
Since it is not guaranteed that faces lying at two ends of the input rank list correctly correspond to the faces of person-\MATH and faces of non person-\MATH , we adopt the [idea / concept?] of a bagging framework \CITE in which randomly selecting subsets to train weak classifiers , and then combining these classifiers help reduce the risk of using noisy training sets .
The details of the Rank-By-Bagging-ProbSVM-InnerLoop method , improving an input rank list by combining weak classifiers trained from subsets annotated by that rank list , are described in Algorithm 2 .
Step 1 : Train a weak classifier , hi .
Step 1 .1 : Select a set Spos including p% of top ranked faces and then randomly select a subset S?pos from Spos .
Label faces in S?pos as positive samples .
Step 1 .2 : Select a set Sneg including p% of bottom ranked faces and then randomly select a subset S? neg from Sneg .
Label faces in S? neg as negative samples .
Step 1 .3 : Use S?pos and S? neg to train a weak classifier , hj , using LibSVM [8] with probability outputs .
Step 2 : Compute ensemble classifier Hi = Pij=1 hj .
Step 3 : Apply Hi to the original face set and form the rank list , Ranki , using the output probabilistic scores .
Step 4 : Repeat steps 1 to 3 until Dist2RankList( Ranki?1 ,Ranki ) <= ? .
Step 5 : Return Hi = Pij=1 hj .
Step 1 : Rankcur = Rank-By-Bagging-ProbSVM-InnerLoop ( Rankprev ) .
Step 2 : dist = Dist2RankList ( Rankprev ,Rankcur ) .
Step 3 : Rankfinal = Rankcur .
Step 4 : Rankprev = Rankcur .
Step 5 : Repeat steps 1 to 4 until dist <= ? .
Step 6 : Return Rankfinal .
Given an input ranked list , Rank-By-Bagging-ProbSVM-InnerLoop is used to improve this list .
We repeat the process a number of times whereby the ranked list output from the previous step is used as the input ranked list of the next step .
In this way , the iterations significantly improve the final ranked list .
The details are described in Algorithm 3 .
To determine the number of iterations of Rank-By-Bagging-ProbSVM-InnerLoop and Rank-By-Bagging-ProbSVM-OuterLoop , we use the \MATH distance \CITE , which is a metric that counts the number of pairwise disagreements between two lists .
The larger the distance , the more dissimilar the two lists are .
The \MATH distance between two lists , \MATH and \MATH , is defined as follows :
Since the maximum value of \MATH is \MATH , where \MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :
Using this measure for checking when the loops stop means that if the ranked list does not change significantly after a number of iterations , it is reasonable to stop .
We used the dataset described by Berg et al. \CITE for our experiments .
This dataset consists of approximately half a million news pictures and captions from Yahoo News collected over a period of roughly two years .
This dataset is better than datasets collected from image search engines such as Google that usually limit the total number of returned images to 1 ,000 .
Furthermore , it has annotations that are valuable for evaluation of methods .
Note that these annotations are used for evaluation purpose only .
Our method is fully unsupervised , so it assumes the annotations are not available at running time .
Only the front of faces were considered since current frontal face detection systems \CITE work in real time and have accuracies exceeding 95\% .
44 ,773 faces were detected and normalized to 86\MATH86 pixels .
We selected fifteen government leaders , including George W. Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals , such as John Paul II ( the Former Pope ) and Hans Blix ( UN ) , because their images frequently appear in the dataset \CITE .
Variations in each person 's name were collected .
For example , George W. Bush , President Bush , U.S. President , etc. , all refer to the current U.S. president .
We performed simple string search in captions to check whether a caption contained one of these names .
The faces extracted from the corresponding image associated with this caption were returned .
The faces retrieved from the different name queries were merged into one set and used as input for ranking .
Figure \REF shows the distribution of retrieved faces from this method and the corresponding number of relevant faces for these fifteen individuals .
In total , 5 ,603 faces were retrieved in which 3 ,374 faces were relevant .
On average , the accuracy was 60 .22\% .
We used an eye detector to detect the positions of the eyes of the detected faces .
The eye detector , built with the same approach as that of Viola and jones \CITE , had an accuracy of more than 95\% .
If the eye positions were not detected , predefined eye locations were assigned .
The eye positions were used to align faces to a predefined canonical pose .
To compensate for illumination effects , the subtraction of the best-fit brightness plane followed by histogram equalization was applied .
This normalization process is shown in Figure \REF .
We then used principle component analysis \CITE to reduce the number of dimensions of the feature vector for face representation .
Eigenfaces were computed from the original face set returned using the text-based query method .
The number of eigenfaces used to form the eigen space was selected so that 97\% of the total energy was retained \CITE . //It is not clear what you mean by " energy " in this context . This is the first time you mention this term . You might want to specify what kind of energy you are talking about .
The number of dimensions of these feature spaces ranged from 80 to 500 .
We evaluated the retrieval performance with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as follows :
Precision and recall are only used to evaluate the quality of an unordered set of retrieved faces .
To evaluate ranked lists in which both recall and precision are taken into account , average precision is usually used .
The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries
The parameters of our method include :
-\MATH : the fraction of faces at the top and bottom of the ranked list that are used to form a positive set \MATH and negative set \MATH for training weak classifiers in Rank-By-Bagging-ProbSVM-InnerLoop .
We empirically selected \MATH ( i .e 40\% samples of the rank list were used ) since a larger \MATH will increase the number of incorrect labels , and a smaller \MATH will cause over-fitting .
In addition , \MATH consists of \MATH samples that are selected randomly with replacement from \MATH .
This sampling strategy is adopted from the bagging framework \CITE .
The same setting was used for \MATH .
-\MATH : the maximum Kendall tau distance \MATH between two rank lists \MATH and \MATH .
This value is used to determine when the inner loop and the outer loop stop .
We set \MATH for balancing between accuracy and processing time .
Note that a smaller \MATH requires more iterations , making the system 's speed slower .
-\MATH : the kernel type is used for the SVM .
The default is a linear kernel that is defined as : \MATH .
We have tested other kernel types , such as RBF or polynomial , but the performance did not change much .
Therefore , we used the linear kernel for simplicity .
We performed a comparison between our proposed method with other ones .
Text Based Baseline ( TBL ) : Once faces corresponding with images whose captions contain the query name are returned , they are ranked in time order .
This is a rather naive method in which no prior knowledge between names and faces is used .
Distance-Based Outlier ( DBO ) : We adopted the idea of distance-based outlier detection for ranking \CITE .
Given a threshold \MATH , for each point \MATH , we counted the number of points \MATH so that \MATH , where \MATH is the Euclidean distance between \MATH and \MATH in the feature space mentioned in section \REF .
This number was then used as the score to rank faces .
We selected a range of \MATH values for experiments : \MATH .}
Densest Sub-Graph based Method ( DSG ) : We re-implemented the densest sub-graph based method \CITE for ranking .
Once the densest subgraph was found after an edge elimination process , we counted the number of surviving edges of each node ( i .e face ) and used this number as the ranking score .
To form the graph , the Euclidean distance \MATH was used to assign the weight for the edge linked between node $p$ and node \MATH .
DSG requires a threshold \MATH to convert the weighted graph to the binary graph before searching for the densest subgraph .
We selected a range of \MATH values that are the same as the values used in DBO : \MATH .
Local Density Score ( LDS ) : This is the first stage of our proposed method .
It requires the input value \MATH to compute the local density score .
Since we do not know the number of returned faces from text-based search engines , we used another input value \MATH , defined as the fraction of neighbors , and estimated \MATH by the formula : \MATH , where \MATH is the number of returned faces .
We used a range of $fraction$ values for experiments : \MATH .
For a large number of returned faces , we set \MATH to the maximum value of 200 : \MATH .
Unsupervised Ensemble Learning Using Local Density Score ( UEL-LDS ) : This is a combination of ranking by local density scores , and the ranked list is used for training a classifier to boost the rank list .
Supervised Learning ( SVM-SUP ) : We randomly selected a portion \MATH of the data with annotations to train the classifier ; and then used this classifier to re-rank the remaining faces .
This process was repeated five times and the average performance was reported .
We used a range of portion \MATH values for experiments : \MATH .
Figure \REF shows a performance comparison of these methods .
Our proposed methods ( LDS and UEL-LDS ) outperform other unsupervised methods such as TBL , DBO , and DSG .
Furthermore , the DBO and DSG methods are sensitive to the distance threshold , while the performance of our proposed method is less sensitive .
It confirms that the similarity measure using shared nearest neighbors is reliable for estimation of the local density score .
The performance of UEL-LDS is slightly better than LDS since the training sets labeled automatically from the ranked list are noisy .
However , UEL-LDS improves significantly even when the performance of LDS is poor .
These performances are worse than that of SVM-SUP using a small number of labeled samples .
Figure \REF shows an examples of the top 50 faces ranked using the TBL , DBO , DSG , and LDS methods .
The performance of DBO is poor since a low threshold is used .
This ranks irrelevant faces that are near duplicates ( rows 2 and 3 in Figure \REF( b ) ) higher than relevant faces .
This explains the same situation with DSG .
In Figure \REF , we show the performance of five single classifiers and that of five ensemble classifiers .
The ensemble classifier \MATH is formed by combining single classifiers from \MATH to \MATH .
It clearly indicates that the ensemble classifier is more stable than single weak classifiers . //You use both plural and singular forms of " classifier " here , so it is a bit confusing if you are talking about a single classifier or more than one . I suggest you use the same form throughout if applicable .]
We conducted another experiment to show the effectiveness of our approach in which learned models are used to annotate new faces of other databases .
We used each name in the list as a query to obtain the top 500 images from the Google Image Search Engine ( GoogleSE ) .
Next , these images were processed using the steps described in section \REF : extracting faces , detecting eyes , and doing normalization .
We projected these faces to the PCA subspace trained for that name and used the learned model to re-rank faces .
There were 4 ,103 faces ( including false positives - non-faces detected as faces ) detected from 7 ,500 returned images .
We manually labeled these faces and there were 2 ,342 relevant faces .
On average , the accuracy of the GoogleSE is 57 .08\% .
In Table \REF , we compare the performance of the methods .
The performance of UEL-LDS was obtained by running the best system , which is shown as the peak of the UEL-LDS curve in Figure \REF .
The performances of SVM-SUP-05 and SVM-SUP-10 correspond to the supervised systems ( cf . section \REF ) that used \MATH of the data set , respectively .
We evaluated the performance by calculating the precision of the top 20 returned faces , which is common for image search engines and recall and precision on all detected faces of the test set .
UEL-LDS achieved comparable performance to the supervised methods and outperformed the baseline GoogleSE .
The precision of the top 20 of SVM-SUP-05 is poorer than that of UEL-LDS due to the small number of training samples .
Figure \REF shows top 20 faces ranked using these two methods .
Our approach works fairly well for well known people , where the main assumption that text-based search engines return a large fraction of relevant images is satisfied .
Figure \REF shows an example where this assumption is broken .
Consequently , as shown in Figure \REF , the model learned by this set performed poorly in recognizing new faces returned by GoogleSE .
Our approach solely relies on the above assumption ; therefore , it is not affected by the ranking of text-based search engines .
The iteration of bagging SVM classifiers does not guarantee a significant improvement in performance .
The aim of our future work is to study how to improve the quality of the training sets used in this iteration .
We presented a method for ranking faces retrieved using text-based correlation methods in searches for a specific person .
This method learns the visual consistency among faces in a two-stage process .
In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely to be relevant or irrelevant faces , respectively .
In the second stage , a bagging framework is used to combine weak classifiers trained on subsets labeled from the ranked list into a strong classifier .
This strong classifier is then applied to the original set to re-rank faces on the basis of the output probabilistic scores .
Experiments on various face sets showed the effectiveness of this method .
Our approach is beneficial when there are several faces in a returned image , as shown in Figure \REF .
A Text Segmentation Based Approach to Video Shot Boundary Detection
Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .
Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle the various transition types caused by photo flashes , rapid camera movement , and object movement is still challenging .
We present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing . //detecting / determining?
This is possible by assuming that each frame is a word and then the shot boundaries are treated as text segment boundaries ( e.g. topics ) .
The text segmentation based approaches in natural language processing can be used .
The experimental results from various long video sequences have proven the effectiveness of our approach .
Recent advances in digital technology have made many video archives readily available .
Therefore scalable , efficient , and effective tools for indexing and retrieving video are needed .
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
There are many types of transitions between shots .
According to TRECVID 's categorization \CITE , shot boundaries can be classified into two main categories : cut and gradual .
A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs over a number of consecutive frames .
With the gradual type , fades and dissolves are common .
A fade is usually a change in brightness with one or several solid black frames in between the key frames , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
Figure \REF shows examples of shot boundary types .
Many approaches have been proposed for shot boundary detection .
The simplest approach is to compute the differences between the color distributions of consecutive frames and use a threshold to classify whether a hard cut occurs .
In order to detect gradual transitions , edge change ratios or motion vectors can be used \CITE .
Since these approaches use threshold-based models for detection , their advantage is they are fast .
Nevertheless , they are sensitive to changes in illumination and motion .
Furthermore , they are difficult to generalize for new datasets .
Recent works \CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .
In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .
Specifically , each frame is considered a word and a set of consecutive frames , forming a shot , is considered a text segment .
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of the following labels :
PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) , and POSTSEG ( word outside a segment ) .
Given a sequence of labeled words , the boundary between text segments can be identified .
The remainder of this paper is organized as follows .
In section \REF , we present an overview of our framework .
Section \REF introduces our experiments on different long video sequences from the TRECVID dataset .
Section \REF concludes the paper .
The shot boundary detection process for a given video is carried out through two main stages .
In the first stage , frames are extracted and labeled with pre-defined labels .
In the second stage , the shot boundaries are identified by grouping the labeled frames into segments .
We use the following six labels to label frames in a video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , and POST -GRAD ( post-frame of a GRADUAL transition ) .
Given a sequence of labeled frames , the shot boundaries and transition types are identified by looking up and processing the frames marked with a non NORM -FRM label .
For example , if we encounter two consecutive frames respectively marked by IN-CUT and POST-CUT , we can declare that a shot boundary occurs at these frames and the transition type is a CUT .
In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare that a GRADUAL shot boundary occurs at these frames .
Figure \REF shows an example of the labeled frames of a shot transition .
To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .
The feature extraction process and classifier learning using a support vector machine ( SVM ) are described in detail below .
We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .
However , using this representation is not discriminative enough for frame categorization since the frames of a shot transition usually strongly relate to their neighboring frames .
For example , an abrupt change in illumination between two consecutive frames is a strong cue for a hard cut , or one solid black frame in between dark and then bright frames might help to identify a fade shot transition .
Therefore , in this study , we do not directly use the above features .
Instead , we use them indirectly to model the difference and motion between the current frame and its neighboring frames .
In particular , for each frame , we compute \MATH distances between the current frame \MATH and neighboring frames ranging from \MATH .
These distances are used to form a feature vector for frame \MATH in the training and testing process later .
In this way , we can have a unified framework for the shot boundary detection and consequently avoid having to give special treatment to the different shot boundary types as described in many of the works that participated the TRECVID benchmark \CITE .
Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing the color distributions of images \CITE .
The first order ( mean ) , the second order ( variance ) , and the third order ( skewness ) color moments are defined as :
where \MATH is the value of the \MATH -th color component of image pixel \MATH , and \MATH is the number of pixels in the image .
Edge orientation histogram has also been widely used in shot boundary detection \CITE .
The basic steps for computing the edge orientation histogram features are as follows :
Extract edges from the input image by using Canny edge detector .
Compute a \MATH -bin histogram of edge and non-edge pixels .
The first \MATH bins are used to represent the edge directions quantized at a \MATH interval and the remaining bin is used for counting the non-edge pixels .
The histogram is normalized by the total number of all the pixels to compensate for different image sizes .
We use color moments and an edge orientation histogram to compute the distances between the current frame \MATH and its neighboring frames as follows :
The input image is converted to a LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \MATH grid .
The color moments and edge orientation histogram are extracted from these sub-images .
For color moments , there are \MATH values .
For the edge orientation histogram , there are \MATH values for each input frame image .
Compute \MATH values , which are the Euclidean distances between the current frame \MATH and its neighboring frames ranging from \MATH .
In other words , we compute \MATH , where \MATH .
These values \MATH are then used to form the feature vector for frame \MATH .
Support Vector Machines ( SVM ) are a statistical learning method based on the structure risk minimization principle \CITE .
They have been very efficiently proved to be useful in many pattern recognition applications \CITE .
In the case of binary classification , the objective of the SVM is to find the best separating hyperplane with a maximum margin .
The form of the SVM classifiers is : \MATH
where \MATH is the d-dimensional vector of an observation example , \MATH is the class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
SVMs were originally designed for binary classification .
There are two common approaches for handling multi-class classification .
The first one is the one-against-all method that combines \MATH binary classifiers , where \MATH is the number of classes .
The \MATH SVM classifier is trained using positive samples as examples of the \MATH class and negative samples as the examples of the other classes .
The second one is the one-against-one method that combines \MATH binary classifiers in which each classifier is trained on examples from the two classes .
There are seven classes in our framework : NORM FRM ( frame of a normal shot ) , PRE CUT ( pre-frame of a CUT transition ) , POST CUT ( postframe
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) , and NORM-FRM ( normal frame that does not belong to any shot transitions ) .
To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?
Using the trained classifier , we can label a sequence of frames with the tags mentioned above .
A gradual transition usually has a " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' pattern and a cut transition usually has a " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " 'pattern .
The shot boundary detection process is started by checking for these transition patterns in the tagged sequence .
Once a pattern is encountered , PRE-xxx and POST-xxx tags are used to identify the shot boundary and the two ends of the shot transition .
Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .
Instead , we use a more flexible matching algorithm in which a match is declared if a portion of the predefined pattern is found in the input sub-sequence .
We used annotated data sets from the TRECVID 2003 test sets for the training and testing .
We divided eight videos , each 30-minute long , into two sets : a training set and a test set .
The number of frames , the number of shot boundaries , and the types of these sets are shown in Table \REF .
Note that , the number of shot boundaries is equal to the number of frames with a PRE-CUT / GRAD label and the number of frames with a PRE-CUT / GRAD label is equal to the number of frames within a POST-CUT / GRAD label .
We used \MATH grid to divide the input image into sub-images .
As for the edge orientation histogram , we used 12-bins for the edge pixels and one bin for the non-edge pixels .
Furthermore , we used 20 neighboring frames before and after the current frame ( \MATH ) for computing the distances .
These parameters were selected from our empirical studies when participating in TRECVID 's tasks .
The extracted features are normalized to zero mean and a unit standard deviation and then stored for training and testing .
Specifically , the normalized vector \MATH
where \MATH is the \MATH-th element of the feature vectors \MATH , respectively , and \MATH is the number of dimensions .
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take the \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
We use LibSVM \CITE to train the SVM classifiers with a RBF kernel .
The optimal \MATH parameters are found by conducting a grid search with a 5-fold cross validation on a subset of 10 ,000 samples stratified selected from the original dataset .
As for the multi-class classification , LibSVM used the one-against-one approach .
The results that were evaluated by a tool provided by TRECVID with a standard measurements , such as the precision , recall , and F1 score , clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process . //for / by?
Specifically , we selected three sampling rates \MATH , which were \MATH and \MATH .
As shown in Figure \REF , the best performance was obtained at a sampling rate of \MATH .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
The first one is GCM , the second one is EOH , and the last one GCM+EOH is a combination of the distances using GCM and the distances using EOH .
The number of dimensions of the feature vectors using GCM and EOH was 20 , while that of feature vectors using GCM+EOH was 40 .
We also compared the proposed method with the baseline method that computes the differences in the color histograms between two consecutive frames , and then decides the shot transition by using a predefined threshold .
In Figure \REF , we superimposed our result on the results reported in the shot boundary detection task of TRECVID 2003 .
Our system achieves a high precision and recall for the CUT transition and this result is comparable to the third-ranked system .
Note that our system is general and has no special treatment for particular shot transitions .
Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .
Therefore , it is generalization is difficult for new test sets .
We have proposed a unified and general framework for shot boundary detection that uses a text segmentation based approach .
Firstly , we label the frames with one of the six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD , and POST -GRAD .
Then we extract the shot boundaries and types from these labeled frames .
In order to label frames , we proposed a new feature type to model the difference and motion in color and the edges between the frames and used it in the classification with SVM classifiers .
The experiments we conducted on various videos from TRECVID 2003 have shown that our approach is effective .
Ent-Boost : Boosting Using Entropy Measures
for Robust Object Detection
Recently , boosting has come to be used widely in object detection applications because of its impressive performance in both speed and accuracy .
However , learning weak classifiers , which is one of the most significant tasks in using boosting , is left to users . //learning / training / identifying / finding?<--Here and throughout , I am not sure that " learning " is the best word choice . If you change it here , it should be changed throughout .
In Discrete AdaBoost , weak classifiers with binary output are too weak to boost when the training data is complex .
Meanwhile , determining the appropriate number of bins for weak classifiers learned by Real AdaBoost is a challenging task because small ones might not accurately approximate the real distribution while large ones might cause over-fitting , increase computation time , and waste storage space .
We have developed Ent-Boost , a novel method for efficiently learning weak classifiers using entropy measures . //method / boosting scheme?
Class entropy information is used to automatically estimate the optimal number of bins through discretization .
Then Kullback-Leibler divergence , which is the relative entropy between probability distributions of positive and negative samples , is used to select the best weak classifier in the weak classifier set .
Experiments showed that strong classifiers learned by Ent-Boost can achieve good performance and be stored compactly . //[be stored compactly / achieve compact storage?]
The results of building a robust face detector using Ent-Boost showed the boosting scheme to be effective .
Building a robust and reliable classifier is always a fundamental problem of pattern recognition .
Several kinds of classifiers , such as neural networks [1] and support vector machines [2] , have been proposed and applied successfully in many object-detection systems .
Boosting [3] and its variants [4] ? [10] have recently gained much attention from researchers because of their excellent performance .
In regard to face detection , for example , the methods described in [4] , [5] , and [10] are state-of-the-art in terms of both accuracy and running speed .
The main idea of boosting is to combine the performance of weak classifiers to form a strong classifier .
Typically , a weak classifier is any classifier whose performance is better than random guessing ( i.e. , its error rate is less than 0 .5 ) .
The performances of these weak classifiers are integrated into the final form of a strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .
In practical problems , designing and learning weak classifiers leave practitioners with two main challenges : computational evaluation and discriminant power .
Generally , for efficient computation , the dimensions of the input space of weak classifiers are reduced be to much smaller than those of the strong classifier[s?] .
In object-detection frameworks [4] , [5] , [11] ? [13] , weak classifiers are usually constructed from one or several features .
For example , a weak classifier can be constructed from one Haar wavelet feature that is evaluated very rapidly through an integral image [4] .
Given a feature type , choosing the suitable way to form a weak classifier that balances efficiency and computation is still an open problem [14] .
Two key trends exist for seeking the most discriminant weak classifier .
The first trend is dealing with the problem of how to design features for best representing the target object .
Besides Haar wavelet features [4] , Gabor wavelets [5] , edge orientation histograms ( EOH ) [11] , orientation dominants [12] , scale invariant feature transform ( SIFT )-based high-level features [13] , and local binary patterns ( LBP ) [15] have also been used .
The second trend is studying how to optimally select the best weak classifier from a weak classifier set .
In Discrete AdaBoost [16] , weak classifiers are threshold-functions whose output is restricted to binary data. //[data / values??I think you need a noun here?binary what?]
This leads weak classifiers to be too weak to boost when handling complex data sets .
For example , in later layers of the cascaded face classifiers [4] , the error rate of weak classifiers is between 0 .4 and 0 .5 .
Meanwhile , in Real AdaBoost [3] , a generalized version of Discrete AdaBoost , weak classifiers are piece-wise functions whose output is a real value representing the confidence-rated prediction .
Normally , to construct such weak classifiers , one splits the input space \MATH into non-overlapping blocks ( or subspaces ) \MATH , \MATH , . . . , \MATH so that the predictions of the weak classifier are the same for all instances falling into the same block .
In the case of one-feature-based weak classifiers , this is equivalent to dividing the real line into intervals .
Typically , most current works [5] , [6] , [8] , [10] , [17] split the data into \MATH bins that are equal in width . This method suffers from the following limitations : //[works / systems?]
-The way to choose the appropriate number of bins is undetermined .
Normally , it has been done by trial and error [6] , [17] ? a tedious task .
In the training cascade of classifiers [6] , [17] , when the complexity of the training data changes over time , using the same number of bins for training every layer is not optimal .
-Choosing a large number of bins might cause over-fitting because of outliers in the case of noisy data [18] .
Furthermore , it might lengthen computation and training time and waste storage space , which is critical in applications with limited resources , for example , face detection on mobile phones .
Choosing a small number of bins , however , might not accurately approximate the real densities of the data distribution and could influence the selection of the best weak classifier .
A deterministic method is therefore needed to automatically and optimally choose the number of bins .
This problem can be formulated as a discretization problem in which subspace boundaries are found by some criteria . //[some criteria?This sounds a bit vague . Could you be more specific?]
Among discretization methods , the entropy-based method [19] has been proved most efficient . Hence , we propose using it to solve the problem .
The entropy-based discretization method is an algorithm that automatically selects appropriate thresholds to split feature values into optimal bins by using entropy measurement .
It is a supervised discretization method that takes into account class information and data distribution , so it is generic and can be applied to any kind of input data .
Furthermore , many studies have shown that the discretization process might help to improve performance in induction tasks [18] and it can also work with a weighted data distribution . Therefore , it is most appropriate for boosting-based methods .
Besides learning weak classifiers , selecting the best weak classifier in the large set of weak classifiers in each round of boosting is also important .
Following the method used in [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples . // [used / proposed?]
The integration of the entropy-based discretization process and optimal weak classifier selection into the current boosting framework formed a new variant of AdaBoost , called Ent-Boost .
Experiments on building a robust face detector have shown the effectiveness of this new boosting scheme .
Originally , Discrete AdaBoost , proposed by Freund and Schapire [16] , was a learning method of combining weak classifiers to form a strong classier .
Given a training set \MATH , where \MATH and \MATH , a weak classifier \MATH has the form \MATH .
Normally , a weak classifier is any classifier whose performance measured by error rate is less than 0 .5 .
Therefore , in many applications [4] , [5] , [7] , it is simplified by associating with one feature \MATH .
Through boosting processing , weak classifiers are combined into a strong classifier \MATH where \MATH are values that measure the performance of the selected weak classifier .
In the boosting process , a distribution \MATH or set of weights over the training samples are maintained and updated so that subsequent weak classifiers focus on the strong-classified samples . //[hard / strong?]
Real AdaBoost [3] is a generalized version of Discrete AdaBoost in that weak classifiers are real-valued functions instead of binary ones and \MATH is found numerically instead of by predescription . //[This method also involves?NOTE : A method cannot propose something .
Do you mean that the creators of this system proposed this?] designing weak classifiers that partition the input space into subspaces so that the predictions are unique in each subspace .
Such weak classifiers are used widely in current state-of-the-art object detection systems [5] , [8] , [17] .
Suppose that \MATH , \MATH , . . . , \MATH is a partition of the domain \MATH on which such weak classifiers $h$ are defined .
The prediction of \MATH depends only on which block \MATH a given instance falls into .
On the other hand , \MATH for all \MATH .
In the case of one-feature-based weak classifier , the histograms of positive and negative samples are computed as follows \MATH where \MATH .
It is proven in [3] that the most appropriate choice for the prediction of the weak classifier on block \MATH to maximize the margin is \MATH where \MATH is a smoothed value in order to handle cases in which \MATH is very small or even zero .
A summary of the Real AdaBoost algorithm is given in Algorithm 1 .
Real AdaBoost is easy to implement , but in practical applications , designing and learning weak classifiers depend on specific applications .
In such face detection systems as [those described in?] [5] , [6] , [8] , and [17] , weak classifiers are usually associated with one feature .
With a very large number of available features ? hundreds of thousands ? [there are many candidates from which to / many choices must be made to?] select one weak classifier for each round of boosting .
Optimally selecting the suitable weak classifier makes the final strong classifier more robust and efficient .
Furthermore , optimal selection can reduce the number of boosting rounds , thus directly shortening training time .
Most studies so far have been focused on how to measure the discriminant power of weak classifiers in order to select the best weak classifier .
Many measurements have been proposed , for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] , and recently , Jensen-Shannon divergence [8] and mutual information [9] ( Table 1 ) .
Meanwhile , few studies have been made on efficiently partitioning subspaces .
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by the above measures give similar performances . //[measurements / measures?]
However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .
The proposed boosting scheme , Ent-Boost , is an integration of adaptive entropy-based subspace splitting and the symmetric KL divergence-based weak classifier selection .
In Ent-Boost , each weak classifier is constructed from one feature and trained on weighted training samples similar to [those used in?] Real AdaBoost .
However , instead of using the equal-width binning method used in Real AdaBoost [6] , [17] which has a hard time predicting the suitable number of bins in advance , we use the entropy-based discretization method [19] to split the input space into subspaces .
This subspace splitting process is totally automatic ; the stopping criteria of the splitting process are determined using minimum description length principles ( MDLP ) . This process will be described in greater detail in the next section .
To select the best weak classifier from the input weak classifier set , we use symmetric KL divergence as in [5] , which measures the distance between two distributions as follows : \MATH where \MATH and \MATH are probability distributions of a discrete random variable .
This formula can be rewritten in entropy terms : \MATH or \MATH where \MATH and \MATH are entropy and \MATH is cross entropy of \MATH and \MATH .
The outline of Ent-Boost is shown in Algorithm 2 .
Note that the discretization process is performed in every round of boosting to adapt to new distributions of samples .
As a result , the number of intervals of the selected weak classifier varies . //[classifier varies / classifiers vary?]
This is different from previous methods , which fix the number of equal-width intervals in advance .
This section briefly describes automatic subspace splitting using entropy-based discretization .
Discretization is a quantizing process that converts continuous values into discrete values . It typically consists of four steps [18] .
Step 1 : Sorting the continuous values of the feature to be discretized .
Step 2 : Evaluating candidate cut-points and selecting the best cut-point for splitting .
A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold .
Step 3 : Splitting the data into two intervals using the cut-point selected in step 2 .
Step 4 : Continuing discretization with each interval until a stopping criteria is satisfied .
The stopping criteria are usually selected by considering a trade-off between lower arity ( the number of intervals or the number of bins ) and its effect on the accuracy of classification tasks .
A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .
Given a set \MATH of sorted continuous values \MATH , candidate cut-points are usually selected as mid-points of every successive pair of \MATH .
On the other hand , candidate cut-points are \MATH .
For each cut-point \MATH that splits set \MATH into two subsets \MATH , the class entropy of a subset \MATH is defined as \MATH where \MATH is the number of classes \MATH , and \MATH is the proportion of examples in \MATH that have class \MATH .
To evaluate the resulting class entropy after set \MATH is partitioned into two sets \MATH and \MATH , the class-information entropy of the partition induced by cut-point T is defined by taking the weighted average of their resulting class entropies \MATH he best cut-point selected in step 2 is the cut-point \MATH for which \MATH is minimal amongst all the candidate cut-points .
Given set S and a potential binary partition \MATH , specified on S by the given cut-point \MATH , a stopping criteria is used to decide whether or not this partition should be accepted .
If the answer is YES , the discretization will continue with each partition given by \MATH ; otherwise , the discretization process will stop .
Suppose \MATH is the probability of a \MATH answer , and \MATH is the probability of a \MATH answer .
Partition \MATH is only accepted if \MATH .
However , in practice , there is no easy way to estimate these probabilities directly .
Instead , Fayyad and Irani [19] proposed using MDLP to indirectly estimate them .
The minimum description length of an object is defined as the minimum number of bits required to uniquely specify that object out of the universe of all objects .
To employ MDLP in choosing the stopping criteria , Fayyad and Irani formulated the above problem as a communication problem between a sender and a receiver .
It is assumed that the sender has the entire set of training examples , while the receiver has the examples without their class labels .
The sender needs to convey needed information for the proper class labeling of the example set to the receiver .
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to be sent before the partition is more than the length of the message required to be sent after the partition .
By inferring from coding hypothesis , the stopping criteria is defined as follows : MDLP Criteria :A partition induced by cut-point \MATH for a set \MATH of \MATH examples is accepted iff :\MATH
where \MATH and \MATH where\MATH is the number of classes in \MATH Extensive experiments [18] , [19] recommended that this method should be the first choice for variable discretization because it gives a small number of cut-points while maintaining consistency .
For our experiments , face and non-face patterns were of size 24x24 . //[what is the unit here?]
A set of 10 ,000 face patterns were collected from the Internet .
Another set of 10 ,000 hard non-face patterns were false positives collected by running a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 examples .
Some examples of the collected 24x24 face and non-face patterns are shown in Figure 2 .
Haar wavelet features , which have been used in many face detection systems [4] , [6] , [14] , were used in our experiments .
These consisted of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
The feature value was defined as the difference of the sum of the pixels within rectangles ( Figure 3 ) .
In total , 134 ,736 features were used for training classifiers .
Figure 4 shows a comparison of the performances of strong classifiers trained by the different boosting schemes : AdaBoost [4] , Real AdaBoost [17] , and Ent-Boost .
Each strong classifier is a combination of 80 weak classifiers ( using more weak classifiers does not much improve the performance ) .
For Real AdaBoost , subspace splitting is done by equal-width binning in which the number of bins is arbitrarily selected to be 64 and 128 .
The curves indicate that the performances of Real AdaBoost and Ent-Boost were better than that of AdaBoost .
In addition , the performance of Real AdaBoost classifiers varied when using different numbers of bins .
Overall , Ent-Boost produced the best result .
As for storage space , the Ent-Boost-based classifier only uses 6 .79 bins on average , which is much fewer than the number used by Real AdaBoost-based classifiers .
Using Ent-Boost , a robust face detector was built .
It was a cascade of Ent-Boost-based classifiers that were trained [through a process similar to that used in] [4] .
The resulting cascade has 25 layers using 3 ,850 features .
The performances of the AdaBoost-based face detector [4] and our Ent-Boost-based face detector on the MIT+CMU test set [1] confirmed the effectiveness of our proposed boosting scheme ( Table 2 ) .
Some detection results are given in Figure 5 .
We have described Ent-Boost , a variant of AdaBoost , which uses entropy measures for automatic subspace splitting and optimal weak classifier selection .
The resultant strong classifier has good performance and achieves compact storage .
Furthermore , this new boosting scheme overcomes the main limitation of Real AdaBoost , which is difficulty in determining the suitable number of bins for subspace splitting .
Because it considers the class information and the distribution of the input data in the splitting process , this method is generic and can be used for other applications .
Experiments have shown promising results , especially in the building of a robust face detector .
ROBUST OBJECT DETECTION USING FAST FEATURE SELECTION FROM HUGE FEATURE SETS
This paper describes an efficient feature selection method which that quickly selects a small subset out of a given huge feature set ; the proposed method for will be useful for building robust object detection systems .
In this filter-based method , features are selected so that not only to maximizeing their relevance with the target class but also to minimizeing their mutual dependency .
As a result , the selected feature set only contains only highly informative and non-redundant features , which significantly improve classification performance when combined together , significantly improve classification performance .
The relevance and mutual dependency of features are measured by using conditional mutual information ( CMI ), in which features and classes are treated as discrete random variables . //[ ,?<--A comma can be used here if the following describes CMI in general .]
Experiments on different huge feature sets have shown that the proposed CMI-based feature selection can both reduce significantly the training time significantly and achieve high accuracy .
One of the fundamental research issues in pattern recognition is feature selection , which is the task of finding a small subset out of a given large set of features .
Improving the method of accomplishing this task is important due to the following three reasons .
First , there are many ways can be used to represent a target object , and this variety leadsleading to a huge feature set .
For example , the number of Haar wavelet features used in [1] for face detection is hundreds of thousands .
However , only small and incomplete training sets are available .
As a result , these systems suffer from the curse of dimensionality and over-fitting .
Second , a huge feature set usually includes many irrelevant and redundant features that can degrade the generalization performance of classifiers , waste storage space , and increase training time [2 , 3] .
Third , selecting an optimal feature subset from a huge feature set can improve the performance and speed of classifiers .
Furthermore , less complex models is are easier to understand and verify .
In face detection , the success of systems such as those in [1 , 4] comes mainly from efficient feature selection methods .
Generally , feature selection methods can be categorized into two kinds : the filter-based approach and the wrapper-based approach [5] .
The filter-based approach is independent of any induction algorithm , while but the wrapper-based approach is associated with a specific induction algorithm to evaluate the quality of the selected feature subset . //[goodness / quality / appropriateness?<--If " goodness " is the word you would usually use in your field for this , it is fine , but I would suggest a different word choice otherwise . " Goodness " seems vague , so in what sense do you mean " good " ?]
In the filter-based approach , features are normally selected based on their individual predictive power . This power is measured by Fisher scores , Pearson correlation [6] , or mutual information [7] .
The major advantage of these measurement methods is their speed and ability to scale to huge feature sets .
However , because the mutual relationships between features is are often not taken into account , leading the selected features might be highly redundant and less informative because two features with high individual predictive power , when combined together , might not bring significant performance improvement . Combining compared with two features of which one of them has low predictive power but is useful when combined with others would thus be more effective for improving performance .
Since wrapper-based feature selection methods use machine learning algorithms as a black box in the selection process , they can suffer from over-fitting in situations of when applied to small training sets . //[when used with / when applied to?]
Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands of features , so using wrapper-based methods is obviously inefficient because of the very high computation costs they incur .
For example , in the state- of- the- art face detection system in [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by using AdaBoost has takentook several weeks . //[by using / generated by?]
Consequently , feature selection methods based on conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of the above approaches for handling large scale feature sets .
The main goal of these CMI-based methods is to select features which that maximize their relevance with the target class and to simultaneously minimize mutual dependency between selected ones . //[idea / goal?]
It doesThese methods do not select a feature similar to ones already selected ones , even if itthe feature is individually powerful , as because selecting it might not do much to increase much information about the target class [7] .
One of the important tasks in using CMI-based methods is mutual information estimation , which involves to computecomputing the probability densities of continuous random variables .
In [9] , Kwak and Choi used a Parzen windows -based density estimation method in which many parameters such as kernel function and window width are complicated to determine .
For simplification , discretizing features is often used on the features . //[discretizing features is often used on the features / the features are often discretized?]
So far , in object detection systems like [8 , 7] treat , features are treated as binary random variables by choosing appropriate thresholds .
However , binarizing features is not a suitable way to handle highly complex data for which it is hard to finding the best threshold is difficult .
Using multiple thresholds to discretize data is better than using a binary approach .
Such a simple method is equal-width binning , which divides the range of feature values into m equally sized bins , where m must be known in advance .
Our method is also a CMI-based feature selection method .
However , the method�fs main distinguishing point is that it employs the entropy-based discretization method [11] to discretize features . //[distinguishing / unique?]
This discretization method is simpler than the Parzen window-s based density estimation method and is more efficient than binary discretization .
Furthermore , contrary to equal-width binning , it can automatically evaluate the optimal number of bins based on data distribution . //[evaluate / determine?]
Experiments show that the proposed method can well capably handle huge feature sets of data such as Haar wavelets [1] and Gabor wavelets [12] for face detection , significantly reducinge the training time while maintaining high classification performance .
FEATURE SELECTION " >
Huge feature sets usually contain four kinds of features : ( i ) irrelevant features , ( ii ) weakly relevant and redundant features , ( iii ) weakly relevant but non-redundant features , and ( iv ) strongly relevant features ; in which ( iii ) and ( iv ) are the objectives of feature selection methods [13] .
To measure the relevance of a feature , an entropy-based measure , which quantifies the uncertainty of random variables , is normally used .
The entropy of a discrete random variable X is defined as : \MATH and the conditional entropy of X after another variable Y is known is defined as \MATH
The mutual dependence between two random variables is measured by mutual information : \MATH .
The conditional mutual information is defined as : \MATH .
In the first step , the most relevant feature F1 , which has the highest largest amount of mutual information , is selected .
However , iIn the second step , however , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .
Therefore , F2 is selected so that maximizingas to maximize the information it can add :\MATH .
Following the same scheme, we iteratively add the feature that brings the highest increase of the information content contained in the current selected feature set . //[the / an?<-- " An " is correct if there is more than one such measure .]
The next feature Ft to be added at iteration t is defined by :\MATH .
To simply estimate mutual information , the easiest way is to discretize features are discretized in binary values by specifying thresholds [8 , 7] .
However , for complex data , doing thisit is not efficient ; therefore , we use the entropy-based method proposed by Fayyad and Irani [11] for discretization .
This method is a supervised method , thus so it is generic and can adapt very well to any kind of data distributions .
Discretization is essentially a quantizing process that converts continuous values into discrete values .
Suppose that we are given a set of instances S , a feature A , and a cut-point T . ( A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold . ) .
The class-information entropy of the partition induced by T is defined as :
Among candidate cut-points , the best candidate cut-point Tmin , which minimizes the entropy function \MATH , is selected to split \MATH into two partitions \MATH and \MATH .
This process can then be repeated recursively forto \MATH and \MATH until some stopping condition is satisfied , thus creating multiple intervals on the feature \MATH .
Using MDLP , the stopping criteria is was proposed by Fayyad and Irani [11] as follows :
MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \MATH wWhere \MATH ,where \MATH , \MATH , and \MATH is are the numbers of classes in \MATH , \MATH , and \MATH , respectively .
Extensive experiments [11 , 14] have shown that this method is one of the best in variable discretization one because it gives a small number of cut-points while maintaining consistency .
The outline of the proposed feature selection method is shown in Algorithm 1 .
For experiments , a set of face and non-face patterns of size 24x24 was used .
A set of 10 ,000 face patterns were collected from the Internet .
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 patterns .
Some examples of the collected 24x24 face and non-face patterns are shown in Figure 1 .
Two types of features ?that are Haar wavelet features and Gabor wavelet features ? were used in our experiments .
Haar wavelet features have been widely used in many face detection systems [1 , 15] .
They consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape . //I�fm not 100 percent clear on what " they " points to here . " These Haar wavelet features , " perhaps? But can features consist of other kinds of features? You may want to clarify here .]
The feature value is defined as the difference of the sum of the pixels within the rectangles .
In total , 134 ,736 features were used for training classifiers .
Gabor wavelet features have also often been used often in face recognition systems [12] and are defined as : \MATH , where \MATH and \MATH define the orientation and scale of the Gabor kernels respectively , \MATH , and the wave vector \MATH , is defined as : \MATH where \MATH , \MATH and \MATH .
The Gabor representation of a face image is computed by convolving the face image with the Gabor filters .
Let \MATH be the face image ; , its convolution with a Gabor filter �� ,_( z ) is defined as : \MATH where \MATH denotes the convolution operator .
Similar to [12] , Gabor kernels at five scales , \MATH , and eight orientations , \MATH , were used .
At each pixel position , 40 Gabor features are computed by convolving the input image with the real part of Gabor filters .
As a result , one \MATH training sample hasthere are \MATH Gabor features for one 24x24 training sample .
To prove the effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods ?that are forward feature selection ( FFS ) [16] and a CMI-based methods using binary features ( CMI-Binary ) [8 , 7] ? on the data set and feature sets mentioned described above .
All classifiers were trained using AdaBoost similar to [1] .
We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results , when not only reducing significantly the training time of the AdaBoost-based face detection systems [1] by ( about 100 times , ) but also maintaining comparable performance .
Figure 2 shows performance of classifiers trained by Haar feature subsets selected by three feature selection methods .
The figureIt indicates that , the proposed method , CMI-Multi , outperforms the others while the performances of FFS and CMI-Binary have were comparable performanceto one another .
The A similar result is was also shown when the three feature selection methods were tested on Gabor wavelet features .
In this case , CMI-based feature selection methods obviously clearly outperformed FFS , and CMI-Multi is was confirmed to be more efficient than CMI-Binary .
Because our proposed method uses same principle as FFS , which only trains weak classifiers once , it is extremely fast compared with AdaBoost [1] .
We built two cascades of AdaBoost classifiers that use CMI-Multi and AdaBoost [1] as feature selection methods .
Testing on the standard benchmark MIT+CMU test set , they hadve comparable performance .
However , CMI-Multi wasis trained faster than was AdaBoost by approximately 70 times .
We have presented a fast feature selection method using conditional mutual information to handle huge feature sets .
The estimation of mutual information is simplified by using an MDLP- based discretization method .
Integrated into AdaBoost-based object detection systems , our proposed methodit can not only reduces the training time significantly , but also achieves high classification performance .
Experiments on two popular feature sets have demonstrated the effectiveness of the proposed method . //[Please note : I am not sure which of the following you mean .--> one composed of such as Haar wavelets and the other composed of Gabor wavelets / ? Haar wavelets and Gabor wavelets ?]
A Multi-Stage Approach to Fast Face Detection
A multi-stage approach that is fast , robust , and easy to train is proposed for a face-detection system .
Motivated by the work of Viola and Jones [1] , this approach uses a cascade of classifiers to yield a coarse-to-fine strategy to significantly reduce detection time while maintaining a high detection rate .
However , our [system / approach?] is distinguished from previous work by two features .
First , a new stage has been added to detect face candidate regions more quickly by using a larger window size and larger moving step size .
Second , support vector machine ( SVM ) classifiers are used instead of AdaBoost classifiers in the last stage , and Haar wavelet features selected by the previous stage are reused for the SVM classifier robustly and efficiently .
By combining AdaBoost and SVM classifiers , the final system can achieve both fast and robust detection because most non-face patterns are rejected quickly in earlier layers , while only a small number of promising face patterns are classified robustly in later layers .
The proposed multi-stage-based system has been shown to run faster than the original AdaBoost-based system while maintaining comparable accuracy .
Face detection is one of the most active research areas in computer vision because of its many interesting applications in fields such as security , surveillance , multimedia retrieval , and human-computer interaction .
For example , face detection is combined with other modules to identify a person in a video sequence [2] .
Face locations , the results of a face detection system , can be used for applications such as face recognition and video indexing [3] .
Although this area has been studied for more than 30 years , developing a fast and robust face detection system that can handle the variations found in different faces in real applications , such as facial expressions , pose changes , illumination changes , complex backgrounds , and low resolutions , is still a challenging research target [4] .
Recently , with advances in machine learning research , neural networks [5] , [6] , support vector machines ( SVM ) [7] , [8] , [9] and AdaBoost [1] , [10] , [11] , [12] , [13] are typical choices for building robust face detectors .
Current research is focusing on feature extractions and appropriate structures for combining classifiers .
Generally , to classify an input pattern of intensities as a face or non-face , features must be extracted and normalized before passing [the image / the pattern / the results?] to a classifier [14] .
Many kinds of features have been used , ranging from simple ones such as intensity values [7] , [5] and eigenspace [15] to complex ones such as wavelets [16] , [1] , [12] , edge orientation histograms [17] , [18] , and Bayesian discriminating features ( BDF ) [19] .
Discriminative and informative features usually increase detection rates and reduce the complexity of training procedures [17] .
In a typical face detector that is scale- and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .
However , the vast majority of the analyzed patterns are non-face .
Statistics from [9] have shown that the ratio of non-face to face patterns is about 50 ,000 to 1 .
Face detectors based on single classifiers such as SVM [7] , [8] , [9] and neural networks [6] , [5] are usually slow because they equally process non-face and face regions in the input image .
To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers has been proposed [8] , [1] , [9] , [20] , [21] , [11] .
In particular , fast and simple classifiers are [recommended to be?] used as filters at the earliest stages to quickly reject a large number of non-face patterns and a slower yet more accurate classifier is then recommended to be used for classifying face-like patterns .
In this way , the complexity of classifiers can be adapted corresponding to the difficulty in the input patterns . / / [is / can be?]
In [8] , nonlinear SVM classifiers using pixel-based features were arranged into a sequence by increasing the number of support vectors , while in [9] , linear SVM classifiers trained at different resolutions were used for rejection and a reduced set of principle component analysis ( PCA )-based features were used with the nonlinear SVM at the classification stage in order to reduce computation time .
In [1] , AdaBoost-based classifiers were arranged in a degeneration decision tree or a cascade .
Using about 10 features of the first two layers , more than 90\% of non-face patterns were rejected .
Recently , a boosting chain [20] and a nested cascade [11] have also been proposed for improvements .
It is believed that the cascade structure of classifiers is the key factor in enhancement of current real-time face detectors . / / It is believed?This sounds vague?who believes this? " May researchers believe , " for example , would be clearer and sound more believable .]
This work is motivated by Viola and Jones [1] , who proposed a framework for fast and robust face detection .
Their success comes mainly from three contributions :
-The cascaded structure of simple-to-complex classifiers reduces computation time dramatically .
-AdaBoost is used to select discriminative and significant features from a pool of a very large number of features and then construct the classifier .
The output classifier built from these selected features is very fast and robust in classification .
Compared to SVM-based classifiers or neural network-based classifiers , AdaBoost-based classifiers are hundreds of times faster .
-Haar wavelet features used for all stages are informative [22] and can be evaluated extremely quickly due to the introduction of the integral image .
However , this framework still has the following problems :
-First , the cascaded classifiers that use AdaBoost and Haar wavelet features are only efficient in quickly rejecting simple non-face patterns .
To robustly classify complex patterns , it is necessary to use a larger number of features and layer classifiers .
This need is apparent because when face and non-face patterns become hard to distinguish , weak classifiers are too weak to boost [22] .
With the first several layers in our experiment ( cf . Figure 1 ) , using some 800 weak classifiers , more than \MATH of non-face patterns were rejected .
However , enabling the later layers to robustly classify a smaller number of remaining patterns requires many more weak classifiers ( around 5 ,660 ) , thus making the training task much more complicated .
-Second , the training process is complicated .
It requires a long time because the training time is proportional to the number of features in the input feature set ( which is normally hundreds of thousands ) and the number of training samples ( which is generally tens of thousands ) .
In our experiment , with 20 ,000 training samples and 134 ,736 features , the average training time for choosing one feature associated with the weak classifier was about 30 minutes on a PC ( Pentium 4 , 2 .8 MHz , 512-MB RAM ) .
Therefore , training a cascade of classifiers with around 6 ,060 features [1] might take on the order of several weeks .
Another thing that complicates the training process is that AdaBoost-based classifiers are constructed by adding features after each round of boosting , so several training parameters must be tuned manually while training .
In practice , for stopping training a classifier , at least the following three parameters must be determined in advance : minimum detection rate , maximum false positive rate , and maximum number of boosting rounds ( or the number of weak classifiers of each layer ) .
Because the complexity of the training sets varies throughout the layers in the cascade , a way to choose these parameters automatically and optimally has not been determined .
For example , in the first layers , it is quite easy to train a classifier with a minimum detection rate of \MATH and a maximum false-positive rate of \MATH .
However , in later layers , choosing the detection rate of \MATH will give a false positive rate greater than \MATH [22] .
Adding more features directly increases computation time and might cause over-fitting .
The authors therefore propose a multi-stage approach to build a face-detection system by adopting the advantages of Viola and Jones' approach and by introducing a method to address the above problems .
Specifically , for quick rejection of non-face patterns , we have reused two key ingredients of Viola and Jones' system , that is , the cascaded structure of simple-to-complex classifiers and AdaBoost trained with Haar wavelet features .
Furthermore , for robust classification and simple training , we propose using SVM classifiers for later layers .
The contribution of this approach is threefold :
-First , to detect the face candidate regions , a new stage ( using a larger window size and a larger moving step size ) has been added .
We use 36 x 36-pixel window-based classifiers with a moving step size of 12 pixels to quickly estimate the candidate face regions .
The idea of using larger windows and moving the step size was adopted in [5] , but it severely degraded performance .
To improve speed while maintaining high accuracy , our approach takes advantage of the combination of the Haar wavelet features and the AdaBoost learning for fast and robust evaluation .
Second , we have investigated how to efficiently reuse the features selected by AdaBoost in the previous stage for the SVM classifiers of the last stage .
Reusing these features brings two advantages : ( i ) Haar wavelet features are very fast in being evaluated and normalized [1] .
Furthermore , these features do not need to be re-evaluated because they have already been evaluated .
( ii ) By using SVM classifiers with powerful generalization , using too many features in the cascade is avoided , with the important results of saving training time and avoiding over-fitting .
Third , the training time of AdaBoost classifiers has been shortened by using simple sampling techniques to reduce the number of features in the feature set .
Experiments showed that for rejection , the performance gained by using a sampled feature set was comparable to that of a full feature set .
Along with using several SVM classifiers instead of many AdaBoost classifiers in later layers , the total training time has been significantly reduced .
Several studies have worked on addressing the drawbacks of Viola and Jones' system .
Wu et al. [23] used direct feature selection to reduce training time while maintaining comparable performance .
Their idea is to separate the training process into two stages : feature selection and classifier construction .
In Viola and Jones' work , features are selected by the discriminative performance of their associated weak classifiers through the boosting process .
This process is very time consuming because all weak classifiers must be trained every time one feature is selected .
With the new proposal of Wu et al. , weak classifiers are trained only once and features are selected by the direct feature selection method , which directly maximizes the learning objective of the output classifier .
They claim that their method is 100 times faster than Viola and Jones' method .
Another direction is to optimally build the cascade to improve its overall performance .
Sun et al. [24] and [25] proposed a scheme to optimally tune parameters in layer classifiers .
However , their approach is somewhat complicated and is not easy to implement .
Xiao et al. [20] and Huang et al. [11] proposed a boosting chain structure in which subsequent layers utilize the historical information of the previous layers .
This significantly reduces the number of features used in each layer .
Discrete AdaBoost uses a binary weak classifier that is too weak to boost in the case of a hard distinguished dataset .
Studies based on RealBoost [26] , such as [12] , [10] , [27] , and [11] , introduced new kinds of weak classifiers that are stronger than binary weak classifiers .
These new real-valued weak classifiers can effectively discriminate face and non-face distributions , so the total number of features used is also reduced dramatically .
Face detection systems such as [27] and [11] only use around 800 features .
However , the main problem with these systems is how to choose the most appropriate number of bins .
A small number of bins might not accurately approximate the real distribution , while a large number of bins might cause over-fitting , increase computation time , and waste storage space .
However , our system can benefit from this approach when building the rejection stage and can thus reduce the training time even further .
The proposed face detection system consists of three stages that classify a 24x24-pixel window as either a face or a non-face .
To detect faces of different sizes and locations , the detector is applied at every location and scale in the input image with a scale factor of 1 .2 , which is similar to other approaches [5] , [6] , [9] .
An outline of this system is given in Figure 2 .
The first stage is a cascade of classifiers used to estimate face candidate regions by evaluating 36x36 input windows , with a moving step of 12 pixels .
If a 36x36-pixel window is detected as the existence of a face , 144 ( i.e. , 12x12 ) likely face positions are collected and passed to the next stage .
The second stage is a cascade of classifiers used to investigate 24x24 window face candidate locations returned from the previous stage .
The main purpose of designing these two stages is trying to filter out a large number of non-face patterns as quickly as possible before passing complex patterns to the final stage classifier .
This is done by taking advantage of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers enable extremely fast computation .
Although the cascade of \MATH AdaBoost classifiers rejects non-face patterns rapidly , it is still influenced by the large number of \MATH patterns that it must process .
For this reason , the first stage , which is a cascade of \MATH classifiers , is added is to decrease the number of analyzed patterns .
To this end , this stage is trained specially to make the classifiers invariant to small face translations .
These classifiers can detect faces that are off-center by up to six pixels in any direction .
An illustration of the difference between 24x24 and \MATH face training samples is depicted in Figure 3 .
The \MATH window is chosen in accordance with the idea in [5] that the classifier can be trained to be invariant to translation by up to \MATH of the original window size .
With this flexible classifier , the moving step size can be increased up to 12 pixels to dramatically reduce the number of analyzed patterns .
The efficiency of this stage will be discussed further in section 6 .3 .
The last stage is a cascade of nonlinear SVM classifiers that reuses features that have been selected by AdaBoost in the second stage classifier .
These feature values are evaluated and scaled to be between 0 and 1 to form a feature vector .
In our experiments , only 100 features were used , making classification faster than it would have been using pixel-based SVM classifiers [8] , [9] .
The same feature set proposed in [1] was used ( cf . Figure 4 ) .
It consists of four kinds of features modeled from adjacent basic rectangles of the same size and shape .
The feature value is defined as the difference of the sum of the pixels within rectangles .
Each feature is parameterized by four parameters : the position within the window \MATH , the width \MATH , and the height \MATH ( cf . Figure 5 ) .
By using integral image definition [1] , the feature values of these rectangles can be computed extremely quickly .
The integral image at location \MATH is defined as \MATH , where \MATH is the integral image and \MATH is the original image .
In practice , \MATH can be computed simply by using the following recurrent function :\MATH , and sum of the pixels within a rectangle can be computed from four integral image values of its vertices , for example , \MATH .
Boosting is used to improve the classification performance of any given simple learning algorithm [28] .
Given \MATH weak classifiers \MATH learned through \MATH rounds of boosting , the strong classifier is formed by a linear combination : \MATH , where \MATH are coefficients found in the boosting process .
Each weak classifier \MATH is associated with a feature \MATH and a threshold \MATH such that the number of incorrectly classified examples corresponding to the weak classifier is minimized : \MATH , where polarity \MATH indicates the direction of the inequality sign .
In each round of boosting , the best weak classifier \MATH that has the lowest error \MATH will be chosen .
The error of each weak classifier is measured with respect to the set of weights over each example of the training set \MATH , where \MATH and \MATH are the respective weight and label of the training example \MATH .
After each round , these weights are updated such that the weak learner will focus much more on the hard examples in the next round .
The main idea of building a cascade of classifiers is to reduce the computation time by giving different treatments to different complexities of input windows ( cf .
Figure 7 ) .
Only input windows that have passed through all layers of the cascade are classified as faces .
Training cascaded classifiers that can achieve both good detection rates and less computation time is quite complex ; a higher detection rate requires more features , but more features correspond to more time needed for evaluation .
To simplify this , the detection rate goal and the false positive rate goal for each layer are usually set beforehand .
Viola and Jones [1] stated that , if the layer classifier has achieved the predefined target goals after 200 features are used , the training process will stop and a new layer will be added .
1 .	<section label= " SVM Classifier " >
The support vector machine is a statistical learning method based on the structure-risk minimization principle .
It has been very efficiently proven in many pattern recognition applications [29] , [8] , [9] .
In the binary classification case , the objective of the SVM is to find the best separating hyperplane with a maximum margin .
The form of SVM classifiers is : \MATH where \MATH is the d-dimensional vector of an observation example , \MATH is a class label , and \MATH is the vector of the \MATH training example .
All the \MATH corresponding to non-zero \MATH are called support vectors .
It is important to choose the appropriate kernel and parameter \MATH in order to obtain the robust SVM classifier .
Although many kernels have been introduced by researchers , the following four kernels are commonly used : \MATH where \MATH , and \MATH are kernel parameters .
Compared to AdaBoost classifiers , SVM classifiers run much more slowly because of the large number of support vectors and the heavy kernel computation .
To control the trade-off between the number of support vectors and errors , Scholkopf et al. [30] proposed using a new parameter \MATH instead of the parameter \MATH .
They proved that the parameter \MATH is an upper bound of the fraction of margin errors and a lower bound of the fraction of support vectors .
The implementations of \MATH and \MATH are provided by LibSVM [31] .
For training , we collected 7 ,500 , 24x24-size face patterns from the Internet . / / size / pixel?
Non-face patterns were generated at different locations and scales from 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Some examples of the collected 24x24 face patterns are shown in Figure 8 .
Face patterns for training the 36x36 classifiers were generated by selecting 36x36 windows containing the 24x24 face window of the input image .
Figure 9 shows some examples of 36x36 face patterns that include various kinds of floating positions and backgrounds .
To train the cascade of 24x24 AdaBoost classifiers used in the rejection stage , the same 7 ,500 face patterns were used for all layers .
Non-face patterns of the training and the validating sets of the first layer in the cascade were selected randomly .
Non-face patterns of the subsequent layer classifiers are false positives collected by the partially trained cascade on the set of non-face images .
For each layer classifier , 7 ,500 non-face patterns were used for training and 7 ,500 other non-face patterns were used for validating .
To compare the performance of classifiers , we implemented a full cascade of classifiers trained by AdaBoost , similar to that used by Viola and Jones [1] .
The training parameters of each layer were set as follows .
The minimum of the detection rate was \MATH , the maximum of the false positive rate was \MATH , and the maximum of the number of features in each layer was 200 .
This setting resulted in a face detector that consists of 38 layers with 6 ,360 features .
All experiments were run on a PC ( Pentium 4 , 2 .8 MHz , 512-MB RAM ) .
The training process was terminated when no more false positives were found in the non-face images of the data set .
If \MATH is the number of Haar wavelet features and \MATH is the number of training patterns , the learning time of AdaBoost to train \MATH weak classifiers is roughly [1] .
Therefore , if the number of training patterns is fixed , the learning time can be shortened when either the number of features in the feature set or the number of weak classifiers in the final cascade is reduced .
In our approach , the cascaded classifiers are only used for efficient rejection , so we can reduce both of these numbers in order to keep the training time for the full system reasonable .
As mentioned in section 4 .1 , each feature is parameterized by a tuple of four parameters \MATH . / / If this ( and other places ) do not display with spaces after the commas , spaces must be insert . A comma should always be followed by a space . I recommend checking this carefully throughout .]
A set of features is then formed by changing these parameters in corresponding steps \MATH .
A feature set , on the other hand , is parameterized by \MATH .
One of the simplest ways to sub-sample the feature set is to change parameters \MATH , for example , from a full feature set \MATH to a reduced feature set \MATH .
Because the full feature set is redundant , this sub-sampling is expected not to significantly affect the rejection performance of AdaBoost classifiers .
We carried out experiments to compare the performance of classifiers trained on these two feature sets : the full feature set \MATH , containing 134 ,736 features and the reduced feature set \MATH , containing 14 ,807 features ( excluding features of small size ) .
Two classifiers were trained up to the maximum of 200 features .
The classifier 's threshold was changed to meet the detection rate of \MATH .
The training set contains 7 ,500 face patterns and 7 ,500 non-face patterns .
Rejection performance was evaluated through the false positive rate on a validation test set that contains 500 ,000 non-face patterns .
All non-face patterns were selected randomly from the training set mentioned above .
The results shown in Figure 10 indicate that the performances of these two classifiers were no different , especially when the number of features was large enough , for example , more than 50 .
As a result , by using the reduced feature set , the training time can be shortened to approximately one-ninth .
Another experiment we conducted showed that , for similar performance , an AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than one trained on the full feature set . / / [Do you need a reference here , or is this still talking about the experiments you report in this paper?]
Therefore , only the sampling parameter \MATH was used in training the 24x24 AdaBoost classifiers .
In our system , the first stage is a cascade of classifiers that processes 36x36 patterns with a moving step size of 12 pixels .
By taking advantage of simplification in training classifiers only for rejection , as demonstrated in section 6 .2 , training this cascade only uses the feature set generated from a 36x36 window with sampling parameters \MATH .
As a result , 12 ,223 features are produced .
The training set contains 12 ,000 face patterns and 12 ,000 non-face patterns .
Since a 36x36 face sample contains a large proportion of background outside the 24x24 face region and the classifier is required to be fast and to keep all possible face regions , a minimum detection rate of \MATH and a maximum of false positive rate of \MATH were set as the training parameters .
In our experiments , after reaching 50 features , the classifier 's performance did not significantly increase , so the maximum number of features for each layer is set to 50 .
To keep a balance between computation speed and robustness , the maximum number of layers is set to three because using more layers would degrade the overall detection rate dramatically .
Figure 11( a ) shows several features of the first 36x36 layer classifier selected by AdaBoost .
They are somehow similar to the features of the first 24x24 layer classifier as shown in Figure 11( b ) . / / [somehow?This sounds vague . How are they similar?]
In addition , Figure 12 shows an example of face candidate regions estimated by using this cascade .
Two main issues surrounding the reuse of features selected by AdaBoost are : ( i ) which layer�fs features should be reused for SVM and ( ii ) how many features should be used .
For comparison of the performance of SVM classifiers , 2 ,450 face patterns and 7 ,500 non-face patterns that were separated from the training set ( section 6 .1 ) were used .
The SVM classifiers were trained with a RBF kernel whose parameter \MATH is \MATH .
The parameter \MATH was set to \MATH .
These parameters were found by using a cross-validation test .
Figure 13 compares the performance of classifiers trained on 200-feature sets selected by different layers in the cascade ( layers 14 , 17 , 20 , and 25 ) .
These comparable performances suggest that the second stage ( using AdaBoost ) can be switched to the final stage ( using SVM ) at any time .
As a result , the total training time of the system can easily be controlled .
To determine the number of features is that would be sufficiently robust , we used the 200-feature set selected in layer 17 to generate different subsets of features with different numbers of features .
Features in each set were selected in the order in which they were added in the training process .
For example , a 25-feature set consists of the first 25 features selected by AdaBoost when training layer 17 .
The results shown in Figure 14 indicate that with more than 100 features , the performance of the classifiers was comparable . / / [to what?]
Basically , the speed of a SVM classifier is proportional to the number of features used , so the greater the number of features used , the slower the classifier will be .
Figure 15 shows the processing speed of SVM classifiers using different subsets of features .
The SVM classifier using 25 features ran the fastest , while the SVM classifier using 200 features was the slowest .
The speeds of SVM classifiers using 100 , 125 , and 175 features were not importantly different because their difference in terms of number of features and number of support vectors were not large enough to have a significant impact .
Therefore , 100 features might be the best trade-off between speed and performance .
We carried out an experiment to show the efficiency of a single SVM classifier over a cascade of AdaBoost classifiers .
In this experiment , 40 ,000 false positives were gathered by running a cascade of 17 AdaBoost classifiers ( CAB17 ) on the set of non-face images mentioned in section 6 .1 .
These false positives then were used as hard non-face patterns to train and test the performance of two classifiers : a single RBF SVM classifier and a cascade of other 18 AdaBoost classifiers .
Of 40 ,000 non-face patterns , 7 ,500 non-face patterns were used along with 7 ,500 face patterns to train these two classifiers .
The remaining 34 ,000 non-face patterns and other 2 ,450 face patterns were used to compare the accuracy of the classifiers .
The cascade of AdaBoost classifiers were trained with the parameters set as in section 6 .1 .
The RBF SVM classifier reused 100 features selected by the last layer of CAB17 as the feature vector and was trained by an RBF kernel whose parameter \MATH is \MATH .
The parameter \MATH was set to \MATH .
These parameters were found by using a cross-validation test .
The result shown in Figure 16 demonstrates that with hard classified patterns that later layers of the cascade will process , the single SVM classifier can achieve higher accuracy than the cascade of AdaBoost classifiers trained by roughly predefined training parameters . / / ?NOTE : I believe that I hyphenated this term in your previous document , but after seeing it used here , I would say that it does not need to be hyphenated. My apologies for any confusion . A better way to express this , however , might be " patterns that have been classified as difficult " or " patterns shown to be difficult to classify .]
Furthermore , the training time of a single SVM ( which takes several hours ) is much shorter than that of a cascade of AdaBoost classifiers ( which might take several weeks ) .
The final system consists of three stages .
In the first stage , the cascaded 36x36 classifiers consist of three layers , making for a total of 120 features .
The second stage consists of 17 layers with 2 ,160 features .
Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and can thus save significant training time ; the training time needed using the new system is approximately 27 times shorter / approximately 27 rounds of training are needed in the new system . / / <--I think that the first choice here is your intended meaning , but please check carefully .
The final stage is a cascade of three SVM classifiers that take 100 features of the last layer in the second stage as the feature vectors .
Each SVM classifier was trained by using 7 ,500 face patterns and 7 ,500 non-face patterns .
The same 7 ,500 face patterns were used in training all these SVM classifiers .
By running the cascade of AdaBoost classifiers of the second stage on the set of non-face images , 40 ,000 false positives were collected and used as non-face patterns to train the SVM classifiers .
The 7 ,500 non-face patterns used to train the first SVM classifier were selected randomly from the 40 ,000 non-face patterns .
Non-face patterns in the subsequent SVM classifiers were false positives collected by the partially cascaded SVM classifiers on these 40 ,000 non-face patterns .
To control the number of support vectors , the parameter \MATH was used instead of the parameter \MATH .
All SVM classifiers were trained by using the RBF kernel with \MATH .
All these parameters were found by using a cross-validation test tool provided by LibSVM [31] . / / ?NOTE : I believe that I hyphenated this term in your previous document , but after seeing it used here , I would say that it does not need to be hyphenated
This training procedure yielded three SVM classifiers whose numbers of support vectors are 4 ,725 , 5 ,043 , and 4 ,847 .
The average evaluating speed of a SVM classifier is approximately 610 WPS ( windows per second ) .
We tested our system on the MIT+CMU frontal-face standard test set [5] , which consists of 124 images with 480 frontal faces ( excluding images containing hand-drawn , cartoon , and small faces ) .
The configuration and rejection performance of the classifiers are listed in Tables 1 and 2 .
The first row presents the number of features of each layer and the second row shows the fraction of the remaining patterns after each layer were processed . / / [fraction / percentage?<--Here and after , you use " percentage " in the graph , so you may want to keep the same term here .]
The last row indicates the fraction of time that each layer consumed . / / [fraction / percentage?]
All these statistics were extracted by running the classifiers on the MIT+CMU test set .
The fraction of the remaining patterns on these two tables indicates that most of the non-face patterns , i.e. , \MATH , were rejected by the first stage , the cascade of 36x36 AdaBoost classifiers .
When the first 24x24 layer classifier was added to the cascade of 36x36 classifiers , this combination rejected 85 .91\% of analyzed patterns compared to \MATH of using only the first layer of the single cascade of 24x24 classifiers .
Furthermore , the rejection of this very large number of patterns was done extremely quickly , only using \MATH of the total processing time . / / [the total / the standard?]
It also shows that most of the processing time used by the AdaBoost+SVM system , \MATH , was used for SVM classifiers .
The detection rate and speed of the classifiers with ten false positives are listed in Table 3 .
It is clear that our multi-stage system ran faster than the single cascade of 24x24 AdaBoost classifiers while achieving comparable detection rates .
This performance was possible for three reasons .
First , the cascade of 36x36 AdaBoost classifiers rejected many of non-face patterns extremely quickly , while slow SVM classifiers only processed a very small number of the remaining patterns .
Second , many images in the MIT+CMU test set contain large portion of background , which [9] mentioned has a ratio of non-face to face patterns of about 50 ,000 to 1 .
Experimental results showed that the AdaBoost+SVM system ran faster than that of the original AdaBoost on \MATH of the total number of images in this test set .
Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers did not severely affect the final performance because they might also be rejected by 24x24 classifiers in later layers .
Some detection results are given in Figure 17 .
We have developed a method to build a fast and robust face detection system based on a multi-stage approach .
The cascaded structure of AdaBoost-based classifiers in the two first stages allows the system to best adapt to various complexities of input patterns , while nonlinear SVM classifiers at the final stage are robust enough to achieve good results .
Extensive experiments demonstrated that a significant computation time is devoted to potential face regions because almost all non-face patterns are rejected quickly by the two first stages , and only a very small number of face-like patterns are processed by the slow SVM classifiers . / / [are / need to be?]
Discriminant Haar wavelet features selected from AdaBoost are used for all stage classifiers to take advantage of their efficient representation and fast evaluation .
Unsupervised Face Re-Ranking By Mining the Web and Video Archives
It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing .
One popular approach has been to learn visual consistency between images returned by these search engines .
Most state-of-the-art methods of learning visual consistency usually learn one specific classifier for each query to re-rank the returned images .
The main drawback with these methods is that they require computational cost and processing time that are unsuitable for handling a large number of queries .
We propose a method in which one generic classifier is learned and is then used for all queries .
Different from query-specific based methods that learn classifiers for recognition concepts encoded in each query , the generic classifier in our method learns relevance between images and the query for re-ranking purposes .
The key contribution of this research is to introduce a query-dependent feature to represent this relevance and an unsupervised method of collecting training samples to learn the generic classifier .
The generic classifier is built automatically and is independent of existing ranking algorithms for input search engines .
The experimental results demonstrated that the proposed method performed very well in various datasets .
Image searches are essential for many search engines .
Most existing image-search engines usually use text information to determine relevance , resulting in poor precision .
To improve the accuracy of retrieval , it is necessary to use visual information from images to re-rank them .
However , understanding content-based images remains a challenging and unsolved problem .
In addition , using visual information requires much greater computational cost than using text .
One popular approach \CITE combining both text and visual information has been to use text information to quickly retrieve a set of candidates and then do post-processing ( i . e . , re-ranking ) on this set to improve precision .
There are two ways of doing post-processing : The first \CITE has been to build a ranker or a classifier specific to the given query using the returned images .
Building such classifiers involves large computational cost and time .
As a result , this way is not scalable for applications that process very large numbers of queries .
The second way \CITE has been to build a generic classifier once and then use it for all new queries .
This is more scalable and can be used for practical applications such as meta-search engines .
We pursued the latter way to solve the problem with face retrieval in which the system enables users to search people's appearances by their names .
Our system re-ranks the faces returned by text-based search engines with a generic classifier that is trained in advance using visual information before returning them to the user .
Building such generic classifiers requires two problems to be solved : finding a good query-relative representation of faces and collecting a large labeled dataset to train the classifier .
Our contribution by addressing these problems is two-fold :
-We propose a general framework for re-ranking faces returned by existing text-based search engines .
We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not in this framework .
The output scores returned by this classifier are used to re-rank the faces .
The more relevant a face is to the query , the higher score is .
This approach is different from existing ones \CITE that learn a classifier to recognize the identity of the returned faces .
For example , it recognizes a face as the appearance of 'personX' or not the appearance of 'personX' .
Instead , the relevance classifier is learned to classify a face being relevant or irrelevant to the query .
As this classifier is independent of the identity of faces , it can be shared for multiple queries ( cf . Figure \REF ) .
We propose a novel representation for each face that models the relevance between that face and the query .
Once this query-dependent feature for each face is extracted , one relevance classifier can be shared by the faces of various queries .
The experimental results demonstrated that the relevance classifier that is independent of the underlying ranking algorithms of existing search engines could significantly boost performance .
-We propose a simple yet efficient mining technique of automatically collecting labeled data to train the generic classifier .
We specifically detected and grouped faces of people appearing in video programs in face tracks in which each face track contained the faces of one person .
To distinguish the face tracks of different people , we assumed that if multiple faces were detected at different locations in one frame , they would be of different people ( cf .
Using this assumption , we collected face tracks whose faces were detected in the same frames to guarantee that each face track was associated with one unique person .
We used video programs from multiple genres and channels to increase the number of such face tracks .
We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .
Since we knew the relevance of these faces to the artificial sets , the labels for each face could be easily generated and no human intervention was needed in this process .
Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .
Collecting training sets from such external sources as video archives is easy and efficient because , first , a large number of videos can be easily obtained .
For example , people can record broadcast videos from different channels within a certain period .
Second , a huge number of faces can be obtained by applying a face detector to all frames .
In addition , the faces of one person appearing in consecutive frames can be automatically grouped with a high degree of accuracy using temporal information .
It is essential for image-search engines to find relevant images with a high degree of precision given queries described by text , e.g. , 'airplane' or 'George Bush' .
Existing image-search engines usually use textual information associated with images such as filenames , image captions , and surrounding text for ranking that leads to poor precision .
To improve precision , visual information is used to re-rank the returned images .
The idea is to rely on the visual consistency between these images to learn visual classifiers that measure the relevance between an image and the input query .
There have been different approaches \CITE to re-ranking images containing general objects and faces returned from text-based search engines .
Work \CITE has extended to topics on models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or the Hierarchical Dirichlet Process to learn generative model-based classifiers .
These models can handle noisy image data to some degree .
However , they have many parameters that need to be tuned such as the number of topics and feature configurations .
In addition , how the best topic is selected associated with the input query to identify the target label is still a difficult issue \CITE .
Textual information has been used to build a text ranker to re-rank the returned images \CITE .
The top images in this ranked list were used as positive samples to train visual classifiers using SVM ( Support vector machines ) .
This method made the training data cleaner and led to improved performance .
A multiple-instance learning framework has been used to learn category models from images associated with keywords \CITE .
The returned images were treated as a positive bag .
Negative bags were collected from image sets corresponding to unrelated keywords .
The learned model was used to re-rank the images .
These researchers re-ranked images containing general objects .
Gaussian mixture models have been used for re-ranking faces to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
Discriminative-approach-based models such as SVM and linear discriminant analysis have been used instead of Gaussian mixture models \CITE .
A densest-graph-based method has been used for finding the face group relevant to the query \CITE .
One specific classifier is built for each query in these approaches .
Therefore , many classifiers must be built , which are not suitable in practice , to handle a large number of queries .
Only one generic classifier has been built in advance \CITE and then used for all queries .
This generic classifier was a relevance classifier that learned relevance between an image and the query .
Each image for specific classifiers is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , e.g. , 'airplane' .
In other words , each specific classifier is associated with one class label implied by the query .
Each image in a generic classifier is classified as relevant or irrelevant to the query .
Therefore , it is independent of class labels and can be used for any query .
This method works well for objects such as cars and flags , but fails to handle faces .
Our method was inspired by the generic-classifier-based approach .
We extended it in two ways : first , query-dependent features specific to faces are proposed , and second , the training data for learning the generic classifier are collected automatically by mining video archives .
Given a set of faces returned by any search engine for a queried person ( e.g. , 'George Bush' ) , our task is to re-rank these faces to improve precision .
To this end , we extract query-dependent features for each face and then use the generic classifier trained in advance to predict scores representing the relevance between that face and the query .
These scores are sorted and used for re-ranking .
The ranked list is then returned to users as shown in Figure \REF( b ) .
This approach is different from the existing approaches \CITE shown in Figure \REF( a ) in which one specific classifier is built for each query .
To build a specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by a query-independent feature such as pixel intensity around facial features such as the eyes , nose , and mouth \CITE .
The label for each face is 'personX' or 'non-personX' meaning that it is relevant or irrelevant to 'personX' .
Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .
The label for each face is relevant or irrelevant to the query .
The query-dependent feature is used to encode this relevance .
Query-dependent features using textual information has been proposed \CITE .
Each feature was treated as binary indicating the presence or absence of query terms in the textual data associated with the input image , e.g. , filenames , image titles , and nearby text .
Extending this query-dependent feature to use visual information is not trivial since we cannot compute the presence or absence of query terms such as 'George Bush' in each face .
Each image in \CITE is represented as a set of visual words .
The top- \MATH visual words that are strongly associated with the set of returned images for the query are selected .
The binary features for each image are computed by evaluating the presence and absence of these visual words in that image .
Since this method is suitable for general objects rather than faces , we propose another method of extracting query-dependent features to train the generic classifier that is described below .
We assumed that there would be visual consistency between faces returned by search engines for a query to be able to model the relevance between a face and that given query .
In the other words , we assumed faces that were relevant to the query would form the largest cluster .
Note that finding such clusters is still difficult since the number of clusters is not known in advance and the accuracy of clustering algorithms always depends on the discriminative power of feature representation .
This assumption is widely accepted in most of the work in this field \CITE .
We consider the problem of finding relevant and irrelevant faces in the input set to be the problem of outlier detection \CITE that is popular in the data-mining community .
We first describe several distance-based methods of outlier detection that use the distance to the \MATH -nearest neighbors to determine observations as outliers or non-outliers .
Then , adaptation is proposed to form a query-dependent feature .
Given threshold \MATH , for each point \MATH , we examine the number of points \MATH so that \MATH , where \MATH is the distance ( e.g. , Euclidean distance ) between \MATH and \MATH in the feature space .
This number of points \MATH is called the neighborhood score of \MATH and is defined as : \MATH where \MATH is the total number of points in the input dataset .
A low value for \MATH indicates \MATH is a candidate of outliers , while a high value for \MATH indicates \MATH is a member of one strong association cluster .
In practice , it is difficult to know \MATH because this depends on the underlying distribution of the input dataset .
For each point \MATH , find its \MATH -nearest neighbors \MATH ; the distance score of \MATH is the sum of the distances between \MATH and its \MATH -nearest neighbors \MATH and is defined as : \MATH
Points with larger values for \MATH have sparser neighborhoods and are more likely outliers than points belonging to dense clusters , which usually have lower values for \MATH .
Similar to the nearest neighbor score , it is difficult to determine the appropriate \MATH value for each dataset .
We consider the generic classifier as an outlier classifier that classifies an input sample as an outlier or a non-outlier .
Each face in our framework is a sample , and non-outliers / outliers mean faces are relevant / irrelevant to the query ( i.e. , target person ) .
As described above , the \MATH and \MATH of outliers and non-outliers might have the distributions in Figure \REF ; these scores can be used as feature values to discriminate non-outliers from outliers .
From this observation , the feature vector is formed by varying parameters such as \MATH and \MATH in the formula of \MATH and \MATH as follows : \MATH .
It requires a sufficient number of training samples to train the relevance classifier using supervised learning methods such as SVM .
The simplest way \CITE of collecting training samples is to pick many names , and pass them to search engines .
After collecting the returned faces , we manually label each face as to whether it is relevant to the input query or not .
This is a tedious task and involves a human-labor cost .
We propose another approach to automatically collecting training samples to train the relevant classifier .
This approach consists of two steps .
First , by mining video archives , we automatically collect a set of faces of \MATH different people \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of people .
Second , we generate a set of subsets \MATH , where \MATH is the set of faces that is picked from \MATH , and \MATH is the number of subsets .
The restriction is that the assumption of visual consistency is satisfied .
In other words , as seen in Figure \REF , \MATH might have several face clusters and the largest cluster is equivalent to the faces relevant to the query if they are returned by a search engine .
As a result , this method can be used to stimulate face sets returned by search engines using many names as mentioned above .
To obtain \MATH , we use a simple technique for faces extracted from video archives .
We specifically use the following heuristics to pick a set of different people appearing in video archives :
-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .
Figure \REF shows an example where this has occurred .
-If two people appear in video programs broadcast by different broadcast stations ( e.g. , CNN , MSNBC , and CCTV ) , they are likely to be different .
If we have large video archives , we can collect a sufficient number of training samples to learn the relevance classifier by using these heuristics .
We form face set Generating \MATH by picking a subset of faces of Generating \MATH and randomly adding faces from other sets Generating \MATH .
To keep satisfying the assumption of visual consistency , the number of faces selected in each set Generating \MATH must be smaller than the number of faces in set Generating \MATH .
We then label faces in set Generating \MATH as relevant to the query associated with Generating \MATH , and the other faces of Generating \MATH as irrelevant to the query .
Once the training samples are collected , we use SVM with a linear kernel to learn the relevance classifier .
TRECVID dataset : We collected all video programs from the TRECVID 2006 dataset \CITE .
There were 527 video programs broadcast on seven channels in three languages including English , Chinese , and Arabic .
We extracted faces from these video programs and grouped faces belonging to one person in each shot in one face track using a similar method to that described in \CITE .
We scanned all face tracks for each channel extracted from the videos broadcast by this channel , and picked face tracks extracted from key frames where several faces were detected at different locations .
To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .
As a result , there were 5 ,126 faces of 19 face tracks selected from the seven channels corresponding to 19 different people .
Note that the system did not know the identity of these faces .
It only knew any two face tracks represented different people .
The number of faces in these face tracks is shown in Figure \REF .
We generated the 133 labeled sets described in Section \REF using these face tracks and used them to train the relevance classifier .
Yahoo News Images : This dataset consists of approximately half a million news photos and captions from Yahoo News collected over a period of roughly two years \CITE .
Using people�fs names as queries , we applied a simple string search to the captions in this dataset to return a list of faces for each queried name .
We used 23 names of celebrities such as George W .
Bush , Vladimir Putin , Ziang Jemin , Tony Blair , and Abdullah Gul .
These names have widely been used in experiments \CITE .
A total of 9 ,136 faces were retrieved in which 3 ,909 faces were relevant .
The accuracy was \MATH on average .
Google Images : We used the same set of people�fs names used in the Yahoo News Images dataset and input them into the Google Image Search Engine .
We crawled a maximum of 500 images from URLs returned by Google for each query .
A total of 9 ,516 faces were extracted in which 5 ,816 faces were relevant .
The accuracy was \MATH on average .
The TRECVID dataset was used for training the generic classifier .
The datasets for Yahoo News Images and Google Images , as shown in Figure \REF , were used for testing .
We used the Viola-Jones face detector \CITE to detect frontal faces in images and video frames .
We simply used a similar technique to that described in \CITE to group faces belonging to one person in one video shot .
Using prior knowledge that faces of the same person in consecutive frames do not change much in locations and appearance , the technique used tracked points to robustly associate these faces in face tracks with a precision of \MATH .
Once faces were extracted , we used the code provided by the authors \CITE to extract features .
Each face was then represented as a point in a very high dimensional feature space .
Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .
There were a total of 13 feature points from which features were extracted .
The features were intensity values lying within a circle with a radius of 15 pixels .
The output feature had 13x149 = 1 ,937 dimensions .
Figure \REF illustrates this feature .
We evaluated the efficiency of retrieval with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as : \MATH .
Precision and recall were only used to evaluate the quality of an unordered set of retrieved faces .
Average precision is usually used to evaluate ranked lists in which both recall and precision are taken into account .
The average precision is computed by taking the average of the interpolated precision measured at 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
The interpolated precision , \MATH , at a certain recall level , \MATH , is defined as the highest precision found for any recall level \MATH :
In addition , we used the mean average precision to evaluate the performance of multiple queries , which is the mean of average precisions computed from queries .
We compared the performance of the Maximum A-Posteriori ( MAP ) algorithm in seven systems in this experiment by testing it on YahooNews Images :
-DistScore-TrainGoogleImages : The training set was the set of annotated faces returned by Google Images Search for 23 people�fs names .
The feature vector was computed using \MATH .
-NNScore-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
The feature vector was computed using \MATH .
DistScore-TrainTRECVID : The feature vector was computed using .
The training set was the set of annotated faces artificially generated with our method described in Section \REF .
-NNScore-TrainTRECVID : The training set was the same as DistScore-TrainTRECVID .
The feature vector was computed using \MATH .
-Krapac[11]-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
We re-implemented the method proposed by Krapac et al. \CITE of extracting the query-dependent feature .
Since this method was proposed to handle images , not faces , we modified it to handle faces .
Each face was specifically represented as a bag of visual words .
We used 13 facial-feature points detected in each face and their descriptors using pixel intensity as visual words .
The codebook was formed by clustering all visual words extracted from all faces of the training set into 200 clusters .
The top-$k$ visual words strongly related to the returned faces of each query and the binary feature vector were computed as described in \CITE .
-Mensink[15]-GaussianModels : This method proposed by Mensink et al. \CITE modeled the returned faces by using two Gaussians , the first for the faces relevant to the target person and the second for the remaining faces .
-Mensink[15]-Friends : This method proposed by Mensink et al. \CITE used linear discriminant analysis to train a specific classifier for each query .
This method used detected people�fs names in captions associated with faces for query expansion to model faces of the target person 's friends .
Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are state-of-the-art that learn a specific classifier for each query .
Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then used for new queries .
Figure \REF compares the performance of these systems when they were tested on the YahooNews Images dataset .
The curves plot the correlation between performance and the number of features for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID .
-DistScore performed significantly better than NNScore .
-The performance of DistScore and NNScore was not affected by selecting the number of features .
Therefore , we could use small numbers of features to reduce the computational cost .
-The performance of the system using training data generated artificially with our method was comparable to that of the system using training data returned by search engines .
-The method of DistScore-TrainTRECVID we propose performed comparably to the state-of-the-art method in the specific classifier-based approach of Mensink[15]-Friends .
It outperformed the method where only visual information was used , i.e. , Mensink[15]-GaussianModels .
-Our proposed method DistScore-TrainTRECVID outperformed the method proposed by Krapac et al. , which was customized to handle faces .
As seen in Figure \REF , DistScore-TrainTRECVID outperformed the original ranking of the Google Images Search Engine if from 20 to 50 features were used .
The results for DistScore-TrainTRECVID on the YahooNews Images set and Google Images set indicate that the relevance classifier with our proposed method was able to generalize well on different queries and was independent of underlying ranking algorithms used in search engines .
Figure \REF shows an example of re-ranking results for the top-30 faces for the query John Paul , which is one of the most difficult cases in the YahooNews Images set .
The results clearly demonstrate that our proposed method outperformed the other state-of-the-art methods .
Our query-dependent feature was based on the nearest neighbors of images in the returned image set that usually have a complexity of \MATH , where \MATH is the total number of images in the set .
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and a Self Adaptive Set of Histograms ( SASH ) \CITE could significantly speed up the nearest neighbor search .
For example , the complexity of the fast lookup of $k$ approximate nearest neighbors is \MATH \CITE .
Studying other techniques to speed up the process of query-feature extraction is our next step in future work .
376
We have presented a novel method of re-ranking face images returned by existing search engines .
Instead of training a specific classifier for each new query , we only trained one generic classifier and used it for ranking new queries .
This helped make the ranking application more scalable .
We propose a simple unsupervised method to train the generic classifier to obtain a large number of labeled faces from video archives .
It uses temporal information to group faces belonging to one person in one shot into one track .
Several heuristics are employed to guarantee that a subset of face tracks has the correct labels used in the training process .
Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .
Enhancing mathematical searches with names of formulas
We present a method to enhance the performance of a mathematical search system .
By targeting mathematical formulas that appear in natural language documents , we collect the names of formulas from the surrounding text and incorporate the correspondence into the search system 's database .
The effectiveness of the approach is demonstrated through experiments using Wikipedia mathematical articles and Wolfram Functions Site data sets .
The mathematical content being published on the Web is increasing day by day , and retrieving mathematical content has become an important issue for many users .
Teachers , students , and researchers need better access to mathematical resources for teaching , studying , and obtaining information for research and development .
Moreover , users need specialized search systems to find formulas that are relevant to their needs .
Internet search engines can detect particular keywords in mathematical formulas but they mostly fail at recognizing mathematical symbols and constructs such as integral and square root symbols , fractions , and matrices .
There are some mathematically oriented search engines on the Internet .
Although such engines provide more accurate and relevant results , they usually do not provide enough information for the user .
Furthermore , these systems do not take into account the semantics of mathematical formulas as revealed by the surrounding natural language text , e.g. , the formula�fs name or the description of its variables .
The Digital Library of Mathematical Functions ( DLMF ) project is a mathematical database available on the Web [8] .
This site provides a major resource of mathematical reference data for special formulas and their applications .
But even this site does not provide a full mathematical search .
Other systems that support mathematical searches are MathFind [4] , MathWebSearch [3] .
These systems , however , provide neither similarity structures nor semantic meanings of their formulas .
The Wolfram Functions Site [7] contains a large number of mathematical formulas and also provides a semantic search for them .
This site and some recent work done by Adeel et al. [2] and Yokoi and Aizawa [1] employ similarity search methods based on MathML but they do not make use of the semantics of the formulas' surrounding text . //[ ? ? propose is unclear in the sense of a website .]
The work presented in this paper focuses on retrieving mathematical formulas on the Web by using mathematical expressions and the surrounding natural language text .
We describe our work toward creating a mathematical database that contains formulas , their names , variable descriptions , and other related information .
We implemented a mathematical search system that uses this information as its base knowledge .
This information is very helpful when performing mathematical search by reducing the need for formula input and solving the notational variation problem where mathematically equivalent formulas follow different notations .
The relationship between formulas and their names can also be used to correct errors in mathematical OCR systems , such as Infty [5] .
It also provides opportunities to make mathematics better understandable and usable for people with disabilities .
The remainder of this paper is organized as follows : we present an overview of our framework in section 2 .
We then describe the results of our experiments in section 3 .
Section 4 concludes the paper and gives avenues of future study .
Mathematical formulas on the Web have many different formats , e.g. , LaTeX and Mathematical Markup Language ( MathML ) [6] .
This diversity makes searches more difficult .
In this paper , we shall use the MathML format for mathematical formulas .
Formulas with other formats can be easily converted to MathML format by using freely available tools .
For our work , we used LaTeXML Converter , which is freely available at \URL .
We automatically collected our mathematical formulas from Wikipedia and the Wolfram Functions Site .
Figure 1 shows a page from a mathematical section on Wikipedia and the information we retrieved on this site , besides the mathematical formulas .
We used heuristics to ensure adequate matching of mathematical formulas with their names .
These heuristics are based on the type settings and distances between the name strings and formulas on the same page .
After collecting mathematical formulas from these resources , we extracted keywords for indexing .
The keywords included formulas' names , operators , variables' names , and so on .
Our system allows two ways of searching : text content and formula content .
In a text content search , users search with extracted keywords , e.g. , " sin " , " Pythagorean " or " trigonometric functions " .
In a formula content search , users directly input the formulas , for example : \MATH .
The system then looks for relevant formula names .
If found , it will return other information related to that formula .
If nothing matching is found , it looks for mathematical formulas which are similar to the input ( including formulas with a similar structure ) .
Evaluating a mathematical search system is not an easy task because we do not have any standard for this task .
The similarity between mathematical formulas is very subjective .
We consider that formulas with the same semantic meaning are relevant . //[The original is unclear the rewrite seems to be what you mean .]
For example , while searching for sin( a ) , we also consider results containing arcsin or cosin .
Our experiments were conducted on a collection of about 16 ,000 mathematical docu-ments on Wikipedia and about 155 ,000 mathematical formulas on the Wolfram Functions Site .
To show the effect of linking the formula with its name , we also set up an experimental search system without using the formula 's names .
Table 1 shows the top 5 search results for the query " sin( a + b ) " .
As can be seen , when the system associates the formulas with their names , it can provide more useful information to the user .
The system also allows the user to input the formula 's name directly .
Table 2 shows the top 10 results for the query " Pythagorean " .
Note that at this time , when the user submits a query that does not match any function 's name in our database , the system can not return anything .
We presented a new framework for mathematical searches where links between formulas and their names are automatically detected in the target documents and then utilized in the search .
Due to unavailability of a standard corpora to evaluate mathematical search systems , our evaluation at this moment remains subjective and limited .
We believe that our approach of incorporating information other than the mathematical formulas themselves showed promising results .
The experimental results showed how helpful this information is to mathematical search users .
However , this is only a first step ; many important issues are left for future study .
Using a formula 's name is only one way of taking into account the semantic meaning of the formula ; we are considering other information such as the formula 's description and its variable 's description .
Currently , our system uses only the links between formulas and their names in the same article .
Therefore , linking formulas across articles should also be taken into account .
Automatic approach to understanding mathematical expressions using MathML Parallel Markup corpora
This paper explores the use of MathML Pallel Markup Corpora for automatic understanding of mathematical expressions , the task of which is formulated as a translation from Presentation to Content MathML Markups . // <the use of capitals implies that these are software applications like PowerPoint or Word . I assume this is the right idea .> .
In contrast to previous research that mainly relied on manually encoded transformation rules , we use a statistical-machine-translation-based method to automatically extract translation rules from parallel markup corpora .
Our study shows that the structural features embedded in the MathML tree can be effectively exploited in the sub-tree alignment and the translation rules extracted from the alignment give a boost to the translation system .
Experimental results on the Wolfram Function Site show that our approach is an improvement over prior rule-based systems . // <Note : It seems that where were two prior systems that were compared . If not , you can go back to using a prior system .> .
One of the most significant discussions regarding the digitization of mathematical and scientific content and its applications is about semantic enrichment of mathematical documents , that is , adding or associating semantic tags - usually concepts - with mathematical expressions .
By encoding the underlying mathematical meaning of an expression explicitly , it is possible to interchange information more precisely between systems that semantically process mathematical objects .
The direct application of this idea enables semantic searches for mathematical expressions whereby the system 's �eunderstanding ' of the intent of the searcher and the contextual meaning of mathematical terms improves search accuracy .
It also benefits computer algebra systems , automatic reasoning systems and multi-lingual translation systems .
However , as is the case with natural language , semantic enrichment of mathematical expressions is a non-trivial task .
- First , mathematical notation , though more rigorous than natural language , is nonetheless at times ambiguous , context-dependent , and varies from community to community .
- Second , the underlying mathematical meaning of an expression needs to follow a semantic markup in a semantically rigorous way .
Because of this , in failing to follow the constraint , the computer might not be able to process that expression .
- The third problem is that new notations tend to be introduced and used when needed so a mechanism is required for referring to mathematical concepts outside of the base collection .
The aim of this paper is to describe a method of automatic semantic enrichment for mathematics that is capable of analyzing and disambiguating mathematical terms .
In our research , MathML \CITE Presentation Markup is used to display mathematical expressions and MathML Content Markup is used to convey mathematical meaning .
The semantic enrichment task then becomes one of generating Content MathML outputs from Presentation MathML expressions .
There are three reasons why we chose MathML markup in our research .
- First , since its first release in 1997 , MathML has grown to become a general format that enables mathematics to be served , received , and processed in a wide variety of applications .
- Second , MathML can be used to encode both mathematical notations and mathematical content .
- Last , large collections of formulas are available in MathML , and we can easily assess these collections .
- In the scope of this paper , we only make use the information within a mathematical expression for disambiguation when translating it into content markup .
The prior solution to this problem is SnuggleTeX \CITE , which was proposed by David McKain .
The system uses rule-based methods for disambiguation and translation .
This solution has two main limitations :
- Since it is a hand-written rule-based system , SnuggleTeX requires mathematical knowledge and human effort to develop .
- Due to the diversity of mathematical expressions , SnuggleTeX is still considered experimental and has difficulty processing complicated mathematical symbols and expressions .
In this paper , we propose an approach that automatically learns semantic inferences in a presentation from parallel markup data . // <The original has too many from to be logically clear . The rewrite is a guess . > .
This approach is based on statistical machine translation .
The underlying mathematical meaning of an expression is inferred from the probability distribution $ p( c | p ) $ that a semantic expression $ c $ is the translation of a presentation expression $ p $ .
The probability distribution is automatically learned from both Presentation and Content MathML markup data , that is , parallel markup MathML data .
The data used in this study was collected from the Wolfram Function Site \CITE .
We also prepared other parallel markup MathML data by annotating mathematical expressions in 20 papers from The Archives of the Association for Computational Linguistics \CITE ( ACL-ARC ) .
There are two main contributions in this paper :
- First , we successfully applied machine translation techniques to solving the problem of mathematic semantic enrichment .
Experimental results show that our system significantly outperforms the current rule-based system and it can handle a lot of practical cases in the semantic enrichment problem .
The quantity and quality of mathematical expressions are continuing to grow , and we believe that our system will be able to cover most mathematical expressions .
- Second , mathematics knowledge such as a symbol 's meanings or structural relations is automatically learned while training ; therefore , the system requires no human effort or expertise , and it is easier to update with more data .
Since new notations keep cropping up , it is important to update the system as quickly as possible .
We performed a ten-fold cross validation on mathematical expressions from six categories of the Wolfram Functions Site to evaluate the effectiveness of our learning method .
We performed another experiment to assess the correlation between the system 's performance and training set size and found that increasing the size of the training data boosted the system 's performance .
We also performed an extensive comparison with prior work \CITE using a data set collected from ACL-ARC scientific papers .
Our experimental results show that our approach works well in dealing with the mathematics semantic enrichment problem and it outperforms the previous work by making significantly fewer errors .
The remainder of this paper is organized as follows : In Section 2 , we give a brief overview of the background and related work on semantic enrichment of mathematical expressions .
We present our method in Section 3 and describe the experimental setup and results in Section 4 .
Section 5 concludes the paper and gives avenues for future work .
Since mathematical formulas contain both mathematical symbols and structures , a special markup is required for their representation .
Until recently , images have been used to represent mathematical formulas on the web .
This type of display does not need any markup language to decode the formulas , but it is hard to process them .
A way of dealing with mathematical formulas in this format is to convert them into another text-based format , for example , InftyReader \CITE .
\TeX{} has been used to encode mathematical formulas in scientific documents .
A formula is printed in a way a person would write by hand , or typeset the equation .
In some web pages , such as on the Wikipedia site , formulas are displayed in both image and \TeX{} formats .
The best known open markup format for representing mathematical formulas for the web is MathML \CITE , which was recommended by the W3C math working group .
It provides a standard way of representing mathematical expressions .
It is an XML application for describing mathematical notations and encoding mathematical content within a text format .
MathML has two types of encoding , content-based encoding , called Content MathML , dealing with the meaning of formulas , and presentation-based encoding , called Presentation MathML , dealing with the display of formulas .
The illustration trees of the Presentation and Content Markup of the expression $ C_{-\frac{17}{2}}= \tilde {\infty} $ are depicted in Figure \REF and Figure \REF .
Besides MathML , there are other markups such as eqn \CITE , OpenOffice .
org Math \CITE , ASCIIMathML \CITE , and OpenMath \CITE , but these markups can be converted into MathML by using freely available tools .
There are not many studies on the semantic enrichment problem .
In this section , we list some of the work on exploiting the meanings of mathematical expressions .
Grigole et al. \CITE proposed an approach to understanding mathematical expressions based on the text surrounding the mathematical expressions .
The main idea of this approach is to use the surrounding text for disambiguation based on word sense disambiguation and lexical similarity .
First , a local context C ( five nouns preceding a target mathematical expression ) is found in each sentence .
For each noun , the system identifies a Term Cluster ( derived from the OpenMath Content Dictionary ) with the highest semantic similarity according to a similarity metric .
The similarity scores obtained are weighted , summed up , and normalized by the length of the considered context .
The Term Cluster with the highest similarity score is assigned as the interpretation .
The approach was evaluated on 451 manually annotated mathematical expressions , and the best result was an F_{0.5} $ score of 68.26 $ .
To deal with the meanings of mathematical formulas , Nghiem et al. \CITE proposed an approach for extracting names or descriptions of formulas by using the natural language text surrounding them .
The most accurate extraction result using data from Wikipedia was $ 68.33 $ percent .
There are two other projects that deal with the semantic meaning of mathematical expressions .
The first is the SnuggleTeX project \CITE , which provides a free and open-source Java library for converting fragments of LaTeX into XML including Content MathML .
The other project is Lamapun \CITE .
This project investigates semantic enrichment , structural semantics , and ambiguity resolution in mathematical corpora .
Unfortunately , there are no evaluations of these systems .
To translate mathematical expressions from the Presentation MathML into Content MathML format , a list of translation rules is required .
Building these translation rules by hand is a large undertaking .
Our task is inherently domain-specific ; therefore , we devised an approach based on statistical machine learning for automatically extracting rules from a dataset .
Statistical machine translation ( SMT ) is by far the most widely studied machine translation method .
SMT uses a very large data set of good translations , that is , a corpus of texts which have already been translated into another language , and it uses those texts to automatically infer a statistical model of translation .
The statistical model is then applied to new texts to make a translation of them .
Tree-based or syntax-based SMT can be used for tree-to-tree translation but it has two drawbacks when it is applied to the problem of translating Presentation into Content MathML .
- The first drawback is that tree-based SMT focuses on generating surface texts rather than tree structures .
Mathematical expressions have strict structures , and it fails to fulfill this requirement .
- The second drawback is there are many long mathematical expressions in real-world data and translating long and complex sentences has been a critical problem in machine translation .
To overcome these limitations , we made two separate rule sets : fragment rules and translation rules .
The details are described in the next section .
The framework of the system is shown in Figure \REF .
The system has three main modules .
- Preprocessing : This module processes MathML expressions by removing error expressions or format tags with no semantic meaning .
- Rule Extraction : This module is given a dataset containing MathML parallel markup expressions , and it extracts translation rules from it . // <The original is ungrammatical and unclear . The rewrite is a guess .> .
- Content MathML Generation : This module is given mathematical expressions in Presentation MathML markup and a set of rules , and it generates Content MathML expressions to enrich the Presentation MathML expressions .
The presentation elements of Presentation MathML are divided into two classes : token elements and layout schemata .
Token elements represent the identifier 's names , function 's names , numbers , etc. // <the identifier 's names means there is one identifier with possibly many names . If this is what you want to say , it is okay . If not , maybe you mean simply " identifier names " . ><Likewise , maybe you mean " function names " .>
Layout schemata build expressions out of parts .
After investigating data on the Wolfram Function Site , we noticed that there are elements that have no specific meaning ; they are used for display purposes only and most of them are layout schemata .
For example , the $ <mtext> </mtext> $ or $ <mspace / > $ tags are used to insert some space between expressions .
Another example is pairs of parentheses ; these are used to indicate that the expressions in the parentheses go together , despite that their structure already encodes that information . // <The original is unclear . The rewrite is a guess . > .
This preprocessing step removes these elements .
We also remove mathematical expressions with error markups such as expressions that have no Content markup .
For simplification , expressions with more than 200 content nodes are also removed .
In the training phase , we use GIZA++ \CITE for aligning Presentation MathML terms and Content MathML terms .
Based on the aligned data , we use heuristics to extract rules that we call " fragment rules " .
Fragment rules are rules that define the translation from the Presentation MathML sub-trees to the Content MathML sub-trees .
These rules are used to break up a large Presentation MathML tree into smaller sub-trees while maintaining the structure of the output Content MathML trees .
These rules are extracted based on the fact that translating a small tree is easier than translating a large one .
Each rule in the fragment rule set is associated with a probability , that is , the frequency at which a rule occurs in the training data .
Once the sub-trees cannot be broken down further , we start to extract other rules , which we call " translation rules " .
We enhance the translation rule set with translation terms extracted by GIZA++ .
The pseudo code for extracting fragment rules is described in Algorithm \REF .
In the previous steps , we get two sets of rules , a fragment rule set and a translation rule set .
We then use these rules for translation .
Given mathematical expressions in Presentation MathML markup , the system will generate Content MathML markup for each expression .
- First , the expression is preprocessed to remove non-semantic elements .
- Second , the fragment rule is applied to the expression until it cannot be divided any further .
- Third , the small sub-expressions in Presentation MathML markup are translated into sub-expressions in Content MathML markup by using the translation rule set .
If no translation rule is found for a sub-expression , that expression is marked as untranslated .
- Last , sub-expressions in Content MathML markup are grouped to form the complete Content MathML expression .
Before the last step , we add a heuristic translation to translate numbers .
The reason for this is that there is an infinite number of rules . // <The rewrite is a guess .> .
The translation algorithm is described in Algorithm \REF .
The experiments were carried out using datasets from the Wolfram Function site .
This site was created as a resource for educational , mathematical , and scientific communities .
It contains the world 's most encyclopedic collection of information about mathematical functions .
All formulas on this site are available in both Presentation MathML and Content MathML format .
The datasets we used contain 205 , 653 mathematical expressions belonging to six categories .
All of these expressions have both MathML Presentation and Content Markups .
Training and testing were performed using ten-fold cross-validation ; for each category , the original corpus was partitioned into ten subsets .
Of the ten subsets , a single subset was retained as the validation data for testing the model , and the remaining subsets were used as training data .
The cross-validation process was repeated ten times , with each of the ten subsets used exactly once as the validation data .
The ten results from the folds then were averaged to produce a single estimation .
To prove the effectiveness of our models with real data , we conducted another experiment on the mathematical expressions in scientific papers .
Currently , we have 20 papers from the ACL archive , and we manually annotated all of the math expressions in these papers with both Presentation Markup and Content Markup . // The original is somewhat vague . The rewrite is a guess . Use it if it is correct . > .
We called this data ACL-ARC .
In the first experiment , the data was not compatible with SnuggleTeX since SnuggleTeX uses ASCII MathML but the Wolfram Functions site does not .
In the second experiment with ACL-ARC data , we compared our model with SnuggleTeX .
Table \REF lists the various data statistics .
Given a Presentation MathML expression $ e $ , we assume that tree $ A $ is the correct Content MathML tree of expression $ e $ and tree $ B $ is the output of the automatic translation .
The basic idea to evaluate the correctness of tree $ B $ is directly comparing it with tree $ A $ .
In the experiments , we extended the conventional definition of " Translation Error Rate " and used a metric which is a combined version of
- the Tree Edit Distance \CITE : the tree edit distance is the minimal cost to transform A into B using edit operations .
There are three types of edit operation : substituting a node , inserting a node , and deleting a node .
- the Translation Error Rate \CITE : the translation error rate is an error metric for machine translation that measures the number of edits required to change a system output into one of the references .
We called the new metric the Tree Edit Distance Rate ( TEDR ) .
TEDR is defined as the rate between ( 1 ) the minimal cost to transform a tree A into another tree B using edit operations and ( 2 ) the maximum number of nodes of A and B . // <The " rate between " is unclear to me . Do you mean , " the ratio of " >
It can be computed using Eq . \REF .
For example , the output tree using the translation system for the expression $ C_{-\frac{17}{2}}= \tilde {\infty} $ is depicted in Figure \REF .
Compared with the reference tree in Figure \REF , we need to substitute X nodes , insert Y nodes , and delete Z nodes , so that $ TED( A , B ) = x $ , while the maximum number of nodes of the two trees is y .
Therefore , $ TEDR( A \rightarrow B ) = \frac{x}{y} = z $ .
It appeared that SnuggleTeX was not applicable to the data from the Wolfram Function site since it uses ASCII MathML but the site does not .
Therefore , we could not do a comparison on this data .
Our experimental results show that our approach gives reasonable results , that is , a 20 percent TEDR with large training data .
For small data ( less than 3000 training samples ) , the results vary from 50 to 75 percent TEDR .
For ACL-ARC data , the experimental results show that our system significantly outperforms SnuggleTeX in terms of the Tree Edit Distance Rate .
Our system had a 24 percent lower TEDR in comparison with SnuggleTeX .
To investigate the correlation between the TEDR score and training set size , we set up an experiment using mathematical expressions in the Elementary Functions category .
We started with one fifth of the data and increased the data by one fifth in each run .
Our experimental results conformed with the theoretical analysis that the more training data we have , the better the results are .
Table \REF and Table \REF show the TEDR of our method on the Wolfram Functions Site data and in comparison with SnuggleTeX on ACL ARC data , respectively .
Table \REF and Figure \REF shows the correlation between TEDR score and training set size .
We discussed the problem of semantic enrichment of mathematical expressions .
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expressions to Content MathML expressions is a significant improvement over prior systems .
As we mentioned before , mathematical notations are context-dependent .
That means we need to consider not only surrounding expressions but also the document that contains the notations in order to generate the correct semantic output .
In the scope of this paper , we only considered the first sort of context information .
Since this is a first attempt to translate from Presentation to Content MathML using a machine learning method , there is room for further improvement .
Possible improvements are
- Increasing the training data so the system can cover more mathematical notations
- Expanding the work by incorporating the surrounding information of mathematical expressions , for example , definitions or other mathematical expressions .
Our approach combining automatic extraction of fragment rules and translation rules has shown promising results .
The experimental results confirm that it would be helpful for automatic understanding of mathematical expressions .
However , this is only a first step ; many important issues remain for future studies .
Currently , our system deals with a limited range of mathematical notations .
In the future , we should consider expanding it to cover all mathematical notations .
Improving protein coreference resolution by simple semantic classification
Current research has shown that major difficulties in event extraction cases for the biomedical domain are related to coreference .
Therefore , coreference resolution is believed to be useful for improving event extraction .
To address coreference resolution in molecular biology literature , the Protein Coreference ( COREF ) task was arranged in the BioNLP-ST 2011 , as a supporting task .
However , the shared task results indicated that transferring coreference resolution methods developed for other domains to the biological domain was not straightforward , due to the domain differences in the coreference phenomena .
We studied the contribution of domain-specific information , including information that indicates the protein type , in a rule-based protein coreference resolution system .
In particular , the domain-specific information is encoded into semantic classification modules for which the output is used in different components of the coreference resolution .
We compared our system with the top four systems in the BioNLP-ST 2011 ; surprisingly , we found that the minimal configuration had outperformed the best system in the BioNLP-ST 2011 .
Analysis of the experimental results revealed that semantic classification , using protein information , had contributed to an increase in performance by 2.3 % on the test data , and 4 .0% on the development data , in F-score .
The use of domain-specific information in semantic classification is important for coreference resolution .
Since it is difficult to transfer domain-specific information across different domains , we need to continue to seek methods to exploit and use it in coreference resolution .
While named entity recognition ( NER ) and relation / event extraction are regarded as standard tasks for biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is being recognized more and more as an important component of IE to achieve a higher performance .
Without coreference resolution , oftentimes , the IE performance issubstantially limited , due to the abundance of coreference relations in natural language text ; information pieces written in text with the involvement of a coreference relation are hard to be captured [ 9 , 14 ] .
There have been several attempts for coreference resolution ; in particular , they have been for newswire texts [ 7 , 8 , 22 , 23 , 28 , 30 ] .
Coreference resolution is also one of the lessons from the BioNLP Shared Task ( BioNLP-ST , hereafter ) 2009 , in which it was communicated that coreference relations in biomedical text substantially hinder the progress of fine-grained IE [ 10 ] .
To address the problem of coreference resolution in molecular biology literature , the Protein Coreference ( COREF ) task was arranged in BioNLP-ST 2011 as a supporting task .
This task definition focuses on protein , as a specific type of entity .
Figure 1 shows an example text segmented into four sentences , S2 - S5 , where coreferential expressions are shown in brackets .
In the figure , protein names P4 - P10 are highlighted in boldface ; the targeted anaphoric expressions of the shared task ( pronouns and definite noun phrases ) are T29 , and T32 , for which the antecedents are indicated by arrows , if found in the text .
In the example , the definite-noun-phrase expression , this transcription factor ( T32 ) , is considered coreferential with the protein mention p65 ( P10 ) .
Without knowing this coreference relation , it becomes difficult to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is a localization of p65 ( out of nucleus ) , according to the framework of BioNLP-ST .
The terminologies used in this paper are similar to those in [ 25 ] .
A new term introduced in the BioNLP-ST is antecedent protein , which indicates the protein mention contained in the antecedent expression , e.g. , p65 in T28 .
There are other coreferential expressions , which are ignored in the context of this COREF task , such as : this complex and the NF-kappa B transcription factor complex ( Figure 1 ) , since our focus is on the antecedent expressions that contain and point to protein mentions .
The best system in the COREF shared task , according to the primary evaluation , found 22 .2% of the anaphoric protein references at the precision of 73 .3% ( 34 .1% F-score ) .
The results are promising , since the authors make use of an external coreference resolution tool originally built for the news domain , without much domain adaptation on the main coreference resolution algorithm .
Modifications are mostly made to the markable detection component and post-processing for the output coreference links [ 11 ] .
However , the external coreference tool " s performance drops for biological texts than for news texts , from 66 .38% to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .
A detailed analysis on the _nal submission of the COREF task participants was reported in the organizer 's papers [ 15 , 31 ] , and is summarized in table 2 .
In this analysis , the submitted predictions on the test data set of the COREF shared task are analyzed according to four types of anaphoric expressions : DNP for definite noun phrases , RELAT for relative pronouns , PRON for other pronouns including personal , possessive , and demonstrative pronouns , and OTHER for catch-all type .
Examples of the coreference types are outlined below :
- " [ . . . ] the phosphorylation status of [ TRAF2 ] had significant effects on the ability of [ the protein ] to bind to CD40 , " ( DNP )
- " Subnuclear fractionation reveals that there are [ two ATF1 isoforms , which ] appear to differ with respect to DNA binding activity , " ( RELAT )
- " This ability of [ CIITA ] to facilitate promoter occupation is undissociable from [ its ] transactivation potential , " ( PRON )
An analysis of the results indicated that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type was 27 .5% F-score and 10 .1 F-score , respectively ; the scores were much lower than the F-score for relative pronouns ( the RELAT type ) , which yielded a 66 .2 % F-score .
Thus , it can be inferred that it is more difficult to resolve definite noun phrases and pronouns than relative pronouns .
The top four official results of the COREF shared task are presented in the top four rows of Table 2 .
In this paper , we compare the contributions of different features in coreference resolution ; two simple types of domain-portable information : discourse preference and number-agreement , is compared , as well as domain-specific information , which is considered to be more difficult to be transferred across different domains .
We implemented a protein coreference system that makes use of syntactic information from the parser output , and protein-indicated information encoded in rule-based semantic classification .
Experimental results showed that domain-specific semantic information is important for coreference resolution , and that simple semantic classification using semantic features helped our system to outperform the best-reported system results in the shared task .
In order to acquire insight into the problem , we took a rule-based approach , analyzing the training data of BioNLP-ST 2011 Coref task .
The performance of the system evaluated on the official test dataset of the COREF task shows a significant improvement over the official winning system of the task .
This section presents the overview and the performance evaluation of our system .
Figure 2 shows the overall design of the system , which includes five main components : preprocessing , markable detection , anaphor selection , antecedent candidate selection , and antecedent prediction .
Processing of each component is briefly described below .
More details of implementation can be found in the method section .
Step 0 - Preprocessing : The input text is preprocessed using NLP tools for sentence segmentation , and syntactic parsing .
We used the Genia Sentence Splitter and Enju Parser [ 15 ] for sentence segmentation and syntactic parsing , respectively .
( Enju parser comes with a default tokenizer and part-of-speech tagger for biological text . ) Row 1 in the example of Table 1 shows three sentences as the output from the Genia Sentence Splitter , and noun phrases as the output from the Enju Parser for the sentence , S3 .
Due to the limited space , only a part of the phrases are shown in the table .
The full parse tree for this sentence is separately shown in Figure 3 .
Step 1 - Markable detection : Text chunks that are candidate coreferential expressions , which are also called markables following the jargon of MUC-7 , are collected .
For the set of markables , noun phrases , which do not include a subordinate clause , are collected as they are analyzed by a syntactic parser ( in our case , Enju ) .
Pronouns are also collected as markables .
Then , for chunks that share the same head word , which is normally the main noun of a noun phrase , only the longest chunk is taken .
Since the Enju parser outputs head-word information for every noun phrase , we make use of this information for our processing , without any modification .
The third row of Table 1 shows the result of markable detection for the sample text .
In the sentence S3 , three noun phrases recognized by the NX and NP tags of the Enju output , role , role for c-Myc in apoptosis , and this role for c-Myc in apoptosis ( Step 0 results ) share the same head-word role ; thus , only the longest noun phrase , this role for c-Myc in apoptosis , is selected .
However , between studies and studies using . . .
apoptosis , the former chunk is selected , since the latter contains a subordinate clause .
Step 2 - Anaphor selection : Candidate anaphoric expressions , which are basically pronouns and definite noun phrases , are determined . A minority of anaphors are indefinite noun phrases or entity names , which act as appositions .
The system first considers all pronouns and definite noun phrases in the markable set as anaphors .
Then , several filters are applied to remove anaphors that are not relevant to the task definition .
We implemented two types of filters : syntactic and semantic .
Syntactic filters are used to filter out pleonastic its , or pronouns , like : he , she , which are not expected to refer to proteins .
Moreover , because our task focuses on protein references , semantic filters can be used to filter out non-protein anaphors at this stage .
In practice , for definite noun phrase type of anaphors , this is accomplished , by using a list of possible head-words of protein references ; for pronouns , their context words are used .
More details of these methods can be found in the following section .
Step 3 - Antecedent candidate selection : For each anaphor , this component collects the antecedent candidates from the preceding expressions .
One of the candidates will become the response antecedent , as a result of the antecedent prediction step .
In theory , all expressions in the set of markables can become antecedent candidates ; however , too many candidates makes it difficult to achieve correct antecedent prediction .
Moreover , we also filter out candidates that violate syntactic or semantic constraints raised by the anaphor .
In our system , this is done by using a particular window size in sentences , together with several syntactic filters .
One of the syntactic filters is based on syntactic relations among phrases outputted from the parser .
The idea behind this filter is that some types of syntactic relations imply the impossibility of coreference relations between its argument noun phrases and the inclusive expressions of these noun phrases .
For example , the two expressions : dominant negative form and its in our example in Table 1 , cannot be coreferential with each other , since they are connected via the preposition of .
Another syntactic filter removes pronouns that are not in the same pronoun family as the anaphor .
This results in the disappearance of this in candidate antecedents of its .
Pronouns in the same family as its are its , it , and itself .
Step 4 - Antecedent prediction : The best candidate in the antecedent candidate set is selected , and a response coreference link is formed .
Antecedent candidates are compared with one another using a comparison procedure .
This procedure implements a decision rule list containing four rules , encoding the following selection preference conditions :
-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate , which does not conflict in number with the anaphor , is selected .
-Rule 2 ( Semantic constraint - SEM-CONS ) : If the anaphor is a protein reference , then a protein candidate is selected .
-Rule 3 ( Discourse preference - DISC-PREF ) : According to the anaphor type , the farther candidate is selected .
-Default rule ( Default discourse preference - DEFAULT ) : The closer candidate is selected .
The rules are implemented using different features of expressions , such as syntactic types of expressions , head noun , semantic types , etc. , in a similar way to [ 22 ] .
Each rule in the decision list compares two candidates , and returns the preferable candidate in concern with the anaphor .
If equility happens , the next rule in the list is applied .
The default and also last rule in the decision rule list is special in the sense that depending on the anaphor , it prefers the closer or the farther candidate .
Because of this particular rule , the decision list never results in the equility result .
In this way , candidates can be sorted , and the best candidate is selected as the antecedent .
Figure 4 illustrates how the decision list works when comparing two candidates : and .
More details concerning the implementation of the main components of our system shown in Figure 2 are presented below .
In this step , we want to filter out those pronouns and definite noun phrases that are not a target of this task . The expressions are comprised of two types : non-anaphoric expressions , and anaphoric expressions , which do not point to proteins .
The term anaphoric is used with the common sense in the NLP community .
Anaphoric expression refers to an expression that has a noun phrase as an antecedent .
Thus , expressions with a sentence or phrase antecedents , or nominal but successive antecedents , are not our target and should be filtered out .
Non-anaphoric expressions include first and second-person pronouns such as I , we , you , and pleonastic it .
First and second-person pronouns are easily recognized by the part-of-speech tags ; thus , we use part-of-speech information for the filtering .
For pleonastic it , we make use of the following four patterns , which are similar to [ 13 ] :
It be [ Adj|Adv| verb ]* that
It be Adj [ for NP ] to VP
It [ seems|appears|means|follows ] [ that ]*
NP [ makes|finds|take ] it [ Adj ]* [ for NP ]* [ to VP|Ving ]
To recognize and filter anaphoric expressions that do not point to proteins , the system is based on the protein semantic classification results determined by the method presented below .
For each anaphoric markable , the system collects a list of antecedent candidates , and select the most probable candidate to be the antecedent of the anaphor .
Basically , all of the expressions detected in the initial expression set are an antecedent candidate , with the exception of anaphoric pronouns .
However , if the list contains too many candidates , then it may be more difficult for the later antecedent-selection algorithm .
Therefore , candidates that are not probable to be an antecedent of the anaphor should be filtered out .
There are several filters that can be used :
Window size Borders are set to include or exclude antecedent candidates .
This is a common method for antecedent candidate filtering , as seen in the previous work [ 3 , 5 , 26 ] .
Since our task focuses on anaphoric coreference , antecedent expressions normally appear not too far ( in sentence distance ) from the anaphors . Thus , using window sizes is a proper technique .
Syntactic dependency relations Since arguments of some dependency relations ( such as poss-arg12 and prep-arg12 ) do not corefer with each other , they can be used to correctly eliminate the number of antecedent candidates .
For instance , two such truncated forms definitely cannot be an antecedent of the protein in this context : two such truncated forms of the protein .
After filtering non-relevant antecedent candidates for an anaphor in the step above , depending on the anaphor type , the remaining candidates are ranked by fixed rules , or by using a pairwise comparison procedure :
The relative pronoun can be said to be the easiest type of coreference resolution , because its antecedent expression is very close to the anaphor , and in many cases , it is right before the anaphor .
For these types of anaphors , any syntactic parser can be used to find the relation between relative pronouns and their arguments .
Our system accomplishes this task .
It simply produces coreference links between the relative pronouns and their arguments .
However , a disadvantage to using this method is that when the parser makes a mistake on finding the correct arguments , the coreference also fails . This is exemplified in the following : " . . .of transcription factor NF-kappa B also encodes a p70 I kappa B protein , I kappa B gamma , which is identical to the C-terminal 607 amino acids of . . . "
This procedure compares two candidate expressions at a time with respect to preferences raised by the anaphor .
The best antecedent expression is selected to form a response coreference link .
In particular , a list of rules is used to compare two candidates of an anaphor in a deterministic manner .
For each rule , both of the candidates are checked against the condition hold by that rule .
If one candidate satisfies and the other does not , the procedure ends with the result that the former will be preferable over the latter .
If both satisfy or both do not satisfy , the procedure proceeds to the next rule in the same manner .
The rules are applied in a successive order , one after another , until the inequality occurs , or until the end-of-the-rule list is reached .
The default rule of the procedure , is in the preference of the closer antecedent candidate .
By definition , two coreferential expressions are identical , which implies a semantic-constraint on coreference relationship .
In other words , semantic types of coreferents must be compatible .
In practice , this compatibility is checked based on a given taxonomy of semantic classes in the following manner : two semantic classes are considered compatible or agreed with each other , when they have a synonym relation , or hypernym-hyponym relation .
In this work , we only focus on the Protein type , ignoring other possible semantic types , so the structure of the taxonomy is not taken into account .
Therefore , the likelihood that two expressions are semantically compatible , is definitely beneficial for antecedent prediction , besides syntactic information .
Focusing on specific entity types , i.e. , Protein type , enables us to find a proper method for determining the likelihood , and method for encoding the likelihood in coreference resolution .
Since gold protein annotations are given , we can use them in combination with syntactic information to judge whether an expression is a protein-referential expression or not .
If an expression is a noun phrase with a single head word , and it contains a protein mention that completely overlaps with the head word , then the expression is classified as Protein .
In another case , when the head noun is either protein or gene , and has a protein mention as its premodifier , such as the Tax protein .
For a coordinated noun phrase , if one of its constituents is classified as a Protein , then that noun phrase is also classified as a Protein .
Pronouns , in particular , possessive pronouns , occupy the majority of anaphoric pronouns in biological texts ( Table 5 ) .
However , they do not contain very much useful information for the resolution ; thus , we need to exploit more information from its context [ 17 ] .
The analysis of BioNLP-ST 2011 also showed that we need a different strategy to resolve such pronouns [ 18 ] .
Fortunately , the key to this problem lies in the context of pronouns .
We implemented a simple function to classify the semantic type of a possessive pronoun , based on its context word .
In particular , we check the noun phrase in which the determiner is its or their ; if the noun phrase contains a protein key word , then the inclusive pronoun is classified into the Protein semantic type .
Protein key words can be a verb , a noun or an adjective that co-occurred with protein mentions , and can be used as a clue to distinguish the protein type from other semantic types .
For example , the word binding in the following noun phrases : its heterodimeric binding partner , or its binding site , is a clue to infer that it must be a protein reference .
For our preliminary experiment , we collect these keywords manually by checking the noun phrases containing its and their in the training data .
Our final protein keyword set includes 12 words : binding , expression , interaction , regulation , phosphatase activity , localization , gene , sequence , region , phosphorylation , transactivation , and transcription .
In future , the protein key words can be collected automatically using the term corpus , or other resources of proteins .
Coreferential definite noun phrases in text are used to include a broader definition of coreference .
In other words , their antecedents do not necessarily exist in the textual context ; in particular , in biomedical scientific papers , many definite noun phrases do not have antecedents , since the referenced concepts can include any concept that is understood by subject matter experts in the domain .
Distinguishing such non-anaphoric definite noun phrases from anaphoric ones is a difficult task .
Knowing their semantic type helps to filter out irrelevant candidate antecedents , thereby increasing the chance of picking up the right antecedent , and increasing the precision of antecedent prediction .
In our implementation , the decision to keep an anaphoric expression for further processing steps for an anaphoric definite noun phrase is based on a protein head word list .
We tested two different head word lists : one is built automatically from the gold anaphoric nominals in gold data ; the other word list contains the top seven common head words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .
Semantic type information can be used in coreference resolution in several ways .
First , in anaphor selection , semantic information can be used to filter out non-protein anaphoric expressions .
Second , in antecedent candidate filtering , semantic agreement between the antecedent candidates and the anaphoric expression is checked .
Those candidates that are not in agreement with the anaphor in semantics are filtered out .
For example , if an anaphor is classified as a protein referent , then the non-protein antecedent candidates are removed from the candidate set of the anaphor .
Finally , in antecedent prediction : semantic agreement can again be used as a constraint when comparing two antecedent candidates to select the more probable candidate .
Our minimal system configuration includes all of the processing and filters from step 0 to step 3 , as explained in the section above ( RB-MIN ) .
For antecedent candidate selection , the window size used in step 4 is set to 2 , which means that antecedent candidates are collected in the two nearest sentences from the anaphor , and the sentence embedding the anaphor .
The statistics measured on the training set of the corpus shows that 97 .0% percent of protein coreference links have antecedents appearing in within 2 sentences .
With this window size , the average number of candidates per anaphor is 6 .1 .
Also , experiments with wider window sizes did not help .
The word list used to filter out anaphoric definite noun phrases in step 2 contains the following words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .
These words are selected from the top appearing head words extracted from the training data .
Premodifiers of definite noun phrases are also limited to numbers and popular premodifiers of proteins , such as nuclear , and transcription .
Using this head word list and premodifiers , the system covers 83 .5 percent of the coreference links .
To keep the minimal configuration simple , step 4 - antecedent selection of the baseline only uses the default comparison rule , which assures that the closest antecedent candidate is selected .
Table 2 compares our system with the top four official results of the COREF shared task in BioNLP-ST 2011 [ 18 ] : UU [ 11 ] , UZ [ 29 ] , CU , and UT [ 4 ] .
The scoring scheme used throughout this paper is the protein coreference evaluation , the primary evaluation method of the COREF shared task [ 18 ] .
This primary evaluation method , which was particularly designed for the shared task , is based on protein coreference links that have been automatically generated from manually annotated coreference links .
The last column ALL shows the overall results , while its preceding three columns PRON , DNP , and RELAT shows the protein resolution results by three major subtypes of anaphors : pronouns , definite noun phrase and relative pronouns , respectively .
Note that the results from RB-MIN with minimal configuration , already surpasses the best results obtained by the UU team , with up to 7 .1% higher performance in F-score .
Since RB-MIN uses similar preprocessing tools as UU [ 11 ] , but less information in antecedent prediction , this gap in performance is likely caused by the different markable detection methods .
UU pointed in their paper that markable detection is one of the challenges of this task [ 11 ] .
In their system , UU used a machine learning approach , and tested two distinguished models for markable detection : one solved both anaphors and antecedents together , the other treated anaphors and antecedents separately .
Meanwhile , our method is basically based on the boundary of noun phrases and pronouns , as is outputted from the parser .
The patterns used to extract the proper noun phrases and pronouns , are manually designed concerning the markable boundaries annotated in the training data .
Breaking down the system performance by the different types of anaphors provides us with insight into what has been accomplished / solved by our methods , and also provides us with improvement opportunities .
Concerning the RELAT type of coreference , we can see that RB-MIN and RB-FULL both achieve comparable results with the best team in BioNLP-ST 2011 .
However , it should be noted that our antecedent prediction for the RELAT type is based solely on the output of the Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See Discussions section ) .
The increase in system performance on the PRON and DNP types by RB-FULL demonstrate the effectiveness of discourse and semantic information in the performance of protein coreference resolution .
Comparing RB-MIN , RB-FULL and RB-MIN+1 , 3 , we found that rule 3 , which stands for discourse preference , works well for the PRON type ( 2 ) .
On the other hand , the major contribution to the improvement of DNP resolution is from rule 2 .
This rule successfully utilizes the domain-specific information , which shows that coreference resolution requires domain-specific information .
To further explore the elements contributed to this significant improvement , we analyzed our system in more detail .
The analyses of the results are provided in the section entitled Discussions .
" >
Table 3 compares various configurations of the rule-based system .
The first , RB-MIN , is the minimal system .
The following three show the contribution of the three rules , NUM-AGREE , SEM-CONS , and DISC-PREF .
RB-FULL is the full system .
To emphasize the contribution of the semantic rules , it also shows RB-FULL-sem system .
The combination of rule 1 , 2 and 3 resulted in a 62 .4% F-score ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contributes to a 4-point F-score increase in the development set , and 2 .3-point F-score increase on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .
However , the result of RB-MIN is still more than 7 points higher than in state-of-the-art performance .
This gain is due to the fact that the rule ensures that the semantic type of antecedents is the same as for their anaphors , thus enabling the correct detection of antecedents .
In other words , if an anaphor is classified as a protein reference , then the antecedent must also be a protein reference .
The following examples illustrate the way rule 2 works .
( Coreference examples in this paper are represented in the following manner : gold anaphoric and antecedent expressions are bracketed , antecedents before anaphors ; gold protein mentions are underlined ; and incorrect response antecedents are in italics . )
- " Therefore , [ IRF-1 ] may be an important contributor to IL-12 signaling , and we speculate that the defective IL-12 responses seen in IRF-1- / - mice might be attributable , in part , to the absence of [ this transcription factor ] . " ( PMID-10358173 )
In this example , without rule 2 , the faulty response antecedent of this transcription factor is part because it is the closet antecedent candidate agreeing with the anaphor on the singular number .
Meanwhile , since this transcription factor is recognized as a protein reference , its closest protein antecedent IRF-1 was successfully detected by RB-FULL .
Another example is :
- " This role for [ c-Myc ] in apoptosis is now confirmed in studies using a dominant negative form of [ its ] heterodimeric binding partner , Max , which . . . " ( PMID-7964516 )
Concerning the anaphoric pronoun its in this example , there are several antecedent candidates : this role , c-Myc , apoptosis , studies , a dominant negative form of its heterodimeric binding partner .
Although studies and a dominant negative form of its heterodimeric binding partner have been crossed out because of disagreement in numbers , and violation of abandoned syntactic constraints , correspondingly , the system would return the incorrect antecedent apoptosis instead of c-Myc .
Fortunately , the containing noun phrase of the anaphor its has the modifier word binding , which is a clue for classifying its as a protein reference ( See Semantic type classification for pronominal anaphors ) .
Rule 2 utilizes semantic classification result to make the correct selection .
In our system , domain-specific semantic information is utilized in two places : anaphor selection and antecedent prediction .
The effect of semantic information in antecedent prediction has been analyzed in the sections above .
In this subsection , we are going to explore the contribution of semantic information in the anaphor selection step .
To classify anaphors into protein or non-protein reference , our system employs a head-word based classifier for definite noun phrases , DEFNP-ANA-SEM , and a context-based classifier for pronouns , PRO-ANA-SEM ( Section Methods ) .
Without limiting the number of anaphors by using semantic information-based filtering , the precision significantly drops , causing a big decrease in the F-score ( Table 4 , RB-FULL without DEFNP-ANA-SEM ) .
This decrease is due to the fact that the semantic filter is the only way to filter out definite noun phrase anaphors .
Without the filter , all definite expressions , which include a huge amount of non-anaphoric expressions , are considered as anaphors .
Besides the anaphoric use , definite noun phrases are also used to refer to entities or concepts in the common domain knowledge shared between readers and writers .
Statistics in [ 21 ] show that only around 30% of definite noun phrases are anaphoric , and the other uses according to their classification include associative , unfamiliar / larger situation , idiom and doubt .
Distinguishing such non-anaphoric definite noun phrases from anaphoric ones is extremely difficult .
In our system , contextual information of possessive pronouns is utilized through the protein key words ( Section Methods ) , and this contributed to a 1 .8% gain in F-score ( Table 4 , RB-FULL without PRO-ANA-SEM ) .
This gain is a good indication for seeking a systematic method to develop and include such contextual information in coreference resolution .
Examples showing the effectiveness of semantic information from the context of pronouns is provided below :
- " This role for [ c-Myc ] in apoptosis is now confirmed in studies using a dominant negative form of [ its ] heterodimeric binding partner , Max , which . . . " ( MID-7964516 )
- " This ability of [ CIITA ] to facilitate promoter occupation is undissociable from [ its ] transactivation potential . " ( PMID-10221658 )
- " In transient transfectin experiments , [ BCL6 ] can repress transcription from promoters linked to [ its ] DNA target sequence and this activity is . . . " ( PMID-8692924 )
- " [ Human immunodeficiency virus type 1 ( HIV-1 ) Tat ] , an early regulatory protein that is critical for viral gene expression and replication , transactivates the HIV-1 long terminal repeat ( LTR ) via [ its ] binding to the transactivation response element ( TAR ) and , . . . " ( PMID-9261367 )
In all the examples above , the appearance of words such as binding , transactivation , DNA target sequence in the noun phrases for which the anaphor plays a role as a determiner , is a contextual indicator for the protein type .
Since the anaphors are predicted as protein reference from their context , the system correctly detects their protein antecedents .
Other challenges specific to the protein coreference task Number agreement is a constraint in English writing .
However , in the data , we found several coreferential expressions that violate this constraint .
The anaphor and antecedent in the following is an instance of this violation :
- " . . .for OTF-2 in DRA gene transcription .
In contrast , [ OTF-1-enriched protein fractions ] did not affect DRA gene transcription although [ it ] functionally enhanced the transcription of another . . . " ( PMID-1560002 )
Coreference annotation and evaluation Current protein coreference evaluation schemes generate protein links ( links between anaphors and antecedent proteins ) from surface links ( links between anaphors and antecedent expressions ) , without concerning the relative position of antecedent proteins in the antecedent expression .
Therefore , when the proteins appear in premodifiers or postmodifers of noun phrases as [ cDNAs encoding EBF or a covalent homodimer of E47 ] in this example :
- " With the aim of identifying genetic targets for these transcription factors , we stably transfected [ cDNAs encoding EBF or a covalent homodimer of E47 ] , individually or together , into immature hematopoietic Ba / F3 cells , which lack [ both factors ] . " ( PMID-9252117 )
Such proteins might not be the correct antecedent proteins .
In future , revision of corpus annotation and evaluation schemes would benefit the ease of automation of coreference resolution .
Parse error Coreference expression boundary is determined mostly based on noun phrase boundary output from the parser .
Therefore , parse error on noun phrase boundary strongly affects the performance of coreference resolution .
Examining the data , we found that many antecedent expressions of plural anaphors are coordinated noun phrases , which are unfortunately difficult cases to many parsers including Enju .
Incorporation of recent works for coordination resolution like [ 20 ] should be useful for improving the performance .
The following example shows a coordination-structured antecedent AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 that failed to be detected by the parser .
The spurious response expression is transcription factors from several families .
- " granulocytic and monocytic lineages , transcription factors from several families are active , including [ AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 ] .
Few of [ these factors ] are expressed exclusively in myeloid cells ; . . . " ( PMID-9291089 )
Our current work has reconfirmed that domain knowledge is indispensable for coreference resolution .
Since the biological domain has richer knowledge resources than any other domain , it would be interesting to continue studying how to exploit and employ domain-specific semantic information in coreference resolution for this domain .
Another conclusion concerns markable detection .
This sub-problem is often regarded as an easy task in coreference resolution systems ; however , in actuality , it is an important subtask , which strongly affects the performance of coreference system .
Sticking to the gold data in designing the markable detection method , as done in this paper , is one employed strategy .
However , from the perspective of coreference data creation , revision of the markable annotations would aid in automatic and robust markable detection .
For future opportunities , more effort should be spent on automating the semantic classification for coreference expressions , using context .
Furthermore , it would be interesting to test the results in this study in a machine-learning framework .
Syntactically annotated corpora have become important resources for natural language processing , due in part to the success of corpus-based methods .
Since words are often considered as primitive units of language structures , the annotation of word segmentation forms the basis of these corpora .
This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language .
Although word segmentation is straight-forward for space-delimited languages like English , this is not the case for languages like Vietnamese for which a standard criterion for word segmentation does not exist .
This work explores the challenges of Vietnamese word segmentation through the detection and correction of inconsistency for VTB .
Then , by combining and splitting the inconsistent annotations that were detected , we are able to observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation .
The analysis and experimental results showed that our methods improved the quality of VTB , which positively affected the performance of its applications .
Treebanks , which are corpora annotated with syntactic structures , have become more and more important for language processing .
In order to strengthen the automatic processing of the Vietnamese language , the Vietnamese Treebank ( VTB ) has been built as a part of the national project , `` Vietnamese language and speech processing ( VLSP ) '' ( Nguyen et al ., 2009b ) .
However , in our preliminary experiment with VTB , when we trained the Berkeley parser ( Petrov et al ., 2006 ) and evaluated it by using the corpus , the parser achieved only 65 .8% in F-score .
This score is far lower than the state-of-the-art performance reported for the Berkeley Parser on the English Penn Treebank , which reported 90 .3% in F-score ( Petrov et al ., 2006 ) .
There are two possible reasons to explain this outcome .
One reason for this outcome is the quality of VTB , including the quality of the annotation scheme , the annotation guidelines , and the annotation process .
The second reason is the difficulty of parsing Vietnamese ; we need to seek new solutions to address this problem .
VTB is annotated with three layers : word segmentation , POS tagging , and bracketing .
This paper focuses on the word segmentation , since the most basic unit of a treebank are words ( Di Sciullo and Edwin , 1987 ) , and defining `` words '' is the first step ( Xia , 2000b ,a ; Sornlertlamvanich et al ., 1997 , 1999 ) .
For languages like English , defining `` words '' is almost trivial , because the blank spaces denote word delimiters .
However , for an isolating language like Vietnamese , for which blank spaces play a role of syllable delimiters , `` What are words ? '' is not a trivial question .
For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words , `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) '' . Word segmentation is expected to break down the sentence at the boundaries of these words , instead of splitting `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .
Note that the terminology `` word segmentation '' also refers to the task of extracting words statistically without concerning a gold-standard for segmentation , as in ( Ha , 2003 ; Le et al ., 2010 ) .
In such a context , the extracted words are more appropriate for building a dictionary , rather than for corpus-based language processing , which are outside of the scope of this paper .
Because of the discussed characteristics of the language , there are challenges in establishing a gold standard for Vietnamese word segmentation .
The difficulties in Vietnamese word segmentation have been recognized by many researchers ( Ha , 2003 ; Nguyen et al ., 2004 , 2006 ; Le et al ., 2010 ) .
Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus as to the methodology for segmenting a sentence into words .
The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation .
While similar problems also occur with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more difficult , because the modern Vietnamese writing system is based on Latin characters , which represent the pronunciation , but not the meaning of words .
All of these characteristics make it diffcult to perform word segmentation for Vietnamese , both manually and automatically , and have thus resulted in different criteria for word segmentation .
However , so far , there have been few studies on the challenges in word segmentation , and the comparison of different word segmentation criteria .
In this paper , a brief introduction of the Vietnamese Treebank VTB and its annotation scheme are provided in Section 2 .
Then , we described our methods for the detection and correction of the problematic annotations in the VTB corpus ( Section 4 .2 ) .
We classified the problematic annotations into several patterns of inconsistency , part of which were manually fixed to improve the quality of the corpus .
The rest , which can be considered as the most difficult and controversial instances of word segmentation , were used to create different versions of the VTB corpus , representing different word segmentation criteria .
Finally , we evaluated these criteria in automatic word segmentation , and its application in text classification and English-Vietnamese statistical machine translation , in Section 4 .
This study is not only beneficial for the development of computational processing technologies for Vietnamese , a language spoken by over 90 million people , but also for similar languages such as Thai , Laos , and so on .
This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language , like English , to a language that has not yet been intensively studied .
Word segmentation in VTB aims at establishing a standard for word segmentation in a context of multi-level language processing .
VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al ., a ) , which can be divided up into three groups : single , compound , and special `` words '' .
Single words contain only one token .
The terminology tokens refers to text spans that are separated from each other by blank spaces .
Compound words have two or more tokens , and are divided into four types : compound words composed by semantic coordination ( semantic-coordinated compound ) , compound words composed by semantic subordination ( semantic-subordinated compound ) , compound words with an affx , and reduplicated words .
Special `` words '' include idioms , locutions , proper names , date times , numbers , symbols , sentence marks , foreign words , or abbreviations .
The segmentation of these types of words forms a basis for the POS tagging , with 18 different POS tags , as shown in Table 2 ( Nguyen et al ., c ) .
Each unit in Table 1 goes with several example words ; English translations are provided in parentheses .
Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed .
The subscript of a token translation is the index of that token in the compound word .
However , for some tokens , we could not find any appropriate English translation , so we gave it an empty translation , marked with an asterisk .
Note that a Vietnamese word or a token in context can have other meanings , in addition to the given translations .
A classifier noun , denoted by the part-of-speech Nc in Table 2 , is a special type of word in Vietnamese .
Classifier nouns are specific to several Southeast Asian languages , like Vietnamese and Thai .
One of the functions of classifier nouns is to express the definiteness .
For example , the common noun `` bàn '' generally means tables , while `` cái bàn '' means a specific table , similar to the table in English .
In this section , we analyzed the VTB corpus to determine whether the difficulties in Vietnamese word segmentation affected the quality of VTB annotations .
The analysis revealed several types of inconsistent annotations , which are also
Vietnamese word segmentation .
Our analysis is based on two types of inconsistencies : variation and structural inconsistency , which are defined below .
Variation inconsistency : is a sequence of tokens , which has more than one way of segmentation in the corpus .
For example , `` con gái/girl '' can remain as one word , or be segmented into two words , `` con '' and `` gái '' .
A variation can be an annotation inconsistency , or an ambiguity in Vietnamese .
While ambiguity cases reflect the difficulty of the language , annotation inconsistencies are usually caused by the confusion in the decision of annotators , which should be eliminated in annotation .
We use the term variation instance to refer to a single occurrence of a variation .
Structural inconsistency : happens when different sequences have similar structures , and thus should be split in the same way , but are segmented in different ways in the corpus .
For example , `` con gái/girl '' and `` con trai/boy '' have similar structures : a combination of a classifier noun and a common noun , Nc + N , so when `` con gái/girl '' is split , and `` con trai/boy '' is not , it is considered as a structural inconsistency of Nc .
It is likely that structural inconsistency at the word segmentation level complicates the higher levels of processing , including POS tagging and bracketing .
The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in the VTB , following the definition for variation inconsistency , above .
In detail , we counted N-gram sequences of different lengths in VTB that have two or more ways of word segmentation , satisfying one of the following two conditions :
N tokens are all in the same phrase , and all have the same depth in phrase .
For example , the 3-gram " nhà tình nghĩa ( house of gratitude ) " in this structure " ( NP ( Nc-H căn ) ( N nhà ) ( A tình nghĩa ) ) , " OR
nhà tình nghĩa " in this structure " ( NP ( Nc-H căn ) ( N nhà ) ( ADJP ( A tình nghĩa ) ) ) , " where the ADJP contains only one word .
Table 3 shows the overall statistics of the variation inconsistency detected by the method described above .
Most of the diffcult cases of word segmentation occur in two-token variations , occupying the majority of variations ( 92 .9% ) .
This ratio of 2-gram variations is much higher than the average ratio of two-token words in Vietnamese , as reported in ( Nguyen et al., 2009a ) , which is 80% .
Variations that have lengths of three and four tokens occupy 6 .1% and 1 .0% , respectively .
We estimated the precision of our method by randomly selecting 130 2-gram variation instances , extracted from the method described above , and manually checked whether the inconsistencies are true .
We found that 129 cases occupying 99 .2% of all extracted 2-grams are true inconsistencies .
Only one instance of inconsistency was an ambiguous sequence giá c , which is one word when it means price , and two words giá / price c / all in đàu có giá c / all have ( their own ) price .
The precision for our method is high , so we can use the extracted variations to provide insights on the word segmentation problem .
We further analyzed the 2-gram variations to understand what types of 2-grams were most confusing for annotators .
The analysis revealed that compound nouns , compound verbs , and compound adjectives are the most difficult cases of word segmentation .
We classified the 2-gram variations according to their POS sequences in case the tokens in the 2-gram are split .
There are a total of 54 patterns of POS sequences . The top 10 confusing patterns , their counts of 2-gram variations , and examples are depicted in Table 4 .
Table 5 and Table 6 show the POS patterns that are a specific POS tag , appearing at the beginning or ending of the sequence .
Investigating the inconsistent 2-grams extracted , we found that most of them are compound words , according to the VTB guidelines ( Section 2 ) .
One of the reasons why the compound words are sometimes split , is because the tokens in those compound words have their own meanings , which seem to contribute to the overall meaning of the compounds .
This can be seen through the examples provided in Table 4 , where the meanings of tokens are given with a subscript .
This scenario has proven to be problematic for the annotators of VTB .
Furthermore , by observing the POS patterns in Table 5 and Table 6 , we can see the potential for structural inconsistency , particularly for closed-set POS tags .
Among them , classifier nouns ( Nc ) and affixes ( S ) are two typical cases of structural inconsistency , which will be used in several settings for our experiments .
The same affx or classifier noun can modify different nouns , so when they are sometimes split and combined in the variations , we can conclude that classifier nouns and affixes involve in-structural inconsistencies .
In the following section , we present our detection method for structural inconsistency for classifier nouns and affixes .
The detection method for structural inconsistency of classifier nouns and affixes is simple .
We collected all affixes and classifier nouns in the VTB corpus , and then extracted 2-grams containing these affixes or classifier nouns , which are also structural inconsistencies .
For example , since " con " is tagged as a classifier noun in VTB , we extracted all 2-grams of " con " including both " con gái / girl " and " con trai / boy " .
Even though the sequence , " con trai " is always split into two words throughout the corpus , it can still be an inconsistency , if we consider similar structures such as " con gái " .
In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent , if we consider the higher analysis levels , POS tagging .
According to the VTB POS-tagging annotation guidelines ( Nguyen et al., c ) , classifier nouns should be separated from the words that they modify .
However , in practice , it is confusing when the classifier noun can be standalone as a single word .
For example a classifier noun , e.g. , " con " in " con trai ( boy ) " , or " con gái ( girl ) " , can also be a simple word , which means " I ( first person pronoun used by a child when talking to his / her parents ) " , or part of a complex noun " con cái ( children ) " .
Therefore , in our experiments , we want to evaluate the " splitting " and " combining " of these cases , in order to see whether the solution is successful for applications of the corpus .
By examining the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like a percentage ( % ) in " 30% " , is split or combined with " 30 " .
Such inconsistent annotations are manually fixed based on their textual context .
By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations .
For example , the character , % , in " 30% " is split , but is combined with a number in " 50 % " , which is considered to be a structural inconsistency .
Note that it can be argued that splitting " N% " into two words or combined in one word is dependent on the blank space in-between N and " % " .
Higher-levels of annotation such as POS tagging is significant , because we may need one or two different POS tags for the different methods of annotation .
Therefore , we think that it is better to carefully preprocess text and segment these special characters in a consistent way .
To improve the quality of the VTB corpus , we extracted the problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency .
Automatically modification is diffcult , since we must check the semantics of the special characters in their contexts .
For example , hyphens in date expressions like " 5-4-1975 " , which refers to the date , " the fifth of April , 1975 , " are combined with the numbers .
However , when the hyphen indicates " ( from ) to " or " around .
. .
or " , as in " 2-3 gi░ sáng " , meaning " around 2 or 3 o’clock in the morning " , we decided to separate it from the surrounding numbers .
As a result , we have fixed 685 inconsistent annotations of 21 special characters in VTB .
The variation inconsistency and structural inconsistency found in Section 3 can also be seen as representatives of different word segmentation criteria for Vietnamese .
We organized the inconsistency detected in seven configurations of the original VTB corpus .
Then , by using these data sets , we could observe the influence of the different word segmentation criteria on three tasks : automatic word segmentation , text classification , and English-Vietnamese statistical machine translation .
Seven data sets corresponding to different segmentation criteria are organized as follows .
ORG : The original VTB corpus .
BASE : The original VTB corpus + Manual modification of special characters done in Section 3 .3 .
VAR_SPLIT : BASE + split all variations detected in Section 3 .1 .
VAR_COMB : BASE + combine all variations detected in Section 3 .1 .
VAR_FREQ : BASE + select the segmentation with higher frequency among all variations detected in Section 3 .1 .
STRUCT_NC : BASE + combine all classifier nouns detected in Section 3 .2 with the words they modify .
STRUCT_AFFIX : BASE + combine all suffxes detected in Section 3 .2 with the words they modify .
These data sets are used in our experiments , as illustrated in Figure 1 .
The names of the data sets are also used to label our experimental configurations .
In this section , we briefly describe the task settings and the methods used for word segmentation ( WS ) , text classification ( TC ) , and English-Vietnamese statistical machine translation ( SMT ) .
We used YamCha ( Kudo and Matsumoto , 2003 ) , a multi-purpose chunking tool , to train our word segmentation models .
The core of YamCha is the Support Vector Machine ( SVM ) machine learning method , which has been proven to be effective for NLP tasks .
For the Vietnamese word segmentation problem , each token is labeled with standard B , I , or O labels , corresponding to the beginning , inside , and outside positions , respectively .
The label of each token is determined based on the lexical features of two preceding words , and the two following words of that token .
Since the Vietnamese language is not inflectional , we cannot utilize inflection features for word segmentation .
Each of the seven data sets is split into two subsets for training and testing our WS models .
The training set contains 8443 sentences , and the test set contains 2000 sentences .
Text classification is defined as a task of determining the most suitable topic from the predefined topics , for an input document .
We implemented a text classification system similar to the system presented in ( Nguyen et al., 2012 ) .
The difference is that we performed the task at the document level , instead of at the sentence level .
The processing of the system is summarized as follows .
An input document is preprocessed with word segmentation and stop-word removals .
Then , the document is represented in the form of a vector of weighted words appearing in the document .
The weight is calculated using standard tf-idf product .
An SVM-based classifier predicts the most probable topic for the vector , which also is the topic for the input document .
In our experiment , for comparison of different word segmentation criteria in topic classification , we only vary the word segmentation model used for this task , while fixing other configurations .
News articles of five topics : music , stock , entertainment , education , and fashion are used .
The sizes of the training and test data sets are summarized in Table 8 .
A	phrase-based SMT system for English-Vietnamese translation was implemented .
In this system , we used SRILM ( Stolcke , 2002 ) to build the language model , GIZA++ ( Och and Ney , 2003 ) to train the word-aligned model , and Moses ( Holmqvist et al., 2007 ) to train the phrase-based statistical translation model .
Translation results are evaluated using the BLUE score ( Papineni et al., 2002 ) .
Both training and test data are word-segmented using the word segmentation models achieved .
For the experiment , we used the VCL_EVC bilingual corpus , 18000 pairs of sentences for training , and 1000 pairs for testing .
Evaluation of word segmentation models trained on different versions of the VTB are given in Table 9 .
The experimental results with text classification and English-Vietnamese statistical machine translation are shown in Table 10 and Table 11 , respectively .
There are two important conclusions that can be drawn from these tables : ( 1 ) The quality of the treebank strongly affects the applications , since our BASE model and most of the other enhanced models improved the performance of TC and SMT systems ; ( 2 ) " Splitting " seems to be a good solution for word segmentation for controversial cases , including the split of variations , affxes , and classifier nouns .
According to the result in Table 9 , the VAR_SPLIT criterion gives the highest WS performance .
With the exception of STRUCT_NC , all of the modifications to the original VTB corpus increase the performance of WS .
However , the word segmentation criterion with higher performance is not necessarily a better criterion , but a criterion should also be judged through applications of word segmentation .
In both SMT and TC experiments , the BASE model , which is based on the manually-modified inconsistency of special characters , achieved better results than the ORG model .
In particular , in the TC experiment , the BASE model achieved 0 .66 point higher than ORG , which is a significant improvement .
The results support the conclusion that the quality of the word-segmentation corpus is very important for building NLP applications .
The SMT results show that three out of six augmented models , VAR_SPLIT , VAR_FREQ and BASE , performed better than the ORG configuration .
Among them , the best-performing model , VAR_SPLIT achieved 36 .91 BLEU score , which is 0 .55 higher than ORG .
In TC results , all six augmented models achieved higher results than ORG .
In general , the augmented models performed better than the ORG .
Additionally , because our automatic methods for inconsistency detection could not cover all of the types of inconsistencies in word segmentation annotation , further improvement of corpus quality is demanded .
Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining affxes with their head nouns resulted in slightly better results for WS and TC , and did not change the performance of SMT .
However , the combination of classifier nouns with their head nouns had negative effects on WS and SMT .
Another part of the scope of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining .
Splitting and combining variations are reflected by VAR_COMB and VAR_SPLIT , while STRUCT_AFFIX and STRUCT_NC represent the combination of affixes or classifier nouns with the words that they modify .
STRUCT_AFFIX and STRUCT_NC are contrasted with BASE where affxes and classifier nouns remain untouched .
Comparing VAR_COMB and VAR_SPLIT in both the TC experiment and SMT experiment , we see that the VAR_SPLIT results are better in both cases .
Since the ratio of combined variations in the ORG corpus is 60 .9% , it can be observed that splitting seems to be better than combining for WS , TC and SMT .
In this paper , we have provided a quantitative analysis of the difficulties in word segmentation , through the detection of problematic cases in the Vietnamese Treebank .
Based on the analysis , we automatically created data that represent the different word segmentation criteria , and evaluated the criteria indirectly through their applications .
Our experimental results showed that manual modification , done for annotation of special characters , and most other word segmentation criteria , significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , in comparison with the use of the original VTB corpus .
Since the VTB corpus is the first effort in building a treebank for Vietnamese , and is the only corpus that is publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building efficient NLP systems .
Face retrieval in large-scale news video datasets
Face retrieval in news video has been identified as a challenging task due to huge variations in the visual appearance of the human face .
Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .
In this paper , we introduce approaches to face retrieval that are scalable to such datasets while maintaining competitive performances with state-of-the-art approaches .
To utilize the variability of face appearances in video , we use a set of face images called face track to represent the appearance of a character in a video shot .
Our first proposal is an approach to extracting face tracks .
We use a point tracker to explore the connections between detected faces belonging to the same character and , then group them into one face track .
We present techniques to make the approach robust to common problems caused by sudden illumination changes , partial occlusions , and scattered appearances of characters in news videos .
In the second proposal , we introduce an efficient approach to matching face tracks for retrieval .
Instead of using all the faces in the face tracks to compute their similarity , our approach selects representative faces for each face track .
The representative faces are sampled from the original face track .
As a result , we significantly reduce the computational cost of face-track matching while taking into account the variability of faces in face tracks to achieve high matching accuracy .
Experiments are conducted on two face-track datasets extracted from real-world news videos , of such .
scales that have never been considered in the literature .
One dataset contains 1,497 face tracks of 41 characters extracted from 370 hours of TRECVID videos .
The other dataset provides 5,567 face tracks of 111 characters observed from a television news program ( NHK News 7 ) over 11 years .
We make both datasets public for the research community .
The experimental results show that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
News videos play an important role as a source of information nowadays because of their rich and relevant contents .
With the advances in modern technology , a huge amount of news videos can be obtained easily .
Accordingly , this creates an urgent demand to retrieve useful information from such news video datasets .
Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
A robust face retrieval system for large-scale news video datasets is indeed of much benefit in a wide range of applications .
For example , by applying face retrieval to a news video dataset , we are returned a list of relevant shots or scenes containing the appearance of a selected well-known character .
With such a list , important events related to the character can be found or summarized.
However , developing an accurate face retrieval system is not a trivial task because of the fact that the imaged appearance of a face changes dramatically under large variations in poses , facial expressions , and complex capturing conditions .
Besides accuracy , efficiency is also an issue in such a face retrieval system because the scales of available datasets are rapidly getting larger , for instance , exceeding thousands of hours of videos with millions of faces of hundreds of characters .
Thus , accurate and efficient approaches to face retrieval are always required.
Generally , a face retrieval system consists of two principal steps .
The first step is extracting the appearance of faces in videos .
, The second step is matching the extracted appearances with a given query so as to return a rank list .
Whereas conventional approaches consider single face images as the basic units in extracting and matching \CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .
A face track contains multiple face images belonging to the same individual character within a video shot .
The face images in a face track may present the corresponding character from different viewpoints and with different facial expressions ( as shown in Figure 1 ) .
By exploiting the plenteous information from the multiple exemplar faces in the face tracks , face track-based approaches are expected to achieve a more robust and stable performance.
Once all the face tracks in the video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .
Because each face track is a set of face images , matching face tracks can essentially be thought of as a problem of matching image sets .
Several approaches have been introduced to deal with this problem \CITE .
They differ in the ways in which the sets are modeled and the similarity between sets is computed .
Using these approaches , the image set has been modeled in different ways , including as distributions \CITE , subspaces \CITE , a convex geometric region in a feature space \CITE , or more general manifolds \CITE .
Although these approaches have shown promising results in benchmark datasets , they require high computational costs to characterize the representation of face tracks , such as computing the convex geometric region in \CITE , the probability in \CITE , and the eigenvectors in \CITE .
Their complexity in modeling face tracks and estimating the similarity between face tracks limits their practicability in large-scale datasets.
This paper provides a threefold contribution toward solving the above problems , .
Robust face-track extraction from news video .
To enhance the performance of face-track matching , face tracks should first be extracted accurately .
For this purpose , we introduce an approach .
motivated by a study of Everingham et al .
The basic idea is to use a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
Our approach incorporates techniques to overcome specific problems with news video caused by sudden illumination change and partial occlusion , in contrast to the approach in \CITE , which failed to deal with , these problems .
Evaluations of a collection of real-world news videos showed that our proposed face-track extraction approach achieved approximately 95% accuracy , a significant improvement compared to the approach in \CITE .
Efficient face-track matching .
We introduce an approach that significantly reduces the computational cost for face-track matching while maintaining a competitive performance with state-of-the-art approaches .
Based on the observation that face tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all the faces in a face track for learning the variation of faces .
Thus , a set of faces is sampled from the original face track for matching .
The size of the set is much smaller than that of the original face track .
The , mean face of the sampled faces in the set is then computed .
The similarity between two face tracks is the distance between their mean faces.
Large-scale face-track datasets from real-world news videos .
We investigated the problem of face retrieval in news video datasets whose scales have never been considered in the literature .
Our first dataset is from 370 hours of TRECVID news videos and contains 405,887 detected faces belonging to 41 individuals .
The second dataset includes 1.2 million faces of 111 individuals observed in the NHK News 7 program over 11 years .
, .
The total number of available face tracks is 5,567 .
The number of occurrences of each individual character varies from 4 to 550 .
Both datasets are published for the research community.
The remainder of this paper is organized as follows .
In Section 2 , we introduce related works in detail .
Sections 3 and 4 describe our approaches to face-track extraction and matching , respectively .
Section 5 presents our experimental settings , and Section 6 provides our .
conclusions.
Face-track extraction .
Face-track extraction is a key step in a video-based face retrieval system .
The existing studies on automatic face-track extraction follow a standard paradigm that consists of two basic steps , detecting faces in frames and grouping faces of the same character into face tracks .
In the first step , the Viola-Jones detector is usually used to detect near frontal faces in frames of videos .
, In the second step , the detected faces of the same character are grouped by using either clustering \CITE or tracking approaches \CITE .
In \CITE , Ramanan et al .
built a color histogram for the hair , face , and torso associated with each detected face in a frame .
A concatenated vector of the normalized color histogram represented the face .
They then clustered all vectors to obtain groups of similar faces , using agglomerative clustering .
The limitations of this approach include its high computational cost for constructing and clustering high-dimensional representation feature vectors and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure that no group contains faces of multiple characters and that groups are not over-fragmented.
On the other hand , Everingham et al .
\CITE and Sivic et al .
In \CITE , an affine covariance tracker of \CITE is used .
This tracker can develop tracks on deforming objects , where the between-frame region deformation can be modeled by an affine geometric transformation plus perturbations .
The outcome is that a face can be tracked ( by the collection of regions on it ) through significant pose variations and expression changes , allowing the association of possibly distant face detections .
The disadvantage of this tracker is its high computational cost for locating and tracking affine covariance regions .
Another way of using a tracker was introduced by Everingham et al .
in \CITE , in which .
they used a Kanade-Lucas-Tomasi ( KLT ) tracker to create a set of point tracks starting at some frame in a shot and continuing until some later frame .
Grouping faces in different frames for one character is based on enumerating the track points shared between faces .
Although using tracking is an efficient solution , it may return poor tracking results because trackers are very sensitive to illumination changes and partial occlusions .
Face-track matching .
There are two major categories of approaches to using multiple exemplars of faces in face tracks ( i.e. , sets of face images ) for robust face matching and recognition .
The approaches in the first category \CITE make use of both face images and the temporal order of their appearances .
The face dynamics within the video sequence are modeled and exploited to improve recognition accuracy .
For instance , Li et al .
Edwards et al .
They then used the trained statistical face model to incorporate identity evidence over a sequence .
In \CITE , Liu and Chen used an adaptive hidden Markov model ( HMM ) for this face recognition problem .
In the training face , they created a HMM for each character to learn the statistics and temporal dynamics using the eigen-face image sequence .
The implicit constraint of these approaches is that the dynamics of faces should be temporally consecutive .
In general , this constraint is not always satisfied.
Without relying on temporal coherence between consecutive images , the approaches in the second category use multiple face images only and .
treat the problem as a set-matching problem .
These approaches are differentiated based on the ways in which the sets are modeled and the similarity between sets is computed .
Shakhnarovich et al .
However , to make the computation tractable , they made the assumption that faces are normally distributed , which may not be true \CITE .
Cevikalp and Triggs \CITE claimed that a face sequence is a set of points and discovered a convex geometric region expanded by these points .
The min-min approach \CITE considered a face sequence as a cluster of points and measured the distance between these clusters .
Subspace methods \CITE viewed a face sequence as points spread over a subspace .
Although these methods can be highly accurate , a lot of computation is needed to represent the distribution of the face sequence , such as computing the convex hulls in \CITE , the probability models in \CITE , and the eigenvectors in \CITE .
For this reason , they are not scalable to large-scale video datasets .
Face datasets .
In evaluating the performance of face-matching approaches , most of the recent works on face retrieval in video use two benchmark datasets: Mobo ( Motion of Body ) \CITE and Honda / UCSD \CITE .
The scales of these datasets are limited , varying from hundreds to thousands of face images of tens of individual characters .
Particularly , Honda / UCSD consists of 75 videos involving 20 individuals .
Each video contains approximately 300-500 frames .
Meanwhile , Mobo provides 96 image sets of 24 individuals .
Hence , there are only 4 image sets for each individual .
One of the largest face datasets recently available is the YouTube Faces dataset \CITE , which .
provides 3,425 videos of 1,595 individual characters .
However , each character has only around 2.15 videos .
Such a small number of samples for each character is not sufficient to stably evaluate a face-matching or recognition approach , which is an important part of a face retrieval system .
In addition , there is no face dataset related to real-world news videos , which is our targeted domain .
In view of all the above-mentioned considerations , we prepare new datasets for evaluating the approaches.
Figure 2 illustrates the overview of our framework .
In the off-line stage , the face tracks in all video shots are extracted using our face-track extraction approach ( described in Section 4 ) .
Each extracted face track contains multiple face images of one individual character , varied under different viewpoints , illumination conditions , and expressions within a shot .
Each single face image in a face track is represented by a feature vector .
The process consisting of face-track extraction and face image representation is performed once for the entire video dataset .
Our contribution here is making the face-track extraction approach robust to sudden illumination changes , scattered appearances of characters , and occlusions.
Given a face track as an input retrieval query , the online stage of our system starts by using our proposed face-track matching algorithm ( described in Section 5 ) to estimate the similarity between a query face track and each face track in the retrieved set containing all face tracks extracted from the dataset in the offline stage .
A ranked list of the evaluated face tracks is returned as the retrieval result of the online stage .
Because the retrieved set is huge , our approach targets an extremely efficient face-track matching strategy while maintaining a competitive performance with state-of-the-art approaches.
Given a video shot with occurrences of multiple characters , face-track extraction is the process of extracting sets of face images .
A set is supposed to contain the face images of only one character who appears in the shot .
Such sets of face images are called face tracks ( sometimes called face sequences ) .
A common strategy in the existing approaches to face-track extraction consists in detecting faces in frames and grouping detected faces of the same character .
Whereas detecting faces is done by using a standard face detector ( e.g. , Viola-Jones face detector ) \CITE , grouping detected faces requires comprehensive techniques to identify faces of the same character.
In this section , we first briefly introduce an approach to face-track extraction proposed by Everingham et al .
We then present the problems with this approach as applied to news video and our proposed solutions.
To group detected faces into face tracks , connections should be established between faces belonging to the same character in different frames .
Motion analysis can be used to investigate such connections .
If two faces in different frames are defined that they are translated faces of each other according to a motion , they are likely faces of the same character .
Everingham et al .
in \CITE proposed the use of a KLT tracker for this purpose .
Their algorithm starts by detecting interest points in the first frame of the shot and propagating them to the next frames based on local appearance matching .
Points that cannot be propagated from one frame to the next are eliminated and replaced with new points .
Given two faces in different frames , if the number of point tracks passing through both faces is larger than half of the total number of point tracks that are not common to both faces , the faces are grouped into one face track.
Although the approach by Everingham et al .
has shown its efficiency and robustness with drama videos \CITE , directly applying the approach to news videos results in poor performance due to the following issues.
Tracking errors due to sudden illumination change .
Because the KLT tracker uses intensity variance for computing the image motion to find the correspondence between points in different frames , it is unreliable when there is a sudden and significant change in illumination .
As shown in Figure 3 ( top ) , points are distracted when a flash occurs .
As a result , the points are badly tracked .
The flash breaks all connections between faces in the frames before and after its occurrence.
Unadaptive track point generation .
In \CITE , the track point generation is totally independent from face appearances .
New points are generated at the first frame of the shot or at a frame in which some existing points cannot be propagated .
As a result , a face that , does not appear in the aforementioned frames , may not contain any point .
Its connections with other faces in the shot cannot be established for grouping.
Tracking errors due to occlusion .
To successfully connect actual faces of the same character in different frames , the track points generated for the first face should be tracked and retained inside the latter faces for a sufficient number of shared points between faces .
However , when occlusion occurs , the points are distracted by occluded regions .
Thus , the number of shared points drops , .
resulting in face connection failure .
As shown in Figure 3 ( bottom ) , when the woman moves the paper , which partially occludes her face in several frames , some points in her facial region are drifted with the paper .
These points are not lost so they are not replaced by new points .
However , they become meaningless in determining the connection between faces.
Based on the observed limitations of the approach in \CITE when applied to news videos , we integrate techniques to bypass these restrictions in our proposed approach to face-track extraction in news videos.
First , unlike in \CITE , our approach does not compare all possible pairs of faces in a shot for face grouping\CITE; .
such pairwise comparison rapidly becomes intractable as the number of faces in a shot increases .
Instead , we group faces into face tracks according to the temporal order of their appearances .
A detected face in the current frame is considered for grouping into existing face tracks formed by previously detected faces only .
By doing this , we avoid greedy pairwise comparison.
Second , as described in our first observation , a sudden illumination change in any frame causes the KLT tracker to fail to track points properly .
Because such illumination changes are very common and mostly occur simultaneously with important characters in a news video , finding a solution to this problem is vital .
We learn that the occurrences of such illumination changes are usually very short ( less than 3 frames ) .
and that the , faces that appear in those frames are less informative for recognition because most of the facial identity characteristics are lost due to over-lighting .
Thus , the faces cannot enrich the information on its corresponding face track , but may only add noise .
Therefore , our solution is to detect and skip all frames containing sudden illumination changes , which .
we call flash frames.
To identify flash frames , we measure the brightness of the frames in the video shot .
If the brightness of a frame is significantly increased compared with its neighbors , the frame is declared a flash frame and skipped in processing .
Particularly , given a frame \SYM with t indicating its frame index , we compute the average luminosity L of the frame \SYM and its consecutive frames \SYM , where i = \SYM; t +W+ 1 , and W is the potential length of a sudden illumination change .
Then , we compare the average luminosity L of each frame \SYM in the set S = \SYM with s = t; t +W to those of \SYM and \SYM .
If L( \SYM ) > L( \SYM ) and L( \SYM ) > L( \SYM ) , \SYM is defined as flash frames according to a predefined brightness sensitive threshold \SYM .
In our experiments , we found that \SYM = 1:25 and W = {1; 2; 3} are optimal for detecting all flash frames with a low false alarm rate.
Given a video shot , our approach starts by finding the first frame in which faces are detected .
All point-tracking and face-grouping processes are initialized from this frame , not at the first frame of the shot as in \CITE .
This helps us to save on computational cost and avoid tracking errors caused by transition effects between shots .
Initial track points will be generated for all detected faces in the frame .
Each face now becomes the first face of a corresponding newly formed face track.
After the initialization , we sequentially process each frame , knowing all flash frames will be skipped .
In a given frame , the points from the previous frame are tracked by the KLT tracker to update their locations .
If there are faces detected , each face is checked against all the existing face tracks formed in the previous frames to find out to which face track the face belongs .
The checking between a face and a face track is based on enumerating the points shared by both the face and the last face that appeared on the face track .
If the enumerated number is larger than half of the total number of points that are not common to both faces , the face is grouped into the face track .
Our grouping criterion here is similar to that in \CITE .
A face that cannot be grouped into any face track is treated as the initial face of a new face track .
We then generate new track points within such faces for tracking and grouping its corresponding faces in latter frames .
In our approach , track points are generated in conjunction with face appearances .
We can ensure that there are track points for all faces that appear in the shot .
Consequently , our approach overcomes the second observed limitation in \CITE.
In other cases , when a face in the current frame is grouped into an existing face track , we prepare points for further tracking .
We remove all points that are inside the last face that appeared on the face track but are not inside the current face , and vice versa .
Because such points are likely tracked incorrectly , eliminating them prevents us from transferring tracking errors to latter frames .
Points that are shared by both faces are kept .
Besides , we generate additional points to replace the removed ones and to provide updated points .
By doing so , our tracking results over a long sequence of frames become more accurate and reliable .
As a result , we can partly bypass the third observed limitation of \CITE .
When a face is partly and slowly occluded , our approach can discard incorrectly tracked points and reproduce points for the face after it has been occluded .
Thus , the connection between faces before and after the occlusion is retained.
Our approach continuously processes the next frame until the end of the shot is reached .
The pseudo-code is presented in Algorithm 1 as follows.
Several approaches to matching face tracks have been proposed ( as presented in Section 2 ) .
However , although these approaches have shown high accuracy in benchmark datasets , their high computational costs limit their practical applications in large-scale datasets .
This motivates us to target a matching approach that provides a good balance between accuracy and computational cost .
The approach should be extremely efficient while achieving a competitive performance with state-of-the-art approaches.
To maintain competitive accuracy , we still use the plenteous information from the multiple faces of a face track to enrich the representation .
However , instead of using all the faces in a face track , we propose taking a subsample of faces .
In doing so , the required computational cost can be reduced while keeping the amount of information sufficient to improve accuracy .
We call our approach k-Faces.
Given a specific value of k , which indicates the expected size of the subsampled set of a face track , the approach starts by dividing each face track into k parts according to the temporal order of appearances .
For each part , one face is selected to represent all faces within the part .
The mean face of k selected faces is then computed .
The similarity between two face tracks is now the distance between their mean faces.
Let mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} denote the mean faces of face tracks A and B , respectively , with N representing the number of dimensions of the feature space .
We use following standard distance types to compute the distance between mA and mB.
Figure 4 illustrates our k-Faces , with the following .
pseudo-code: .
Clearly , the higher the value of k selected , the more faces in each face track selected to compute the representative face and the .
, better the approximations , which may result in higher accuracies .
However , the computational cost can overly increases .
By using k as a predefined parameter , k-Faces provides users with flexibility in balancing the accuracy they expect and the cost they can afford ( or the time they can spend waiting for the result ).
Besides , because k-Faces averages multiple faces for the representative face of a face track , the effects of noisy or outlier faces on estimating the similarity of face tracks will be substantially reduced.
In this section , we present our experiments to evaluate the proposed approaches .
The experiments are divided into two parts; .
the first , evaluates the performance of the proposed approach in face-track extraction , and the second in .
face-track matching.
We tested our proposed approach to face-track extraction on 8 video sequences from different video broadcasting stations , including NHK News 7 , ABC News , and CNN News.
All shot boundaries are provided in advance .
A face detector based on the Viola-Jones approach \CITE is used to detect near frontal faces in every frame of the video sequences .
A conservative threshold is used to reduce the number of false positives ( i.e. , a non-face classified as a face ).
Ground-truth information on the face tracks in videos is manually prepared .
Each face track of a character appearing in a video shot is annotated by indexes of the frames in which the first face and the last face of that character occur .
An approach is considered as exactly extracting a face track if it provides precise starting and ending frame indexes of the face track , compared to ground-truth annotation .
Note that if a character moves out of the frame and then moves back into it again , annotators will divide the appearance of that character into two independent face tracks in ground-truth annotation .
Table 1 shows the number of frames , faces , and face tracks .
In this experiment , we directly compare our approach with that proposed by Everingham et al .
in \CITE.
As shown in Table 2 , by detecting flash frames , our approach successfully overcomes the problem of face-track fragmentation due to illumination changes .
Meanwhile , the approach by Everingham et al .
almost completely fails to do that .
In addition , the results also show that our approach is superior to that of Everingham et al .
in handling problems caused by partial occlusion and the appearance of a character in the middle of a shot .
The only face tracks that we could not extract exactly are those fully occluded in some frames during their occurrences .
In those cases , all points in the face regions are drifted to the background region .
After such full occlusions , there is no clue to regrouping the face of that person .
, Using only a tracker is not enough to handle this problem .
One can apply visual information-based clustering to group the fragmented face track , as in \CITE , but this .
obviously , requires extra cost .
Nevertheless , we observe that full occlusion rarely happens in news video because the characters featured in the news are recorded with care , especially the important and well-known ones .
This is a special characteristic of news videos .
The last column of the table shows the overall extraction performance of both approaches .
These facts clearly indicate that our approach is robust and outperforms that of Everingham et al .
in \CITE.
In terms of speed , our approach is approximately 2 times slower than that of Everingham et al .
However , our complexity is somehow linear to the total number of faces , because we consequently enlarge face tracks according to the temporal order by checking new faces with only the last face that appeared on each face track .
Meanwhile , Everingham et al .
compared all pairs of faces in the shot .
Their complexity is polynomial to the total number of faces .
If the number of faces increases , the gap in speed between our approach and that by Everingham et al .
will narrow rapidly.
Because all the problems presented here , such as those due to flash , occlusion , and in-the-middle face appearance , are practically observed , overcoming them is vital for the practical application of our approach .
In this experiment , we show that our proposed techniques and solutions to the problems are robust and efficient enough for extracting face tracks in real-world news videos by successfully extracting 94% of all face tracks .
Based on our observations , other complex techniques can be applied to handle the problems .
However , the trade-off between obtaining the 6% remaining face tracks and incurring an overly high computational cost should be considered with care.
Due to the limitations of existing public datasets , we prepared new datasets for the experiments .
Face tracks are extracted from videos of the datasets by using our proposed approach to face-track extraction ( see section 4.2 ) .
The identity of the character associated with each extracted face track is given by annotators .
Because our approach extracts face tracks in each video shot , the shot boundaries of videos are required .
A simple shot boundary detector based on a color histogram of frames is used .
The whole process , including shot boundary detection and face-track extraction , is fully automated.
TRECVID dataset .
We used TRECVID news videos from 2004 to 2006 .
This dataset contains 370 hours of videos in different languages , such as English , Chinese , and Arabic .
The total number of frames that we processed was approximately 35 million .
Among those , 20 million faces were grouped into 157,524 face tracks .
We filtered out short face tracks that had less than 10 faces , which .
resulted in 35,836 face tracks .
Finally , we annotated 1,497 face tracks containing 405,887 faces of 41 well-known individual characters .
NHKNews7 dataset .
This dataset consists of observations from the NHK News 7 program over 11 years .
After the annotation process , 1,259,320 faces of 111 individuals are provided .
The total number of face tracks is 5,567 .
Each character has from 4 to 550 face tracks .
In this dataset , we discard face tracks with fewer than 100 faces and more than 500 faces .
Compared to the TRECVID dataset , the NHKNews7 dataset is much more challenging.
Table 4 shows a , comparison between our datasets and some public benchmark datasets .
Based on the results , it is obvious that our datasets are superior over the other datasets , such as MoBo and Honda / UCSD , on all statistical terms , including number of videos , number of characters , and average face-track length .
Compared to the YouTube Faces dataset , , we provide much more face tracks ( or video shots ) per character , although our datasets have smaller numbers of characters ( or subjects ) .
Thus , our datasets are more relevant in evaluating a face retrieval system.
Figure 5 presents statistical information on our datasets .
The datasets can be downloaded from http: / / satohlab .
ex.nii.ac.jp / users / ndthanh / NIIFacetrackDatasets .
However , due to copyright issues , the face images in the face tracks cannot be published .
Instead , we provide a feature vector , used in \CITE , for each face image .
The feature vector of a face is extracted by computing the descriptors of the local appearance of the face around each of the located facial features .
Before extracting the descriptors , the face is geometrically normalized to reduce the effect of pose variation .
An affine transformation is estimated , which transforms the located facial feature points to a canonical set of feature positions .
Then , the appearance descriptors around each facial feature are computed .
The final feature representation of the face is formed by concatenating all the descriptors of its facial features.
We compared k-Faces with several approaches , including those based on pair-wise distances , MSM \CITE and CMSM \CITE.
Given two face tracks having multiple face images represented as feature vectors , pair-wise-based approaches compute the distances between each possible pair of feature vectors in two face tracks .
The maximum distance , the minimum distance , or the mean distance of the computed pair-wise distances is the used as the similarity measurement between two face tracks .
We refer to the approaches as pair:max , pair:min , and pair:mean , respectively ( see Figure 6 for the illustration ) .
The pair:min ( sometimes called min-min ) is a state-of-the-art approach widely used in other studies \CITE.
Regarding \CITE , if the pair-wise-based approaches are representative of nonparametric sample-based approaches , MSM and CMSM are representative of approaches based on a parametric model .
MSM , introduced by Yamaguchi et al .
The similarity between the sets is computed using the angle between subspaces .
CMSM is an extension of MSM , in which subspaces of the sets are projected onto a constraint subspace .
In doing so , the subspaces are expected to be more separable .
All of these approaches have shown their robustness in benchmark datasets , such as MoBo , HondaUCSD , and YouTube Faces .
Therefore , it is appealing to compare our k-Faces with them for a comprehensive evaluation.
Besides evaluating k-Faces with different values of k and different types of distance ( e.g. , Euclidean , L1 , and cosine ) , we try another criterion for selecting k representative faces in a face track .
In the original way , we proposed selecting these faces by partitioning the face track according to the temporal order and choosing the middle face of each partition .
However , another criterion that is based on clustering can be applied in selecting these representative faces .
In this new way , all the faces in a face track will be clustered to k groups by using a clustering algorithm .
The centroid of each group is selected .
Then , the mean of k centroids is used as the representative face for the face track .
In this experiment , we use the standard K-Means for clustering .
We refer to the former k-Faces as k-Faces.Temporal and to the latter k-Faces as k-Faces.KMeans.
We evaluate the performance of a face-track matching approach by computing the average precision of the rank list that it returned .
In particular , in each dataset , a face track is alternatively picked out as a query face track , while the remaining face tracks are used as the retrieved database .
Given a query , the average precision of the returned ranked list is computed , .
Finally , the mean of all average precision ( MAP ) values for all queries is reported as the overall evaluation metric for the approach with the given database.
Let r denote a rank in the returned face-track list , Pre( r ) the precision at rank r of the list , Nl the length of the list , Nhit the total number of face tracks matched with the query face track q , and I sMatched( k ) a binary function returning 1 if the face track at rank r is matched with q ( based on ground-truth annotations ) and , zero otherwise .
Then , the MAP of the evaluated approach can be computed as follows: \MATH
The MAP is a standard metric for evaluating retrieval and matching systems .
Besides the MAP , we record the processing times of the approaches in each dataset to compare their efficiency.
Figure 7 presents the mean average precision ( MAP ) of all the evaluated approaches in our two datasets , Trecvid and NHKNews7 .
Generally , all the MAPs vary from 64.61% to 76.54% in the Trecvid dataset .
Meanwhile , in the NHKNews7 dataset , the best MAP is 60.99% , and the worst is 42.75% .
The difference in the MAPs between the two datasets can be explained by following reasons .
First , the number of characters in NHKNews7 is larger than that in Trecvid , 111 characters in NHKNews7 compared to 41 characters in Trecvid .
This clearly increases the probability of mismatching face tracks .
Second , the videos in NHKNews7 were recorded over a long time ( i.e. , 11 years ) .
Thus , besides facial variations in each face track caused by the environmental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) , the face tracks of the character themselves also reflect the biological variations of the character over time; .
for instance , a character may look older after several years ( see Figure 8 , for example ) .
For these reasons , matching faces in NHKNews7 becomes more challenging , which .
resulted in decreased MAP( s ) for all the evaluated approaches.
A clear and consistent observation from both datasets is that pair:min ( i.e. , min-min ) always achieves the best MAPs , which are 76.54% and 60.99% in the two datasets , respectively .
Among the distance types , L1 is the optimal for use with pair:min .
A reasonable replacement is the Euclidean distance .
However , there is a minor accuracy gap between pair:min using L1 and pair:min using the Euclidean distance .
In addition , computing the Euclidean distance between two feature vectors is more expensive than computing their L1 distance .
The results also show that pair:min is better than pair:mean .
This is because pair:mean uses the mean of all pair-wise distances between two face tracks as the similarity score .
By computing the mean , pair:mean reduces the effect of noisy pairs .
At the same time , it eliminates the influence of pairs containing identical faces , which can help to instantly determine that the faces belong to the same character .
Thus , the discriminative power of the computed similarity score is reduced , compared to that computed by pair:min .
This causes the difference in MAPs between pair:min and pair:min .
More generally , this explains why such a gap between pair:min and pair:mean is larger in NHKNews7 than in Trecvid .
Because the average length of face tracks on NHKNews7 is longer ( i.e. , each face track contains more sample faces of a character ) , there is a greater chance that two face tracks of the same character contain identical faces.
Regarding our k-Faces , its MAP increases when k increases .
Between k-Faces.Temporal and k-Faces.KMeans , the impact of k on the MAP of k-Faces.KMeans is less significant .
Because k-Faces.KMeans always uses all the faces in a face track for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
In contrast , k plays an important role in k-Faces.Temporal .
The higher the k set , the more representative faces of each face track selected .
Thus , the final mean face of each face track becomes more reliable and accurate .
The advantages of k-Faces.KMeans is that it can achieve high accuracy even when k is very small .
However , its disadvantage is the high computational cost of clustering faces on a high-dimensional feature space ( i.e. , 1,937 dimensions ) .
When k is large enough , there is no substantial difference in MAP between k-Faces.KMeans and k-Faces.Temporal.
In both datasets , when k increases from 2 to 20 , the MAPs of k-Faces approaches grow rapidly .
However , the MAPs become stable from k = 20 upward .
Because further increasing k does not help improve accuracy but increases the computational cost , we select k = 20 for investigating the trade-off between the accuracy and computational cost of k-Faces approaches in comparison to others .
Table 5 shows the MAP and processing time of each approach .
Processing time is divided into two parts , preprocessing and matching .
The preprocessing time refers to the time required to preprocess face tracks before matching .
In k-Faces approaches , the preprocessing of face tracks includes selecting representative faces and computing their mean face .
In MSM and CMSM , preprocessing includes computing subspaces for face tracks .
The matching time is averaged over one query run .
The time unit used is seconds.
As shown in Table 5 , k-Faces.KMeans and k- Faces.Temporal achieve almost equal accuracy and consume the same amount of time for one query in both datasets .
However , k-Faces.Temporal is hundreds of times ( 240 times in Trecvid and 360 times in NHKNews7 ) faster than k-Faces.Temporal in the preprocessing phase .
This suggests that in terms of both accuracy and efficiency , selecting representative faces based on temporal sampling is better than that based on clustering .
,
Compared to state-of-the-art approaches , our k- Faces.Temporal is thousands of times faster than the best approach , which is pair:min , and hundreds of times faster than MSM and CMSM in both datasets .
In terms of accuracy , k-Faces takes second place , with 73.65% in the Trevid dataset , after pair:min .
The difference in MAP between our approach and pair:min is 2.89% .
Meanwhile , k- Faces.Temporal is significantly better than MSM and CMSM , which respectively achieved 69.20% and 64.62% accuracy .
In the NHKNews7 dataset , k-Faces.Temporal is better than CMSM , but worse than pair:min and MSM .
One may question why MSM performed poorly in the Trecvid dataset , but was superior to k-Faces.Temporal in NHKNews7 .
The reason for this is the fact that the face tracks in the NHKNews7 dataset are larger than those in the Trecvid dataset .
Therefore , more sample faces in each face track can be used to obtain a reliable subspace .
As expected , the results of this experiment show that our proposed approach is extremely efficient while achieving comparable performance with state-of-the-art approaches .
In this paper , we investigate face retrieval in large-scale news video datasets .
Our contribution is threefold .
First , we present the practical problems encountered when a tracker is used to extract face tracks in news videos .
Based on these , we introduce techniques and solutions to overcome these problems to achieve robust face-track extraction .
Second , we present an approach for face-track matching that significantly reduces the computational cost while achieving competitive performance compared with state-of-the-art approaches .
Third , we prepare datasets , evaluate state-of-the-art face retrieval approaches , and publish real-world face-track datasets of such scales that have never been considered in the literature.
Recommend-Me : recommending query regions for image search
This paper presents a novel recommendation system , named Recommend-Me , to facilitate users in searching and exploring images in unknown image databases .
Given an initial query image , Recommend-Me automatically shows its recommendations to users .
The recommendations indicate which and how frequent items in the initial query image occur in the database .
In this way , users can make their own decisions before any actual search .
If there is a recommendation matching their search intention , relevant search results are ensured .
Otherwise , users should refine their initial query image for a better query sample .
Or , they can start exploring the database by using the recommended items as hints .
Recommend-Me helps users to avoid unnecessary trials and poor searching experiences .
We describe an efficient approach for Recommend-Me to deal with quantifying occurrences of multiple candidate items in the images of the database .
Instead of scanning the database for each candidate item , the approach enumerate occurrences of multiple candidate items simultaneously by investigating pairs of highly similar regions , knowing one pair is formed by a region in the initial image and a region in an image of the database .
We formulate the problem of finding such pairs as an optimization problem , which can be solved by a branch-and-bound algorithm .
Experiments conducted on a real-life and publicly available dataset demonstrate the efficiency , robustness , and promising application of our system .
Thanks to the advances of modern technology , a large amount of digital images can be easily created and stored nowadays .
The resulting exponential growth of image repositories , however , has created an urgent need for effective ways of searching images .
Moreover , image search has gained interest in recent years because of its importance and wide range of applications . In a typical scenario , users supply a query item , which is usually a region cropped from an image .
The search system then returns a list of relevant images retrieved from a database .
The images are expected to contain the query item .
Extensive studies have been conducted with an eye to improving the performance of this sort of search \CITE .
However , regardless of the powerfulness of state-of-the-art search techniques , there are still cases in which users are disappointed with their search results .
The reason is that relevant items are not in the database .
Under such circumstances , whatever the search technique is , results are obviously irrelevant and unexpected .
A normal user without prior knowledge about a database has no choice but to search it by trial-and-error .
We decided to tackle this problem to help users in searching and exploring images in unknown databases .
Our proposal is a novel recommendation system , named Recommend-Me .
The envisioned scheme can be described as follows ( see Figure \REF for an example ) .
Given an unknown database and an initial query image , Recommend-Me automatically presents its recommendations to the user .
One recommendation is one item , bounded by a rectangular region , in the initial query image .
Each recommended item is assigned a number to show in how many images of the database it occurs .
Items with larger assigned numbers will be more recommended .
By providing such recommendations , Recommend-Me helps users to :
- avoid unexpected search experience with poor queries that are subjectively ( and sometimes randomly ) selected ,
- rapidly refine the initial query image before any actual search , if the recommendations show that current search intention can not return relevant results ,
- explore the database using the recommendations as hints .
Recommend-Me is a pure visual recommendation system .
No extra information or knowledge is required for an input besides an initial query image and a database . //<" and the name of the database " ? ? Or " and the location and name of the database " ? ?>
To automatically generate recommendations , we need to address several issues .
First , there tends to be a huge pool of candidate items in the initial query image .
Basically , any rectangular region in the image can be considered as a candidate item .
Examining all of them would incur an enormous computational cost .
Second , even if a candidate item is known , enumerating its occurrences in the database is a not trivial task because it is subject to many variations in viewpoint , scale , rotation , occlusion , etc.
Furthermore , the cost of scanning all regions of the images of the database will inevitably be prohibitive for practical purposes .
In this paper , we employ state-of-the-art techniques such as SIFT and the Bag-of-Words ( BoW ) model to handle the task of matching regions with the above variations .
Our main focus is to devise an efficient approach for quantifying occurrences of candidate items in the database in order to generate recommendations .
The advantage in efficiency comes from our use of various methodologies .
Based on the observation that users are mostly interested in object-like items , we decided to use a selective search approach proposed by Van de Sande et al. \CITE to sample regions bounding object-like items in all images as a preprocessing step .
By applying this approach instead of a naive sampling approach such as sliding windows , we were able to dramatically reduce the number of items ( i.e. regions ) that need to be processed in each image .
Given two sets of regions , one containing regions of candidate items in the initial query image and the other containing regions of items in images of the database , our task is to find occurrences of all candidate items in the database .
This task can be equivalently treated as finding pairs of matched regions , knowing that a pair is formed by a region in one of the sets with a region in the other .
So , if the top region pairs are found with sufficiently high similarity scores , we can enumerate the occurrences of the items .
Based on this insight , we can boost efficiency yet again by formulating the problem as an optimization problem that can be solved by applying a branch-and-bound algorithm .
In order to do that , we introduce a novel representation based on a hierarchical structure describing a set of region pairs and a corresponding function bounding the similarity scores of pairs over such a set .
Related Work .
On the topic of discovering common items , Recommend-Me is related to recent studies on mining common items in image databases such as \CITE .
However , in contrast to these studies , Recommend-Me targets items which are shared by both an image database and the user 's particular interest inthe input initial image .
Meanwhile , \CITE only aims at finding common items within the database .
One can employ these techniques to solve our problem by first identifying common items among the images of the database , then looking them up in the initial query image again to make recommendations .
However , doing that incurs the extra cost of mining unnecessary items that appear in the database , but not in the initial query image .
One of the most related studies to ours is that of Zha et al in \CITE .
They introduced a system , called Visual Query Suggestion ( VQS ) , that simultaneously provides both keyword and image suggestions to users .
There are clear differences between Recommend-Me and VQS .
VQS requires an initial text query for formulating the suggestion , and its suggestions are both keywords and images .
On the other hand , Recommend-Me takes an image as input , and its outputs are regions in the image .
Recommend-Me is a query suggestion system based on pure visual information .
Above all , although both Recommend-Me and VQS aim at helping users search for images , their targeted problems are different .
VQS proposes to help users to overcome their tendency to formulate ambiguous queries by precisely expressing search intents , assuming the relevant items are always available .
Meanwhile , Recommend-Me helps users to select queries based on the existence of relevant items in the retrieved database .
To the best of our knowledge , Recommend-Me is the first attempt at this sort of targeted suggestion scheme .
From a technical point of view , our solution is motivated by recent work on object localization and subimage retrieval based on branch-and-bound optimization \CITE .
However , ours is differentiated from the other studies in that we represent sets of region pairs , instead of only sets of regions .
ESS and ESR \CITE use coordinate intervals for their presentation .
In contrast , we utilize hierarchical structures in order to do that , since our regions are discrete .
Although coordinate intervals as in ESS ( or ESR ) can be extended to represent sets of region pairs , such a criterion in the context of the branch-and-bound algorithm may suffer from the curse-of-dimensionality problem since the number of dimensions required at least doubles .
Finally , ESS , ESR and Recommend-Me differ in that they have different approaches to constructing a bounding quality function and to computing bounding values over the sets .
The rest of this paper is organized as follows .
Section 2 presents an overview of the system .
The details of how we find region pairs with the highest similarity scores are given in Section 3 .
Section 4 presents our experiments and evaluations .
Section 5 concludes the paper .
The framework of Recommend-Me consists of four main steps .
Figure \REF summarizes the pipeline .
Step 1 : Select candidate items in images .
Using all possible rectangular regions in images as candidate items is overly expensive in the subsequent processing .
More importantly , users are often attracted by object-like items .
Thus , we employ an approach proposed by Van de Sande et al. \CITE for item selection .
The approach starts by over-segmenting an image into disjoint regions .
Then , it performs a greedy search < ? ?> algorithm that iteratively merges the two most similar regions together until the whole image becomes a single region .
All regions throughout the hierarchy are considered to be candidate items .
Each item is represented by its rectangular bounding region .
Step 2 : Find top region pairs with the highest similarity scores .
There will be a pool of region pairs if we compare each region in the initial query image with each region in images of the database .
However , only region pairs with sufficiently high similarity scores are meaningful for identifying occurrences of candidate items .
In this step , we use the approach explained in Section 3 to find the top \MATH ( the expected number of returned region pairs ) of such pairs in the pool .
Step 3 : Group overlapping regions .
Given \MATH region pairs returned in Step 2 and assuming each region pair in the \MATH pairs is formed by a candidate item and its corresponding match , we can enumerate the number of occurrences of the items .
However , there are likely several regions that overlap each other due to the merging done in Step 1 .
These regions would be perceived as the same item by users .
Thus , we propose to use maximal clique analysis to group such regions so that the recommendations will be consistent .
One clique is one group of regions .
Step 4 : Formulate recommendations .
Finally , for each group of regions , we count the number of images containing at least one match of one member region of the group .
The number indicates how frequent the item , represented by the group , occurs in the database .
Using those numbers , we rank all groups and show them as recommendations to users .
A representative of each group is a rectangular region located by averaging the coordinates of all member regions of the group .
In this section , we describe our approach for efficiently finding the top \MATH similar region pairs in the pool of all possible region pairs .
Given two sets of regions \MATH and \MATH , the set of all possible region pairs can then be represented as \MATH .
By using the similarity function \MATH , we have to solve the following optimization problem in order to find the region pair \MATH with the highest similarity score .
Because \MATH elements , it is expensive to perform this maximization exhaustively .
We hence propose to use a branch-and-bound algorithm \CITE to solve the problem .
Once \MATH is found , we can obtain the other top region pairs by continuing the search process with the remaining search space , in which the found top pairs have been eliminated .
A general branch-and-bound algorithm works by hierarchically dividing the parameter space into disjoint parts ; this is called the branching step .
In the bounding step , each part is assigned an upper bound for which the quality function could take on any of the members of the part . //<The rewrite is grammatical but I don 't know what " the quality function could take on any of the members of the part " . Inparticular what are these members and can they be computed in a function ?>
Those parts of the parameter space with higher upper bound values are examined first .
Thus , many portions of the parameter space can be eliminated if their upper bound values imply that they cannot contain the maximum .
In our problem , the parameter space is the set of all region pairs \MATH , and the quality function is the similarity function \MATH .
Assuming we can organize regions in \MATH and \MATH into two hierarchical structures \MATH and \MATH respectively , so that :
- all regions are leaf nodes of the structures and non-leaf nodes are {\it virtual} nodes ,
- if each node is represented by a histogram \MATH with \MATH bins , the value in each bin of a child node is constrained to be equal or smaller than the value in the same bin of its parent node .
Given such structures , we show in what follows how the branch-and-bound algorithm can be used to solve our problem .
Let \MATH and \MATH denote two nodes on \MATH and \MATH .
And let \MATH denote the set containing all leaf nodes explored from \MATH .
If \MATH is a leaf node , \MATH .
Otherwise , given \MATH with \MATH being direct child nodes of \MATH , \MATH can be recursively defined as follows : \MATH
In a similar way , we have : \MATH
Letting \MATH indicate the set of node pairs formed by pairing nodes in \MATH with nodes in \MATH , we get : \MATH . //<the rewrite is a guess .>
Thus , if \MATH and \MATH are roots of \MATH and \MATH respectively , \MATH will be exactly the entire search space \MATH .
Branching Step .
Dividing up the search space ( i.e. set of region pairs ) covered by \MATH can be done straightforwardly by utilizing the hierarchical structures \MATH , \MATH at certain nodes \MATH , \MATH . //<The rewrite is better if it is correct .>
Regarding \REF , \REF and \REF , \MATH can be divided into disjoint parts as follows : \MATH
Or , \MATH
The way to divide can be based on the sizes of \MATH and \MATH .
We divide the larger one first .
The branching step is illustrated in Figure \REF .
Bounding Step .
An essential requirement for the branch-and-bound algorithm is the quality bounding function \MATH used to determine whether a part of the search space should be examined . //<Or " determine the extent that " ? ?>
In particular , \MATH bounds the upper values of \MATH over a set of node pairs ( i.e. region pairs ) .
Let us assume that we are evaluating the upper bound of \MATH over all region pairs in \MATH .
Among the several distance formulas for estimating the similarity of two regions , we will use the Normalized Histogram Intersection ( NHI ) distance since it is well-balanced between computational efficiency and robustness~\cite{ESR} .
We will then rely on NHI to define \MATH bounding the values of \MATH , with : \MATH
Referring to constraint ( b ) in constructing \MATH and \MATH , we have : \MATH
As a result , the bounding value \MATH over \MATH can be clearly observed as : \MATH . //<I 'm not sure what observed means in this context . Do you mean " can be derived as " ? ?>
We can efficiently evaluate \MATH for the set of region pairs \MATH because \MATH relies only on the histogram representation of single rectangular regions \MATH and \MATH .
Moreover , the normalization terms , which indicate the minimum number of visual words inside any member region of \MATH , \MATH , are computed once by using the integral image technique .
Inspired by \CITE , we devised the algorithm to work in a best-first manner . //<The original describes the way you decided to write the algorithm . In contrast , the rewrite describes the way the algorithm works .>
The algorithm examines the set having the highest bounding value \MATH . //<" next " is unclear . The rewrite is a guess .>
The algorithm stops if the set contain only one pair of region .
Otherwise , the set is then divided into disjoint subsets for further search .
Pseudo-code for the algorithm using a priority queue to store sets of region pairs , is given as follows .
To obtain more than one region pair , we simply repeat the loop in Algorithm 1 until the expected number of region pairs \MATH is reached .
So far , our approach is based on an assumption that the sets of regions are already organized into hierarchical structures which satisfy constraints ( a ) and ( b ) .
In the remaining of this section , we show how to organize such sets , given the initial query image and image database .
There are two type of region set .
One is a set containing regions of one image .
The other is a set containing regions of multiple images ( i.e. database ) .
With the first type of set , by applying the selective search approach introduced in \CITE for item selection , regions in each image are already organized into a binary tree .
Because such binary tree were constructed by bottom-up merging of regions , a parent region on the trees spatially covers its child regions in the image space ( see Figure \REF ) .
As a result , constraint ( b ) is satisfied .
However , because we want to use all regions corresponding to all nodes throughout the tree as candidate item regions , constraint ( a ) will be violated if we keep using the tree for the branch-and-bound algorithm .
In other words , all current non-leaf nodes of the tree will be treated as {\it vitual nodes} and will not be used as candidate item regions .
Our solution to this problem is straightforward .
We generate and attach a new leaf node to each non-leaf node of the current tree .
The generated node is exactly the same as the non-leaf node it is attached to , which now becomes a virtual node .
By doing that , we keep the spatial covering property of the original binary tree for the new hierarchical structure .
Moreover , all non-leaf nodes will be taken into account as candidate item regions via their attachments .
The new hierarchical structure therefore satisfies both constraints .
Figure X is an illustration of this organization .
With a set containing regions from multiple images , we perform a two-stage organization procedure .
In the first stage , regions in each image are organized into a hierarchical structure , as presented above .
If multiple hierarchical structures are returned by the first stage , we use their root nodes as the initial elements to construct an yet another hierarchical structure over them by divisive clustering . //<the rewrite is a guess .>
We start with the full set of the elements .
Then , we perform splits recursively as one moves down the hierarchy .
In each splitting step , the split set is divided into $k$ parts by using $k$-means clustering .
Once the hierarchical structure is completed , we compute a histogram representation for all of its non-leaf nodes .
The value at each histogram bin of a non-leaf node is the maximum of all values in the same bin of its child nodes .
This is to ensure constraint ( b ) is satisfied .
Finally , by unifying the results of both stages , we have a unique hierarchical structure over the set of regions of multiple images , which satisfies both constraints .
We illustratie this in Figure Y .
So , given the initial query image and a database , we now can construct two hierarchical structures .
One is for the regions of the initial query image .
The other is for the regions of all images in the database .
Both structures then become the input for our approach to find the top region pairs with the highest similarity scores for making recommendations .
Note that because the hierarchy of the regions of images in the database is independent of the query , we construct it only one time .
A recommendation is a good one if it exactly locates an item in the database .
We call such recommendations" hit recommendations " , and a good recommendation system should accurately provide them to users .
More importantly , users always expect that hit recommendations are ranked higher than false recommendations ( if there are a number of them ) on the list of the recommendations by the system .
Based on these insights , we evaluated the Recommend-Me system using two evaluation metrics : precision in pesenting recommendations and rank of the first hit recommendation on the list .
Given an initial query image with ground-truth annotations indicating the bounding box of an item known to existin the database , Recommend-Me will provide a recommendation if at least one of its recommendations is a hit recommendation .
We used an approach from the Pascal VOC challenge to clarify whether a recommendation is a hit recommendation or not .
In particular , the intersection area between a hit recommendation and an item should be larger than half their union area .
Because our target is not to improve search techniques but to facilitate query selection , the search simply relies on standard techniques if users use an recommendation as a search query .
To evaluate the efficiency of Recommend-Me in finding \MATH region pairs with the highest similarity scores , we computed the number of evaluations for the quality bounding function in the branch-and-bound algorithm .
This number was then divided by the size of all possible region pairs formed by regions in the initial query image and regions in images of the database . //< " the size of all possible region pairs " is unclear to me . Do you mean " the average size of all possible region pairs " or " the sizes of all possible region pairs " ?>
The fraction was taken to be the efficiency improvement of Recommend-Me .
Note that regions in images were pre-selected as in Step 1 of our framework .
Feature presentation .
We employed a BoF model to represent the features of images and regions in the images .
Visual words in the images were located by using dense grid sampling and a Different-of-Gaussian ( DoG ) detector .
A codebook of 2000 visual words was built using the standard K-Means algorithm .
%to cluster points on a set of random images .% Additionally , the set of points of interest obtained by the DoG in the query image was used to remove regions without any such points inside .
This helped us to eliminate less meaningful regions such as the sky , solid color regions , etc. , from the recommendations .
Region selection in images .
As mentioned above , we used the approach first introduced in \CITE on different color channels for the region selection .
In this experiment , we used two color channels , RGB and Hue , since the regions generated on those channels can cover 99 .72\% of the area of the annotated item regions in our dataset .
A virtual root node was created to compose two color-dependent binary trees into one unique binary tree for each image .
In addition , the rectangular regions which did not contain any visual word or were smaller than 40 x 40 pixels were discarded .% since they can form a meaningful recommendation .
Maximal clique analysis algorithm .
Given the set of regions in the initial query image , we built a graph in which two regions were connected if they nearly overlapped each other ( we use the approach of Pascal VOC with a tighter threshold , 0 .8 ) .
The Bron-Kerbosch algorithm was then applied to find all maximal cliques in the graph .
One clique was one group of regions .
Figure \REF shows the results of the evaluation .
The reported precision , rank of the first hit recommendation , and efficiency improvement are averages of Recommend-Me on 375 different initial query images and an individual value of \MATH .
The results show that Recommend-Me can make hit recommendations to users with high precision ( approximately 80 .27\% ) .
On the returned list of all recommendations , hit recommendations usually take the first two places on the list .
This help users to avoid choosing false recommendations ( if such false recommendations are highly ranked ) .
There were two types of false recommendation in the top places of the list .
The first type consisted of background regions ( e.g. trees , buildings , roads ) , which are easily found in many images .
The second type was items lacking manual annotations such as windows , cars , and humans .
Thus , recommendations about those items are not counted as hit recommendations .
However , if users are interested in using them as hints to explore the database , they may still be very helpful .
Clearly , one can realize that the performance of Recommend-Me is influenced by \MATH .
By increasing \MATH , we can obtain more region pairs with sufficiently high similarity scores .
Thisgives more chances to get region pairs of the annotated item , thus improving precision .
However , the trade-off is that more unexpected items are returned .
This results in a drop in the average rank of the first hit recommendation .
Figure 5( a ) demonstrates this circumstance .
When \MATH increases from 2000 to 10000 , the precision increases from 74 .67\% to 81 .97\% ; meanwhile , the average rank of the first hit recommendation drops to 2 .27 from 1 .78 .
It is worth noting that keeping increasing \MATH may not always give better precision , since precision relies on not only \MATH but also the robustness of the region comparison techniques .
Recommend-Me cannot provide any hit recommendation for around 20\% of the initial query images due to the fact that our region comparison technique cannot deal with significant variations in the items .
Figure 5( b ) shows another circumstance when we increase \MATH .
It is the decline in the efficiency improvement of Recommend-Me .
This is because the branch-and-bound algorithm has to visit more parts of the total search space in order to find extra local opitimals .
However , in all of our evaluations , Recommend-Me still performed around three times faster than the exhaustive search .
This advantage will be important for practical applications .
Figure \REF presents examples of hit recommendations returned byRecommend-Me .
We described oursystem , named Recommend-Me , for making visual query suggestions .
Given an initial query image and a retrieved database , Recommend-Me gives recommendations that impose conditions on which and how frequent items in the initial query image appear in the database .
Such recommendations help users to select the search query , to rapidly refine the initial query image or to explore the database .
An efficient solution to make Recommend-Me practical was also presented .
To the best of our knowledge , Recommend-Me is the first attempt at developing a targeted suggestion scheme .
