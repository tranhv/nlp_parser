Using SOM based Graph Clustering for Extracting Main Ideas from Documents
Using SOM based Graph Clustering for Extracting Main Ideas from Documents
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In this paper , we would like to present a graph clustering system for grouping the similar documents and extracting the main ideas in documents . 
In this paper , we would like to present a graph clustering system for grouping the similar documents and extracting the main ideas in documents . 
5 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

To cluster the documents , we need a model for representing the documents . 
To cluster the documents , we need a model for representing the documents . 
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The traditional approaches used a word set based model or a vector based model for representing the documents . 
The traditional approaches used a word set based model or a vector based model for representing the documents . 
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

These models discard the important structural information of documents such as word position , the semantic relations of words in document …
These models discard the important structural information of documents such as word position , the semantic relations of words in document …
8 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Recently , some research works using the graph for representing the documents have been appeared . 
Recently , some research works using the graph for representing the documents have been appeared . 
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

We use the graph to becreated by analyzing the co-occurrence and position of two words in a section of document . 
We use the graph to becreated by analyzing the co-occurrence and position of two words in a section of document . 
10 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

After representing the documents by using graph , we used self organizing map ( SOM ) with two dimensional output layer for grouping the graphs . 
After representing the documents by using graph , we used self organizing map ( SOM ) with two dimensional output layer for grouping the graphs . 
11 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

One of the advantages of SOM is to cluster the data without specifying the number of clusters . 
One of the advantages of SOM is to cluster the data without specifying the number of clusters . 
12 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Besides , two-dimensional SOM output layer can be put on the computer display and it can help to access the similar documents on the computer display . 
Besides , two-dimensional SOM output layer can be put on the computer display and it can help to access the similar documents on the computer display . 
13 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

We use the graph distance based on the maximum common sub-graph ( mcs ) which is discovered by maximal frequent sub-graph algorithm and the updated operation of neurons on SOM ouput layer based on the weighted means graphs and the genetic algorithm .
We use the graph distance based on the maximum common sub-graph ( mcs ) which is discovered by maximal frequent sub-graph algorithm and the updated operation of neurons on SOM ouput layer based on the weighted means graphs and the genetic algorithm .
14 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved

In this paper , we would like to present the research results of a system for clustering the documents and extracting the main ideas in documents . 
In this paper , we would like to present the research results of a system for clustering documents and extracting their main ideas . 
18 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17::mogrammar-det 18:17:preserved 19:18:preserved 20:19:preserved 21:20:bigrammar-det 22:21:preserved 23:22:preserved 24,25::unaligned 26:23:preserved

To cluster the documents , we need a model for representing the documents . 
To cluster documents , we need a model for representing the documents . 
19 0:0:preserved 1:1:preserved 2::mogrammar-det 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved

In the previous approaches , a word set based model or a vector based model were used for representing the documents . 
In previous approaches , a word - set - based model or a vector - based model were used for representing the documents . 
20 0:0:preserved 1:21:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6,7,8:5,6,7,8,9:typo 9:10:preserved 10:11:preserved 11:12:preserved 12,13:13,14,15:typo 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19::unaligned 20:22:preserved 21:23:preserved

These models discard the important information such as position , the semantic relation of words .
These models , however , discard the important information such as position and the semantic relation of words .
21 0:0:preserved 1:1:preserved 2:5:preserved 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:paraphrase 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved :2:unaligned :3,4:unaligned

Recently , some research works using the graph for representing the document have beeen appeared </CITE> .
Recent studies use graphs as an alternative , and we apply this model to representing documents </CITE> .
22 0,2,3,4,5,6,7,8,10,11,12,13,14:0,1,2,3,4,5,6,8,9,10,11,12,13,15:paraphrase 1:7:preserved 9:14:preserved 15:16:preserved 16:17:preserved

After using the graph model for representing the documents , we need to develop a system for clustering the graphs . 
Prevous approaches to text clustering rely on word - set - based or vector - based models to represent documents . 
23 0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,18,19:0,1,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18:paraphrase 8:19:preserved 12:2:preserved 17:4:preserved 20:20:preserved

We use SOM neural network for clustering the graphs and extracting the main ideas from the documents . 
The SOM neural network clustering methodology , 
24 0,1,5,7,8,9,10,11,12,13,14,15,16,17:0,5,6:paraphrase 2:1:preserved 3:2:preserved 4:3:preserved 6:4:preserved

SOM neural network has been developed by T . Kohonen since 1980 and it has been used for clustering </CITE> . 
developed by Kohonen since 1980 , is utilized to cluster the graphs </CITE> . 
25 0,1,2,3,4,12,13,14,15,16,17,18,20:5,6,7,8,9,10,11:paraphrase 5:0:preserved 6:1:preserved 7,8,9:2:paraphrase 10:3:preserved 11:4:preserved 19:12:preserved :13:unaligned

One of the strong points of the SOM neural network is the capability of data clustering without defining number of clusters . 
SOM is superior in its capability of clustering data without having to define the number of clusters . 
26 0,3,4:2,3,10:paraphrase 1::unaligned 2:13:preserved 5::unaligned 6::mogrammar-det 7,8,9:0:paraphrase 10:1:preserved 11:4:bigrammar-det 12:5:preserved 13:6:preserved 14:8:preserved 15:7:preserved 16:9:preserved 17:12:bigrammar-wform 18:14:preserved 19:15:preserved 20:16:preserved 21:17:preserved :11:mogrammar-prep

This capability is very important and better than the traditional clustering algorithms such as kmeans . 
This capability is very important and better than the traditional clustering algorithms such as the kmeans . 
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved :14:mogrammar-det

Besides , SOM can put the documents on a document map and help to access the content of similar documents . 
Besides , SOM can put the documents on a document map and help to access the content of similar texts . 
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:paraphrase 20:20:preserved

We study the method for calculating the distance between two graphs based on the maximal common sub-graph . 
We study the method for calculating the distance between two graphs based on the maximal common subgraph , 
29 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16,17:16,17:typo

We use the maximal frequent sub-graph discovery algorithm for discover-ing the maximal common sub-graph . 
which is determined by the maximal frequent subgraph 
30 0,1,5,6,7,8,9,10,11,12,14:0,1,2,3:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 13:7:typo

The maximal common sub-graph is calculated by using the maximal frequent subgraph with 100% support . 
discovery algorithm with 100% support . 
31 0,1,2,3,4,5,6,7,8,9,10,11:0,1:paraphrase 12:2:preserved 13:3:preserved 14:4:preserved 15:5:preserved

The method of adjustment of the weighted graph on the nodes of SOM output layer is based on the weighted means graph and genetic algorithm . 
The method of adjustment of the weighted graph on the nodes of SOM output layer is based on the weighted means graph and genetic algorithm . 
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

The remainder of this paper is organized as follows : 
The remainder of this paper is organized as follows : 
33 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

2 ) Using graph model for representing the documents 
2 ) Using graph model for representing the documents 
34 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

3 ) SOM neural network
3 ) SOM neural network
35 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

4 ) Using SOM for clustering graphs and extracting main ideas from documents 
4 ) Using SOM for clustering graphs and extracting main ideas from documents 
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

5 ) Experiment and discussions 
5 ) Experiment and discussions 
37 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

6 ) Conclusion and future work
and 6 ) Conclusion and future work
38 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved

We introduce two approaches of using graph for document representation . 
We introduce two approaches of using graph for document representation . 
43 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The first one is the approach of J. Tomita et al.</CITE> . 
The first approach , developed by Tomita et al.</CITE> 
44 0:0:preserved 1:1:preserved 2,3,4,6,11:3,4,5:paraphrase 5:2:preserved 8,7:6:paraphrase 9:7:preserved 10:8:preserved

They used subject graph for representing document . 
uses subject graphs for representing documents . 
45 0,1:0:bigrammar-vtense 2:1:preserved 3:2:bigrammar-nnum 4:3:preserved 5:4:preserved 6:5:bigrammar-nnum 7:6:preserved

In this approach , the graph is created by following steps :
In this approach , the graph is created by the following steps :
46 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved :9:mogrammar-det

− Extracting the frequent terms in the document .
− Extracting the frequent terms in the document .
47 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

− Calculating the significance from the co-occurrence frequency of two terms in a unit of document such as sentence , paragraph … .
− Calculating the significance from the co-occurrence frequency of the two terms in a unit of document such as sentence , paragraph … .
48 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved :9:mogrammar-det

If the co-occurrence frequency of two terms is greater than a threshold , we create an edge to connect these terms .
If the co-occurrence frequency of two terms is greater than a threshold , we create an edge to connect these terms .
49 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

An example of subject graph for document representation is shown in figure 1 .
An example of the subject graph for document representation is shown in figure 1 .
50 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :3:mogrammar-det

The second one is the approach of Adam Schenker and Mark Last </CITE> . 
The second one is the approach developed by Adam Schenker and Mark Last </CITE> . 
51 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:bigrammar-prep 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

In this approach , each term appearing in the document , except for stop words such as “ a ” , “ the ” , “ of ”  , “ and ” , “ or ” … which contain little information becomes a node in the graph representing the document . 
In this approach , each term appearing in the document , becomes a node , except for stop words such as “ a ” , “ the ” , “ of ”  , “ and ” , and “ or ” which contain little information . 
52 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:15:preserved 12:16:preserved 13:17:preserved 14:18:preserved 15:19:preserved 16:20:preserved 17:21:preserved 18:12:preserved 19:23:preserved 20:14:preserved 21:25:preserved 22:26:preserved 23:27:preserved 24:24:preserved 25:29:preserved 26:30:preserved 27:31:preserved 28:28:preserved 29:33:preserved 30:34:preserved 31:35:preserved 32:32:preserved 33:38:preserved 34:39:preserved 35:40:preserved 36,44,45,46,47,48,49:36,37:paraphrase 37:41:preserved 38:42:preserved 39:43:preserved 40:44:preserved 41:11:preserved 42:22:preserved 43:13:preserved 50:45:preserved

This is accomplished by labeling each node with the term it represents . 
This is accomplished by labeling each node with the term it represents . 
53 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

They create only a single node for each word even if a word appears more than once in the text . 
Each word is represented by only a single node even when the word appears more than once in the text . 
54 0,1,6,7,10:0,2,3,4,10:paraphrase 2:5:preserved 3:6:preserved 4:7:preserved 5:8:preserved 8:1:preserved 9:9:preserved 11:11:bigrammar-det 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Thus each node in the graph represents a unique word . 
Thus each node in the graph represents a unique word . 
55 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

This node is labeled with an unique term .
For example , this is labeled with an unique term .
56 0,1:0,1,2,3:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved

Second , if word a immediately precedes word b , somewhere in a section s of the document , then there is a directed edge from the node corresponding to a to the node corresponding to b with the label s . 
In the graph-based model , the authors create the graph by representing each word , except stop words, in the document as one unique node in the graph , regardless of how many times it appears in the text . The direction from one node to another corresponds to the relative positions of the words , and is illustrated by a directed edge starting from the node representing the preceding word . 
57 0,2,5,6,8,10,12,13,14,19,20,22,28,30,34,35,36,37,39,40:0,2,3,6,7,9,10,11,12,15,16,17,21,22,23,25,26,27,29,31,32,33,34,35,36,37,38,40,41,43,46,47,49,50,51,52,53,54,55,56,58,59,63,64,65,66,67,68,69,71:paraphrase 1:4:preserved 3:13:preserved 4:60:preserved 7:70:preserved 9:14:preserved 11:18:preserved 15:30:preserved 16:1:preserved 17:20:preserved 18:28:preserved 21:57:preserved 23:61:preserved 24:62:preserved 25:42:preserved 26:5:preserved 27:24:preserved 29:45:preserved 31:48:preserved 32:8:preserved 33:44:preserved 38:19:preserved 41:39:preserved

Section they have defined are as title , which contains the text related to the document ‘s title and any provided keywords; link  , which is text appearing in hyperlink on the document; and text  , which comprises any of the readable text in the document . 
The edges are then labeled according to the section that contains the words . These sections include title , link or the clickable hyperlinks , and the remaining  called text . Nodes corresponding infrequent words are excluded from the graph .
58 0,1,2,3,5,8,12,15,16,17,19,20,21,24,25,26,27,28,29,30,32,33,34,35,36,37,38,39,41,42,43,45:0,1,3,4,5,8,9,12,14,15,16,20,22,23,27,28,30,31,32,33,34,35,36,37,39,40:paraphrase 4:2:preserved 6:17:preserved 7:18:preserved 9:10:preserved 10:7:preserved 11:29:preserved 13:6:preserved 14:11:preserved 18:25:preserved 22:19:preserved 23:24:preserved 31:21:preserved 40:26:preserved 44:38:preserved 46:13:preserved

An example of directed graph representation is given in figure 2 . 
An example of directed graph representation is given in figure 2 . 
59 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

The oval indicates nodes and their corresponding term labels , the edges are labeled according the title ( TI ) , link ( L ) or text ( TX ) . 
The oval indicates nodes and their corresponding term labels , the edges are labeled according the title ( TI ) , link ( L ) or text ( TX ) . 
60 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

We also use a semantic edge called TS ( text similarity ) for connecting two similar terms . 
We also use a semantic edge called TS ( text similarity ) for connecting two similar terms . 
61 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

For example board and plank are two similar meaning terms , they mean a piece of wood . 
For example , " board " and " plank " both means " a piece of wood " . 
62 0:0:preserved 1:1:preserved 2:4:preserved 3:6:preserved 4:8:preserved 5,6,7,8,9,11,12:3,5,7,9,10,11,12,17:paraphrase 10:2:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved

We used Wordnet for measuring word similarity </CITE> .
We used Wordnet for measuring word similarity </CITE> .
63 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

The second approach is richer than the first approach . 
The second approach is superior to the first . 
64 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,8:4,5:paraphrase 6:6:preserved 7:7:preserved 9:8:preserved

Suppose that we have two sentences . 
Suppose that we have two sentences : 
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-others

The first sentence is “ Cat eats mouse ” , the second one is “ Mouse eats cat ” . 
" Cats eat mice " and " Mice eats cats " . 
66 0,1,2,3,4,8,9,10,11,12,13,14,18:0,4,5,6,10:paraphrase 5:1:bigrammar-nnum 6:2:bigrammar-inter 7:3:bigrammar-nnum 15:7:bigrammar-nnum 16:8:preserved 17:9:bigrammar-nnum 19:11:preserved

In the first approach , two sentences are the same meaning because it does not care the position of words in sentence . 
The first approach fails to distinguish the meanings of these sentences beacuse it ignores the positions of the words ,
67 0,5,7,9,11,13,14,15,20,21,22:3,4,5,9,11,13,8:paraphrase 1:0:preserved 2:1:preserved 3:2:preserved 4:19:preserved 6:10:preserved 8:6:preserved 10:7:bigrammar-nnum 12:12:preserved 16:14:preserved 17:15:bigrammar-nnum 18:16:preserved 19:18:preserved :17:mogrammar-det

But in second approach , these sentences are not the same meaning . 
while according to the second approach ,
68 0,1,5,6,7,8,10,11,12:0,1,2:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 9:3:preserved

This is true , because the position of words is expressed in the representation model . 
the word positions matter and are expressed in the representation model .
69 0,1,2,3,4:3,4:paraphrase 5:0:preserved 6,7,8:1,2:para-colocation 9:5:bigrammar-inter 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved

We used the second approach for representing the document in our proposed system .
We used the second approach for representing the document in our proposed system .
70 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

SOM has been developed by T Kohonen </CITE> . 
SOM has been developed by T Kohonen </CITE> . 
76 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

After learning phase , the SOM will create a mapping between the high dimension objects of training set with a smaller dimension clusters . 
After the learning phase , SOM will be used to create a mapping between the high dimension objects of training set with a smaller dimension clusters . 
77 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:1:preserved 5:5:preserved 6:6:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved :7,8,9:bigrammar-others

In our system , we use the 2D SOM , because it is suitable for placing 2D SOM output layer on the computer display .
In our system , we use the 2D SOM , because it is suitable for placing 2D SOM output layer on the computer display .
78 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

In SOM neural network structure , there is a weight expressing the measure of a link between the input and the output . 
In SOM neural network structure , there is a weight expressing the measure of a link between the input and the output . 
83 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

The learning process will adjust these weights based on the training data set . 
The learning process will adjust these weights based on the training data set . 
84 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The result of this learning process will create the clusters of similar documents in the nodes of SOM output layer . 
The result of this learning process will create the clusters of similar documents in the nodes of SOM output layer . 
85 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

The learning pattern will belong to the cluster with minimum distance to this cluster . 
The learning pattern will belong to the cluster with minimum distance to this cluster . 
86 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The traditional learning algorithm of SOM neural network is listed as follows :
The traditional learning algorithm of SOM neural network is listed as follows :
87 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Step 1 : Randomly initialize the weight of SOM output layer
Step 1 : Randomly initialize the weight of SOM output layer
88 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Initialize N c( t ) ( the radius of neighboring area ) and set time t=1
Initialize N c( t ) ( the radius of neighboring area ) and set time t=1
89 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Step 2 : Present an input vector v( t ) and normalize the input vector v( t )
Step 2 : Present an input vector v( t ) and normalize the input vector v( t )
90 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Calculate the Euclidean distance ( dE  ) from input vector v( t ) to all weight vectors of all nodes in SOM output layer and choose the neuron with the minimum distance from input vector v( t ) to the weight vector ( winner ) as follows :
Calculate the Euclidean distance ( dE  ) from input vector v( t ) to all weight vectors of all nodes in SOM output layer and choose the neuron with the minimum distance from input vector v( t ) to the weight vector ( winner ) as follows :
91 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved

Where i , j is the valid index which is established base on the size of SOM output layer .
Where i , j is the valid index which is established base on the size of SOM output layer .
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Step 3 : Update the weight of nodes in the neighboring area of winner ( ic , jc ) by using the following formula :
Step 3 : Update the weight of nodes in the neighboring area of winner ( ic , jc ) by using the following formula :
94 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Where γ is a constant determining the learning rate 0 ≤ γ ≤ 1 and </Eq>
Where γ is a constant determining the learning rate 0 ≤ γ ≤ 1 and </Eq>
96 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Step 4 : Update t = t + 1 , present next input vector and go back to step 2 until satisfying the convergence criteria or exceeding the maximum number of iterations .
Step 4 : Update t = t + 1 , present next input vector and go back to step 2 until satisfying the convergence criteria or exceeding the maximum number of iterations .
97 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

A graph G = ( V ,E ) , consists of a set of vertices </Eq> , and a set of edges </Eq> . 
A graph G = ( V ,E ) , consists of a set of vertices </Eq> , and a set of edges </Eq> . 
104 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

Let Lv and LE be the set of vertex and edge labels , respectively , and let V  : </Eq> be the labeling functions that assign labels to each vertex and edge . 
Let Lv and LE be the set of vertex and edge labels , respectively , and let V  : </Eq> be the labeling functions that assign labels to each vertex and edge . 
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

The size of a graph G , denoted </Eq> is the cardinality of the edge set ( i.e. , </Eq> ) . 
The size of a graph G , denoted </Eq> is the cardinality of the edge set ( i.e. , </Eq> ) . 
106 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

A graph G1 = ( V1 ,E1 ) is a sub-graph of another graph G2 = ( V2 ,E2 ) , denoted </Eq> , if there exists a 1-1 mapping </Eq> , such that </Eq> implies </Eq> . 
A graph G1 = ( V1 ,E1 ) is a sub-graph of another graph G2 = ( V2 ,E2 ) , denoted </Eq> , if there exists a 1-1 mapping </Eq> , such that </Eq> implies </Eq> . 
107 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved

Further , f preserves vertex labels , i.e. , </Eq> , and preserves edge labels , i.e. ,</Eq> . 
Further , f preserves vertex labels , i.e. , </Eq> , and preserves edge labels , i.e. ,</Eq> . 
108 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

f is also called a sub-graph isomorphism from G1 to G2 . 
f is also called a sub-graph isomorphism from G1 to G2 . 
109 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

If </Eq> , we also say that G2 is a super-graph of G1 . 
If </Eq> , we also say that G2 is a super-graph of G1 . 
110 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Note also that two graphs G1 and G2 are isomorphic </Eq> . 
Note also that two graphs G1 and G2 are isomorphic </Eq> . 
111 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Let D be a set of graphs , then we write </Eq> . 
Let D be a set of graphs , then we write </Eq> . 
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

G is said to be a maximal common sub-graph of </Eq> , and </Eq> , such that </Eq> . 
G is said to be a maximal common sub-graph of </Eq> , and </Eq> , such that </Eq> . 
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Let D be a database ( a set ) of graphs . 
Let D be a database ( a set ) of graphs . 
114 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

The support of a graph G in D denoted </Eq> is ratio of number of graphs of D containing graph G and </Eq> . 
The support of a graph G in D denoted </Eq> is ratio of number of graphs of D containing graph G and </Eq> . 
115 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

Graph G is called frequent if </Eq> , where minsup is a user-specified minimum support threshold . 
Graph G is called frequent if </Eq> , where minsup is a user-specified minimum support threshold . 
116 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Let </Eq> be the set of frequent graphs of D for a given support minsup . 
Let </Eq> be the set of frequent graphs of D for a given support minsup . 
117 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

A graph </Eq> is said to be maximal frequent graph if there exists no graph </Eq> such that </Eq> </CITE> .
A graph </Eq> is said to be maximal frequent graph if there exists no graph </Eq> such that </Eq> </CITE> .
118 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

The input data of a SOM is a set of graph representing the documents . 
The input data of a SOM is a set of graphs representing the documents . 
123 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-nnum 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

After training SOM , the input graphs will be grouped into nodes on the SOM output layer </CITE> .
After the SOM is being trained , the input graphs will be grouped into nodes on the SOM output layer </CITE> .
124 0:0:preserved 1:3,4,5:para-passact 2:2:preserved 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved :1:mogrammar-det

Each neuron on the SOM layer output is a weighted graph . 
Each neuron on the SOM layer output is a weighted graph . 
125 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

This graph is initialized based on the input value of SOM neural network .
This graph is initialized based on the input value of SOM neural network .
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

C . Distance between two graphs H Bunke </CITE> proposed a formula for calculating the distance between two graphs . 
C . Distance between two graphs H Bunke </CITE> proposed a formula for calculating the distance between two graphs . 
127 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Given graph G1 and G2 , the distance of two graphs G1 and G2 denoted as d( G1 ,G2 ) is calculated as follows :
Given graph G1 and G2 , the distance of two graphs G1 and G2 denoted as d( G1 ,G2 ) is calculated as follows :
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Where “ mcs ” is the maximal common graph and </Eq> is the size of graph G ( number of vertices and edges of graph G ) .
Where “ mcs ” is the maximal common graph and </Eq> is the size of graph G ( number of vertices and edges of graph G ) .
130 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

Consider figure 3.a and 3.b as follows :
Consider figure 3.a and 3.b as follows :
131 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Figure ( 3.a ) is graph G1 , and figure ( 3.b ) is graph G2  , figure ( 3.c ) is the maximal common graph of graph G1 and G2 . 
Figure ( 3.a ) is graph G1 , and figure ( 3.b ) is graph G2  , figure ( 3.c ) is the maximal common graph of graph G1 and G2 . 
132 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

The distance of graph G1 and G2 is : </Eq>
The distance of graph G1 and G2 is : </Eq>
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

We compute mcs(  ) by using SPIN – an algorithm for mining the maximal frequent sub-graph in graph mining </CITE> .
We compute mcs(  ) by using SPIN – an algorithm for mining the maximal frequent sub-graph in graph mining </CITE> .
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

SPIN takes two graphs as input and mines for maximal frequent sub-graphs with 100% support . 
SPIN takes two graphs as input and mines for maximal frequent sub-graphs with 100% support . 
135 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The maximal frequent sub-graph with maximal size is used to compute the size of the maximal common sub-graph in formula ( 2 ) .
The maximal frequent sub-graph with maximal size is used to compute the size of the maximal common sub-graph in formula ( 2 ) .
136 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

In the training process of SOM neural network , we need to adjust the weight of neurons laying in the neighboring area of winning neuron . 
In the training process of SOM neural network , we need to adjust the weight of neurons laying in the neighboring area of the winning neurons . 
140 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:24:preserved 24:25:bigrammar-nnum 25:26:preserved :23:mogrammar-det

When using the SOM neural network for clustering the graph , each node in the SOM layer output is a graph , we call this kind of graph as weighted graph . 
When using the SOM neural network for clustering the graph , each node in the SOM layer output is a graph , and we call this kind of graph as weighted graph . 
141 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21,22:paraphrase 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved

To adjust the weighted graph , H. Bunke </CITE> used the weighted means graph of a pair of weighted graphs . 
To adjust the weighted graph , H. Bunke </CITE> used the weighted means graph of a pair of weighted graphs . 
142 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Given two graph G1 and G2 , the graph G denotes weighted means graph of weighted graph G1 and graph G2 if there is a number </CITE> such as </Eq> , we have  :
Given two graph G1 and G2 , the graph G denotes weighted means graph of weighted graph G1 and graph G2 if there is a number </CITE> such as </Eq> , we have  :
143 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved

And </Eq> ( 4 )
And </Eq> ( 4 )
145 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

From formulas ( 3 ) and ( 4 ) , we have </Eq>
From formulas ( 3 ) and ( 4 ) , we have </Eq>
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

To adjust the weight of nodes of SOM output layer , we use formula ( 1 ) . 
To adjust the weight of nodes of SOM output layer , we use formula ( 1 ) . 
147 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

We can write formula ( 1 ) as follows : </Eq> ( 5 ) and </Eq> ( 6 )
We can write formula ( 1 ) as follows : </Eq> ( 5 ) and </Eq> ( 6 )
148 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

If we replace G1 by x , G by ynew and G2 by yold , the operator  ” - ” by the distance between two graphs , the formula ( 5 ) and ( 6 ) will be formula ( 7 ) and ( 8 ) as follows : </Eq> ( 7 ) and </Eq> ( 8 )
If we replace G1 by x , G by ynew and G2 by yold , the operator  ” - ” by the distance between two graphs , the formula ( 5 ) and ( 6 ) will be formula ( 7 ) and ( 8 ) as follows : </Eq> ( 7 ) and </Eq> ( 8 )
149 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved

If </Eq> , the formulas ( 7 ) and ( 8 ) will be formula ( 3 ) , ( 4 ) and graph G is the weighted means graph of graph G1 and graph G2 </CITE> .
If </Eq> , the formulas ( 7 ) and ( 8 ) will be formula ( 3 ) , ( 4 ) and graph G is the weighted means graph of graph G1 and graph G2 </CITE> .
150 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved

From formulas ( 7 ) and ( 8 ) , we have formula ( 9 ) as follows : </Eq>
From formulas ( 7 ) and ( 8 ) , we have formula ( 9 ) as follows : </Eq>
151 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

We used the genetic algorithm </CITE> for seeking the weighted means graph of two weight graphs G1 and G2 with the following steps  :
We used the genetic algorithm </CITE> for seeking the weighted means graph of two weight graphs G1 and G2 with the following steps  :
152 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

1 ) Overview of the genetic algorithm Genetic Algorithms ( GA ) are programs that simulate the logic of Darwinian selection . 
1 ) Overview of the genetic algorithm Genetic Algorithms ( GA ) are programs that simulate the logic of Darwinian selection . 
153 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

GA is a algorithm which makes it easy to search a large search space . 
GA is a algorithm which makes it easy to search a large search space . 
154 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The general algorithm is as follows  :
The general algorithm is as follows  :
155 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Generate initial population .
Generate initial population .
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

Assign fitness function to all individuals of population 
Assign fitness function to all individuals of population 
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Generation = 1 
Generation = 1 
158 0:0:preserved 1:1:preserved 2:2:preserved

REPEAT Select individuals from population of current generation
REPEAT Select individuals from population of current generation
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Create new off-springs with crossover operation
Create new off-springs with crossover operation
160 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Create new off-springs with mutation operation
Create new off-springs with mutation operation
161 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Compute new fitness for all individuals
Compute new fitness for all individuals
162 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Delete all the unfit individuals to give space to new off-springs
Delete all the unfit individuals to give space to new off-springs
163 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Check if best solution is found
Check if best solution is found
164 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

generation = generation + 1
generation = generation + 1
165 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

UNTIL best solution is found or generation >= MaxLoop
UNTIL best solution is found or generation >= MaxLoop
166 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

The Maxloop is user defined constant and determine the maximal generation pf genetic algorithm .
The Maxloop is user defined constant and determine the maximal generation pf genetic algorithm .
167 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

2 ) Initilalizing the population
2 ) Initilalizing the population
168 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

The graph is represented by an adjacency matrix , the vertex set of graph is </Eq> . 
The graph is represented by an adjacency matrix , the vertex set of graphs is </Eq> . 
169 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-nnum 14:14:preserved 15:15:preserved 16:16:preserved

The chromosome is a set of graph candidates of weighted means graphs ( set of matrices ) . 
The chromosome is a set of graphs candidates of weighted means graphs ( set of matrices ) . 
170 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-nnum 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Set of initial graphs are initialized randomly based of graph G1 and G2 .
Set of initial graphs are initialized randomly based of graph G1 and G2 .
171 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Crossover operation in this case will be the crossover of two matrices . 
Crossover operation in this case will be the crossover of two matrices . 
172 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The details of crossover operation are as follows :
The details of crossover operation are as follows :
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

a ) Before crossover : </Eq>
a ) Before crossover : </Eq>
174 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

b ) After crossover : </Eq>
b ) After crossover : </Eq>
175 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

3 ) Mutation operation
3 ) Mutation operation
176 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

− Select random positions i ,j in matrix representing the selected chromosome .
− Select random positions i ,j in matrix representing the selected chromosome .
177 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

− Determine the random value of 0 or 1 ( add or delete the edge of graph )
− Determine the random value of 0 or 1 ( add or delete the edge of graph )
178 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

− Assign this random value to aij and aji
− Assign this random value to aij and aji
179 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

4 ) Firness function
4 ) Fitness function
181 0:0:preserved 1:1:preserved 2:2:typo 3:3:preserved

Maximize the following function :
Maximize the following function :
182 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

The less value of this function gives the more fitness value of chromosome .
The less value of this function gives the more fitness value of chromosome .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

5 ) GA parameters
5 ) GA parameters
185 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

We used the following GA parameters : GA population size .
We used the following GA parameters : GA population size .
186 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Main ideas of documents are the sentences containing as much as the words determined by the order of occurrence on the weighted graphs . 
Main ideas of documents are the sentences containing as many words determined by the order of occurrence on the weighted graphs . 
192 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10:9:paraphrase 11::mogrammar-det 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved

Main idea is created based on the weighted graph representing a group of similar documents . 
Main idea is created based on the weighted graph representing a group of similar documents . 
193 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

A typical weighted graph of SOM output layer is shown in figure 4 .
A typical weighted graph of SOM output layer is shown in figure 4 .
194 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The Precision , Recall and F-measure are used for evaluating the result of clustering . 
The Precision , Recall and F-measure are used for evaluating the result of clustering . 
201 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The clustering to be created by human experts is called manual clustering . 
The clustering to be created by human experts is called manual clustering . 
202 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

We compare the clustering result of documents to be created by our system with the result of manual clustering .
We compare the clustering result of documents to be created by our system with the result of manual clustering .
203 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Consider a set of n documents , Let m be the number of clusters to be created from n documents by manual clustering .
Consider a set of n documents , Let m be the number of clusters to be created from n documents by manual clustering .
204 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

Methods Let k be the number of clusters in the clustering result to be created by our system . 
Methods Let k be the number of clusters in the clustering result to be created by our system . 
205 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

In evaluation process , we have m ≤ k . 
In the evaluation process , we have m ≤ k . 
206 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved :1:mogrammar-det

To evaluate the system , we use Precision , Recall and Fmeasure and calculate them by two methods .
To evaluate the system , we use Precision , Recall and Fmeasure and calculate them by two methods .
207 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Let </Eq> in figure 5 , cluster mi to be created by manual clustering is A∪B; this cluster contains a+b documents , The cluster ki to be created by our system is A∪C; this cluster contains a + c documents .
Let </Eq> in figure 5 , cluster mi to be created by manual clustering is A∪B; this cluster contains a+b documents . The cluster ki to be created by our system is A∪C; this cluster contains a + c documents .
208 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:40:bigrammar-others 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:21:preserved

Two above clusters have an intersection A which contains common documents of two clusters .
Two above clusters have an intersection A which contains common documents of two clusters .
209 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The Precision is a measure of the number of documents that match those in the manual clustering against the number of documents in the system cluster . 
The Precision is a measure of the number of documents that match those in the manual clustering against the number of documents in the system cluster . 
210 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

If P=1 then all documents in cluster ki are in cluster mi </Eq>
If P=1 then all documents in cluster ki are in cluster mi </Eq>
211 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The Recall between two cluster mi and cluster ki denoted as R ( recall ) and is calculated by formula ( 11 ) . 
The Recall between two cluster mi and cluster ki denoted as R ( recall ) and is calculated by formula ( 11 ) . 
212 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

If R =1 then all documents in cluster mi are in cluster ki </Eq>( 11 )
If R =1 then all documents in cluster mi are in cluster ki </Eq>( 11 )
213 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The Precision and Recall can be combined to F-Measure .
The Precision and Recall can be combined to F-Measure .
214 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The F-Measure is calculated by formula ( 12 ) : </Eq> ( 12 )
The F-Measure is calculated by formula ( 12 ) : </Eq> ( 12 )
215 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

High value of α gives more influence to recall while low value gives it to precision . 
High value of α gives more influence to recall while low value gives it to precision . 
216 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

A common value of α in formula ( 12 ) is 0.5 . 
A common value of α in formula ( 12 ) is 0.5 . 
217 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

This formula can be written as : </Eq> ( 13 )
This formula can be written as : </Eq> ( 13 )
218 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Brew C. </CITE> propose a method to evaluate the clustering result as follows . 
Brew C. </CITE> propose a method to evaluate the clustering result as follows . 
219 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

For each cluster in the clustering result to be created by system , we calculate the F-measure comparing with all clusters to be created by manual method . 
For each cluster in the clustering result to be created by system , we calculate the F-measure comparing with all clusters to be created by manual method . 
220 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

Then we select the maximal value of F-measure for this cluster ( each column in table 1 and table 2 ) .
Then we select the maximal value of F-measure for this cluster ( each column in table 1 and table 2 ) .
221 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

The process is repeated for the remain system clusters . 
The process is repeated for the remain system clusters . 
222 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

High value of the total of F-measure gives the accuracy of clustering system .
High value of the total of F-measure gives the accuracy of clustering system .
223 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The test set contains 500 scientific documents belonging to 5 different topics such as database , data mining , computer network , web programming , artificial intelligence . 
The test set contains 500 scientific documents belonging to 5 different topics such as database , data mining , computer network , web programming , and artificial intelligence . 
224 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:25,24:paraphrase 25:26:preserved 26:27:preserved 27:28:preserved

Each topic has 100 documents . 
Each topic has 100 documents . 
225 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

The size of SOM output layer is 8x8; The repeated cycle of training algorithm is 5 ,000; The cycle of adjusting the radius of neighboring area is 50 . 
The size of SOM output layer is 8x8; The repeated cycle of training algorithm is 5 ,000; The cycle of adjusting the radius of neighboring area is 50 . 
226 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

The directed graph is used for representing the documents .
The directed graph is used for representing the documents .
227 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

1 ) Method of clustering the vectors
1 ) Method of clustering the vectors
228 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Result : Method of manual clustering has 5 clusters , each cluster has 100 documents . 
Result : The method of manual clustering has 5 clusters , each of which has 100 documents . 
229 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12,13:paraphrase 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved :2:mogrammar-det

In this experiment , we used the vector model for representing the documents . 
In this experiment , we used the vector model for representing the documents . 
230 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Number of result clusters is 8 . 
The number of result clusters is 8 . 
231 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved :0:mogrammar-det

Comparing the results created by vector clustering and result created by manual clustering . 
We comparing the results created by vector clustering and those created by manual clustering . 
232 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:0,9:paraphrase 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

We use formulas </CITE> for calculating the Precision , Recall , F-measure .
We use formulas </CITE> for calculating the Precision , Recall , F-measure .
233 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Table 2 provides the results of calculation .
Table 2 provides the results of calculation .
234 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

The sum of maximal value of F-measure for vector clustering is 0.32 + 0.34 + 0.54 + 0.43 + 0.43 = 2.06
The sum of maximal value of F-measure for vector clustering is 0.32 + 0.34 + 0.54 + 0.43 + 0.43 = 2.06
235 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

2 ) Method of clustering the graphs
2 ) Method of clustering the graphs
236 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

In this experiment , we use graph model for representing the documents . 
In this experiment , we use the graph model for representing the documents . 
237 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:6:preserved 11:12:preserved 12:13:preserved :11:bigrammar-others

Number of result clusters is 6 . 
The number of result clusters is 6 . 
238 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved :0:mogrammar-det

Comparing the results created by graph clustering and result created by manual clustering . 
We comparing the results created by graph clustering and those created by manual clustering . 
239 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:0,9:paraphrase 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

We use formulas </CITE> for calculating the Precision , Recall , F-measure . 
We use formulas </CITE> for calculating the Precision , Recall , F-measure . 
240 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Table 2 provides the results of calculation .
Table 2 provides the results of calculation .
241 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Discussion : We test our proposed solution with many data sets and calculate the sum of max value of F-measure , we hold that the sum of max of F-Measure for graph cluster is higher PC Man than the sum of max of F-measure for vector clustering . 
Discussion : We test our proposed solution with many data sets and calculate the sum of max value of F-measure , we hold that the sum of max of F-Measure for graph cluster is higher PC Man than the sum of max of F-measure for vector clustering . 
242 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved

This result encourages us to continue developing the method of using the graph for representing the documents .
This result encourages us to continue developing the method of using the graph for representing the documents .
243 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

The sum of maximal value of F-measure for graph clustering is 0.54+0.32+0.68+0.56+0.54=2.64
The sum of maximal value of F-measure for graph clustering is 0.54+0.32+0.68+0.56+0.54=2.64
244 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Comparing to the vector based clustering algorithm , our proposed algorithm still remains the core parts of the SOM learning algorithm . 
Comparing to the vector - based clustering algorithm , our proposed algorithm still remains the core parts of the SOM learning algorithm . 
248 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved :4:bigrammar-others

Our proposed algorithm only changes the formula of calculation the distance between the input patterns and the weighted graphs and the adjustment way of weighted graphs .
Our proposed algorithm only changes the formula of calculating the distance between the input patterns and the weighted graphs and the adjustment way of weighted graphs .
249 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:bigrammar-wform 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

The time complexity of our proposed algorithm focuses on the time complexity of calculation the distance between two graphs and the calculation of weighted means graph based on the genetic algorithm . 
The time complexity of our proposed algorithm focuses on the time complexity of calculation the distance between two graphs and the calculation of weighted means graph based on the genetic algorithm . 
250 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

Figure 6 provides the chart to compare the average processing time of two algorithms with test set containing 100 , 150 , 200 , 250 , 300 , 350 ,400 , 450 , 500 documents . 
Figure 6 provides the chart to compare the average processing time of two algorithms with test set containing 100 , 150 , 200 , 250 , 300 , 350 ,400 , 450 , 500 documents . 
251 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

The computer to be used for testing is PC Pentium 4 , 3GB with 500 M byte RAM .
The computer to be used for testing is PC Pentium 4 , 3GB with 500 M byte RAM .
252 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

The value in this chart in figure 6 holds that our proposed algorithm has average processing time estimated 1.7 times higher than the vector based clustering algorithm . 
The value in this chart in figure 6 holds that our proposed algorithm has average processing time estimated 1.7 times higher than the vector based clustering algorithm . 
253 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

However , as we mentioned above the accuracy of our proposed methods is higher than the vector based clustering graph . 
However , as we mentioned above the accuracy of our proposed methods is higher than the vector based clustering graph . 
254 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Moreover , it also open a new way to improve the quality of document clustering by using the SOM neural network .
Moreover , it also open a new way to improve the quality of document clustering by using the SOM neural network .
255 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

In this paper , we would like to present the result of building a graph based clustering system by using the SOM neural networks . 
In this paper , we present the result of building a graph - based clustering system by using the SOM neural networks . 
261 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7::unaligned 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 15,14:13,11,12:typo 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved

We use the graph model for representing the documents . 
We use the graph model for representing the documents . 
262 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The graph model can represent the structural information of documents such as semantic relation of words , position of words in documents , concepts implicit present in documents . 
The graph model can represent the structural information of documents such as the semantic relation of words , the position of words in documents , and concepts implicitly present in documents . 
263 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24,25:paraphrase 23:26:preserved 24:27:bigrammar-wform 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved :12:mogrammar-det :18:mogrammar-det

After clustering the document , on the SOM output layer are the weighted graph . 
After clustering the document , on the SOM output layer are the weighted graph . 
264 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

These weighted graph contain the words which help to choose the main ideas form set of documents . 
These weighted graph contain the words which help to choose the main ideas form a of documents . 
265 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:bigrammar-det 15:15:preserved 16:16:preserved 17:17:preserved

We choose the maximal common sub-graph to calculate the distance between graphs . 
We choose the maximal common sub-graph to calculate the distance between graphs . 
266 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The approach of maximal frequent sub-graph with 100% support is used to find the maximal common sub-graph . 
The approach of maximal frequent sub-graph with 100% support is used to find the maximal common sub-graph . 
267 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

To adjust the weighted graphs on the nodes of SOM output layer , we use the weighted means graph concept and genetic algorithms . 
To adjust the weighted graphs on the nodes of the SOM output layer , we use the weighted means graph concept and genetic algorithms . 
268 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:9:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved :16:mogrammar-det

We test our propose methods on the corpus of Vietnamese articles and analyze the results . 
We test our proposed methods on the corpus of Vietnamese articles and analyze the results . 
269 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-vtense 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

We continue to do the research in calculating the distance between graphs and the way to reduce the time complexity of our proposed algorithm . 
We continue to do the research in calculating the distance between graphs and the way to reduce the time complexity of our proposed algorithm . 
270 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

We also study the capability of using the conceptual graph for representing documents and enhancing the richness of document representation model .
We also study the capability of using the conceptual graph for representing documents and enhancing the richness of document representation model .
271 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Acknowledgment
Acknowledgment
272 0:0:preserved

The authors would like to express our thanks to Ministry of Science and Technology for financial support to Fundamental research project numbered 202806 and the valuable comments of reviewers .
The authors would like to express our thanks to Ministry of Science and Technology for financial support to Fundamental research project numbered 202806 and the valuable comments of reviewers .
273 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

