A Text Segmentation Based Approach to Video Shot Boundary Detection
A Text Segmentation Based Approach to Video Shot Boundary Detection
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .
Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle various transition types caused by photo flashes , rapid camera movement and object movement is still challenging .
Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle the various transition types caused by photo flashes , rapid camera movement , and object movement is still challenging .
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:40:preserved 39:41:preserved 40:42:preserved :23:mogrammar-det

In this paper , we present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing .
We present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing . //detecting / determining?
12 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33:1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,0:preserved

By the formulation that each frame is considered as a word and shot boundaries are treated as boundaries of text segments ( e .g topics ) .
This is possible by assuming that each frame is a word and then the shot boundaries are treated as text segment boundaries ( e.g. topics ) .
15 0,1,2:0,1,2,3,4:paraphrase 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 9:9:preserved 10:10:preserved 11:11,12:para-freeword 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17,18,19,20:21,19,20:para-colocation 21:22:preserved 22,23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved :13:mogrammar-det

Text segmentation based approaches that have been well studied in natural language processing can be adopted .
The text segmentation based approaches in natural language processing can be used .
18 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 10,11,9,12,13,14:6,7,8,5,9,10:preserved 15:11:paraphrase :0:mogrammar-det

Experimental results on various long video sequences show the effectiveness of our approach .
The experimental results from various long video sequences have proven the effectiveness of our approach .
21 0:1:preserved 1:2:preserved 2:3:bigrammar-prep 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8,9:paraphrase 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved :0:mogrammar-det

Recent advances in digital technology have made many video archives available .
Recent advances in digital technology have made many video archives readily available .
26 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11,10:paraphrase 11:12:preserved

Therefore scalable , efficient and effective tools for indexing and retrieving video are needed .
Therefore scalable , efficient , and effective tools for indexing and retrieving video are needed .
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as continuous frames from a single camera at a time .
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:37:preserved 37:38,36:bigrammar-nnum 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:45,46:paraphrase 45:47:preserved

By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
29 0:0:preserved 1:1,2:paraphrase 2:3:preserved 3:4:preserved 4:5:preserved 5:7,6:paraphrase 6:8:preserved 7:9:preserved 8:10:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:paraphrase 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved 33:36:bigrammar-others 34:37:preserved 35:38:preserved 36:39:preserved 37:40:preserved 38:41:preserved 39:42:preserved 40:43:preserved

There are many types of transitions between shots .
There are many types of transitions between shots .
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

According to TRECVID 's categorization \CITE , shot boundaries can be classified into two main categories : cut and gradual .
According to TRECVID 's categorization \CITE , shot boundaries can be classified into two main categories : cut and gradual .
33 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs in a number of consecutive frames .
A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs over a number of consecutive frames .
34 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:bigrammar-prep 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

With the gradual type , fades and dissolves are common .
With the gradual type , fades and dissolves are common .
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

A fade is usually a change in brightness with one or several solid black frames in between , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
A fade is usually a change in brightness with one or several solid black frames in between the key frames , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
37 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16,17,18,19:paraphrase 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37:preserved 35:38:preserved 36:39:preserved 37:40:preserved 38:41:preserved 39:42:preserved 40:43:preserved 41:44:preserved

Figure \REF shows examples of shot boundary types .
Figure \REF shows examples of shot boundary types .
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Many approaches have been proposed for shot boundary detection .
Many approaches have been proposed for shot boundary detection .
41 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The simplest approach is to compute the differences between color distributions of consecutive frames and use a threshold to classify whether a hard cut occurs .
The simplest approach is to compute the differences between the color distributions of consecutive frames and use a threshold to classify whether a hard cut occurs .
42 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :9:mogrammar-det

In order to detect gradual transitions , edge change ratio or motion vectors can be used \CITE .
In order to detect gradual transitions , edge change ratios or motion vectors can be used \CITE .
43 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-nnum 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Since these approaches use threshold-based models for detection , their advantage is fast speed .
Since these approaches use threshold-based models for detection , their advantage is they are fast .
44 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12,13:12,13,14:paraphrase 14:15:preserved

Nevertheless , they are sensitive to changes of illumination and motion .
Nevertheless , they are sensitive to changes in illumination and motion .
45 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-prep 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Furthermore , they are difficult to generalize for new datasets .
Furthermore , they are difficult to generalize for new datasets .
46 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Recent works \CITE use machine learning methods for making decision and show impressive results on test videos of TRECVID \CITE which is a de-facto benchmark for evaluation of various techniques in shot boundary detection .
Recent works \CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .
49 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-nnum 10:10:preserved 11:11,12:paraphrase 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26,27:29:paraphrase 28:31:preserved 29:32,33:paraphrase 30,31,32,33:34,35,36,37:preserved :16:mogrammar-det :30:mogrammar-det

In this study , we propose a new approach inspired from natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem of text segmentation .
In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .
52 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:11:preserved 10:12:bigrammar-prep 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:bigrammar-prep 32:35:preserved 33:36:preserved 34:37:preserved :13:mogrammar-det

Specifically , each frame is considered as a word and a set of consecutive frames , forming a shot , is considered as a text segment .
Specifically , each frame is considered a word and a set of consecutive frames , forming a shot , is considered a text segment .
53 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved

Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of labels such as
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of the following labels :
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 23,24,22:22,23,24:paraphrase

PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) and POSTSEG ( word outside a segment ) .
PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) , and POSTSEG ( word outside a segment ) .
59 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved

Given a sequence of labeled words , the boundary between text segments can be identified .
Given a sequence of labeled words , the boundary between text segments can be identified .
60 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The remaining of the paper is organized as follows .
The remainder of this paper is organized as follows .
63 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:bigrammar-det 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

In section \REF , we present an overview of our framework .
In section \REF , we present an overview of our framework .
64 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Section \REF introduces experiments on different long video sequences from TRECVID dataset .
Section \REF introduces our experiments on different long video sequences from the TRECVID dataset .
65 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved :3:mogrammar-det :11:mogrammar-det

Section \REF concludes the paper .
Section \REF concludes the paper .
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Given a video , the shot boundary detection process is carried out through two main stages .
The shot boundary detection process for a given video is carried out through two main stages .
71 0,1,2:6,7,8:preserved 4,5,6,7,8:0,1,2,3,4:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved :5:mogrammar-prep

In the first stage , frames are extracted and labeled by pre-defined labels .
In the first stage , frames are extracted and labeled with pre-defined labels .
72 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-prep 11:11:preserved 12:12:preserved 13:13:preserved

In the second stage , shot boundaries are identified by grouping labeled frames into segments .
In the second stage , the shot boundaries are identified by grouping the labeled frames into segments .
73 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved :5:mogrammar-det :12:mogrammar-det

We use the following six labels to label frames in video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , POST -GRAD ( post-frame of a GRADUAL transition ) .
We use the following six labels to label frames in a video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , and POST -GRAD ( post-frame of a GRADUAL transition ) .
76 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:45:preserved 45:46:preserved 46:47:preserved 47:48:preserved 48:49:preserved 49:50:preserved 50:51:preserved 51:52:preserved 52:53:preserved 53:54:preserved 54:55:preserved 55:56:preserved 56:57:preserved 57:58:preserved 58:59:preserved 59:60:preserved 60:61:preserved 61:63,62:bigrammar-others 62:64:preserved 63:65:preserved 64:66:preserved 65:67:preserved 66:68:preserved 67:69:preserved 68:70:preserved 69:71:preserved 70:72:preserved 71:73:preserved :10:mogrammar-det

Given a sequence of labeled frames , shot boundaries and transition types are identified by looking up and processing frames marked by non NORM -FRM label .
Given a sequence of labeled frames , the shot boundaries and transition types are identified by looking up and processing the frames marked with a non NORM -FRM label .
79 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:21:preserved 20:22:preserved 21:23:bigrammar-prep 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved :7,20:mogrammar-det :24:mogrammar-det

For example , if we encounter two consecutive frames marked by IN-CUT and POST-CUT respectively , we can declare that a shot boundary occurs at these frames and the transition type is CUT .
For example , if we encounter two consecutive frames respectively marked by IN-CUT and POST-CUT , we can declare that a shot boundary occurs at these frames and the transition type is a CUT .
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:9:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:33:preserved 33:34:preserved :32:mogrammar-det

In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare a GRADUAL shot boundary occurs at these frames .
In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare that a GRADUAL shot boundary occurs at these frames .
81 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19,18:bigrammar-others 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved

Figure \REF shows an example of labeled frames of a shot transition .
Figure \REF shows an example of the labeled frames of a shot transition .
84 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :6:mogrammar-det

To label a frame in video , firstly we extract features for that frame and then use a classifier , that has been trained by annotated frames in advance , to classify it into one of six categories mentioned above .
To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .
87 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:10:preserved 8:8:preserved 9:9,11:bigrammar-vtense 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:bigrammar-others 21:24:preserved 22:25:preserved 23:26:preserved 24:29:preserved 25,26:31,32:preserved 27:27:preserved 28:28:preserved 30,31,32,33,34,35:34,35,36,37,38,39:preserved 36,37,38,39:41,42,43,44:preserved :5:mogrammar-det :12:mogrammar-det :30:mogrammar-det :40:mogrammar-det

The feature extraction process and classifier learning using support vector machine ( SVM ) are described in details below .
The feature extraction process and classifier learning using a support vector machine ( SVM ) are described in detail below .
88 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:bigrammar-nnum 18:19:preserved 19:20:preserved :8:mogrammar-det

We use two typical features that are color moments , edge direction histogram for representing visual information of each frame .
We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .
92 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:bigrammar-others 6:7:preserved 7:9:preserved 8:10:preserved 9:11:bigrammar-others 10:12:preserved 11:13:preserved 12:14:preserved 13,14:16,17:paraphrase 15,16,17,18,19,20:19,20,21,22,23,24:preserved :8:mogrammar-det :18:mogrammar-det

However , using this representation is not discriminative enough for frame categorization since frames of a shot transition usually have strong relation to their neighbor frames .
However , using this representation is not discriminative enough for frame categorization since the frames of a shot transition usually strongly relate to their neighboring frames .
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19,20,21:20,21:paraphrase 22:22:preserved 23:23:preserved 24:24:bigrammar-wform 25:25:preserved 26:26:preserved :13:mogrammar-det

For example , an abrupt change in illumination between two consecutive frames is a strong cue for a hard cut , or one solid black frames in between dark and then bright frames might help to identify a fade shot transition .
For example , an abrupt change in illumination between two consecutive frames is a strong cue for a hard cut , or one solid black frame in between dark and then bright frames might help to identify a fade shot transition .
94 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:bigrammar-nnum 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved

Therefore , in this study , we do not directly use above features .
Therefore , in this study , we do not directly use the above features .
97 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved :11:mogrammar-det

Instead , we use them indirectly to model the difference and motion between the current frame and its neighbor frames .
Instead , we use them indirectly to model the difference and motion between the current frame and its neighboring frames .
98 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:bigrammar-wform 19:19:preserved 20:20:preserved

Specifically , for each frame , we compute \MATH distances between the current frame \MATH and neighbor frames ranging from \MATH .
In particular , for each frame , we compute \MATH distances between the current frame \MATH and neighboring frames ranging from \MATH .
99 0:1,0:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved

These distances are used to form a feature vector for frame \MATH in training and testing process later .
These distances are used to form a feature vector for frame \MATH in the training and testing process later .
100 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :13:mogrammar-det

By this way , we can have a unified framework for shot boundary detection and consequently avoid to have special treatments for different shot boundary types as described in many works participated the TRECVID benchmark \CITE .
In this way , we can have a unified framework for the shot boundary detection and consequently avoid having to give special treatment to the different shot boundary types as described in many of the works that participated the TRECVID benchmark \CITE .
101 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17,18:18:bigrammar-wform 19:21:preserved 20:22:bigrammar-nnum 21:23:bigrammar-prep 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:35:preserved 31:36,37:bigrammar-others 35,33,34,32:41,39,40,38:preserved :11:mogrammar-det :20:unaligned :24:mogrammar-det :33:mogrammar-prep :34:mogrammar-det

Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing color distributions of images \CITE .
Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing the color distributions of images \CITE .
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved :18:mogrammar-det

The first order ( mean ) , the second order ( variance ) and the third order ( skewness ) color moments are defined as :
The first order ( mean ) , the second order ( variance ) , and the third order ( skewness ) color moments are defined as :
106 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved

where \MATH is the value of the \MATH -th color component of the image pixel \MATH , and \MATH is the number of pixels in the image .
where \MATH is the value of the \MATH -th color component of image pixel \MATH , and \MATH is the number of pixels in the image .
107 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12::mogrammar-det 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved

Edge orientation histogram has also been used widely in shot boundary detection \CITE .
Edge orientation histogram has also been widely used in shot boundary detection \CITE .
110 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:6:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The basic steps to compute edge orientation histogram feature are as follows :
The basic steps for computing the edge orientation histogram features are as follows :
111 0:0:preserved 1:1:preserved 2:2:preserved 3,4:3,4:paraphrase 5:6:preserved 6:7:preserved 7:8:preserved 8:9:bigrammar-nnum 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :5:mogrammar-det

Extract edges from the input image by using Canny edge detector .
Extract edges from the input image by using Canny edge detector .
114 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Compute a \MATH -bin histogram of edge and non-edge pixels .
Compute a \MATH -bin histogram of edge and non-edge pixels .
117 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The first \MATH bins are used to represent edge directions quantized at \MATH interval and the remaining bin is used for counting non-edge pixels .
The first \MATH bins are used to represent the edge directions quantized at a \MATH interval and the remaining bin is used for counting the non-edge pixels .
118 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:25:preserved 23:26:preserved 24:27:preserved :8:mogrammar-det :13:mogrammar-det :24:mogrammar-det

The histogram is normalized by the number of all pixels to compensate for different image sizes .
The histogram is normalized by the total number of all the pixels to compensate for different image sizes .
119 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved :10:mogrammar-det

We use color moments and edge orientation histogram to compute distances between the current frame \MATH and it neighbor frames as follows :
We use color moments and an edge orientation histogram to compute the distances between the current frame \MATH and its neighboring frames as follows :
123 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:bigrammar-wform 18:20:bigrammar-wform 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved :5:mogrammar-det :11:mogrammar-det

The input image is converted to LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \MATH grid .
The input image is converted to a LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \MATH grid .
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :6:mogrammar-det

The color moments and edge orientation histogram are extracted from these sub-images .
The color moments and edge orientation histogram are extracted from these sub-images .
129 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

For color moments , there are \MATH values .
For color moments , there are \MATH values .
130 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

For edge orientation histogram , there are \MATH values for each input frame image .
For the edge orientation histogram , there are \MATH values for each input frame image .
131 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :1:mogrammar-det

Compute \MATH values which are the Euclidean distance between current frame \MATH and its neighbor frames ranging from \MATH .
Compute \MATH values , which are the Euclidean distances between the current frame \MATH and its neighboring frames ranging from \MATH .
132 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:bigrammar-nnum 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:bigrammar-wform 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved :10:mogrammar-det

In other words , we compute \MATH where \MATH .
In other words , we compute \MATH , where \MATH .
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved

These values \MATH are then used to form the feature vector for frame \MATH .
These values \MATH are then used to form the feature vector for frame \MATH .
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The Support Vector Machines ( SVM ) is a statistical learning method based on the structure risk minimization principle \CITE .
Support Vector Machines ( SVM ) are a statistical learning method based on the structure risk minimization principle \CITE .
140 0::mogrammar-det 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:bigrammar-inter 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved

It has been very efficiently proved in many pattern recognition applications \CITE .
They have been very efficiently proved to be useful in many pattern recognition applications \CITE .
141 0,1:0,1:bigrammar-inter 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6,7,8:paraphrase 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved

In the binary classification case , the objective of the SVM is to find a best separating hyperplane with a maximum margin .
In the case of binary classification , the objective of the SVM is to find the best separating hyperplane with a maximum margin .
142 0:0:preserved 1:1:preserved 2:4:preserved 3:5:preserved 4:2:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:bigrammar-det 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved :3:mogrammar-prep

The form of SVM classifiers is : \MATH
The form of the SVM classifiers is : \MATH
145 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved :3:mogrammar-det

where \MATH is the d-dimensional vector of an observation example , \MATH is a class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
where \MATH is the d-dimensional vector of an observation example , \MATH is the class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
148 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-det 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved

SVM were originally designed for binary classification .
SVMs were originally designed for binary classification .
151 0:0:bigrammar-nnum 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

To handle the case of multi-class classification , there are two common approaches .
There are two common approaches for handling multi-class classification .
152 0,1:5,6:paraphrase 5:7:preserved 6:8:preserved 8,9,10,11,12:0,1,2,3,4:preserved

The first one is the one-against-all method that combines \MATH binary classifiers where \MATH is the number of classes .
The first one is the one-against-all method that combines \MATH binary classifiers , where \MATH is the number of classes .
153 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved

The \MATH SVM classifier is trained by positive samples being examples of the \MATH class and negative samples being examples of the other classes .
The \MATH SVM classifier is trained using positive samples as examples of the \MATH class and negative samples as the examples of the other classes .
154 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:paraphrase 7:7:preserved 8:8:preserved 9:9:paraphrase 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:paraphrase 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :19:mogrammar-det

The second one is the one-against-one method that combines \MATH binary classifiers in which each classifier is trained on examples of two classes .
The second one is the one-against-one method that combines \MATH binary classifiers in which each classifier is trained on examples from the two classes .
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-prep 21:22:preserved 22:23:preserved 23:24:preserved :21:mogrammar-det

There are seven classes in our framework : NORM FRM ( frame of a normal shot ) , PRE CUT ( pre-frame of a CUT transition ) , POST CUT ( postframe
There are seven classes in our framework : NORM FRM ( frame of a normal shot ) , PRE CUT ( pre-frame of a CUT transition ) , POST CUT ( postframe
160 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) and NORM-FRM ( normal frame which does not belong to any shot transitions ) .
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) , and NORM-FRM ( normal frame that does not belong to any shot transitions ) .
161 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:bigrammar-others 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:45:preserved 45:46:preserved 46:47:preserved 47:48:preserved 48:49:preserved

To learn this classifier , we manually annotate frames in the training data .
To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?
162 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-vtense 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Using the trained classifier , we can label a sequence of frames with tags mentioned above .
Using the trained classifier , we can label a sequence of frames with the tags mentioned above .
167 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :13:mogrammar-det

A gradual transition usually has the pattern " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' and a cut transition usually has the pattern " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " ' .
A gradual transition usually has a " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' pattern and a cut transition usually has a " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " 'pattern .
168 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-det 6:31:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved 31:30:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:bigrammar-det 39:61:preserved 40:39:preserved 41:40:preserved 42:41:preserved 43:42:preserved 44:43:preserved 45:44:preserved 46:45:preserved 47:46:preserved 48:47:preserved 49:48:preserved 50:49:preserved 51:50:preserved 52:51:preserved 53:52:preserved 54:53:preserved 55:54:preserved 56:55:preserved 57:56:preserved 58:57:preserved 59:58:preserved 60:59:preserved 61:60:preserved 63:62:preserved

The shot boundary detection process is started by checking these transition patterns in the tagged sequence .
The shot boundary detection process is started by checking for these transition patterns in the tagged sequence .
169 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :9:mogrammar-prep

Once a pattern is encountered , PRE-xxx and POST-xxx tags are used to identify the shot boundary and the two ends of the shot transition .
Once a pattern is encountered , PRE-xxx and POST-xxx tags are used to identify the shot boundary and the two ends of the shot transition .
170 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Since the classifier occasionally produce false predictions due to variations caused by photo flashes , rapid camera movement and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many truth shot boundaries .
Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-inter 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:40,38,39:paraphrase 38,37:42,41:preserved :9:mogrammar-det

Instead , we use a more flexible matching algorithm in which a match is declared if a portion of the predefined pattern is found in the input sub-sequence .
Instead , we use a more flexible matching algorithm in which a match is declared if a portion of the predefined pattern is found in the input sub-sequence .
174 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

We used annotated data sets from TRECVID 2003 test sets for training and testing .
We used annotated data sets from the TRECVID 2003 test sets for the training and testing .
181 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved :6:mogrammar-det :12:mogrammar-det

We divided 8 videos , each 30-minute length , into two sets : training set and testing set .
We divided eight videos , each 30-minute long , into two sets : a training set and a test set .
182 0:0:preserved 1:1:preserved 2:2:spelling 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-wform 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:18:paraphrase 17:19:preserved 18:20:preserved :13:mogrammar-det :17:mogrammar-det

The number of frames , the number of shot boundaries and types of these sets are shown in Table \REF .
The number of frames , the number of shot boundaries , and the types of these sets are shown in Table \REF .
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved :12:mogrammar-det

Note that , the number of shot boundaries is equal to the number of frames with PRE-CUT / GRAD label and the number of frames with PRE-CUT / GRAD label is equal to the number of frames with POST-CUT / GRAD label .
Note that , the number of shot boundaries is equal to the number of frames with a PRE-CUT / GRAD label and the number of frames with a PRE-CUT / GRAD label is equal to the number of frames within a POST-CUT / GRAD label .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:bigrammar-prep 38:41:preserved 39:42:preserved 40:43:preserved 41:44:preserved 42:45:preserved :16:mogrammar-det :27:mogrammar-det :40:mogrammar-det

We used \MATH grid for dividing the input image into sub-images .
We used \MATH grid to divide the input image into sub-images .
189 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:4,5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

As for edge orientation histogram , we used 12-bins for edge pixels and one bin for non-edge pixels .
As for the edge orientation histogram , we used 12-bins for the edge pixels and one bin for the non-edge pixels .
190 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:19:preserved 17:20:preserved 18:21:preserved :2:mogrammar-det :11:mogrammar-det :18:mogrammar-det

Furthermore , we used 20 neighbor frames before and after the current frame ( \MATH ) for computing the distances .
Furthermore , we used 20 neighboring frames before and after the current frame ( \MATH ) for computing the distances .
191 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-wform 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

These parameters were selected from our empirical studies when participating TRECVID 's tasks .
These parameters were selected from our empirical studies when participating in TRECVID 's tasks .
192 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :10:mogrammar-prep

The extracted features are normalized to zero mean and unit standard deviation and then stored for training and testing .
The extracted features are normalized to zero mean and a unit standard deviation and then stored for training and testing .
195 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :9:mogrammar-det

Specifically , the normalized vector \MATH
Specifically , the normalized vector \MATH
196 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

where \MATH is the \MATH-th element of the feature vectors \MATH respectively , \MATH is the number of dimensions .
where \MATH is the \MATH-th element of the feature vectors \MATH , respectively , and \MATH is the number of dimensions .
199 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13,14:bigrammar-others 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved

In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take the \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
204 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved :26:mogrammar-det

We use LibSVM \CITE to train SVM classifiers with RBF kernel .
We use LibSVM \CITE to train the SVM classifiers with a RBF kernel .
207 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved :6:mogrammar-det :10:mogrammar-det

The optimal \MATH parameters are found by conducting a grid search with 5-fold cross validation on a subset 10 ,000 samples stratified selected from the original dataset .
The optimal \MATH parameters are found by conducting a grid search with a 5-fold cross validation on a subset of 10 ,000 samples stratified selected from the original dataset .
208 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved :12:mogrammar-det :19:mogrammar-prep

As for multi-class classification , LibSVM used the one-against-one approach .
As for the multi-class classification , LibSVM used the one-against-one approach .
209 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved :2:mogrammar-det

The results that were evaluated by a tool provided by TRECVID with standard measurement such as precision , recall and F1 score clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
The results that were evaluated by a tool provided by TRECVID with a standard measurements , such as the precision , recall , and F1 score , clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
214 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:bigrammar-nnum 14:16:preserved 15:17:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19,20,21:23,24,25:preserved 22,23,24,25,26,27,29,28,30,31,32,33,34,35,36,37,38,39,40,41:27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46:preserved :12:mogrammar-det :18:mogrammar-det

We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process .
We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process . //for / by?
217 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Specifically , we selected three sampling rates \MATH which are \MATH and \MATH .
Specifically , we selected three sampling rates \MATH , which were \MATH and \MATH .
218 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9,8:bigrammar-others 9:10:bigrammar-vtense 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

As shown in Figure \REF , the best performance is obtained with the sampling rate of \MATH .
As shown in Figure \REF , the best performance was obtained at a sampling rate of \MATH .
221 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10:9,10:bigrammar-vtense 11:11:bigrammar-prep 12:12:bigrammar-det 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

In Table \REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
224 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:paraphrase 5:6:preserved 6:7:preserved 7:9:paraphrase 8:10:preserved 9:11:preserved 10:12:preserved 11,12:13,14:paraphrase 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:20:preserved 18:21:preserved 19,20,21,23:23,24,25,27:preserved 22:26:bigrammar-det :19:mogrammar-det :22:mogrammar-det

The first one is GCM , the second one is EOH and the last one GCM+EOH is combination of distances using GCM and distances using EOH .
The first one is GCM , the second one is EOH , and the last one GCM+EOH is a combination of the distances using GCM and the distances using EOH .
225 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:19:preserved 18:20:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 24,25,23:28,29,27:preserved :18:mogrammar-det :21:mogrammar-det :26:mogrammar-det

The number of dimensions of feature vectors using GCM and EOH is 20 while that of feature vectors using GCM+EOH is 40 .
The number of dimensions of the feature vectors using GCM and EOH was 20 , while that of feature vectors using GCM+EOH was 40 .
226 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:bigrammar-vtense 12:13:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:bigrammar-vtense 21:23:preserved 22:24:preserved :5:mogrammar-det

We also compare the proposed method with the baseline method that computes differences in color histograms between two consecutive frames and then decides a shot transitition by using a predefined threshold .
We also compared the proposed method with the baseline method that computes the differences in the color histograms between two consecutive frames , and then decides the shot transition by using a predefined threshold .
227 0:0:preserved 1:1:preserved 2:2:bigrammar-vtense 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:23,22:bigrammar-others 21:24:preserved 22:25:preserved 23::mogrammar-det 24:27:preserved 25:28:spelling 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved :12:mogrammar-det :15:mogrammar-det :26:mogrammar-det

In Figure \REF , we superimpose our result on the results reported in the shot boundary detection task of TRECVID 2003 .
In Figure \REF , we superimposed our result on the results reported in the shot boundary detection task of TRECVID 2003 .
230 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-vtense 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Our system achieves high precision and recall for the CUT transition and the result is comparable with the third-ranked system .
Our system achieves a high precision and recall for the CUT transition and this result is comparable to the third-ranked system .
231 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:bigrammar-det 13:14:preserved 14:15:preserved 15:16:preserved 16:17:bigrammar-prep 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved :3:mogrammar-det

Note that our system is general and has no special treatment for particular shot transition .
Note that our system is general and has no special treatment for particular shot transitions .
232 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:bigrammar-nnum 15:15:preserved

Many previous shot boundary detectors usually divided the system into sub-systems in which special treatments were proposed to handle different types of shot transitions .
Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .
238 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15,16:15,16:bigrammar-vtense 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Therefore , it is difficult to generalize for new test sets .
Therefore , it is generalization is difficult for new test sets .
239 0:0:preserved 1:1:preserved 2,3,4:5,6:paraphrase 5,6:4:bigrammar-wform 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved :3,2:moproblematic

Different from these approaches , in this paper , we have proposed a unified and general framework for shot boundary detection using a text segmentation based approach .
We have proposed a unified and general framework for shot boundary detection that uses a text segmentation based approach .
242 9,10,11,12,13,14,15,16:0,1,2,3,4,5,6,7:preserved 17,18,19,20:8,9,10,11:preserved 21:12,13:paraphrase 23,22,24,25,26:15,16,14,17,18:preserved

Firstly , we label frames by one of six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD and POST -GRAD .
Firstly , we label the frames with one of the six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD , and POST -GRAD .
244 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:bigrammar-prep 6:7:preserved 7:8:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37:preserved :4:mogrammar-det :9:mogrammar-det

Then we extract shot boundaries and types from these labeled frames .
Then we extract the shot boundaries and types from these labeled frames .
245 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :3:mogrammar-det

In order to label frames , we proposed a new feature type to model the difference and motion in color and edge between frames and used it in classification with SVM classifiers .
In order to label frames , we proposed a new feature type to model the difference and motion in color and the edges between the frames and used it in the classification with SVM classifiers .
248 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:22:bigrammar-nnum 22:23:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved :21:mogrammar-det :24:mogrammar-det :30:mogrammar-det

Experiments on various videos of TRECVID 2003 have shown that our approach is effective .
The experiments we conducted on various videos from TRECVID 2003 have shown that our approach is effective .
249 0:1:preserved 1:4:preserved 2:5:preserved 3:6:preserved 4:7:bigrammar-prep 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved :0:mogrammar-det

