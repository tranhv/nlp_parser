0
<document>
<document>
1
<abstract>
<abstract>
2
<p>
<p>
3
Generating short summary videos for rushes is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .
Generating short summary videos for rushes is a challenging task due to the difficulty in eliminating redundancy and determining the important objects and events to be placed in the summary .
4
Redundancy elimination is difficult since repetitive segments , which are takes of the same scene , usually have different lengths and motion patterns .
Redundancy elimination is difficult since repetitive segments , which are takes of the same scene , usually have different lengths and motion patterns .
5
This makes approaches using one keyframe for shot representation failed in doing clustering .
This makes approaches using one keyframe for a shot representation fail when trying to form a cluster .
6
In addition , even repetitive segments can be determined precisely , the summary generated by concatenating together selected segments still has longer duration than the upper limit .
In addition , even repetitive segments can be precisely determined , but the summary generated by concatenating together the selected segments still takes longer than the upper limit .
7
It is questionable to select a sub-segment so that it conveys information of the scene as much as possible .
It is questionable to select a sub-segment so that it conveys information of the scene as much as possible .
8
In this paper , we introduce two approaches to these problems .
,We introduce two approaches to solve these problems .
9
In the first approach , one keyframe is used for shot representation in doing clustering; and sub-segments are selected using motion information for generating the summary .
In the first approach , one keyframe is used for representing a shot when forming a cluster; and sub-segments are selected using the motion information for generating the summary .
10
Meanwhile , in the second approach , all frames of a shot are used for clustering; and a simple skimming method is used to select sub-segments .
Meanwhile , in the second approach , all the frames of a given shot are used for clustering; and a simple skimming method is used to select the sub-segments .
11
Experimental results on the TRECVID 2008 dataset and comparison between the two approaches are reported .
The experimental results on the TRECVID 2008 dataset and a comparison between the two approaches are also reported .
12
</p>
</p>
13
</abstract>
</abstract>
14
<section label = " Introduction ">
<section label = " Introduction ">
15
<p>
<p>
16
With the availability of multimedia databases growing at an exponential rate , users are increasingly requiring assistance in accessing digital video contents .
With the availability of multimedia databases growing at an exponential rate , users are increasingly requiring assistance in accessing digital video contents .
17
Video summarization is a significant research that helps to meet these needs by developing a condensed version of a full length digital video with the most important contents \CITE .
Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \CITE .
18
Summary videos can help users to browse and navigate large video archives efficiently and effectively .
Summary videos can help users more efficiently and effectively browse and navigate through large video archives .
19
</p>
</p>
20
<p>
<p>
21
Generating summary videos for BBC rushes \CITE is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .
Generating summary videos for BBC rushes \CITE is a challenging task due to the difficulty with redundancy elimination and determining the most important objects and events to be placed in the summary .
22
Since the length of the summary is limited to 2\% duration of the original video , there is a trade-off between recall and usability ( e.g user friendly through smooth presentation , being easy to understand ) .
Since the length of the summary is limited to 2\% duration of the original video , there is a trade-off between the recall and usability ( e.g. user friendly through smooth presentation , / being easy to understand ) .
23
High recall , i.e many objects and events ( called scenes ) are included in the summary , usually reduce the number of frames for each scene .
High recall , i.e. many objects and events ( called scenes ) included in the summary , usually reduces the number of frames for each scene .
24
For example , the maximum duration for the summary of a 30 minute length video is 36 seconds ( \MATH ) .
For example , the maximum duration for a summary of a 30 minute video is 36 seconds ( \MATH ) .
25
If the summary consists of 20 scenes , the average duration for each scene is 1.8 seconds .
If the summary consists of 20 scenes , the average duration for each scene is 1.8 seconds .
26
For the event such as " `Woman attacks man on bench on left and runs off with large bag .
For an event such as " `Woman attacks man on bench on left and runs off with large bag .
27
" ', with this length constraint , it is difficult to present it in a pleasant tempo and rhythm .
" ', with this length constraint , it would be difficult to present it in a pleasant tempo and rhythm .
28
On the contrary , smooth presentation of events consumes a lot number of frames , that decrease the recall .
On the other hand , a smooth presentation of these events would consume a large number of frames , which would decrease the recall .
29
</p>
</p>
30
<p>
<p>
31
In general , generating summary videos consists of the following steps :
In general , generating summary videos consists of the following steps :
32
Video segmentation : This step decomposes the original video into segments , such shots or sub-shots .
Video segmentation : This step breaks down the original video into segments , such as shots or sub-shots .
33
Each segment should be aligned such that it is a part of a scene .
Each segment should be aligned so that it is a part of a scene .
34
Redundancy elimination : This step groups segments that belong to the same take into clusters .
Redundancy elimination : This step groups the segments that belong to the same take into clusters .
35
Only one representative segment is used for the final summary video .
Only one representative segment is used for the final summary video .
36
The others are discarded .
The others are discarded .
37
</p>
</p>
38
<p>
<p>
39
Junk elimination : This step removes color bars , clapboards , all black or all white frames that are unnecessary for the final summary video .
Junk elimination : This step removes the color bars , clapboards , and the all black or all white frames that are unnecessary in the final summary video .
40
Summary generation : This step selects frames from representative segments of clusters and concatenate to form the final summary video .
Summary generation : This step selects the frames from the representative segments of clusters and concatenates them to form the final summary video .
41
</p>
</p>
42
<p>
<p>
43
While the steps of video segmentation and junk elimination are easy to handle , the steps of redundancy elimination and summary generation are difficult .
While the steps for video segmentation and junk elimination are easy to handle , the steps for redundancy elimination and summary generation are difficult .
44
For example , as for redundancy elimination , the question is how to represent a segment into a feature vector and how to compute the similarity between two segments having different length and motion pattern .
For example , as for redundancy elimination , the question is how to represent a segment in a feature vector and how to compute the similarity between two segments having different lengths and motion patterns .
45
In the other case , assume that we have selected appropriate segments , the total length of these segments are usually larger than that of the final summary .
In the other case , assuming that we have selected the appropriate segments , the total length of these segments is usually larger than that of the final summary .
46
The question is how to determine the important part of the selected segment such that it conveys information of the scene as much as possible .
The question is how to determine the most important parts of the selected segments so that they convey as much of the information of the scene as possible .
47
</p>
</p>
48
<p>
<p>
49
In this paper , we present two approaches for handling these difficult steps .
In this paper , we present two approaches for handling these difficult steps .
50
The first approach represents each segment by one key-frame and groups similar segments by doing clustering on these key-frames .
The first approach represents each segment by using one key-frame and groups similar segments by clustering them on these key-frames .
51
Then the portion of each segment that has high motion is used to include into the final summary .
Then the portion of each segment that has the highest motion is included in the final summary .
52
Meanwhile , the second approach uses another strategy for redundancy elimination .
Meanwhile , the second approach uses another strategy for redundancy elimination .
53
Specifically , for each segment , a set of frames are extracted by sampling at a certain time interval ( e.g 5 frames ) .
Specifically , for each segment , a set of frames are extracted by sampling at a certain time interval ( e.g. 5 frames ) .
54
The clustering process is performed on the frames of all segments .
The clustering process is performed on the frames of all the segments .
55
Then , the segments that share a large enough number of frames with respect to their size are merged into one cluster .
Then , segments that share a large enough number of frames with respect to their size are merged into one cluster .
56
In order to generate the final summary , with each representative segment , the middle part is selected with the skim rate of 2 frames .
In order to generate the final summary , with each representative segment , the middle part is selected with a skim rate of 2 frames .
57
</p>
</p>
58
<p>
<p>
59
This paper is organized as follows : section \REF introduces details of the first approach; , while section \REF presents details of the second approach .
This paper is organized as follows : section \REF introduces the details of the first approach; , while section \REF presents the details of the second approach .
60
Section \REF describes experimental results on the TRECVID 2008 dataset .
Section \REF describes our experimental results on the TRECVID 2008 dataset .
61
Finally , section \REF and section \REF conclude the paper .
Finally , section \REF and section \REF conclude the paper .
62
</p>
</p>
63
</section>
</section>
64
<section label = " System NII-1 ">
<section label = " System NII-1">
65
<subsection label = " Video Decomposition ">
<subsection label = " Video Decomposition ">
66
<subsubsection label = " Shot Boundary Detection ">
<subsubsection label = " Shot Boundary Detection ">
67
<p>
<p>
68
From the definition , all rushes are unedited; therefore it must consist of hard cut only .
By definition , all rushes are unedited; therefore they must consist of hard cuts only .
69
The shot boundary detection algorithm in \CITE is used to determine shot boundary and partition the input video into shots .
The shot boundary detection algorithm in \CITE is used to determine the shot boundary and to partition the input video into shots .
70
A local color histogram is extracted by dividing a video frame into \MATH blocks .
A local color histogram is extracted by dividing a video frame into \MATH blocks .
71
The \MATH distance is used to compute the distance between each blocks of frames \MATH and \MATH .
The \MATH distance is used to compute the distance between each block of frames \MATH and \MATH .
72
Next , these values are sorted into an ascending order .
Next , these values are sorted into ascending order .
73
The sum of the middle eight of these 16 values are used to define a cut between frames \MATH and \MATH if these values exceed a threshold \MATH .
The sum of the middle eight of these 16 values is used to define the cut between frames \MATH and \MATH if these values exceed the threshold \MATH .
74
However , this algorithm cannot distinguish between hard cut and the large objects motion .
However , this algorithm cannot distinguish between hard cuts and the motion of large objects .
75
To overcome this problem , motion-based features are computed for each video frame using the Lucas-Kanade point-based tracking functions provided in the OpenCV toolkit\footnote{http : //opencvlibrary.sourceforge.net / } .
To overcome this problem , motion-based features are computed for each video frame using the Lucas-Kanade point-based tracking functions provided in the OpenCV toolkit\footnote{http : //opencvlibrary.sourceforge.net / } .
76
The magnitude is computed from the motion vector for each frame .
The magnitude is computed from the motion vector for each frame .
77
Therefore , if the algorithm detected a cut between frames \MATH and \MATH , whose magnitude is larger than a threshold \MATH , these cuts are rejected since they are motions from large objects .
Therefore , if the algorithm detected a cut between frames \MATH and \MATH , whose magnitude is larger than the threshold \MATH , these cuts are rejected since they are the motions of large objects .
78
Finally , the short shots with less than 25 frames ( 1 second ) are removed .
Finally , short shots of less than 25 frames ( 1 second ) are removed .
79
</p>
</p>
80
</subsubsection>
</subsubsection>
81
<subsubsection label = " Sub-Shot Detection ">
<subsubsection label = " Sub-Shot Detection ">
82
<p>
<p>
83
The sub-shot segmentation algorithm in \CITE is used to divide shots into smaller units .
The sub-shot segmentation algorithm in \CITE is used to divide shots into smaller units .
84
A first frame of the shot is chosen as the base frame \MATH and next frame \MATH for comparison .
The first frame of the shot is chosen as the base frame \MATH and the next frame \MATH for a comparison .
85
The \MATH distance used to compute the distance of frame sequence until the sum of the sorted value of lower eight is larger than a threshold \MATH .
The \MATH distance used to compute the distance of the frame sequence until the sum of the sorted value of the lower eight is larger than the threshold \MATH . //[distance / length?]
86
The frames from \MATH to \MATH , then , form a sub-shot and frame \MATH is used as the next base frame .
The frames from \MATH to \MATH , then , form a sub-shot and frame \MATH is used as the next base frame .
87
Finally , the short sub-shots with less than 25 frames are removed .
Finally , the short sub-shots of less than 25 frames are removed .
88
</p>
</p>
89
</subsubsection>
</subsubsection>
90
<subsubsection label = " Keyframes Extraction ">
<subsubsection label = " Keyframes Extraction ">
91
<p>
<p>
92
We employ a keyframe extraction algorithm proposed in \CITE to extract the representative keyframes from each sub-shot .
We use the keyframe extraction algorithm proposed in \CITE to extract the representative keyframes from each sub-shot .
93
In this approach , cosine distance is used to measure the difference between neighboring frames in each sub-shot .
In this approach , the cosine distance is used to measure the difference between neighboring frames in each sub-shot .
94
Keyframes are selected at the midpoints between two consecutive high curvature points where the high curvature points are detected from the curve of the cumulative frame difference .
Keyframes are selected at the midpoints between two consecutive high curvature points where the high curvature points are detected from the curve of the cumulative frame difference .
95
</p>
</p>
96
</subsubsection>
</subsubsection>
97
</subsection>
</subsection>
98
<subsection label = " Junk Elimination ">
<subsection label = " Junk Elimination ">
99
<subsubsection label = " Color Bars ">
<subsubsection label = " Color Bars ">
100
<p>
<p>
101
The characteristics of color bars are vertically averaged , and the color histograms for each block in the same column should be similar .
The characteristics of color bars are vertically averaged , and the color histograms for each block in the same column should be similar .
102
We employ the algorithm proposed in \CITE by using \MATH distance to compute histogram differences between any two neighboring blocks in each column .
We used the algorithm proposed in \CITE by using the \MATH distance to compute the histogram differences between any two neighboring blocks in each column .
103
Next , we sort these values into an ascending order .
Next , we sort these values into ascending order .
104
If the value of the \MATH is smaller than threshold \MATH , then these sub-shot is defined as a color bar sub-shot .
If the value of the \MATH is smaller than the threshold \MATH , then these sub-shots are defined as a color bar sub-shot .
105
</p>
</p>
106
</subsubsection>
</subsubsection>
107
<subsection label = " Single Color Frames ">
<subsection label = " Single Color Frames ">
108
<p>
<p>
109
From the properties of single color image , a dominant color in its global histogram is large .
From the properties of a single color image , the dominant color in its global histogram is large .
110
If the value of the \MATH of global color histogram is larger than threshold \MATH , then these sub-shots are defined as a single color sub-shot .
If the value of the \MATH of the global color histogram is larger than the threshold \MATH , then these sub-shots are defined as a single color sub-shot .
111
</p>
</p>
112
</subsubsection>
</subsubsection>
113
<subsubsection label = " Clapper Boards ">
<subsubsection label = " Clapper Boards ">
114
<p>
<p>
115
In rushes videos , there are many types of clapper boards , appearance but the same type of clapper boards is often used in the same movie .
In rushes videos , there are many types of clapper boards , but the same type of clapper board is often used in the same movie .
116
The clapper boards have many types , such as scale , rotation , and illumination changes .
There are many types of clapper boards , such as scale , rotation , and illumination changes .
117
The NDK algorithm , proposed in \CITE , is invariant to image scaling , translation , rotation , illumination changes , and affine or 3D projection .
The NDK algorithm , proposed in \CITE , is invariant to image scaling , translation , rotation , illumination changes , and affine or 3D projection .
118
A set of 80 example keyframes of clapper boards are extracted from the development set and used as a set of queries .
A set of 80 example keyframes of clapper boards were extracted from the development set and is used as a set of queries .
119
Next , we extract the keypoints of the keyframes given from section \REF and match them with the query .
Next , we extract the keypoints of the keyframes given from section \REF and match them with the query .
120
If a result of the NDK algorithm returns a match between a keyframe with a query then we define the sub-shot is a clapper board sub-shot .
If the result of the NDK algorithm returns a match from a keyframe with a query then the sub-shot is defined as a clapper board sub-shot .
121
</p>
</p>
122
</subsubsection>
</subsubsection>
123
</subsection>
</subsection>
124
<subsection label = " Redundancy Elimination ">
<subsection label = " Redundancy Elimination ">
125
<p>
<p>
126
The unused keyframes containing of story units for generate video summary are removed .
The unused keyframes containing story units for the generated video summary are removed .
127
However , rushes videos containing of repetitive story , such as retake scenes , are unedited .
However , rushes videos containing a repetitive story , such as a retake of scenes , are unedited .
128
To create the efficiently of rushes videos , the repetitive contents must be eliminated .
To efficiently create rushes videos , the repetitive contents must be eliminated .
129
Generally , a group of continuous contents often share some properties .
Generally , a group of continuous contents often share some properties .
130
From this characteristic , clustering technique can be used to separate the data into groups of similar contents .
With this characteristic in mind , a clustering technique can be used to separate the data into groups of similar contents .
131
Each group , called cluster , consists of contents that are similar between themselves and dissimilar to contents of other groups .
Each group , called a cluster , consists of contents that are similar between themselves and dissimilar to the contents of other groups .
132
GreedyRSC , proposed in \CITE , is used to find clusters with high precision and the number of clusters is automatically determined .
GreedyRSC , proposed in \CITE , is used to find clusters at high precision and the number of clusters is automatically determined .
133
</p>
</p>
134
<p>
<p>
135
To do the clustering on keyframes , three different features , including mean , variance , and skewness , are extracted from local color histogram .
To do clustering on keyframes , three different features , including the mean , variance , and skew , are extracted from the local color histogram .
136
These values are used to represent the keyframes content and defined as follows :
These values are used to represent the keyframes content and are defined as follows :
137
<p>
<p>
138
Figure \REF shows an example of clustering result .
Figure \REF shows an example of a clustering result .
139
</p>
</p>
140
</subsection>
</subsection>
141
<subsection label = " Summary Generation ">
<subsection label = " Summary Generation ">
142
<p>
<p>
143
So far , we completely remove the unused contents from rushes video and reduce repetition of the story contents .
So far , we have completely removed the unused contents from rushes videos and reduced any repetition of the story contents .
144
The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of summary is 2\% of the original video ) , less repetitive of content , and must have many objects and events as possible .
The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of a summary is 2\% of the original video ) , less repetitive content , and must have as many objects and events as possible .
145
To reach this objective , the important keyframes should be selected to generate summary video .
To reach this objective , only the most important keyframes should be selected to generate a summary video .
146
</p>
</p>
147
<p>
<p>
148
To generate summary , we first compute its maximum duration in seconds \MATH ,
To generate a summary , we first compute its maximum duration in seconds \MATH ,
149
where \MATH is the maximum duration for the summary .
where \MATH is the maximum duration for the summary .
150
<p>
<p>
151
Second , we compute quota length for each cluster based on the cluster size \MATH .
Second , we compute the quota length for each cluster based on the cluster size \MATH .
152
<\p>
<\p>
153
<p>
<p>
154
Third , merge consecutive sub-shots in each cluster into shots and compute the priority of each shot based on priority of shot weighted duration and shot weighted average motion magnitude using the following equation : \MATH</p>
Third , merge the consecutive sub-shots in each cluster into shots and compute the priority of each shot based on the priority of the shot weighted duration and shot weighted average motion magnitude using the following equation : \MATH</p>
155
<p>
<p>
156
Next , these \MATH values are sorted into descending order and the first shot is selected .
Next , these \MATH values are sorted into descending order and the first shot is selected .
157
</p>
</p>
158
<p>
<p>
159
Forth , sort sub-shots in the selected shot in descending order based on the average motion magnitude .
Fourth , sub-shots in the selected shot in descending order are sorted based on the average motion magnitude .
160
Select sub-shots from top to bottom until the quota length for that shot is reached .
The sub-shots are selected from top to bottom until the quota length for that shot is reached .
161
</p>
</p>
162
<p>
<p>
163
Fifth , for each selected sub-shot , extract 25 frames ( 1 second ) around the middle to generate the final summary .
Fifth , for each selected sub-shot , 25 frames ( 1 second ) around the middle are extracted to generate the final summary .
164
</p>
</p>
165
<section label = " System NII-2 ">
<section label = " System NII-2 ">
166
<subsection label = " Video Decomposition ">
<subsection label = " Video Decomposition ">
167
<p>
<p>
168
This system is adopted with some modifications from the system developed for the same task last year \CITE .
This system has some modifications from the system developed for the same task last year \CITE .
169
Specifically , the original video is decomposed into segments , which are shots with hard cut transition .
Specifically , the original video is broken down into segments , which are shots with a hard cut transition .
170
These segments are further decomposed into fragments so that each fragment represents a portion of a scene .
These segments are further broken down into fragments so that each fragment represents a portion of a scene .
171
In order to reduce the computation time , we only extract a subset of frames from the original video by sampling at a five frame interval ( i.e extract frames 0th , 5th , 10th , and so on ) .
In order to reduce the computation time , we only extract a subset of the frames from the original video by sampling it at a five frame interval ( i.e. extract frames 0 , 5th , 10th , and so on ) .
172
For each frame , we use grid color moments with the same configuration as in \CITE for feature representation .
For each frame , we use grid color moments with the same configuration as in \CITE for feature representation .
173
The segment boundary , which is located at hard cut transition , is determined by using a loose threshold on the Euclidean distance between two consecutive frames .
The segment boundary , which is located at the hard cut transition , is determined by using a loose threshold on the Euclidean distance between two consecutive frames .
174
Meanwhile , the fragment boundary is determined by using a strict threshold to detect dramatic motion .
Meanwhile , the fragment boundary is determined by using a strict threshold to detect any dramatic motion .
175
<p>
<p>
176
</subsection>
</subsection>
177
<subsection label = " Redundancy Elimination ">
<subsection label = " Redundancy Elimination ">
178
<p>
<p>
179
Instead of selecting one keyframe to represent one fragment as many other systems do , we use all frames of each fragment for redundancy elimination .
Instead of selecting one keyframe to represent one fragment as many other systems do , we use all the frames of each fragment for the redundancy elimination .
180
We use GreedyRSC \CITE to do clustering on the set of all sampled frames extracted from the original video .
We use GreedyRSC \CITE to do the clustering on the set of all the sampled frames extracted from the original video .
181
The number of clusters is determined automatically by this method .
The number of clusters is determined automatically using this method .
182
Frames that belong to the same cluster are assigned the same label .
Frames that belong to the same cluster are assigned the same label .
183
By this discretization process , we can cast one fragment as one string whose characters are labels of its frames .
By this discretization process , we can cast one fragment as one string whose characters are the labels of its frames .
184
We compute the similarity value between two fragments by counting the number of shared characters between two strings and being normalized to the size of each string .
We compute the similarity between two fragments by counting the number of shared characters between two strings and being normalized to the size of each string .
185
If this value is larger than a threshold , these two segments are merged into one cluster .
If this value is larger than the threshold , these two segments are merged into one cluster .
186
We found that this approach is more effective than the approach using one keyframe for one fragment since the more number of keyframes is used , the more information is available to make right decision .
We found that this approach is more effective than the approach using one keyframe for one fragment since the more keyframes that are used , the more information is available to make the right decision .
187
</p>
</p>
188
</subsection>
</subsection>
189
<subsection label = " Junk Elimination ">
<subsection label = " Junk Elimination ">
190
<p>
<p>
191
We select junk frames such as color bar frames , single color ( black or white ) frames to form the reference junk frame set .
We select junk frames such as color bar frames , and single color ( black or white ) frames to form the reference junk frame set .
192
To check whether a fragment is a junk , we compare the frames of this fragment to the frames of the reference junk frame set .
To check whether a fragment is junk , we compare the frames of this fragment to the frames of the reference junk frame set .
193
The similarity between two frames is the Euclidean distance between two grid color moment feature vectors .
The similarity between two frames is the Euclidean distance between two grid color moment feature vectors .
194
We empirically select thresholds for each type of junk .
We empirically select the thresholds for each type of junk .
195
If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered as junk and all fragments of the cluster containing junk fragment are eliminated .
If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered junk and all the fragments of the cluster containing the junk fragment are eliminated .
196
In our system, we only check fragments that are located at two ends of the original video for reducing computation time .
In our system, we only check the fragments that are located at the two ends of the original video for reducing the computation time .
197
However, by using the clustering result, junk fragments that are not checked against the reference junk frame set are also removed .
However, by using the clustering result, junk fragments that are not checked against the reference junk frame set are also removed .
198
</p>
</p>
199
</subsection>
</subsection>
200
<subsection label = " Summary Generation ">
<subsection label = " Summary Generation ">
201
<p>
<p>
202
For each cluster, we merge adjacent fragments into longer fragments and select the longest fragment as the representative fragment to be included in the final summary .
For each cluster, we merge adjacent fragments into longer fragments and select the longest fragment as the representative fragment to be included in the final summary .
203
Since the length of these fragments is still larger than the maximum length of the final summary, we employ a simple strategy to shrink these fragments as follows .
Since the length of these fragments is still larger than the maximum length of the final summary, we use the following simple strategy to shrink these fragments .
204
</p>
</p>
205
<p>
<p>
206
First, we assign a quota, which is the maximum duration, for each fragment by dividing the maximum duration for the summary to the number of clusters .
First, we assign a quota, which is the maximum duration, for each fragment by dividing the maximum duration for the summary to the number of clusters .
207
</p>
</p>
208
<p>
<p>
209
Second, for each fragment, we extract the portion which is expanded from the central of the fragment .
Second, for each fragment, we extract the portion that is expanded from the central part of the fragment .
210
This portion covers a duration twice as much as the fragment quota by selecting frames with sampling rate of 2 frames .
This portion covers a duration twice the size of the fragment quota by selecting the frames with a sampling rate of two frames .
211
Specifically, we select frames \MATH, \MATH, ..., \MATH, \MATH, ..., \MATH, \MATH, where \MATH is the middle frame of the fragment, and \MATH is half of number of frames computed from the quota\MATH and frame rate ( 25fps ) \MATH :
Specifically, we select frames \MATH, \MATH, ..., \MATH, \MATH, ..., \MATH, \MATH, where \MATH is the middle frame of the fragment, and \MATH is half the number of frames computed from the quota\MATH and the frame rate ( 25fps ) \MATH :
212
</section>
</section>
213
<section label=" Experimental Results ">
<section label=" Experimental Results ">
214
<p>
<p>
215
We have tested our approaches with 40 videos of TRECVID 2008 test set .
We have tested our approaches on 40 videos from the TRECVID 2008 test set .
216
Table \REF shows a comparison between these approaches for the measures used in evaluation of this task \CITE .
Table \REF presents a comparison between these approaches for the measures used in evaluation of this task \CITE .
217
The system NII-2 achieves higher recall ( IN ) than the system NII-1 since NII-1 only uses one keyframe for each sub-shot and has shorter duration ( DU ) for summary videos .
The NII-2system achieves a higher recall ( IN ) than the NII-1 system because NII-1 only uses one keyframe for each sub-shot and has a shorter duration ( DU ) for summary videos .
218
However, NII-1 has a better score in quality .
However, NII-1 has better quality .
219
The summary videos generated by NII-1 have fewer duplications ( RE ), are presented in a smoother way ( TE ) and are easy to judge for inclusions ( TT ) .
The summary videos generated by NII-1 have less duplication ( RE ), are presented in a smoother way ( TE ), and are easy to judge for inclusions ( TT ) .
220
</p>
</p>
221
<p>
<p>
222
In terms of efficiency, NII-2 is much better .
In terms of efficiency, NII-2 is much better .
223
The clapper board detection process using NDK consumes around half of processing time of NII-1 but performance is low due to large variations of clapper boards in videos ( see Figure \REF ) .
The clapper board detection process using NDK consumes around half of the processing time of NII-1, but its performance is low due to the large variations in clapper boards in the videos ( see Figure \REF ) .
224
The comparable performance in junk elimination of both systems suggests that simple methods are more favorable .
The comparable performance in the junk elimination of both systems suggests that simpler methods are more favorable .
225
In addition, by using simple features and sampling frames in the original video, NII-2 significantly speeds up the processing time ( computed from the time taking the input video to the time picking the summary video ) to quasi real-time .
In addition, by using simple features and sampling frames in the original video, NII-2 significantly increases the processing time ( computed from the time the input video is taken to the time the summary video is picked ) to quasi real-time .
226
</p>
</p>
227
<p>
<p>
228
Practical summarization systems usually have good balance between fraction of inclusions and user-friendliness .
Practical summarization systems usually have a good balance between the fraction of inclusions and user-friendliness .
229
In Table \REF, we show performance of such systems .
In Table \REF, we present the performance of such systems .
230
The 14 systems listed in this table have IN score larger than the median ( 0.45 ); and other scores such as RE and TE larger than half of maximum score ( 2.5 ) .
The 14 systems listed in this table have an IN score that is above the median ( 0.45 ); and other scores, such as RE and TE, are larger than half of the maximum score ( 2.5 ) .
231
Compared to other systems listed in this list, our system NII-2 is one of the fastest systems .
Compared to the other systems listed in this table, our NII-2system is one of the fastest .
232
</p>
</p>
233
<p>
<p>
234
Compared to the other systems participating in this task of TRECVID 2008, NII-1 has good performance in measures such as DU and TT ( see Figure \REF and Figure \REF; while NII-2 achieves good performance in measure IN ( see Figure \REF ) .
Compared to the other systems participating in this task of TRECVID 2008, NII-1 performed better in such measures as DU and TT ( see Figure \REF and Figure \REF; while NII-2 performs well in the IN measure ( see Figure \REF ) .
235
</p>
</p>
236
</section>
</section>
237
<section label= " Discussion ">
<section label= " Discussion ">
238
<p>
<p>
239
One of most difficult steps is redundancy elimination .
One of the most difficult steps is redundancy elimination .
240
Lack of discriminative representation of segments and robust clustering methods is the main reason \CITE .
The lack of discriminative representation of the segments and robust clustering methods is the main reason \CITE .
241
Two typical cases that usually happen in clustering result are fragmentation and outliers .
Two typical cases that usually happen in clustering results are fragmentation and outliers .
242
Fragmentation is the case that samples of one cluster are put into several different clusters .
Fragmentation is where samples of one cluster are put into several different clusters .
243
Outliers are irrelevant and noisy samples in one cluster due to poor determination of cluster boundary .
Outliers are irrelevant and noisy samples in one cluster due to the poor determination of the cluster boundary .
244
Therefore, it is necessary to develop robust methods for detection of repetitive segments .
Therefore, it is necessary to develop robust methods for detecting repetitive segments .
245
Using all frames of one segment instead of using one keyframe as proposed in NII-2 is one of the efforts toward this direction .
Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .
246
Although the result is not very high as expected, we still believe that this approach is promising .
Although the results are not as high as expected, we still believe that this approach is promising .
247
</p>
</p>
248
</section>
</section>
249
<section label = " Conclusion ">
<section label = " Conclusion ">
250
<p>
<p>
251
We have presented two different approaches for generating short summary for rushes video .
We have presented two different approaches for generating a short summary for rushes videos .
252
In the first approach, NII-1, redundancy elimination is done by doing clustering on the set of keyframes extracted from sub-shots .
In the first approach, NII-1, clustering the set of keyframes extracted from the sub-shots helps to eliminate redundancy .
253
With each representative segment of each cluster, the portion that has high degree of motion is selected to form the summary .
With each representative segment of each cluster, the portion with the highest degree of motion is selected to form the summary .
254
This approach achieves good performance in usability score but low performance in recall .
This approach has a good usability score but is not very good at recall .
255
In the second approach, NII-2, all frames of each sub-shot are used to compute the similarity among sub-shots in clustering process .
In the second approach, NII-2, all the frames of each sub-shot are used to compute the similarity among the sub-shots in the clustering process .
256
With each representative segment of each cluster, the middle part is selected to form the summary with skipping rate of 2 frames .
With each representative segment of each cluster, the middle part is selected to form the summary with a skipping rate of two frames .
257
This approach achieves good performance in recall and reasonable performance in usability score .
This approach is good for recall and has a reasonably good usability score .
258
Compared to other systems participating in TRECVID 2008 summarization task, NII-2 is among best systems that have good balance between recall and usability .
Compared to other systems participating in the TRECVID 2008 summarization task, NII-2 is among the best systems with a good balance between recall and usability .
259
</p>
</p>
260
</section>
</section>
261
</document>
</document>
