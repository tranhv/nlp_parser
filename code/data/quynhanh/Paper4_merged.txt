Two-stage Incremental Working Set Selection for Fast Support Vector Training on Large Datasets
Two-stage Incremental Working Set Selection for Fast Support Vector Training on Large Datasets
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

We propose iSVM - an incremental algorithm that achieves high speed in training support vector machines ( SVMs ) on large datasets . 
We propose iSVM - an incremental algorithm that achieves high speed in training support vector machines ( SVMs ) on large datasets . 
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

In the common decomposition framework , iSVM starts with a minimum working set ( WS ) , and then iteratively selects one training example to update the WS in each optimization loop .
In the common decomposition framework , iSVM starts with a minimum working set ( WS ) , and then iteratively selects one training example to update the WS in each optimization loop .
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

iSVM employs a two-stage strategy in processing the training data . 
iSVM employs a two-stage strategy in processing the training data . 
8 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In the first stage , the most prominent vector among randomly sampled data is added to the WS . 
In the first stage , the most prominent vector among randomly sampled data is added to the WS . 
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

This stage results in an approximate SVM solution . 
This stage results in an approximate SVM solution . 
10 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

The second stage uses temporal solutions to scan through the whole training data once again to find the remaining support vectors ( SVs ) . 
The second stage uses temporal solutions to scan through the whole training data once again to find the remaining support vectors ( SVs ) . 
11 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

We show that iSVM is especially efficient for training SVMs on applications where data size is much larger than number of SVs . 
We show that iSVM is especially efficient for training SVMs on applications where data size is much larger than number of SVs . 
12 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples , iSVM takes less than one hour to train an SVM with 94% testing accuracy , compared to seven hours with LibSVM – one of the state-of-the-art SVM implementations . 
On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples , iSVM takes less than one hour to train an SVM with 94% testing accuracy , compared to seven hours with LibSVM – one of the state-of-the-art SVM implementations . 
13 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved

We also provide analysis and experimental comparisons between iSVM and the related algorithms .
We also provide analysis and experimental comparisons between iSVM and the related algorithms .
14 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

In recent years support vector machine ( SVM ) \CITE has been successfully applied in various machine learning applications .
In recent years support vector machine ( SVM ) \CITE has been successfully applied in various machine learning applications .
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

However , scalability still remains one of biggest challenges for SVM in particular and kernel-based methods in general . 
However , scalability still remains one of biggest challenges for SVM in particular and kernel-based methods in general . 
20 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

It is due to the fact that training an SVM requires solving a quadratic programming ( QP ) problem in which , for the worst case , the complexity becomes </Eq> for time and </Eq> for memory requirement , where l is the number of training examples \CITE .
It is due to the fact that training an SVM requires solving a quadratic programming ( QP ) problem in which , for the worst case , the complexity becomes </Eq> for time and </Eq> for memory requirement , where l is the number of training examples \CITE .
21 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved

There have been number of approaches to scalability problem of SVM training . 
There have been a number of approaches to scalability problem of SVM training . 
22 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :3:mogrammar-det

Among them decomposition is the most widely implemented method in various SVM software and libraries , e.g. LibSVM \CITE , SVM light \CITE , CoreSVM \CITE , HeroSVM \CITE , and SimpleSVM \CITE . 
Among them , decomposition is the most widely implemented method in various SVM software and libraries , e.g. LibSVM \CITE , SVM light \CITE , CoreSVM \CITE , HeroSVM \CITE , and SimpleSVM \CITE . 
23 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:2:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:16:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:20:preserved 24:25:preserved 25:26:preserved 26:24:preserved 27:28:preserved 28:29:preserved 29:27:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved :30:bigrammar-others

The main idea of decomposition algorithms is to divide training data into two sets : an active working set ( WS ) whose coefficients can be updated , and an inactive set whose coefficients are temporally fixed \CITE . 
The main idea of decomposition algorithms is to divide training data into two sets : an active working set ( WS ) whose coefficients can be updated , and an inactive set whose coefficients are temporally fixed \CITE . 
24 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

The extreme case of this decomposition approach is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that does optimization on a set of only two examples . 
The extreme case of this decomposition approach is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that does optimization on a set of only two examples . 
25 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

For each optimization loop , SMO scans through the whole training data to find a good pair of vectors , and then updates coefficients of the two selected vectors analytically . 
For each optimization loop , SMO scans through the whole training data to find a good pair of vectors , and then updates coefficients of the two selected vectors analytically . 
26 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

In order to find a good pair in the next iteration , it is required to update the violation of optimality criteria of all training vectors . 
In order to find a good pair in the next iteration , it is required to update the violation of optimality criteria of all training vectors . 
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

It is very expensive for applications where the number of updates ( training data ) is huge .
It is very expensive for applications where the number of updates ( training data ) is huge .
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

In this paper , we introduce a two-stage incremental WS selection method in training SVMs . 
In this paper , we introduce a two-stage incremental WS selection method in training SVMs . 
29 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The proposed algorithm starts from finding an initial SVM solution on a minimum WS ( two vectors from opposite classes in a two-class classification task ) . 
The proposed algorithm starts from finding an initial SVM solution on a minimum WS ( two vectors from opposite classes in a two-class classification task ) . 
30 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

It then iteratively selects one training vector to update the WS . 
It then iteratively selects one training vector to update the WS . 
31 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

The selection of new vectors is divided into two stages . 
The selection of new vectors is divided into two stages . 
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In the first stage , only the most prominent vector among a fixed number of sampling data is added to the WS while all others remain in the training data . 
In the first stage , only the most prominent vector among a fixed number of sampling data is added to the WS while all others remain in the training data . 
33 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

From the second stage , all training examples are checked once again . 
From the second stage , all training examples are checked once again . 
34 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

For both stages , each optimality violated vector is used to update the WS and find a new SVM solution . 
For both stages , each optimality violated vector is used to update the WS and find a new SVM solution . 
35 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

We show that with this two-stage WS selection strategy the proposed iSVM has a linear time complexity in number of training examples and cubic in number of SVs . 
We show that with this two-stage WS selection strategy , the proposed iSVM has a linear time complexity in number of training examples and cubic in number of SVs . 
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :9:bigrammar-others

Experiments on large benchmark datasets show that iSVM is very fast when working on applications where number of SVs is much smaller than number of training data . 
Experiments on large benchmark datasets show that iSVM is very fast when working on applications where number of SVs is much smaller than number of training data . 
37 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples iSVM takes less than one hour to train a SVM with 94% testing accuracy .
On the KDD-CUP 1999 network intrusion detection dataset with nearly five millions training examples , iSVM takes less than one hour to train a SVM with 94% testing accuracy .
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :14:bigrammar-others

With LibSVM \CITE – one of the state-of-the-art SMO implementations , it takes seven hours .
With LibSVM \CITE – one of the state-of-the-art SMO implementations , it takes seven hours .
39 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The rest part of this paper is organized as follows . 
The rest of this paper is organized as follows . 
40 0:0:preserved 1,2:1:paraphrase 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved

In Section II we briefly describe SVM training problem and decomposition algorithms for solving it . 
In Section II , we briefly describe SVM training problems and decomposition algorithms for solving them . 
41 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:bigrammar-nnum 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:bigrammar-nnum 15:16:preserved :3:unaligned

We introduce iSVM and its complexity analysis in section III . 
We introduce iSVM and its complexity analysis in section III . 
42 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In section IV we discuss relations between iSVM and related SVM training algorithms . 
In section IV , we discuss the relations between iSVM and related SVM training algorithms . 
43 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved :3:unaligned :6:mogrammar-det

Experiments for evaluating iSVM and comparison with other algorithms are reported in section V . 
Experiments for evaluating iSVM and comparison with other algorithms are reported in section V . 
44 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Section VI is for conclusion .
Section VI is for conclusion .
45 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

In support vector learning \CITE , we are given a set of l training examples </Eq> with labels </Eq> . 
In support vector learning \CITE , we are given a set of l training examples </Eq> with labels </Eq> . 
51 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The main task of training an SVM is to solve the following optimization problem :
The main task of training an SVM is to solve the following optimization problem :
52 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

( 1 ) where </Eq> is a kernel function calculating dot product between two vector </Eq> and </Eq> in some feature space ; C is a parameter penalizing each "  noisy "  example in the given training data . 
( 1 ) where </Eq> is a kernel function calculating dot product between two vector </Eq> and </Eq> in some feature space ; C is a parameter penalizing each "  noisy "  example in the given training data . 
53 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

The optimal coefficients </Eq> will form a decision function :
The optimal coefficients </Eq> will form a decision function :
54 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

( 2 ) Problem ( 1 ) involves l variables </Eq> and </Eq> parameters </Eq> . 
( 2 ) Problem ( 1 ) involves l variables </Eq> and </Eq> parameters </Eq> . 
55 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

The </Eq> number of parameters </Eq> quickly exceeds memory capacity of a normal computer when the number of training examples gets larger than , say , 100 ,000 .
The </Eq> number of parameters </Eq> quickly exceeds memory capacity of a normal computer when the number of training examples gets larger than , say , 100 ,000 .
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

This over demanding in memory requirement causes the main difficulty in training SVMs .
This over demanding in memory requirement causes the main difficulty in training SVMs .
57 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The main idea of decomposition algorithms , e.g. \CITE , is to break down the QP problem ( 1 ) of size l into a series of much smaller QPs , and iteratively perform optimization on these sub-problems . 
The main idea of decomposition algorithms , e.g. \CITE , is to break down the QP problem ( 1 ) of size l into a series of much smaller QPs , and iteratively perform optimization on these sub-problems . 
62 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

In each iteration decomposition algorithms divide l training examples into two categories : a set of active vectors W that corresponding coefficients </Eq> can be updated and a set of inactive vectors that corresponding coefficients are temporally fixed . 
In each iteration , decomposition algorithms divide l training examples into two categories : a set of active vectors W corresponding coefficients </Eq> can be updated , and a set of inactive vectors corresponding coefficients are temporally fixed . 
63 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19,32:3,26:bigrammar-others 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

Active vectors are updated by some optimization method to minimize objective function L on W .
Active vectors are updated by some optimization method to minimize objective function L on W .
64 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

After that fixed vectors are checked and used for updating the working set W . Optimization loop will stop when all optimality conditions are satisfied . 
After that fixed vectors are checked and used for updating the working set W . Optimization loop will stop when all optimality conditions are satisfied . 
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

The extreme case of the decomposition method is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that optimizes a set of only two vectors . 
The extreme case of the decomposition method is the Sequential Minimal Optimization ( SMO ) algorithm \CITE that optimizes a set of only two vectors . 
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

The power of SMO resides in the fact that updating scheme could be done analytically . Call </Eq> and </Eq> be chosen vectors , then the best new values of </Eq> and </Eq> in terms of reducing best the objective function L in ( 1 ) are ( ignoring the box constraint </Eq> ) :
The power of SMO resides in the fact that updating scheme could be done analytically . Call </Eq> and </Eq> be chosen vectors , then the best new values of </Eq> and </Eq> in terms of reducing best the objective function L in ( 1 ) are ( ignoring the box constraint </Eq> ) :
67 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved

Input : Training data </Eq> Initialize a feasible solution 
Input : Training data </Eq> Initialize a feasible solution 
68 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Set iteration t = 0
Set iteration t = 0
69 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

While StoppingCondition is not satisfied
While StoppingCondition is not satisfied
70 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

Select a pair of vectors </Eq>
Select a pair of vectors </Eq>
71 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Update </Eq> analytically
Update </Eq> analytically
72 0:0:preserved 1:1:preserved 2:2:preserved

Update violation states </Eq>
Update violation states </Eq>
73 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

Set t = t + 1
Set t = t + 1
74 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Endwhile
Endwhile
75 0:0:preserved

Output : Coefficients ( 3 ) </Eq> . 
Output : Coefficients ( 3 ) </Eq> . 
76 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

This updating scheme leads to the reduction of objective function L an amount of ( 4 )
This updating scheme leads to the reduction of objective function L an amount of ( 4 )
77 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Based on this reduction rate different heuristics have been proposed to select the best pair of vectors </Eq> with a reasonable cost </Eq> .
Based on this reduction rate , different heuristics have been proposed to select the best pair of vectors </Eq> with a reasonable cost </Eq> .
78 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved :5:bigrammar-others

The analytical solution property of SMO makes it become a core optimizer for many SVM implementations . 
The analytical solution property of SMO makes it become a core optimizer for many SVM implementations . 
79 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

In TABLE I we describe main procedures in an SMO implementation .
In TABLE I we describe main procedures in an SMO implementation .
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

The most expensive procedure of the SMO is updating violation of optimality criteria </Eq> of all training vectors in step 5 . 
The most expensive procedure of the SMO is updating violation of optimality criteria </Eq> of all training vectors in step 5 . 
86 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

This calculation is used to select the best pair of vectors in the next iteration; and it is required for every training example .
This calculation is used to select the best pair of vectors in the next iteration , and it is required for every training example .
87 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14,15:bigrammar-others 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

Step 5 becomes very expensive when the number of updates ( or training data ) is huge . Moreover , as only SVs ( training vectors </Eq> with corresponding coefficients </Eq> will contribute to form the final decision function ( 2 ) , step 5 of SMO can be very inefficient when many training examples are not SVs . 
Step 5 becomes very expensive when the number of updates ( or training data ) is huge . Moreover , as only SVs ( training vectors </Eq> with corresponding coefficients </Eq> will contribute to form the final decision function ( 2 ) , step 5 of SMO can be very inefficient when many training examples are not SVs . 
88 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved

To improve the efficiency shrinking technique \CITE can be applied to remove non-support vectors . 
To improve the efficiency , shrinking technique \CITE can be applied to remove non-support vectors . 
89 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :4:bigrammar-others

However , there has been no way to determine whether a training example is a SV or not from the beginning .
However , there has been no way to determine whether a training example is a SV or not from the beginning .
90 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

In this section we introduce an incremental strategy for selecting SVs . 
In this section , we introduce an incremental strategy for selecting SVs . 
91 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :3:bigrammar-others

The main idea is using temporal SVM solutions to determine good candidates of SVs and optimization process is performed only on a small set of selected candidates . 
The main idea is temporal SVM solutions can be used to determine good candidates of SVs , and optimization process is performed only on a small set of selected candidates . 
92 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:4:paraphrase 6:5:preserved 7:6:preserved 8:10,7,8,9:paraphrase 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved :16:unaligned

The following subsections describe in detailed the main steps of our proposed algorithm iSVM in TABLE II .
The following subsections describe in detail the main steps of our proposed algorithm iSVM in TABLE II .
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:spelling 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Input : Training data </Eq>
Input : Training data </Eq>
94 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

Select the first working set </Eq> . 
Select the first working set </Eq> . 
95 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Find the first temporal solution </Eq> . 
Find the first temporal solution </Eq> . 
96 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Set </Eq>
Set </Eq>
97 0:0:preserved 1:1:preserved

Set iteration t = 0
Set iteration t = 0
98 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

While StoppingCondition is not satisfied
While StoppingCondition is not satisfied
99 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

Select one vector </Eq> in T
Select one vector </Eq> in T
100 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Update </Eq>
Update </Eq>
101 0:0:preserved 1:1:preserved

Find new solution </Eq> on </Eq>
Find new solution </Eq> on </Eq>
102 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Set t = t + 1
Set t = t + 1
103 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

Endwhile
Endwhile
104 0:0:preserved

Output : Coefficients </Eq>
Output : Coefficients </Eq>
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

As SMO is used for optimization on the selected working set </Eq> in step 0 and step 5 , a minimum set of two vectors are selected to build the first working set </Eq> . For a two-class classification task , this is simply selecting any two training vectors from two opposite classes ( in our implementation they are first vectors belonging to the positive and negative classes in the given training data ) . 
As SMO is used for optimization on the selected working set </Eq> in step 0 and step 5 , a minimum set of two vectors are selected to build the first working set </Eq> . For a two-class classification task , this is simply selecting any two training vectors from two opposite classes ( in our implementation they are first vectors belonging to the positive and negative classes in the given training data ) . 
109 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved 59:59:preserved 60:60:preserved 61:61:preserved 62:62:preserved 63:63:preserved 64:64:preserved 65:65:preserved 66:66:preserved 67:67:preserved 68:68:preserved 69:69:preserved 70:70:preserved 71:71:preserved 72:72:preserved 73:73:preserved 74:74:preserved

Compared with previously proposed methods , this initialization step is simpler . 
Compared with previously proposed methods , this initialization step is simpler . 
110 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

In \CITE the authors suggested to select randomly a set of p training instances , where p is a training parameter . 
In \CITE the authors suggested to select randomly a set of p training instances , where p is a training parameter . 
111 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

In \CITE , two closest vectors were recommended to form the first working set ( for the best reduction rate in ( 4 ) ) . 
In \CITE , two closest vectors were recommended to form the first working set ( for the best reduction rate in ( 4 ) ) . 
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

However , our preliminary experiments indicated that the result of iSVM does not depend much on this initialization scheme .
However , our preliminary experiments indicated that the result of iSVM does not depend much on this initialization scheme .
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

There have been different schemes proposed to update the WS in the common decomposition frame work . 
There have been different schemes proposed to update the WS in the common decomposition frame work . 
118 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

In fact , this is a distinctive step for each algorithm . 
In fact , this is a distinctive step for each algorithm . 
119 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Different updating strategies will produce different results in terms of convergence speed and final solution . 
Different updating strategies will produce different results in terms of convergence speed and final solution . 
120 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

In iSVM we propose a two-stage process for expanding and updating the WS . 
In iSVM we propose a two-stage process for expanding and updating the WS . 
121 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

In the first stage , iSVM tries to find a good approximation of SVM solution as quickly as possible . 
In the first stage , iSVM tries to find a good approximation of SVM solution as quickly as possible . 
122 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

It then scans through the whole training data once again to examine all remaining vectors one-by-one .
It then scans through the whole training data once again to examine all the remaining vectors one-by-one .
123 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :13:bigrammar-others

Potential SVs will be used to update the WS and find a new and better SVM .
Potential SVs will be used to update the WS and find a new and better SVM .
124 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

1 ) Re-sampling Selection : In support vector learning , if we know in advance which training example will be SV , we can remove all non-support vectors without changing the optimal solution . 
1 ) Re-sampling Selection : In support vector learning , if we know in advance which training example will be SV , we can remove all non-support vectors without changing the optimal solution . 
125 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved

An effective non-support vector removal strategy will guarantee an efficient algorithm . 
An effective non-support vector removal strategy will guarantee an efficient algorithm . 
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

We do this by a twostage data processing procedure .
We do this by a twostage data processing procedure .
127 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

In the first stage , step 3 of iSVM examines only a small number training examples and selects the most prominent vector to add to the WS . 
In the first stage , step 3 of iSVM examines only a small number training examples and selects the most prominent vector to add to the WS . 
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

The selection is based on violation of optimality criteria of a training vector with respect to a temporal solution found in previous iteration . 
The selection is based on violation of optimality criteria of a training vector with respect to a temporal solution found in previous iteration . 
129 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

At iteration t , </Eq> is selected based on the following criterion : ( 6 ) is the temporal solution </Eq> at iteration t . 
At iteration t , </Eq> is selected based on the following criterion : ( 6 ) is the temporal solution </Eq> at iteration t . 
130 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

The selection heuristic ( 5 ) is exactly the maximal violating heuristic that has been used by SMO \CITE and other early decomposition implementations .
The selection heuristic ( 5 ) is exactly the maximal violating heuristic that has been used by SMO \CITE and other early decomposition implementations .
131 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

The difference is that SMO updates all </Eq> and then scans through all of them to select the best pair . 
The difference is that SMO updates all </Eq> and then scans through all of them to select the best pair . 
132 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

iSVM uses temporal solutions St to examine a fixed number of training examples . 
iSVM uses temporal solutions St to examine a fixed number of training examples . 
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Difference in complexity between the two strategies will be analyzed in more detail in subsection D .
Difference in complexity between the two strategies will be analyzed in more detail in subsection D .
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

In the first stage , only the most prominent ( in terms of optimality violation ) vector is removed from T , all other vectors remain in the training data . 
In the first stage , only the most prominent ( in terms of optimality violation ) vector is removed from T while all other vectors remain in the training data . 
135 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:paraphrase 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

They can be re-examined in the next iterations and/or in the second phase . 
They can be re-examined in the next iterations and/or in the second phase . 
136 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

There are two reasons for the re-examination . 
There are two reasons for the re-examination . 
137 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Firstly , only the best vector is added to the WS , not every temporally violated vector . 
Firstly , only the best vector is added to the WS , not every temporally violated vector . 
138 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

The second reason is that temporal solutions are still not close enough to the optimal solution ( as only a small number of vectors are examined ) . 
Secondly , temporal solutions are still not close enough to the optimal solution ( as only a small number of vectors are examined ) . 
139 0,1,2,3,4:0,1:paraphrase 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved 25:22:preserved 26:23:preserved 27:24:preserved

Removing training examples from very beginning might mistakenly remove the true SVs .
Removing training examples from the very beginning might result in the mistaken removal of the true SVs .
140 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7,8:8,9,10,11,12,13:paraphrase 9:14:preserved 10:15:preserved 11:16:preserved 12:17:preserved :4:mogrammar-det

The first stage will finish when we are sure at some extend that </Eq> is a good approximation . 
The first stage will finish when we are sure to some extent that </Eq> is a good approximation . 
141 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-prep 10:10:preserved 11:11:spelling 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

In iSVM , we use the following heuristics for ending the first stage : i ) None of N randomly selected vectors violates optimality criteria , or ii ) Size of </Eq> is bigger than a predefined number ( 1 ,000 in our experiments ) , or iii ) All training vectors are examined once in average .
In iSVM , we use the following heuristics for ending the first stage : i ) None of N randomly selected vectors violates optimality criteria , or ii ) Size of </Eq> is bigger than a predefined number ( 1 ,000 in our experiments ) , or iii ) All training vectors are examined once in average .
142 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved

The first condition has been used in various situations including kernel matrix approximation \CITE and CoreSVM \CITE . 
The first condition has been used in various situations , including kernel matrix approximation \CITE and CoreSVM \CITE . 
143 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved :9:bigrammar-others

This heuristic is based on the fact that with a sample size of N = 59 , we still can catch one among 5% most violating vectors \CITE . 
This heuristic is based on the fact that with a sample size of N = 59 , we still can catch one among 5% most violating vectors \CITE . 
144 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

The second and third conditions mean that the temporal solution will be considered stable and close enough to the optimal solution when a big number of training examples are examined .
The second and third conditions mean that the temporal solution will be considered stable and close enough to the optimal solution when a big number of training examples are examined .
145 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

After a good approximation has been achieved we can switch to the second stage to remove non-support vectors without much affect to the final solution .
After a good approximation has been achieved , we can switch to the second stage to remove non-support vectors without affecting much the final solution .
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 20,21,19:20,21:para-freeword 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

2 ) Final Scanning : Based on the assumption that phase one produces a good approximate solution , phase two examines all training examples remaining in T one-by-one . 
2 ) Final Scanning : Based on the assumption that phase one produces a good approximate solution , phase two examines all the training examples remaining in T one-by-one . 
147 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :22:mogrammar-det

If vector </Eq> violates optimality criteria with respect to temporal solution </Eq> then it is immediately used to update </Eq> to form a new solution . 
If vector </Eq> violates optimality criteria with respect to temporal solution </Eq> , then it is immediately used to update </Eq> to form a new solution . 
148 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :12:bigrammar-others

Otherwise , it is removed from training data T .
Otherwise , it is removed from training data T .
149 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The algorithm will stop when all training examples in T are examined .
The algorithm will stop when all training examples in T are examined .
150 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The SMO is used to minimize the objective function L on the selected set of vectors </Eq> . 
The SMO is used to minimize the objective function L on the selected set of vectors </Eq> . 
155 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

It is very efficient because </Eq> is usually much smaller than the whole training data . 
It is very efficient because </Eq> is usually much smaller than the whole training data . 
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Moreover , SMO can start optimization on Wt from </Eq> - the optimal solution on </Eq> in previous iteration . 
Moreover , SMO can start optimization on Wt from </Eq> - the optimal solution on </Eq> in previous iteration . 
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The difference is only about one newly added vector . 
The difference is only about one newly added vector . 
158 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

This makes SMO converge very fast .
This makes SMO converge very fast .
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

In this section we analyze the computational complexity of the proposed iSVM algorithm . 
In this section we analyze the computational complexity of the proposed iSVM algorithm . 
164 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

At each iteration t , step 3 takes time </Eq> to examine one training example . 
At each iteration t , step 3 takes time </Eq> to examine one training example . 
165 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

In the first phase , each training example is examined at most once ( when the stopping condition iii ) is applied ) . 
In the first phase , each training example is examined at most once ( when the stopping condition iii ) is applied ) . 
166 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

The second phase scans training data once more . 
The second phase scans the training data once more . 
167 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved :4:mogrammar-det

Thus , each training example is examined at most twice . 
Thus , each training example is examined at most twice . 
168 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Totally step 3 takes time </Eq> .
Totally step 3 takes time </Eq> .
169 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Theoretically solving a QP problem of size </Eq> in step 3 takes time </Eq> . 
Theoretically solving a QP problem of size </Eq> in step 3 takes time </Eq> . 
170 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

However , solution </Eq> at iteration t – 1 is used as an initial point , then step 5 requires only time </Eq> per iteration ( in fact it can be done in </Eq> by an efficient updating procedure \CITE ) . 
However , solution </Eq> at iteration t – 1 is used as an initial point , then step 5 requires only time </Eq> per iteration ( in fact , it can be done in </Eq> by an efficient updating procedure \CITE ) . 
171 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved :28:bigrammar-others

Totally step 5 takes time </Eq> .
Totally , step 5 takes time </Eq> .
172 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved :1:bigrammar-others

From the second phase the shrinking technique is applied ( a non support vector in </Eq> is replaced by a new vector found by step 3 ) , then size of the final working set approximates the number of final support vectors . 
From the second phase , the shrinking technique is applied ( a non support vector in </Eq> is replaced by a new vector found by step 3 ) , then size of the final working set approximates the number of final support vectors . 
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:4:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved 42:43:preserved :28:bigrammar-others

In total , the time complexity of iSVM is </Eq> , which is linear with number of training examples l and cubic with number of support vectors .
In total , the time complexity of iSVM is </Eq> , which is linear with number of training examples l and cubic with number of support vectors .
174 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

Time complexity of the SMO algorithm described in TABLE I is </Eq> where </Eq> is the number of SMO iterations . 
Time complexity of the SMO algorithm described in TABLE I is </Eq> where </Eq> is the number of SMO iterations . 
175 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

In iSVM , SMO is used to solve the QP on a small set of selective training examples . 
In iSVM , SMO is used to solve the QP on a small set of selective training examples . 
176 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

From the complexity analysis above we can see that iSVM has advantage over the traditional SMO implementation when number of final SVs is much smaller than data size .
From the complexity analysis above , we can see that iSVM has an advantage over the traditional SMO implementation when the number of the SVs is much smaller than the data size .
177 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:21:preserved 19:22:preserved 20:23:paraphrase 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:30:preserved 27:31:preserved 28:32:preserved :12:mogrammar-det :20:mogrammar-det :29:mogrammar-det

iSVM belongs to the decomposition family of SVM training algorithms . 
iSVM belongs to the decomposition family of SVM training algorithms . 
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In this section we discuss properties of iSVM and its relation to conventional methods .
In this section , we discuss properties of iSVM and its relation to conventional methods .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :3:bigrammar-others

In the initialization step , iSVM selects any two vectors from opposite classes ( for a two-class classification task ) .
In the initialization step , iSVM selects any two vectors from opposite classes ( for a two-class classification task ) .
185 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Based on calculation ( 2 ) , a closer pair of vectors will produce a better reduction in objective function L . 
Based on calculation ( 2 ) , a closer pair of vectors will produce a better reduction in objective function L . 
186 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

However , our preliminary experiments indicate that final solutions are not affected much by this initialization scheme which has been used by CoreSVM \CITE or SimpleSVM \CITE . 
However , our preliminary experiments indicate that final solutions are not affected much by this initialization scheme which has been used by CoreSVM \CITE or SimpleSVM \CITE . 
187 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

Other initialization strategies include selecting randomly a set of p vectors \CITE , or using all training data from the beginning \CITE .
Other initialization strategies include selecting randomly a set of p vectors \CITE , or using all training data from the beginning \CITE .
188 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

For updating the working set , several algorithms share the same way of adding only one vector to the WS in each optimization loop , e.g. SimpleSVM \CITE , CoreSVM \CITE .
For updating the working set , several algorithms share the same way of adding only one vector to the WS in each optimization loop , e.g. SimpleSVM \CITE , CoreSVM \CITE .
189 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

Different selection criteria have been proposed , including optimality violation \CITE , probabilistic estimation \CITE . 
Different selection criteria have been proposed , including optimality violation \CITE , probabilistic estimation \CITE . 
190 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

iSVM differs from others in its two-stage strategy . 
iSVM differs from others in its two-stage strategy . 
191 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

In the first stage , temporal solutions are not good enough to justify which training example is surely an SV or not , so iSVM re examines them in the second phase . 
In the first stage , temporal solutions are not good enough to justify which training example is surely an SV or not , so iSVM re examines them in the second phase . 
192 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

Note that the working set grows from a minimum size , thus the first phase runs very fast because size of the working set </Eq> is small and only the best among N examined vectors is added to the working set . 
Note that the working set grows from a minimum size ; thus , the first phase runs very fast because the size of the working set </Eq> is small and only the best among N examined vectors is added to the working set . 
193 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:12:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:21:preserved 20:22:preserved 21:20:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:23:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:31:preserved 39:41:preserved 40:42:preserved 41:43:preserved :10:unaligned :40:mogrammar-det

It is not clearly described in \CITE and \CITE that training examples are re-examined or not . 
It is not clearly described in \CITE and \CITE that training examples are re-examined or not . 
194 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

If they are removed from T too early from beginning then it is with high probability that many good training examples ( or SVs ) might be removed . 
If they are removed from T too early from beginning , then it is highly likely many good training examples ( or SVs ) might be removed . 
195 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13,14,15,16:14,15:paraphrase 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved

In contrast , if they are remaining in T all the time and re-examined many times then the computation is not efficient .
In contrast , if they remain in T all the time and re-examined many times , then the computation is not efficient .
196 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5:bigrammar-vtense 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Comparing with other approximation methods , iSVM uses a rather simple stopping condition . 
Comparing with other approximation methods , iSVM uses a rather simple stopping condition . 
197 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

In our point of view , CoreSVM uses a looser condition : none of N sampled training data violates the optimality condition with respect to temporal solution at iteration </Eq> . 
In our point of view , CoreSVM uses a looser condition : none of N sampled training data violates the optimality condition with respect to temporal solution at iteration </Eq> . 
198 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

In the experiment section we will show that using this stopping condition will lead to trained SVMs with smaller number of SVs , faster training time , but bigger variation in predictive performance .
In the experiment section , we will show that using this stopping condition will lead to trained SVMs with smaller number of SVs , faster training time , but bigger variation in predictive performance .
199 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:4:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:23:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved :27:bigrammar-others

In this section we describe our experiments to evaluate iSVM and comparisons with other SVM training algorithms .
In this section , we describe our experiments to evaluate iSVM and comparisons with other SVM training algorithms .
204 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved :3:bigrammar-others

We select four datasets from different domains : web page categorization from UCI machine learning repository ( " Web " ) , text-decoding used in IJCNN 2001 conference competition ( " IJCNN"  ) , extended USPS hand written digit recognition data for discriminating '0' and '1' ( " zero-one"  ) , and KDD-CUP 1999 network intrusion detection datasets used in the KDD 1999 conference competition ( " KDD-CUP99"  ) .
We select four datasets from different domains : web page categorization from UCI machine learning repository ( " Web " ) , text-decoding used in IJCNN 2001 conference competition ( " IJCNN"  ) , extended USPS hand written digit recognition data for discriminating '0' and '1' ( " zero-one"  ) , and KDD-CUP 1999 network intrusion detection datasets used in the KDD 1999 conference competition ( " KDD-CUP99"  ) .
205 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved 59:59:preserved 60:60:preserved 61:61:preserved 62:62:preserved 63:63:preserved 64:64:preserved 65:65:preserved 66:66:preserved 67:67:preserved 68:68:preserved 69:69:preserved

All of the datasets summarized in TABLE III have nearly or more than 50 ,000 training examples . 
All of the datasets summarized in TABLE III have nearly or more than 50 ,000 training examples . 
206 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

All of our experiments were conducted on a PC Windows machine with a 3GHz CPU and 2GB RAM memory .
All of our experiments were conducted on a PC Windows machine with a 3GHz CPU and 2GB RAM memory .
207 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

In the first experiment , we compare training performance of iSVM with LibSVM \CITE – one of the best SMO implementation , CoreSVM \CITE – a recent proposed algorithm that has achieved a remarkable performance on the KDD-CUP 1999 dataset . 
In the first experiment , we compare training performance of iSVM with LibSVM \CITE – one of the best SMO implementation , CoreSVM \CITE – a recent proposed algorithm that has achieved a remarkable performance on the KDD-CUP 1999 dataset . 
211 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved

Comparison criteria include training time , number of support vector , testing accuracy . 
Comparison criteria include training time , number of support vector , and testing accuracy . 
212 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10,11:bigrammar-others 11:12:preserved 12:13:preserved 13:14:preserved

Parameters were chosen for achieving good accuracy on testing data : Gaussian kernel </Eq> with </Eq> for " Web " , </Eq> for " IJCNN " , </Eq> for " USPS zero-one " , </Eq> for " KDD-CUP"  . 
Parameters were chosen for achieving good accuracy on testing data : Gaussian kernel </Eq> with </Eq> for " Web " , </Eq> for " IJCNN " , </Eq> for " USPS zero-one " , </Eq> for " KDD-CUP"  . 
213 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

As both iSVM and CoreSVM \CITE use probabilistic trick to speedup training process in step 3 training results are random variables .
As both iSVM and CoreSVM \CITE use probabilistic trick to speedup in step 3 training results are random variables .
214 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12,13,14,15,16:14,11,12,13:paraphrase 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved

We conduct this experiment ten times and estimate statistics of these variables ( for CoreSVM we randomly mix the original data ten times and run training program on these mixed data ) .
We conduct this experiment ten times and estimate statistics of these variables ( for CoreSVM we randomly mix the original data ten times and run training program on these mixed data ) .
215 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

From experimental data reported in TABLE IV we can see that iSVM runs much faster than LibSVM on the " zero-one " and " KDD-CUP99 " where the number of SVs is much smaller than number of training data ( 0.45% and 0.01% correspondingly ) . 
From experimental data reported in TABLE IV we can see that iSVM runs much faster than LibSVM on the " zero-one " and " KDD-CUP99 " where the number of SVs is much smaller than number of training data ( 0.45% and 0.01% correspondingly ) . 
216 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved

This shows the advantage of the two-stage incremental WS selection strategy . 
This shows the advantage of the two-stage incremental WS selection strategy . 
217 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

On the KDD-CUP 1999 data CoreSVM has an incredible training time : two seconds in average for training on a nearly five millions training examples . 
On the KDD-CUP 1999 data , CoreSVM has an incredible training time : two seconds in average for training on a nearly five millions training examples . 
218 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :5:bigrammar-others

In the next experiment we investigate and analyze more to show that iSVM with an early stopping condition can also achieve this training time performance . 
In the next experiment , we investigate and analyze more to show that iSVM with an early stopping condition can also achieve this training time performance . 
219 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :4:bigrammar-others

However the cost is big variation of trained machines .
However , the cost is the huge variation of trained machines .
220 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5,6:paraphrase 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved

In the second experiment we try iSVM with different stopping conditions on the KDD-CUP 1999 dataset . 
In the second experiment , we try iSVM with different stopping conditions on the KDD-CUP 1999 dataset . 
225 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :4:bigrammar-others

The first one is right after the first stage finished , or when there is no vector out of 59 randomly selected training data violates temporal optimality conditions . 
The first one is right after the first stage finished , or when there is no vector out of 59 randomly selected training data violates the temporal optimality conditions . 
226 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :25:mogrammar-det

Other stopping conditions are based on the total number of examined examples ( by the second stage ) : 10% , 20% , 40% and 80% of the whole data .
Other stopping conditions are based on the total number of examined examples ( by the second stage ) : 10% , 20% , 40% and 80% of the whole data .
227 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

This experiment is also conducted ten times to have estimation of training times and testing accuracies . 
This experiment is also conducted ten times to have estimation of training times and testing accuracies . 
228 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

As we can see in Figure 2 , with an early stopping condition iSVM runs faster but produces SVMs with higher variation in predictive accuracy ) .
As we can see in Figure 2 , with an early stopping condition , iSVM runs faster but produces SVMs with higher variation in predictive accuracy ) .
229 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved :13:bigrammar-others

Especially , the first stage finishes after a very small number of iterations ( 32 in average ) . 
Especially , the first stage finishes after a very small number of iterations ( 32 in average ) . 
230 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

It means that the trained machines are determined by a maximum of only 1900 training examples , corresponding to 0.04% of the whole data . 
It means that the trained machines are determined by a maximum of only 1900 training examples , corresponding to 0.04% of the whole data . 
231 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

This number explains why iSVM takes only 2.8 seconds to train on the KDD-CUP 1999 data with nearly five million records .
This number explains why iSVM takes only 2.8 seconds to train on the KDD-CUP 1999 data with nearly five million records .
232 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

In our point of view the same phenomenon happens for CoreSVM . 
From our point of view , the same phenomenon happens for CoreSVM . 
233 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

The big variation of the trained machine indicates that i ) is a too loose stopping condition for the KDD-CUP 1999 data case . 
The huge variation of the trained machine indicates that i ) is a too loose stopping condition for the KDD-CUP 1999 data case . 
234 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

We draw variations in predictive accuracy and average training time of iSVM and CoreSVM in Figure 2 for a comparison .
We draw variations in predictive accuracy and average training time of iSVM and CoreSVM in Figure 2 for comparison .
235 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18::mogrammar-det 19:18:preserved 20:19:preserved

Note that there has been different approaches tackling the KDD-CUP 1999 problem . 
Note that there has been different approaches tackling the KDD-CUP 1999 problem . 
237 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

In TABLE V we describe performances produced by different methods , including data random sampling , active SVM learning \CITE , clustering-based \CITE , CoreSVM \CITE , LibSVM \CITE , and iSVM . 
In TABLE V we describe performances produced by different methods , including data random sampling , active SVM learning \CITE , clustering-based \CITE , CoreSVM \CITE , LibSVM \CITE , and iSVM . 
238 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

iSVM achieves superior generalization performance and faster than SMO , random selection , cluster-based , and active learning .
iSVM achieves superior generalization performance and faster than SMO , random selection , cluster-based , and active learning .
239 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

We have introduced a new incremental algorithm for training SVMs . 
We have introduced a new incremental algorithm for training SVMs . 
245 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

iSVM differs from other methods in its twostage strategy to process training data . The first phase aims at finding a good approximate SVM solution as quickly as possible . 
iSVM differs from other methods in its twostage strategy to process training data . The first phase aims at finding a good approximate SVM solution as quickly as possible . 
246 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

The second phase uses temporal solutions to find out the remaining SVs . 
The second phase uses temporal solutions to find out the remaining SVs . 
247 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Analysis and experimental result indicate that iSVM has advantage over conventional SMO implementation on applications where number of training examples is much larger than number of SVs . 
The analysis and the experimental result indicate that iSVM has advantage over conventional SMO implementation on applications where number of training examples is much larger than number of SVs . 
248 0:1:preserved 1:2:preserved 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved :0:mogrammar-det :3:mogrammar-det

Training SVMs with large number of SVs is our research issue in the future .
Training SVMs with large number of SVs is our research issue in the future .
249 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

