<document>
﻿<document>
0 0:0:preserved

Incorporating Statistical Background Model and Joint Probabilistic Data Association Filter into Motorcycle Tracking
Incorporating Statistical Background Model and Joint Probabilistic Data Association Filter into Motorcycle Tracking
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Multi - target tracking is an attractive research field due to its widespread application areas and challenges .
Multi - target tracking is an attractive research field due to its widespread application areas and challenges .
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Every point tracking method includes two mechanisms : object detection and data association .
Every point tracking method includes two mechanisms : object detection and data association .
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

This paper is a combination between a statistical background modeling method for foreground object detection and Joint Probabilistic Data Association filter ( JPDAF ) in the context of motorcycle tracking .
This paper is a combination between a statistical background modeling method for foreground object detection and Joint Probabilistic Data Association filter ( JPDAF ) in the context of motorcycle tracking .
8 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

A major limitation of JPDAF is its inability to adapt to changes in the number of targets , but in this work , it is modified so that we can successfully apply JPDAF with known number of targets at each time instant .
A major limitation of JPDAF is its inability to adapt to changes in the number of targets , but in this work , it is modified so that we can successfully apply JPDAF with known number of targets at each time instant .
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved

The experimental system works well with the number of targets less than 10 / frame and be able to self-evolve with gradual and " once-off " background changes .
The experimental system works well with the number of targets less than 10 / frame and be able to self-evolve with gradual and " once-off " background changes .
10 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

Motion understanding is an essential function of human vision . 
Motion understanding is an essential function of human vision . 
15 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Consequently , object tracking takes the crucial role in computer vision .
Consequently , object tracking takes the crucial role in computer vision .
16 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Multi - target tracking has widespread applications in both military ( air defense , air traffic control , ocean surveillance ) and civilian areas (  for automatical surveillance demands in public or secret places ) , especially when human labour becomes more and more expensive .
Multi - target tracking has widespread applications in both military ( air defense , air traffic control , ocean surveillance ) and civilian areas (  for automatical surveillance demands in public or secret places ) , especially when human labour becomes more and more expensive .
17 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved

Object tracking , in general , is a challenging problem .
Object tracking , in general , is a challenging problem .
18 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Its complexities arise due to the following factors : loss of information caused by projection from 3D to 2D space , complex object motions , complex object shapes , partial and full object occlusions , scene illumination changes , and realtime processing requirements .
Its complexities arise due to the following factors : loss of information caused by projection from 3D to 2D space , complex object motions , complex object shapes , partial and full object occlusions , scene illumination changes , and realtime processing requirements .
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved

There are three main categories of object tracking </CITE> : point tracking , kernel tracking , and silhouette tracking .
There are three main categories of object tracking </CITE> : point tracking , kernel tracking , and silhouette tracking .
20 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

While kernel and silhouette tracking concern object shapes , point tracking considers an object as a
While kernel and silhouette tracking concern object shapes , point tracking considers an object as a
21 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

point and just focuses on its position and motion , which can be represented by state vector .
point and just focuses on its position and motion , which can be represented by state vector .
22 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Filtering is a class of methods that is suited for solving the dynamic state estimation problems of point tracking .
Filtering is a class of methods that is suited for solving the dynamic state estimation problems of point tracking .
23 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

In multi-target tracking , we have a task of finding a correspondence between the current targets and measurements , named data association .
In multi-target tracking , we have a task of finding a correspondence between the current targets and measurements , named data association .
24 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Data association is a complicated problem especially in the presence of occlusions , misdetections , entries , and exits of objects .
Data association is a complicated problem especially in the presence of occlusions , misdetections , entries , and exits of objects .
25 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

There are many statistical techniques for data association </CITE> , among them , Joint Probabilistic Data Association ( JPDA) is the method that aims to find a correspondence between measurements and objects at the current time step based on enumerating all possible associations and computing the association probabilities , it is a widely used technique for data association (  </CITE> , </CITE> ) .
There are many statistical techniques for data association </CITE> , among them , Joint Probabilistic Data Association ( JPDA) is the method that aims to find a correspondence between measurements and objects at the current time step based on enumerating all possible associations and computing the association probabilities . It is a widely used technique for data association (  </CITE> , </CITE> ) .
26 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 49,48:49,48:bigrammar-others 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved 59:59:preserved 60:60:preserved 61:61:preserved 62:62:preserved 63:63:preserved

However , to have a good JPDA filter ( JPDAF ) , it is required to have accurate measurements , that means we need to have good object detection results .
However , to have a good JPDA filter ( JPDAF ) , it is required to have accurate measurements . That means we need to have good object detection results .
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19,20:20,19:bigrammar-others 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

Every tracking method requires an object detection mechanism , there are those which just need the detection at the first time objects appear , while the others need it in every frame , point tracking belongs to this type .
Every tracking method requires an object detection mechanism . There are those which just need the detection at the first time objects appear , while the others need it in every frame . Point tracking belongs to this type .
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 9,8:9,8:bigrammar-others 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32,33:32,33:bigrammar-others 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved

One effective way for foreground object detection is to give an accurate background model .
One effective way for foreground object detection is to give an accurate background model .
29 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Recently , L . Li et al proposed a foreground object detection method by statistical modeling of complex backgrounds </CITE> .
Recently , L . Li et al proposed a foreground object detection method by statistical modeling of complex backgrounds </CITE> .
30 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

This work used a Bayesian framework for incorporating three type of features : spectral , spatial and temporal features into a representation of complex background containing both stationary and nonstationary objects .
This work used a Bayesian framework for incorporating three types of features , i.e. , spatial and temporal features into a representation of complex background containing both stationary and nonstationary objects .
31 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-nnum 10:10:preserved 11:11:preserved 12,13:13,14:paraphrase 14:12:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

With the statistics of background features , the method is able to : represent the appearances of both static and dynamic background pixels , self-evolve to gradual as well as sudden " once - off " background changes .
With the statistics of background features , the method is able to represent the appearances of both static and dynamic background pixels and self - evolve to gradual as well as sudden " once - off " background changes .
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:bigrammar-others 24:23,25,24:typo 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved

Taking advantage of the excellent object detection results from this method , this paper employs JPDAF for vehicle tracking in the motorcycle lane .
Taking advantage of the excellent object detection results from this method , this paper employs JPDAF for vehicle tracking in the motorcycle lane .
33 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

A major limitation of JPDAF is its inability to adapt to changes in the number of targets , because it is confused between a measurement originated from a new object appearance and a false alarm .
A major limitation of JPDAF is its inability to adapt to changes in the number of targets , because it is confused between a measurement originated from a new object appearance and a false alarm .
34 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

However , in the context of motorcycle surveillance , we has proposed a strategy to detect new objects entering and objects leaving the observation area , so that we can successfully apply JPDAF with known number of targets at each time instant . 
However , in the context of motorcycle surveillance , we has proposed a strategy to detect new objects entering and objects leaving the observation area , so that we can successfully apply JPDAF with known number of targets at each time instant . 
35 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved

The experimental system has good results with the number of targets less than 10 / frame , including of detecting and tracking the wrong-wayed motorcycles .
The experimental system has good results with the number of targets less than 10 / frame , including results detecting and tracking the wrong-wayed motorcycles .
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:paraphrase 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Motorcycle tracking in particular and traffic tracking in generral , is an interesting but challenging application .
Motorcycle tracking in particular and traffic tracking in generral is an interesting but challenging application .
37 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9::bigrammar-others 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved

Its main difficulties can be enumerated as: the severely occlusions when traffic density is high ( especially in rush hours ) , the shadows of big vehicles , and the real-time processing demand of a traffic surveillance system .
Its main difficulties can be enumerated as the severely occlusions when traffic density is high ( especially in rush hours ) , the shadows of big vehicles , and the real-time processing demand of a traffic surveillance system .
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-others 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

This paper is the next step ( after </CITE> ) of the effort finding the most satisfied approach for automatical traffic surveillance in big cities of Vietnam .
This paper is the next step ( after </CITE> ) to find the most satisfying approach for automatical traffic surveillance in big cities of Vietnam .
39 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10::unaligned 12,13,11:10,11:paraphrase 14:12:preserved 15:13:preserved 16:14:bigrammar-wform 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved

The remains of this paper is organized as follow : section II is the main ideas for statistical modeling of complex background proposed in </CITE> , section III reviews the background of JPDAF , a complete algorithm and experimented results on simulated data of JPDAF are presented at the end of this section , section IV is the combination of statistical background model and the modified JPDAF so that they can be applied in the motorcycle tracking situation , the experimental results of this combination are submitted in section V , and the conclusion is after all others .
The rest of this paper is organized as Section II is the main ideas for statistical modeling of complex background proposed in </CITE> . Section III reviews the background of JPDAF , a complete algorithm and experimented results on simulated data of JPDAF are presented at the end of this section , section IV is the combination of statistical background model and the modified JPDAF so that they can be applied in the motorcycle tracking situation . The experimental results of this combination are submitted in section V , and section VI presents our conclusion .
40 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10:8:paraphrase 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25,26:23,24:bigrammar-others 27:25:preserved 28:26:preserved 29:27:preserved 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:preserved 37:35:preserved 38:36:preserved 39:37:preserved 40:38:preserved 41:39:preserved 42:40:preserved 43:41:preserved 44:42:preserved 45:43:preserved 46:44:preserved 47:45:preserved 48:46:preserved 49:47:preserved 50:48:preserved 51:49:preserved 52:50:preserved 53:51:preserved 54:52:preserved 55:53:preserved 56:54:preserved 57:55:preserved 58:56:preserved 59:57:preserved 60:58:preserved 61:59:preserved 62:60:preserved 63:61:preserved 64:62:preserved 65:63:preserved 66:64:preserved 67:65:preserved 68:66:preserved 69:67:preserved 70:68:preserved 71:69:preserved 72:70:preserved 73:71:preserved 74:72:preserved 75:73:preserved 76:74:preserved 77:75:preserved 78,79:76,77:bigrammar-others 80:78:preserved 81:79:preserved 82:80:preserved 83:81:preserved 84:82:preserved 85:83:preserved 86:84:preserved 87:85:preserved 88:86:preserved 89:87:preserved 90:88:preserved 91:89:preserved 92,94,95,96,97:91,92,93,95,90:paraphrase 93:94:preserved 98::unaligned

Let </Eq> be a pixel in a video frame at time </Eq> with its Decartes co-ordinate , </Eq> be the feature vector extracted at </Eq> .
Let </Eq> be a pixel in a video frame at time </Eq> with its Decartes co-ordinate , </Eq> be the feature vector extracted at </Eq> .
46 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Then using Bayes formula we can determine the probability that </Eq> belongs to background given </Eq> as follow : </Eq> ( 1 ) where </Eq> implies that </Eq> belongs to background .
Then using the Bayes formula , we can determine the probability that </Eq> belongs to a background given </Eq> as follow : </Eq> ( 1 ) where </Eq> implies that </Eq> belongs to the background .
47 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:2:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:34:preserved 31:35:preserved :9:mogrammar-det :15:mogrammar-det :33:mogrammar-det

Similarly , the probability that </Eq> belongs to a foreground object given </Eq> is : </Eq> ( 2 ) where </Eq> refers that </Eq> is a foreground point .
Similarly , the probability that </Eq> belongs to a foreground object given </Eq> is : </Eq> ( 2 ) where </Eq> refers that </Eq> is a foreground point .
48 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

According to Bayesian decision rule , </Eq> will be classified as background point if : </Eq> ( 3 )
According to Bayesian decision rule , </Eq> will be classified as a background point if : </Eq> ( 3 )
49 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :11:mogrammar-det

Undergoing some transformations , ( 3 ) is equivalent to : </Eq> ( 4 ) where </Eq> is the probability that </Eq> is classified as background , </Eq> is the probability that </Eq> is observed at </Eq> , and </Eq> is the probability that </Eq> is observed when </Eq> has already been classified as background .
Undergoing some transformations , ( 3 ) is equivalent to : </Eq> ( 4 ) where </Eq> is the probability that </Eq> is classified as the background , </Eq> is the probability that </Eq> is observed at </Eq> , and </Eq> is the probability that </Eq> is observed when </Eq> has already been classified as the background .
50 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:25:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:30:preserved 42:43:preserved 43:44:preserved 44:45:preserved 45:46:preserved 46:47:preserved 47:48:preserved 48:49:preserved 49:50:preserved 50:51:preserved 51:52:preserved 52:53:preserved 53:54:preserved 54:56:preserved 55:57:preserved :42:mogrammar-det :55:mogrammar-det

Thus , we can use </Eq> , </Eq> and </Eq> , which will be modeled and estimated based on statistics in subsection B and C , to judge whether a point comes from background or foreground .
Thus , we can use </Eq> , </Eq> and </Eq> , which will be modeled and estimated based on statistics in subsection B and C , to judge whether a point comes from a background or a foreground .
51 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:34:preserved 34:35:preserved 35:37:preserved 36:38:preserved :33:mogrammar-det :36:mogrammar-det

To estimate </Eq> , </Eq> and </Eq> , we need a data structure to take into account the statistical information relevant to feature vector </Eq> at </Eq> over a sequence of frames .
To estimate </Eq> , </Eq> and </Eq> , we need a data structure to take into account the statistical information relevant to feature vector </Eq> at </Eq> over a sequence of frames .
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

Each feature type at </Eq> has a table of statistics defined as : </Eq> ( 5 ) where </Eq> grasps the </Eq> at time </Eq> based on the classification results at </Eq> up to time </Eq> , and </Eq> takes note the statistics of the </Eq> feature vectors which have the highest frequencies at </Eq> , each </Eq> contains : </Eq> ( 6 ) where </Eq> is the dimension of </Eq> .
Each feature type at </Eq> has a table of statistics defined as : </Eq> ( 5 ) where </Eq> grasps the </Eq> at time </Eq> based on the classification results at </Eq> up to time </Eq> , and </Eq> takes note the statistics of the </Eq> feature vectors which have the highest frequencies at </Eq> , each </Eq> contains : </Eq> ( 6 ) where </Eq> is the dimension of </Eq> .
57 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved 59:59:preserved 60:60:preserved 61:61:preserved 62:62:preserved 63:63:preserved 64:64:preserved 65:65:preserved 66:66:preserved 67:67:preserved 68:68:preserved 69:69:preserved 70:70:preserved 71:71:preserved

In table </Eq> , the </Eq> are kept sorting in descending order with respect to </Eq> , the frequence of </Eq> .
In table </Eq> , the </Eq> are kept sorting in descending order with respect to </Eq> , the frequence of </Eq> .
58 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Then , the first </Eq> members in </Eq> will be used to estimate </Eq> , </Eq> and </Eq> in subsection C .
Then , the first </Eq> members in </Eq> will be used to estimate </Eq> , </Eq> and </Eq> in subsection C .
59 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Another important issue in background modeling is feature selection .
Another important issue in background modeling is feature selection .
60 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Herein , three types of features : spectral , spatial and temporal features are combined for complex background modeling .
Three types of features (namely spectral, spatial and temporal features) are combined for complex background modeling. 
61 0,1,6:4:paraphrase 2:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 7,8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18,19:15:preserved

1 ) Feature selection for static background pixels : due to the constancy in shape and appearance of a pixel comes from a static background object , spectral and spatial features , in this case are its color and gradient , are exploited .
1 ) Feature selection for static background pixels : due to the constancy in shape and appearance of a pixel coming from a static background object , spectral and spatial features , in this case being its color and gradient , are exploited .
62 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-vtense 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:41:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:35:bigrammar-vtense 42:42:preserved 43:43:preserved

Let </Eq> be the color vector and </Eq> be the gradient vector of a pixel </Eq> , then we respectively need two tables </Eq> and </Eq> to make them learned .
Let </Eq> be the color vector and </Eq> be the gradient vector of a pixel </Eq> , then we respectively need two tables </Eq> and </Eq> to make them learned .
63 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

Because two types of features are used , the decision rule in ( 4 ) must be modified with the assumption that color and gradient vectors are independent : </Eq> ( 7 )
Because two types of features are used , the decision rule in ( 4 ) must be modified with the assumption that color and gradient vectors are independent : </Eq> ( 7 )
64 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

With color and gradient features , we need a quantization measure that is less sensitive to illumination changes , so a normalized distance measure based on the inner product of two vectors is adopted </CITE> : </Eq> ( 8 ) where </Eq> , </Eq> and </Eq> are identified with each other if </Eq> .
With color and gradient features , we need a quantization measure that is less sensitive to illumination changes , so a normalized distance measure based on the inner product of two vectors is adopted </CITE> : </Eq> ( 8 ) where </Eq> , </Eq> and </Eq> are identified with each other if </Eq> .
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved

2 ) Feature selection for dynamic background pixels : as for a dynamic background object , its motion is usually in a small range ( so that it is still referred to background ) and has a period : waving tree branches and their shadows for example .
2 ) Feature selection for dynamic background pixels : as for a dynamic background object , its motion is usually in a small range ( so that it is still referred as a background ) and has a period, for example, waving tree branches and their shadows .
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:bigrammar-prep 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37,38:38:bigrammar-others 39:41:preserved 40:42:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved 45,46:39,40:para-freeword 47:47:preserved :32:mogrammar-det

Hence , the color co-occurrence feature is used to take advantage of these properties .
Hence , the color co-occurrence feature is used to take advantage of these properties .
67 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Let </Eq> and </Eq> be the color features at time </Eq> and </Eq> at pixel </Eq> , then the color co-occurrence vector at time </Eq> and pixel </Eq> is
Let </Eq> and </Eq> be the color features at time </Eq> and </Eq> at pixel </Eq> , then the color co-occurrence vector at time </Eq> and pixel </Eq> is
68 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

defined as </Eq> .
defined as </Eq> .
69 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

In this case , another distance measure is used : </Eq> ( 9 )
In this case , another distance measure is used : </Eq> ( 9 )
70 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

So far , we have already had a data structure for statistics , now for the procedure of feature learning .
So far , we have already had a data structure for statistics , now for the procedure of feature learning .
75 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

There are two kinds of background changes , so we will have different learning strategy for each one .
There are two kinds of background changes , so we will have different learning strategy for each one .
76 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

1 ) Gradual background changes : once we have the classification result at pixel </Eq> ( subsection D ) and time </Eq> , its statistics at the next time instant will be updated as follow : </Eq> ( 10 ) where </Eq> , </Eq> is the learning rate .
1 ) Gradual background changes : once we have the classification result at pixel </Eq> ( subsection D ) and time </Eq> , its statistics at the next time instant will be updated as follows : </Eq> ( 10 ) where </Eq> , </Eq> is the learning rate .
77 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:bigrammar-inter 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved

If </Eq> is classified as background point at time </Eq> , then </Eq> , else </Eq> .
If </Eq> is classified as background point at time </Eq> , then </Eq> , else </Eq> .
78 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

If the input feature vector </Eq> is identified with </Eq> then </Eq> , otherwise , </Eq> .
If the input feature vector </Eq> is identified with </Eq> then </Eq> , otherwise , </Eq> .
79 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Besides , if there is no </Eq> in table </Eq> identified with </Eq> , the last component in </Eq> will be replaced by new one : </Eq> ( 11 )
Besides , if there is no </Eq> in table </Eq> identified with </Eq> , the last component in </Eq> will be replaced by new one : </Eq> ( 11 )
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

2 ) " Once - off " background changes : an " once - off " background change occurs when there is a suddenly change in illumination , or a moving foreground object stopping and becoming a background instance , that means when background becomes foreground suddenly or vice versa .
2 ) " Once - off " background changes : an " once - off " background change occurs when there is a suddenly change in illumination , or when a moving foreground object stops and becomes a background instance, or when a background becomes a foreground suddenly .
81 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28,29:paraphrase 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:bigrammar-vtense 34:35:preserved 35:36:bigrammar-vtense 36:37:preserved 37:38:preserved 38,39:39:preserved 40,41:40:paraphrase 42:41:preserved 43:43:preserved 44:44:preserved 45:46:preserved 46:47:preserved 47::unaligned 48,49::unaligned 50:48:preserved :42:mogrammar-det :45:mogrammar-det

When this happens , we have : </Eq> ( 12 )
When this happens , we have : </Eq> ( 12 )
82 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Or </Eq> ( 13 ) where M is a high percentage threshold ( 80 % ~ 90 % ) .
Or </Eq> ( 13 ) where M is a high percentage threshold ( 80 % ~ 90 % ) .
83 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Thus , ( 13 ) can be considered as a condition to check if an " once - off " background change is happening .
Thus , ( 13 ) can be considered as a condition to check if an " once - off " background change is happening .
84 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

In that case , the statistics of foreground should be turned to background statistics : </Eq> ( 14 ) for </Eq> .
In that case , the statistics of the foreground should be turned to background statistics : </Eq> ( 14 ) for </Eq> .
85 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved :7:mogrammar-det

This learning process is also proved that </Eq> will converge to 1 as long as the background features are observed frequently </CITE> .
This learning process is also proved that </Eq> will converge to 1 as long as the background features are observed frequently </CITE> .
86 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

1 ) Change detection : In order to have a proper feature selection as mentioned in C , we need to know whether a pixel is static or dynamic .
1 ) Change detection : In order to have a proper feature selection as mentioned in C , we need to know whether a pixel is static or dynamic .
91 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

Therefore , color - based background differencing and interframe differencing methods are applied to detect changes .
Therefore , color - based background differencing and interframe differencing methods are applied to detect changes .
92 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Background differencing calculates the difference between background </Eq> and input frame , while interframe differencing performs the same work on consecutive frames .
Background differencing calculates the difference between background </Eq> and input frame , while interframe differencing performs the same work on consecutive frames .
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Let </Eq> and </Eq> be the background difference and interframe difference respectively .
Let </Eq> and </Eq> be the background difference and interframe difference respectively .
94 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

If </Eq> and </Eq> , pixel </Eq> is referred to nonchange background point .
If </Eq> and </Eq> , pixel </Eq> is referred to nonchange background point .
95 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

If </Eq> , </Eq> is classified as dynamic point , then color co - occurrence features are used for background / foreground classification , otherwise , </Eq> is a static point , so color and gradient features are used in the next step .
If </Eq> , </Eq> is classified as dynamic point , then color co - occurrence features are used for background / foreground classification , otherwise , </Eq> is a static point , so color and gradient features are used in the next step .
96 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved

2 ) Background / Foreground classification : Let </Eq> be the input feature at pixel </Eq> and time </Eq> .
2 ) Background / Foreground classification : Let </Eq> be the input feature at pixel </Eq> and time </Eq> .
97 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The probabilities are estimated as follow : </Eq> ( 15 ) where </Eq> , </Eq> is the set of </Eq> that are identified with </Eq> : </Eq> ( 16 )
The probabilities are estimated as follow : </Eq> ( 15 ) where </Eq> , </Eq> is the set of </Eq> that are identified with </Eq> : </Eq> ( 16 )
98 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

If there is no </Eq> identified with </Eq> , </Eq> and </Eq> .
If there is no </Eq> identified with </Eq> , </Eq> and </Eq> .
99 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

As saying above , if </Eq> is a static pixel , we will have </Eq> and </Eq> , thus , </Eq> and </Eq> are used as their tables of statistics .
As saying above , if </Eq> is a static pixel , we will have </Eq> and </Eq> , thus , </Eq> and </Eq> are used as their tables of statistics .
100 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

After calculating the probabilities as ( 15 ) , ( 7 ) is used to classified </Eq> as background or foreground .
After calculating the probabilities as ( 15 ) , ( 7 ) is used to classified </Eq> as background or foreground .
101 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Note that in this case : </Eq> .
Note that in this case : </Eq> .
102 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Similarly , if </Eq> is a dynamic pixel , </Eq> and ( 4 ) is used as the classification criterion .
Similarly , if </Eq> is a dynamic pixel , </Eq> and ( 4 ) is used as the classification criterion .
103 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

3 ) Foreground object segmentation : after finishing background / foreground classification for all pixels , an " oil spreading " algorithm is applied to find connected regions of foreground pixels .
3 ) Foreground object segmentation : after finishing the background / foreground classification for all pixels , an " oil spreading " algorithm is applied to find connected regions of foreground pixels .
104 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved :8:mogrammar-det

Then some Heuristic technologies are used to separate objects sticking each other due to shades .
Then some Heuristic technologies are used to separate objects sticking to each other due to shades .
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:10:preserved 14:15:preserved 15:16:preserved :14:mogrammar-prep

To make the background differencing in change detection step more accurate , the background image should be regularly updated .
To make the background differencing in the change detection step more accurate , the background image should be regularly updated .
110 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:6:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :13:mogrammar-det

Let </Eq> and </Eq> be the background and input frame at </Eq> and time </Eq> .
Let </Eq> and </Eq> be the background and input frame at </Eq> and time </Eq> .
111 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

If </Eq> is referred to a nonchange background point , the background at </Eq> is updated as : </Eq> ( 17 ) where </Eq> .
If </Eq> is referred to a nonchange background point , the background at </Eq> is updated as : </Eq> ( 17 ) where </Eq> .
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Otherwise , if </Eq> classified as a background point ( static or dynamic ) , the background at </Eq> is replaced by the new one : </Eq> ( 18 )
Otherwise , if </Eq> classified as a background point ( static or dynamic ) , the background at </Eq> is replaced by the new one : </Eq> ( 18 )
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

Figure 1 presents the complete algorithm of foreground object detection .
Figure 1 presents the complete algorithm of foreground object detection .
114 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Let </Eq> be the number of objects at time </Eq> , and </Eq> be the number of measurements received .
Let </Eq> be the number of objects at time </Eq> , and </Eq> be the number of measurements received .
121 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The set of objects and measurements at time t can be respectively denoted as : </Eq> ( 19 ) </Eq> ( 20 )
The set of objects and measurements at time t can be respectively denoted as : </Eq> ( 19 ) </Eq> ( 20 )
122 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Let </Eq> , </Eq> denote the joint association event between objects and measurements , where </Eq> is the particular event which assigns measurement </Eq> to object </Eq> .
Let </Eq> , </Eq> denote the joint association event between objects and measurements , where </Eq> is the particular event which assigns measurement </Eq> to object </Eq> .
123 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

The joint association event probability is : </Eq> ( 21 ) where </Eq> is the sequence of measurements up to time </Eq> , </Eq> is the normalization constant .
The joint association event probability is : </Eq> ( 21 ) where </Eq> is the sequence of measurements up to time </Eq> , </Eq> is the normalization constant .
124 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

The first term </Eq> is the likelihood function of the measurements , given by : </Eq> ( 22 ) where </Eq> is the number of false alarms , </Eq> is the probability of number of false alarms , which is usually Poisson distributed , </Eq> is the likelihood that measurement </Eq> is originated from target </Eq> .
The first term </Eq> is the likelihood function of the measurements , given by : </Eq> ( 22 ) where </Eq> is the number of false alarms , </Eq> is the probability of number of false alarms , which is usually Poisson distributed , </Eq> is the likelihood that measurement </Eq> is originated from target </Eq> .
125 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved

The second term </Eq> is the prior probability of a joint association event , given by : </Eq> ( 23 ) where </Eq> is the probability of detection of an object with the assumption that target detection occurs independently over time with known probability .
The second term </Eq> is the prior probability of a joint association event , given by : </Eq> ( 23 ) where </Eq> is the probability of detection of an object with the assumption that target detection occurs independently over time with known probability .
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved

Thus , the probability of a joint association event is : </Eq> ( 24 )
Thus , the probability of a joint association event is : </Eq> ( 24 )
127 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The state estimation of object </Eq> is : </Eq> ( 25 )
The state estimation of object </Eq> is : </Eq> ( 25 )
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Let the association probability for a particular association between measurement </Eq> and object </Eq> be defined by : </Eq> ( 26 )
Let the association probability for a particular association between measurement </Eq> and object </Eq> be defined by : </Eq> ( 26 )
129 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Hence , ( 25 ) becomes : </Eq> ( 27 ) where </Eq> is the state estimation from Kalman filter </CITE> with the assumption on association between measurement </Eq> and object </Eq> .
Hence , ( 25 ) becomes : </Eq> ( 27 ) where </Eq> is the state estimation from Kalman filter </CITE> with the assumption on association between measurement </Eq> and object </Eq> .
130 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

Note that : </Eq> , and in fact , it is difficult to propose a model for exactly estimating </Eq> in ( 26 ) as the theory , so we want to normalize </Eq> so that </Eq> before using in ( 27 ) . 
Note that : </Eq> , and in fact , it is difficult to propose a model for exactly estimating </Eq> in ( 26 ) as the theory , so we want to normalize </Eq> so that </Eq> before using in ( 27 ) . 
131 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved

Hence : </Eq> ( 28 ) and ( 27 ) becomes : </Eq> ( 29 )
Hence : </Eq> ( 28 ) and ( 27 ) becomes : </Eq> ( 29 )
132 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Figure 2 below is the complete algorithm of JPDAF at each time instant </Eq> .
Figure 2 below is the complete algorithm of JPDAF at each time instant </Eq> .
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Figure 3 is the experimental results of JPDAF performed on simulated data of 8 targets in 100 time steps .
Figure 3 is the experimental results of JPDAF performed on simulated data of 8 targets in 100 time steps .
135 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The left one of each image pair is the simulated data and the right one is the estimated track of each target .
The left one of each image pair is the simulated data and the right one is the estimated track of each target .
136 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Targets ’ positions are initialized in the area of [ 0 . . 500 ] x [ 0 . .50 ] , false alarms are taken randomly in the area of [ 0 . . 200 ] x [ 0 . .200 ] , </Eq> = 0 . 98 , </Eq> .
Targets ’ positions are initialized in the area of [ 0 . . 500 ] x [ 0 . .50 ] , false alarms are taken randomly in the area of [ 0 . . 200 ] x [ 0 . .200 ] , </Eq> = 0 . 98 , </Eq> .
137 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved

Statistical background model is applied to detect moving objects in the motorcycle lane with the parameters in Table 1 .
Statistical background model is applied to detect moving objects in the motorcycle lane with the parameters in Table 1 .
144 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The color and gradient vectors are obtained by quantizing their domains to 256 resolution levels , while for color cooccurrence vectors , the number of quantized levels is 32 , </Eq> = 0 . 005 is used for the distance measure in ( 8 ) while </Eq> = 2 is used for ( 9 ) .
The color and gradient vectors are obtained by quantizing their domains to 256 resolution levels , while for color cooccurrence vectors , the number of quantized levels is 32 , </Eq> = 0 . 005 is used for the distance measure in ( 8 ) while </Eq> = 2 is used for ( 9 ) .
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved

Using the measurements achieved from detection stage , JPDAF performs data association between the current measurements and targets .
Using the measurements achieved from the detection stage , JPDAF performs data association between the current measurements and targets .
151 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:5:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :14:mogrammar-det

At each time </Eq> , basing on the accuracy of detection results , we can propose a strategy to detect new objects entering the observation area .
At each time </Eq> , based on the accuracy of detection results , we can propose a strategy to detect new objects entering the observation area .
152 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-vtense 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

If : </Eq> so that </Eq> ( 30 ) where </Eq> and </Eq> are respectively the Decartes coordinates of object </Eq> at time </Eq> and measurement </Eq> at time </Eq> is a small positive number .
If : </Eq> so that </Eq> ( 30 ) where </Eq> and </Eq> are respectively the Decartes coordinates of object </Eq> at time </Eq> and measurement </Eq> at time </Eq> is a small positive number .
153 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

Then </Eq> is considered as a measurement originated from a new object .
Then </Eq> is considered as a measurement originated from a new object .
154 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

That means if a measurement is not " too close " with any target at the last time instant , it is implied that a new target has occurred .
That means if a measurement is not " too close " with any target at the last time instant , it is implied that a new target has occurred .
155 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

Besides , if an object is at the end of the observation area and it is not a new object or it is misdetected more than 3 time instant , it will be removed .
Besides , if an object is at the end of the observation area and it is not a new object or it is misdetected more than 3 time instant , it will be removed .
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved

To increase the accuracy of JPDAF , beside the spatial distance , the information of color histogram should be incorporated into the likelihood </Eq> in ( 22 ) .
To increase the accuracy of JPDAF , beside the spatial distance , the information of color histogram should be incorporated into the likelihood </Eq> in ( 22 ) .
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

Hence , Bhattachayya distance is employed to calculate the " distance " between the reference color model </Eq> and the candidate color model </Eq> of each target , ( details in </CITE> ) : </Eq> ( 31 ) where reference color model of a target is chosen as its last state and the candidate color model is its current measurement .
Hence , Bhattachayya distance is employed to calculate the " distance " between the reference color model </Eq> and the candidate color model </Eq> of each target , ( details in </CITE> ) : </Eq> ( 31 ) where reference color model of a target is chosen as its last state and the candidate color model is its current measurement .
158 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved 56:56:preserved 57:57:preserved 58:58:preserved 59:59:preserved 60:60:preserved

Moreover , for increasing the accuracy , the reference and candidate model are divided into two sub - regions ( Figure 4 ) , then the color likelihood of a candidate model is produced : </Eq> ( 32 )
Moreover , for increasing the accuracy , the reference and candidate model are divided into two sub - regions ( Figure 4 ) , then the color likelihood of a candidate model is produced : </Eq> ( 32 )
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved

Let </Eq> be the spatial distance likelihood </Eq> which attained by Kalman filter between measurement </Eq> and target </Eq> </CITE> , then the likelihood </Eq> in ( 24 ) is defined as : </Eq> ( 33 ) where </Eq> because spatial distance information has a higher priority than color in this context .
Let </Eq> be the spatial distance likelihood </Eq> which attained by Kalman filter between measurement </Eq> and target </Eq> </CITE> , then the likelihood </Eq> in ( 24 ) is defined as : </Eq> ( 33 ) where </Eq> because spatial distance information has a higher priority than color in this context .
161 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved

In our application , we chose </Eq> .
In our application , we chose </Eq> .
162 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

The below is some results of object detection ( Figure 5 ( a ) ) , the left image of each pair is the input frame and the right one is the detection result .
The below is some results of object detection ( Figure 5 ( a ) ) . The left image of each pair is the input frame and the right one is the detection result .
169 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15,16:15,16:bigrammar-others 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved

The experimental sequences are taken from the motorcycle lane in a cloudy weather and the illumination changes are easily seen , but the detection algorithm still works very well .
The experimental sequences are taken from the motorcycle lane in a cloudy weather and the illumination changes are easily seen , but the detection algorithm still works very well .
170 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

The background is learned rapidly , figure 5 ( b ) is a learned background after 60 frames , together with the statistics of background features , the results of background / foreground classification step is very accurate , there are almost no misclassified background point .
The background is learned rapidly . Figure 5 ( b ) is a learned background after 60 frames . Together with the statistics of background features , the results of background / foreground classification step is very accurate , and there are almost no misclassified background point .
171 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5,6:bigrammar-others 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18,19:19,18:bigrammar-others 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:39,38:bigrammar-others 39:40:preserved 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:45:preserved 45:46:preserved 46:47:preserved

But the difficulty is in the segmentation step , when the object density at the end of the observation area is high , many occlusions usually occurs and the segmentation step will usually make mistakes ( Figure 6 ) .
But the difficulty is in the segmentation step , when the object density at the end of the observation area is high , many occlusions usually occurs and the segmentation step will usually make mistakes ( Figure 6 ) .
172 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved

Figure 5 ( c ) is an example of " once - off " background change , there was a motorbike stopping close to the pavement for a while and it became background soon after that .
Figure 5 ( c ) is an example of " once - off " background change during which there was a motorbike stopping close to the pavement for a while and it became background soon after that .
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16,17:paraphrase 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved

Table II is the quantitative results of object detection .
Table II is the quantitative results of object detection .
175 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The system was tested on ten sequences which has the object density < 10 objects / frame , each sequence has an average length of 10 seconds and uses the first 30 frames ( 1 second ) for initial background learning ( " + 30 " in Length column ) .
The system was tested on ten sequences which has the object density < 10 objects / frame , each sequence has an average length of 10 seconds and uses the first 30 frames ( 1 second ) for initial background learning ( " + 30 " in Length column ) .
176 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved

The precision rate = 100 % demonstrated that there is no background object which is classified as foreground , and the mistake percentages in the recall rate is caused by the incorrect segmentation .
The precision rate = 100 % demonstrated that there is no background object which is classified as foreground , and the mistake percentages in the recall rate is caused by the incorrect segmentation .
177 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved

The results of JPDAF depends on the object detection results , if objects are correctly detected , the tracking algorithm will works very well .
The results of JPDAF depends on the object detection results : if objects are correctly detected , the tracking algorithm will works very well .
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:16:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:10:bigrammar-others 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

In general , this system works well with a reasonable number of targets / frame ( < 10 targets / frame) .
In general , this system works well with a reasonable number of targets / frame ( < 10 targets / frame) .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

With the strategy for detecting objects entering and exiting the observation area , the JPDAF can also detect and track the motorcycles driven in wrong direction .
With the strategy for detecting objects entering and exiting the observation area , the JPDAF can also detect and track the motorcycles driven in wrong direction .
185 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Figure 7 shows some tracking results , including of the tracking of wrong - wayed motorcycle ( Figure 7 ( d ) , object 10 ) .
Figure 7 shows some tracking results , including of the tracking of wrong - wayed motorcycle ( Figure 7 ( d ) , object 10 ) .
186 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Table III shows the statistics of full correct tracks in the ten sequences above ( the mis - tracked objects in any frame are not counted) .
Table III shows the statistics of full correct tracks in the ten sequences above ( the mis - tracked objects in any frame are not counted) .
187 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Since JPDAF is an NP-hard problem ( the number of possible joint association events at each time instant </Eq> is </Eq> , the computation cost of JPDAF is one of its major weak points .
Since JPDAF is an NP-hard problem ( the number of possible joint association events at each time instant </Eq> is </Eq> , the computation cost of JPDAF is one of its major weak points .
188 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved

All of these experiments are deployed on a Pentium IV 2 . 4 Ghz , 512 MB RAM , due to the high cost of object detection and tracking algorithm , the processing rate is 2s / frame with the frame size is 360 x 240 and the sequence rate is 30 frames / s .
All of these experiments are deployed on a Pentium IV 2 . 4 Ghz , 512 MB RAM , and due to the high cost of object detection and tracking algorithm , the processing rate is 2s / frame with the frame size is 360 x 240 and the sequence rate is 30 frames / s .
189 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:19:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:45:preserved 45:46:preserved 46:28:preserved 47:48:preserved 48:49:preserved 49:50:preserved 50:51:preserved 51:52:preserved 52:53:preserved 53:54:preserved 54:55:preserved 55:56:preserved :47:bigrammar-others

This paper is a next step on the way searching an efficient approach for a motorcycle surveillance system after using Particle filter in </CITE> .
This paper is a next step on the way searching an efficient approach for a motorcycle surveillance system after using Particle filter in </CITE> .
196 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Some improvements have been achieved in object detection step which has more accurate results in the whole observation area and the ability to efficiently adapt to illumination changes and " once - off " changes .
Some improvements have been achieved in object detection step which has more accurate results in the whole observation area and the ability to efficiently adapt to illumination changes and " once - off " changes .
197 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

However , occlusions have not been strictly handled and the computation cost is one of the major limitations .
However , occlusions have not been strictly handled and the computation cost is one of the major limitations .
198 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

In the future , we hope thatmany new multi - target tracking methods will be applied in this context and the best selection will be produced .
In the future , we hope thatmany new multi - target tracking methods will be applied in this context, and the best selection will be produced .
199 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:bigrammar-others 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

