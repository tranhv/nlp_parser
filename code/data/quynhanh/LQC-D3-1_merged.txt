Exploiting Motion Patterns for Action Recognition with Depth Sequences
Exploiting Motion Patterns for Action Recognition with Depth Sequences
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Dense trajectory-based approaches on 2D video have been demontrated state of the art at action recognition since it can capture most discriminative motion patterns . 
Dense trajectory-based approaches have been used to recognize actions in 2D video because they can capture most discriminative motion patterns . 
6 0:0:preserved 1:1:preserved 2:2:preserved 3,8,9,10,11,12,13:5,6,9:paraphrase 4:10:preserved 5:11:preserved 6:3:preserved 7:4:preserved 14,15:8,7:para-colocation 16:12:paraphrase 17:13:bigrammar-nnum 18:14:preserved 19:15:preserved 20:16:preserved 21:17:preserved 22:18:preserved 23:19:preserved 24:20:preserved

However , there are not many studies related to exploiting the discriminative motion patterns in depth video . 
However , there are not many studies related to exploiting discriminative motion patterns in depth video . 
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10::mogrammar-det 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved

In this work , we extend the dense trajectory-based approach on depth video and show its effectiveness for action recognition . 
In this study , we extend the dense trajectory-based approach to depth video and show its effectiveness at recognizing actions . 
8 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-prep 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:bigrammar-prep 18,19:18,19:para-colocation 20:20:preserved

To achieve an effective extension , we extract dense trajectories on 2D videos transformed from depth video . 
In particular , dense trajectories are extracted from depth video . 
9 0,1,2,3,4,10,11,12,13:0,1:paraphrase 5:2:preserved 6,7,8,9:3,4,5,6:para-passact 14:7:preserved 15:8:preserved 16:9:preserved 17:10:preserved

The 2D videos are formed from views which can capture the discriminative motion patterns similar to observing actions from different directions . 
These 2D videos are formed from views which capture the discriminative motion patterns similar to observing actions from different directions . 
10 0:0:bigrammar-det 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9:8:bigrammar-vtense 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved

We evaluate this approach on framework of action recognition using the benchmark MSR Action 3D , MSR Gesture 3D and 3D Action Pairs datasets . 
We evaluate this approach to action recognition on the benchmark MSR Action 3D , MSR Gesture 3D and 3D Action Pairs datasets . 
11 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-prep 5,6,9:7:para-freeword 7:5:preserved 8:6:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved

Evaluation results show that our proposed approach is effective for action recognition on depth video and outperforms the state-of-the-art approaches .
The results of the evaluation indicate that our approach is effective at recognizing actions in depth video and outperforms other state-of-the-art approaches .
12 0,1:0,1,2,3,4:para-colocation 2:5:paraphrase 3:6:preserved 4:7:preserved 5,6:8:paraphrase 7:9:preserved 8:10:preserved 9:11:bigrammar-prep 10,11:12,13:para-colocation 12:14:bigrammar-prep 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:bigrammar-det 18:20:preserved 19:21:preserved 20:22:preserved

Action recognition in videos has been one of the active research fields in the computer vision </CITE> , due to its wide applications in areas like surveillance , video retrieval , human-computer interaction , and smart environments . 
The recognition of actions depicted in videos is an active topic of research in the computer vision field </CITE> , and it has a diverse range of applications in areas like surveillance , video retrieval , human-computer interaction , and smart environments . 
17 1,0:0,1,2,3:para-colocation 2:5,4:paraphrase 3:6:preserved 4,5,6,7,8:7,8:para-freeword 9:9:preserved 10,11:10,11,12:para-colocation 12:13:preserved 13:14:preserved 14,15:15,17,16:para-colocation 16:18:preserved 17:19:preserved 18,19,20,21:20,21,22,23,24,25,26:para-freeword 22:27:preserved 23:28:preserved 24:29:preserved 25:30:preserved 26:31:preserved 27:32:preserved 28:33:preserved 29:34:preserved 30:35:preserved 31:36:preserved 32:37:preserved 33:38:preserved 34:39:preserved 35:40:preserved 36:41:preserved 37:42:preserved

Due to the diversity and complexity of actions , as well as complicated environment ( e.g background clutter and illumination variation ) , action recognition is still a challenging problem . 
As a result of the diversity and complexity of actions , as well as the complicated nature of most environments  ( e.g. , background clutter and illumination variation ) , action recognition is still a challenging problem . 
18 0,1:0,1,2,3:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12,13:15,14,16,17,18,19:para-colocation 14:20:preserved 15:21,22:typo 16:23:preserved 17:24:preserved 18:25:preserved 19:26:preserved 20:27:preserved 21:28:preserved 22:29:preserved 23:30:preserved 24:31:preserved 25:32:preserved 26:33:preserved 27:34:preserved 28:35:preserved 29:36:preserved 30:37:preserved

Recent approaches can be divided into three major categories : silhouette-based </CITE> , salient point-based </CITE> and trajectory-based </CITE> . 
Recent approaches can be divided into three major categories : silhouette-based </CITE> , salient point-based </CITE> and trajectory-based </CITE> . 
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

All approaches basically try to capture motion information that appears in videos , since the motion is crucial information for presenting actions . 
All of these approaches basically try to capture motion information appearing in videos , since it is crucial information for presenting actions . 
20 0:0,1,2:paraphrase 1:3:preserved 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8,9:10:bigrammar-wform 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14,15:15:bigrammar-det 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Based on recent works </CITE> , exploiting discriminative motion patterns has been demonstrated successful at action recognition .
Recent studies </CITE> have shown that exploiting discriminative motion patterns is a successful means of action recognition .
21 0,1,2,3,5,10,11,12,14:0,1,3,4,5,10,11,13,14:para-freeword 4:2:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 13:12:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Most existing studies mainly have investigate on video sequences captured by traditional 2D cameras .
Most of the existing studies deal with video sequences captured by traditional 2D cameras .
22 0:0:preserved 1:3:preserved 2:4:preserved 3:1,2:para-freeword 4,5,6:5,6:paraphrase 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Although , there are many improvements on motion pattern-based approach for action recognition in the domain of 2D video </CITE> , the mentioned challenges  ( e.g. background clutter , illumination variation ) are still difficult to handle . 
Although , there have been many improvements to the motion pattern-based approach for 2D video </CITE> , the above - mentioned challenges  ( e.g. background clutter and illumination variation ) are still difficult to meet . 
23 0:0:preserved 1:1:preserved 2:2:preserved 3:3,4:bigrammar-vtense 4:5:preserved 5:6:preserved 6:7:bigrammar-prep 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11,12,13,14,15,16,17,18:13,14:para-freeword 19:15:preserved 20:16:preserved 21,23,22:17,18,19,20,21:para-colocation 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved 28:26:paraphrase 29:27:preserved 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:paraphrase 37:35:preserved :8:mogrammar-det

With the development of new RGB-D cameras , e.g. Kinect camera , capturing RGB images as well as together with depth maps has become more easily in real time . 
Meanwhile , the advent of RGB-D cameras , e.g. , the Kinect camera , has made it easier to capture depth maps together with RGB images in real time . 
24 0,2,4,12,15,16,17,23,24,25:0,3,15,16,18,19,17:para-freeword 1:2:preserved 3:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:typo 9:11:preserved 10:12:preserved 11:13:preserved 13:24:preserved 14:25:preserved 18:22:preserved 19:23:preserved 20:20:preserved 21:21:preserved 22:14:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved :1:unaligned :10:mogrammar-det

The depth maps can enrich information for cues , such as body shape and motion information . 
The depth maps enrich the information available as cues , such as the shape of body and its motions . 
25 0:0:preserved 1:1:preserved 2:2:preserved 3,4:3:paraphrase 5:5:preserved 6:7,6:bigrammar-prep 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11,12:15,12,13,14:para-colocation 13:16:preserved 14,15:17,18:para-colocation 16:19:preserved :4:mogrammar-det

In addition , depth information is less sensitive to the challenges RGB information usually deals with . 
In addition , the depth information in these maps is less sensitive to the problems affecting RGB information . 
26 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:9:preserved 6:10:preserved 7:11:preserved 8:12:preserved 9:3:preserved 10,13,14,15:6,7,8,13,14,15:para-freeword 11:16:preserved 12:17:preserved 16:18:preserved

Due to these advantages , recent research trend concentrates on exploiting depth maps for action recognition </CITE> . 
Because of these advantages , recent research has concentrated on exploiting depth maps for action recognition </CITE> . 
27 0,1:0,1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8:7,8:paraphrase 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

However , to the best of our knowledge , none success is related to combining discriminative motion pattern-based approach , the state-of-the-art on 2D video , on in depth video . 
However , to the best of our knowledge , none have succeeded in developing a discriminative motion pattern-based approach , the state-of-the-art for 2D video , for depth video . 
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10,11,12,13,14:10,11,13,14,12:paraphrase 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:bigrammar-prep 23:23:preserved 24:24:preserved 25:25:preserved 27,26:26:bigrammar-prep 28:27:preserved 29:28:preserved 30:29:preserved

In this paper , we investigate this approach with on depth sequences .
In this paper , we investigate this approach for depth sequences .
29 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9:8:bigrammar-prep 10:9:preserved 11:10:preserved 12:11:preserved

Key idea of motion pattern-based approach is to capture discriminative trajectories in video .
The key idea of the motion pattern-based approach is to capture discriminative trajectories in video .
30 0:1:preserved 1:2:preserved 2:3:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved :0:mogrammar-det :4:mogrammar-det

Therefore , in order to effectively exploit this approach on depth video , it is necessary to extract the trajectories from depth video . 
Therefore , to effectively exploit this approach on depth video , it is necessary to extract trajectories from depth video . 
31 0:0:preserved 1:1:preserved 2,3,4:2:paraphrase 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18::mogrammar-det 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved

To do that , a straightforward method is to consider depth value as intensity value , extract trajectories on the 2D video , and apply standard motion analysis technique for 2D data .
To do that , a straightforward method is to consider the depth value as an intensity value , extract trajectories from the 2D video , and apply standard motion analysis techniques for 2D data .
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:bigrammar-prep 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:bigrammar-nnum 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved :10:mogrammar-det :14:mogrammar-det

Unfortunately , the method will lead to inherent limitation of the 2D trajectory-based approaches , and results in couple of confused motion patterns which cannot be distinguished by observing 2D videos from one view . 
Unfortunately , this method will expose the inherent limitations of the 2D trajectory-based approaches and will result in confused motion patterns that cannot be distinguished by observing 2D videos from one view . 
33 0:0:preserved 1:1:preserved 2:2:bigrammar-det 3:3:preserved 4:4:preserved 5,6:5:paraphrase 7:7:preserved 8:8:bigrammar-nnum 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14,18,19,23:21:para-freeword 15:14:preserved 16:15,16:bigrammar-vtense 17:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved 28:26:preserved 29:27:preserved 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved :6:mogrammar-det

For example , forward punch and hammer may be confused actions , if we view them from front , since they contain indistinguishable front view movements respectively : ``lift arm up'' and ``stretch out'' . 
For example , a forward punch and hammer may be confused if we view them from the front because they contain ``lift arm up'' and ``stretch out'' movements that are indistinguishable when viewed from the front . 
34 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10,11,18,24,26,27:28,29,31,32,33,34:para-freeword 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:17:preserved 19:18:para-freeword 20:19:preserved 21:20:preserved 22:30:preserved 23:35:preserved 25:27:preserved 28:21:preserved 29:22:preserved 30:23:preserved 31:24:preserved 32:25:preserved 33:26:preserved 34:36:preserved :3:mogrammar-det :16:mogrammar-det

However , if we properly use the depth information in motion pattern analysis , it is possible to extract discriminative motion patterns which is inaccessible from one fixed view but available from multiple different view directions .
However , if we properly use the depth information in the motion pattern analysis , it is possible to extract discriminative motion patterns which are inaccessible from one fixed view but discernible from different views .
35 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:bigrammar-wform 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30,32:31:para-freeword 31:32:preserved 33:33:preserved 34,35:34:para-colocation 36:35:preserved :10:mogrammar-det

To deal with such cases , we consider getting more information on such actions from various directions .
To deal with such cases , we try to get more information on such actions from various directions .
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8:7,8,9:paraphrase 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved

Information achieved from the view directions can provide clearer cues to discriminate such actions . 
Information from different views can provide clearer cues to discriminate such actions . 
37 0:0:preserved 1,3,4,5:2,3:para-freeword 2:1:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved

To collect such information from depth video , we propose a method to virtually project depth maps to multiple view images , as shown in figure </fig> . 
To collect such information from depth video , we propose a method that virtually projects depth maps onto multiple view images , as shown in Figure </fig> . 
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:paraphrase 13:13:preserved 14:14:bigrammar-nnum 15:15:preserved 16:16:preserved 17:17:bigrammar-prep 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:typo 26:26:preserved 27:27:preserved

2D videos which are formed by the projections are easily obtained from depth data . 
2D videos can be easily obtained from the projections . 
39 0:0:preserved 1:1:preserved 2,3,4,5,8,12,13:2,3:para-freeword 6:7:preserved 7:8:preserved 9:4:preserved 10:5:preserved 11:6:preserved 14:9:preserved

Motion features are then calculated to generate corresponding projection representations . 
Motion features are then calculated to generate corresponding projection representations . 
40 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Finally , depth video representation is formed by fusing the projection representations .
Finally , a depth video representation is formed by fusing the projection representations .
41 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :2:mogrammar-det

In our experiments , we adopt dense trajectory-based approach </CITE> to exploit discriminative motion patterns since that is the state-of-the-art approach for action recognition on domain of 2D videos . 
In our experiments , we use a dense trajectory-based approach </CITE> that exploits discriminative motion patterns; this is the state-of-the-art approach for recognizing actions depicted in 2D videos . 
42 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10,11:11,12:bigrammar-wform 12:13:preserved 13:14:preserved 14:15:typo 15,22,23,24,25,26:22,23,24,25:para-freeword 16:16:paraphrase 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 27:26:preserved 28:27:preserved 29:28:preserved :6:mogrammar-det

To evaluate the effectiveness of proposed method , we conduct experiments on MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
To evaluate the effectiveness of the proposed method , we conducted experiments on the MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
43 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-vtense 10:11:preserved 11:12:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved :5,13:mogrammar-det

Experimental results show that our proposed method is shown to outperform the state-of-the-art methods at action recognition using depth data . 
The results show that our method outperforms other state-of-the-art methods at recognizing actions using depth data . 
44 0,7,8,9,10,11:0,6,7:para-freeword 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5:paraphrase 12:8:preserved 13:9:preserved 14:10:preserved 15,16:11,12:para-colocation 17:13:preserved 18:14:preserved 19:15:preserved 20:16:preserved

Our key contributions of this paper are as follows    : (1) we propose an effective method to exploit trajectories in depth video , (2) we perform comprehensive experiments on the challenging benchmark dataset and indicate that our proposed method is the best compared with the state-of-the-art depth-based methods .
Our key contributions are as follows    : (1) we propose an effective method to exploit trajectories in depth video , and (2) we perform comprehensive experiments on a challenging benchmark dataset and indicate that our method is the best of the state-of-the-art depth-based methods .
45 0:0:preserved 1:1:preserved 2:2:preserved 3,4,5,42,43:39:para-freeword 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:20,19:para-freeword 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved 28:26:preserved 29:27:bigrammar-det 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:preserved 37,38:35:paraphrase 39:36:preserved 40:37:preserved 41:38:preserved 44:40:preserved 45:41:preserved 46:42:preserved 47:43:preserved 48:44:preserved

After a brief review of the related work in Section , our action recognition framework is presented in Section . 
After a brief review of the related work in Section , our action recognition framework is presented in Section . 
46 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The proposed method is described in Section . 
The proposed method is described in Section . 
47 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Section presents the experimental settings and results . 
Section describes the experimental settings and results . 
48 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

In section </ref> we provide some concerned discussions . 
We discuss the results in section . 
49 0,2,3,4,5,6,7:0,1,2,3,4:para-freeword 1:5:preserved 8:6:preserved

The summaries of our work are given in Section .
A summary of our work is given in Section .
50 0:0:bigrammar-det 1:1:bigrammar-nnum 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-wform 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

In terms of action recognition in 2D video , there are three popular approaches used in several action recognition systems , including silhouette-based , salient point-based and trajectory-based . 
There are three popular approaches to action recognition in 2D video    : silhouette-based , salient point-based and trajectory-based . 
55 0,1,2,14,15,16,17,18,19,20,21,23:5,11:para-freeword 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:13:preserved 9:0:preserved 10:1:preserved 11:2:preserved 12:3:preserved 13:4:preserved 22:12:preserved 24:14:preserved 25:15:preserved 26:16:preserved 27:17:preserved 28:18:preserved

The silhouette-based approach , as described in </CITE> , is powerful since it encodes a great deal of information in a sequence of images . 
The silhouette-based approach , as described in </CITE> , is powerful since it encodes a great deal of information in a sequence of images . 
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

However , it is sensitive to pose changes , noise , and occlusions . 
However , it is sensitive to pose changes , noise , and occlusions . 
57 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Besides , it depends on the accuracy of localization , background subtraction , or tracking for exactly extracting region of interest . 
In addition , it depends on the accuracy of the localization , background subtraction , and tracking when extracting the region of interest . 
58 0:0,1:para-freeword 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:paraphrase 14:16:preserved 15,16:17:paraphrase 17:18:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved :9:mogrammar-det :19:mogrammar-det

Another approach based on salient points , generates a compact video representation and accepts background clutter , occlusions and scale changes . 
Another approach , based on salient points , generates a compact video representation and can deal with background clutter , occlusions , and scale changes . 
59 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14,15,16:paraphrase 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:2,21,22:para-freeword 19:23:preserved 20:24:preserved 21:25:preserved

The effectiveness of this approach is also showed in several works </CITE> . 
This approach has been shown to be effective in several studies </CITE> . 
60 0,1,2,5,6,7:2,3,4,5,6,7:para-freeword 3:0:preserved 4:1:preserved 8:8:preserved 9:9:preserved 10:10:paraphrase 11:11:preserved 12:12:preserved

However , in case of recognizing complicated motions , the salient point-based approach has to deal with several challenges ; due to the lack of a relationship among salient points . 
However , in the case of recognizing complicated motions , the salient point-based approach has to deal with several challenges due to the lack of relationships among the salient points . 
61 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:27:bigrammar-det 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25,26:25:bigrammar-nnum 27:26:preserved 28:28:preserved 29:29:preserved 30:30:preserved :3:mogrammar-det

In recent studies </CITE> , the trajectory-based approach captures motion patterns in video . 
Recent studies </CITE> have used the trajectory-based approach to capture motion patterns in video . 
62 4,8,0:3,4,8,9:para-freeword 1:0:preserved 2:1:preserved 3:2:preserved 5:5:preserved 6:6:preserved 7:7:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

Although the motion patterns provide even very complicated , structured motion , which are perceived easily by human subject , as shown by Johansson </CITE> .
Although motion patterns are very complicated , structured motion can be easily perceived by humans , as has been shown by Johansson </CITE> .
63 0:0:preserved 1::mogrammar-det 2:1:preserved 3:2:preserved 4,5:3:paraphrase 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:15:preserved 12,19,13:9,10,17,18:para-freeword 14:12:preserved 15:11:preserved 16:13:preserved 17,18:14:paraphrase 20:16:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved

Most recent and effective methods exploiting depth information are categorized into two major directions . 
The most recent methods of exploiting depth information can be categorized into two major types . 
64 0:0,1:bigrammar-det 1:2:preserved 2,3:4:para-freeword 4:3:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:bigrammar-vtense 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:paraphrase 14:15:preserved

The first one is to adapt 2D techniques based methods for depth data . 
The first one is to adapt 2D techniques to depth data . 
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10:8:paraphrase 11:9:preserved 12:10:preserved 13:11:preserved

The second one is to propose 3D techniques for directly exploiting depth data .
The second one is to devise 3D techniques for directly exploiting depth data .
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

For the first direction , X.Yang et al. </CITE> propose the Depth Motion Maps ( DMM ) which is alble to capture global activities in depth sequences . 
Regarding the first direction , X.Yang et al. </CITE> propose Depth Motion Maps ( DMMs ) to capture global activities in depth sequences . 
67 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10::mogrammar-det 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:bigrammar-nnum 16:15:preserved 17,18,19::unaligned 20:16:preserved 21:17:preserved 22:18:preserved 23:19:preserved 24:20:preserved 25:21:preserved 26:22:preserved 27:23:preserved

The DMM are generated by stacking motion energy of depth maps projected to three orthogonal Cartesian planes . 
DMMs are generated by stacking the motion energy of depth maps projected on three orthogonal Cartesian planes . 
68 0::mogrammar-det 1:0:bigrammar-nnum 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-prep 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved :5:mogrammar-det

And the Histogram of Oriented Gradients  ( HOG ) </CITE> are computed from the DMM to represent an action video . 
A Histogram of Oriented Gradients  ( HOG ) </CITE> is computed from the DMMs to represent an action video . 
69 0,1:0:paraphrase 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:paraphrase 11:10:preserved 12:11:preserved 13:12:preserved 14:13:bigrammar-nnum 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved

The approach can accumulate more silhouette information from the projections . 
The approach can accumulate more silhouette information from the projections . 
70 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

However , as silhouette extraction is not trivial due to objectuve challenges , such as occlusion , data quality , the effectiveness of the approach is significantly affected . 
However , silhouette extraction is not a trivial task due to problems such as occlusion and poor data quality . 
71 0:0:preserved 1:1:preserved 2::unaligned 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:7,8:para-colocation 8:9:preserved 9:10:preserved 10,11:11:paraphrase 12,16,19,20,21,22,23,24,25,26,27:15,16:para-freeword 13:12:preserved 14:13:preserved 15:14:preserved 17:17:preserved 18:18:preserved 28:19:preserved :6:mogrammar-det

Another approach proposed by L Xia and J.K Aggarwal </CITE> presents a filtering method to extract spatio-temporal interest points from depth videos  ( DSTIPs ) . 
Another approach , proposed by L. Xia and J.K. Aggarwal </CITE> , is to use filtering to extract spatio-temporal interest points from depth videos  ( DSTIPs ) . 
72 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4,5,7,8:5,6,8,9:typo 6:7:preserved 9:10:preserved 10,11,13:2,11,12,14,13:para-freeword 12:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved

In this approach , they extend a work of Dollar et al. </CITE> to adapt for depth data . 
It is an extension of the work of Dollar et al. </CITE> to depth data . 
73 0,1,2,3,4,5,6,14,15:0,1,2,3,5,7:para-freeword 7:6:preserved 8:4:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved

Firstly , 2D and 1D filters ( e.g. Gaussian and Gabor filters ) are applied respectively on to the spatial dimensions and temporal dimension in depth video . 
First , 2D and 1D filters ( e.g. Gaussian and Gabor filters ) are respectively applied to the spatial dimensions and temporal dimension of the depth video . 
74 0:0:typo 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14,15:15,14:para-colocation 16,17:16:bigrammar-prep 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:bigrammar-prep 25:25:preserved 26:26:preserved 27:27:preserved :24:mogrammar-det

A correction function then is used to suppress points as depth noises . 
A correction function is used to suppress points that are depth noise . 
75 0:0:preserved 1:1:preserved 2:2:preserved 3,9:8,9:para-freeword 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 10:10:preserved 11:11:bigrammar-nnum 12:12:preserved

Finally , points with the largest responses by this filtering method will be selected as the DSTIPs for each video . 
The points with the largest responses resulting from this filtering are selected as the DSTIPs for each video . 
76 0,1,7,10:6,7:para-freeword 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 8:8:preserved 9:9:preserved 11,12:10:bigrammar-vtense 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved :0:mogrammar-det

Besides , a depth cuboid similarity feature ( DCSF ) is proposed to describe a 3D cuboid around the DSTIPs with supporting size to be adaptable to the depth . 
In addition , a depth cuboid similarity feature ( DCSF ) is used to describe a 3D cuboid around the DSTIPs with supporting size to be adaptable to the depth . 
77 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:paraphrase 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved

This work has demonstrated the effectiveness of 2D techniques applied for depth data . 
That study demonstrated the effectiveness of using 2D techniques on depth data . 
78 0,1:0,1:paraphrase 2,3:2:bigrammar-vtense 4:3:preserved 5:4:preserved 6:5:preserved 7:7:preserved 8:8:preserved 9,10:6,9:para-freeword 11:10:preserved 12:11:preserved 13:12:preserved

Nevertheless , the authors have not solved inherent limitations of the 2D techniques . 
Nevertheless , the authors did not overcome the inherent limitations of the 2D techniques . 
79 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:4,5:bigrammar-vtense 6:6:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :7:mogrammar-det

Therefore , this approach has to deal with the challenges as mentioned in 2D video data .
That is , this approach still faces the challenges mentioned in the introduction .
80 0,10:0,1:para-freeword 1:2:preserved 2:3:preserved 3:4:preserved 4,5,6,7:5,6:paraphrase 8:7:preserved 9:8:preserved 11:9:preserved 12:10:preserved 13,14,15:12,11:paraphrase 16:13:preserved

For the second direction , </CITE> uses a bag of 3D points to characterize a set of salient postures .
Regarding the second direction , </CITE> uses a bag of 3D points to characterize a set of salient postures .
81 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The 3D points are extracted on the contours the planar projections of the 3D depth map . 
The 3D points are extracted from the contours of planar projections of the 3D depth map . 
82 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7,8:bigrammar-prep 8::mogrammar-det 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

And then , about 1% 3D points are sampled to calculate feature . 
About 1% of the 3D points are sampled to calculate a feature . 
83 0,1,2:2,3:para-freeword 3:0:preserved 4:1:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:11:preserved 12:12:preserved :10:mogrammar-det

Unlike </CITE> , works </CITE> use occupancy patterns to represent features in action videos .
Unlike </CITE> , the methods described in </CITE> use occupancy patterns to represent features in action videos .
84 0:0:preserved 1:1:preserved 2:2:preserved 3:3,4,5,6:paraphrase 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved

A.W. Vieira et al. </CITE> proposed a new feature descriptor , called Space-Time Occupancy Patterns ( STOP ) . 
A.W. Vieira et al. </CITE> proposed a new feature descriptor , called Space-Time Occupancy Patterns ( STOP ) . 
85 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

This descriptor is formed by sparse cells divided by the sequence of depth maps in a 4D space-time grid .
This descriptor is formed from the sparse cells of a 4D space-time grid dividing up a sequence of depth maps .
86 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-prep 5:6:preserved 6:7:preserved 7,8,14:13,14:para-freeword 9:15:bigrammar-det 10:16:preserved 11:17:preserved 12:18:preserved 13:19:preserved 15:9:preserved 16:10:preserved 17:11:preserved 18:12:preserved 19:20:preserved :5:mogrammar-det :8:unaligned

The values of the sparse cells are determined by points inside to be on the silhouettes or moving parts of the body . 
The points inside the sparse cells are typically on the silhouette or on the moving parts of an object . 
87 0:0:preserved 1,9,2,3,4,5,7,8,10,11,12,15:1,2,3,4,5,6,7,10:para-freeword 6::unaligned 13:8:preserved 14:9:preserved 16:11,12:bigrammar-prep 17:14:preserved 18:15:preserved 19:16:preserved 20,21:17,18:paraphrase 22:19:preserved :13:mogrammar-det

J. Wang et al. </CITE> presented semi-local features , called Random Occupancy Pattern ( ROP ) features , from randomly sampled 4D sub-volumes with different sizes and different locations . 
J. Wang et al. </CITE> created semi-local features , called Random Occupancy Pattern ( ROP ) features , from randomly sampled 4D sub-volumes of different sizes and at different locations . 
88 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:bigrammar-prep 24:24:preserved 25:25:preserved 26:26,27:bigrammar-prep 27:28:preserved 28:29:preserved 29:30:preserved

The random sampling is performed under a weighted scheme to effectively explore the large dense sampling space . 
The random sampling is performed according to a weighted scheme in order to effectively explore the large dense sampling space . 
89 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:paraphrase 6:7:preserved 7:8:preserved 8:9:preserved 9:10,11,12:paraphrase 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved

Besides , authors also apply a sparse coding approach to robustly encode these features . 
The authors also used sparse coding to robustly encode these features . 
90 0,1:0:paraphrase 2:1:preserved 3:2:preserved 4:3:paraphrase 5::mogrammar-det 6:4:preserved 7:5:preserved 8,9:6:paraphrase 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved

The work by J. Wang et al. </CITE> designed a feature , to describe the local ``depth appearance'' for each joint , named Local Occupancy Patterns ( LOP ) . 
J. Wang et al. </CITE> designed features , named Local Occupancy Patterns ( LOPs ) , to describe the local ``depth appearance'' of each joint of the body . 
91 0,1,2,3,4,5,6:0,1,2,3:para-colocation 7:4:preserved 8:5:preserved 9,10:6:bigrammar-nnum 11:7:preserved 12,13,14,15,16,17,19,20,22,23,24,25:16,8,9,10,11,17,18,19,20,21,23,24,25,26,27:para-freeword 18:22:bigrammar-prep 21:15:preserved 26:12:preserved 27:13:bigrammar-nnum 28:14:preserved 29:28:preserved

The LOP features are computed based on 3D point cloud around a particular joint . 
LOP features are computed on the basis of a 3D point cloud around a particular joint . 
92 0::mogrammar-det 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5,6:4,5,6,7:paraphrase 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved :8:mogrammar-det

Moreover , they concatenate the LOP features with skeleton information-based features and apply Short Fourier Transform to obtain the Fourier Temporal Pyramid features at each joint . 
Moreover , they concatenate the LOP features with skeleton information-based features and apply a Short Fourier Transform to obtain Fourier Temporal Pyramid features at each joint . 
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18::mogrammar-det 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved :13:mogrammar-det

The Fourier features are utilized in a novel actionlet ensemble model to represent each action video .
The Fourier features are utilized in a novel actionlet ensemble model to represent each action in the video .
94 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14,15:bigrammar-prep 15:17:preserved 16:18:preserved :16:mogrammar-det

Recently , Oreifej and Liu </CITE> presented a new descriptor for depth maps , named Histogram of Oriented 4D Surface Normals ( HON4D ) . 
Recently , Oreifej and Liu </CITE> presented a new descriptor for depth maps , named Histogram of Oriented 4D Surface Normals ( HON4D ) . 
95 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

To construct the HON4D , firstly , the 4D normal vectors are computed from the depth sequence . 
To construct the HON4D ,  4D normal vectors are first computed from the depth sequence . 
96 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4::unaligned 5,6:4:para-freeword 7:12:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9,10:paraphrase 13:11:preserved 14::unaligned 15:13:preserved 16:14:preserved 17:15:preserved

At the next step , the 4D normal vectors is distributed into spatio-temporal cells . 
Next , these 4D normal vectors are distributed into spatio-temporal cells . 
97 0,1,2,3:0:paraphrase 4:1:preserved 5:2:bigrammar-det 6:3:preserved 7:4:preserved 8:5:preserved 9:6:typo 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved

To quantize the 4D normal vectors , the 4D space is quantized by using vertices of a regular polychoron . 
To quantize the 4D normal vectors , the 4D space is quantized by using the vertices of a regular polychoron . 
98 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :14:mogrammar-det

The quantization , then , is refined by additional projectors to make the 4D normal vectors in each cell denser and more discriminative . 
The quantization is refined by additional projectors to make the 4D normal vectors in each cell denser and more discriminative . 
99 0:0:preserved 1:1:preserved 2,3,4,5:2:para-freeword 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved

Afterwards , the HON4D features in cells are concatenated to represent a depth action video . 
Afterwards , the HON4D features in the cells are concatenated to make depth video depicting actions . 
100 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:paraphrase 11::mogrammar-det 12,13,14:12,13,14,15:para-colocation 15:16:preserved :6:mogrammar-det

In general , the works </CITE> have shown the effectiveness of the approaches to directly exploit 3D data for human action recognition . 
A number of studies </CITE> have shown the effectiveness of approaches that directly exploit 3D data for human action recognition . 
101 0,1,2,22:20:para-freeword 3,4:0,1,2,3:para-colocation 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11::mogrammar-det 12:10:preserved 13:11:paraphrase 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved

However , constraints existed in these approaches , such as human segmentation in </CITE> , or human location in </CITE> , have partially limited their applicability on practical applications .
However , constraints , such as segmentation of the human body in </CITE> , or the location of the body in </CITE> , have somewhat limited their applicability .
102 0:0:preserved 1:1:preserved 2,3,4,5,6:2:paraphrase 7:3:preserved 8:4:preserved 9:5:preserved 10,11:9,6,7,8,10:para-colocation 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 17,16:19,18,17,16,15:para-colocation 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22,26,27,28:24:para-freeword 23:25:preserved 24:26:preserved 25:27:preserved 29:28:preserved

Inspired by results of Shotton et al. </CITE> and L. Xia et al. </CITE> , the works in </CITE> developed skeleton-based methods from sequence of depth maps .
Inspired by the results of Shotton et al. </CITE> and L. Xia et al. </CITE> , some studies </CITE> have developed skeleton-based methods from sequences of depth maps .
103 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15,16:16,17:paraphrase 17,19:19,20:paraphrase 18:18:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:bigrammar-nnum 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved :2:mogrammar-det

 </CITE> proposed an EigenJoints-based action recognition system using a Naive-Bayes-Nearest-Neighbor classifier . 
 </CITE> proposed an EigenJoints-based action recognition system using a Naive-Bayes-Nearest-Neighbor classifier . 
104 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

The system is able to capture the characteristics of posture , motion and offset information of frames . 
The system is able to capture the characteristics of posture , motion and offset of the frames . 
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13,14,15,16:13,14,15,16:para-colocation 17:17:preserved

In addition , non-quantization of descriptors and distance computation in this work are showed effective for action recognition . 
In addition , non-quantization of descriptors and distance computations have proved to be effective for action recognition . 
106 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8,9,10,11:7,8:para-colocation 12,13:9,10,11,12:paraphrase 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved

In work of J. Luo et al. </CITE> to better represent the 3D joint features a new discriminative dictionary learning algorithm ( DL-GSGC ) was proposed that . 
J. Luo et al. </CITE> proposed a new discriminative dictionary learning algorithm ( DL-GSGC ) to better represent 3D joint features . 
107 0,1,2,11,24,26,8,9,10,12,13,14,15,16,17,18,19,20,25:5,6,7,8,9,10,11,15,16,17,18,19,20:para-freeword 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 21:12:preserved 22:13:preserved 23:14:preserved 27:21:preserved

incorporated both group sparsity and geometry constraints . 
This algorithm incorporates both group sparsity and geometric constraints . 
108 0:0,1,2:paraphrase 1:3:preserved 2:4:preserved 3:5:preserved 4:6:preserved 5:7:bigrammar-wform 6:8:preserved 7:9:preserved

Besides , to keep temporal information , a temporal pyramid matching method was used on each sequence of depth maps . 
In addition , to keep temporal information , a temporal pyramid matching method can be used on each sequence of depth maps . 
109 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13,14:bigrammar-vtense 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved

Actually , most skeleton information-based approaches has achieved the state-of-the-art performance on benchmark datasets . 
Most of the skeleton information-based approaches have state-of-the-art performance on benchmark datasets . 
110 14,0,1:12:para-freeword 2,3:2,3,1,0:para-colocation 4:4:preserved 5:5:preserved 6,7:6:paraphrase 8::unaligned 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved

However , due to depending on skeleton information , these approaches have to deal with limitations when skeleton information is not available or incorrect .
However , their dependence on skeleton information is a detriment when such information is not available or incorrect .
111 0:0:preserved 1:1:preserved 2,3,4,8,9,10,11,12,13,14,15:2,3,7,8,9:para-freeword 5:4:preserved 6:5:preserved 7:6:preserved 16:10:preserved 17:11:paraphrase 18:12:preserved 19:13:preserved 20:14:preserved 21:15:preserved 22:16:preserved 23:17:preserved 24:18:preserved

Different from the previous approaches , we use a dense trajectory-based approach for action recognition . 
Different from the previous studies , we take a dense trajectory-based approach to action recognition . 
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:paraphrase 5:5:preserved 6:6:preserved 7:7:paraphrase 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-prep 13:13:preserved 14:14:preserved 15:15:preserved

We do not require to segment human body , like </CITE> . 
We do not require the human body to be segmented , unlike the methods in </CITE> . 
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,6,7:7,4,5,6,8,9:para-freeword 8:10:preserved 9:11,12,13,14:para-freeword 10:15:preserved 11:16:preserved

In addition skeleton extraction as in </CITE> is not required in our work . 
Moreover , unlike the methods in </CITE> , no skeleton has to be extracted . 
114 0,1,3,4,7,8,10,11,12,9,2,5:0,1,2,3,4,7,8,10,11,12,13,9,5:para-freeword 6:6:preserved 13:14:preserved

We investigate the benefit of generating 2D transformed videos from depth data , as mentioned in </CITE> . 
We investigate the benefit of generating 2D transformed videos from depth data , as mentioned in </CITE> . 
115 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Moreover , we leverage the effectiveness of trajectory feature to represent an action video . 
Moreover , we leverage the trajectory feature to represent actions in video . 
116 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8:5,6:paraphrase 9:7:preserved 10:8:preserved 11,12:9:bigrammar-nnum 13:11:preserved 14:12:preserved :10:mogrammar-prep

To the best of our knowledge , no work has previously been proposed to adapt the dense trajectory-based approach to human action recognition in depth video . 
To the best of our knowledge , no study has previously proposed to adapt the dense trajectory-based approach to human action recognition in depth video . 
117 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:paraphrase 9:9:preserved 10:10:preserved 11,12:11:paraphrase 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

We conduct evaluations on recognition accuracy in depth video using dense trajectories proposed by H. Wang et al. </CITE> .
We evaluated the recognition accuracy of our method on depth video using the dense trajectories proposed by H. Wang et al. </CITE> .
118 0:0:preserved 1,2,3:1,2:paraphrase 4:3:preserved 5:4,5,6,7:paraphrase 6:8:bigrammar-prep 7:9:preserved 8:10:preserved 9:11:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved :12:mogrammar-det

In this section , we present a unified action recognition framework on depth data. 
Here , we present a unified action recognition framework for depth data. 
123 0,1,2:0:para-freeword 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:bigrammar-prep 12:10:preserved 13:11:preserved

We extract discriminative motion patterns from multiple views and then apply a bag-of-words ( BoW ) model to compute feature vectors for the feature fusion scheme . 
We extract discriminative motion patterns from multiple views and apply a bag-of-words ( BoW ) model to them to compute feature vectors for the feature fusion scheme . 
124 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9:8:para-freeword 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16,17,18:para-freeword 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved

The motivation of using a bag-of-words model to action recognition is to handle a variable number of motion patterns produced by arbitrary movements from various subjects . 
The motivation behind using the bag-of-words model is that it can handle a variable number of motion patterns produced by arbitrary movements from various subjects . 
125 0:0:preserved 1,2,3,4,5,6,7,8,9:1,2,3,4,5,6:para-colocation 10:7:preserved 11,12:8,9,10,11:para-freeword 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

The fused feature vectors computed from a bag-of-words model are input of classifiers in training and testing phases . 
The fused feature vectors computed from the bag-of-words model are then input to the classifiers in the training and testing phases . 
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-det 7:7:preserved 8:8:preserved 9:9:preserved 10:11,10:para-freeword 11:12:bigrammar-prep 12:14:preserved 13:15:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved :13:mogrammar-det :16:mogrammar-det

Following subsections provide concise descriptions about processes in our framework .
The following steps are concise descriptions of the processes in our framework .
127 0,1:0,1,2:para-freeword 2:3:paraphrase 3:4:preserved 4:5:preserved 5:6:bigrammar-prep 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved :7:mogrammar-det

Projection : In this step , key problem is to find appropriate action representation to effectively captures discriminative motion patterns . 
Projection : In this step , the problem is to find an action representation that effectively captures discriminative motion patterns . 
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6,7:6,7:paraphrase 8:8:preserved 9:9:preserved 10:10:preserved 11,12,13:11,13,12:paraphrase 14:14:para-freeword 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Currently , capturing the motion patterns has not been achieved specific successes on 3D data in comparison with 2D data . 
Currently , capturing motion patterns in 3D data has had less success than in 2D data . 
129 0:0:preserved 1:1:preserved 2:2:preserved 3::mogrammar-det 4:3:preserved 5:4:preserved 6,8,9,7,10,11,12,13,14,15,16,17,18,19:8,5,6,7,9,10,11,12,13,14,15:para-freeword 20:16:preserved

Therefore , at this step , we try to present each 3D action through a combination of 2D actions . 
Therefore , in this step , we try to represent each 3D action as a combination of 2D actions . 
130 0:0:preserved 1:1:preserved 2:2:bigrammar-prep 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:paraphrase 10:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-prep 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

To do that , M depth maps are projected onto N view planes to obtain corresponding 2D action videos .
To do that , M depth maps are projected onto N view planes to obtain corresponding 2D action videos .
131 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

After the projection , each 2D motion video is abstracted by several local motion patterns .
After the projection , each 2D motion video is abstracted by using several local motion patterns .
132 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved :11:bigrammar-others

Feature Extraction : In order to capture discriminative motion patterns on the 2D videos , we adopt the trajectory based approach .
Feature Extraction : Here , we use a trajectory-based approach to capture discriminative motion patterns in the 2D videos .
133 0:0:preserved 1:1:preserved 2:2:preserved 3,4,5,16,17,18,19,20,14,15:3,4,6,7,8,9,5,10:para-freeword 6:11:preserved 7:12:preserved 8:13:preserved 9:14:preserved 10:15:bigrammar-prep 11:16:preserved 12:17:preserved 13:18:preserved 21:19:preserved

With this approach , we can keep away from the challenges from human body segmentation as well as skeleton extraction . 
With this approach , we can avoid problems related to segmentation of the human body as well as skeleton extraction . 
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6,7,8,10,11,9:6,7,8,9:para-freeword 12,13,14:13,10,11,12,14:para-colocation 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Trajectory-aligned descriptors are then calculated on the extracted trajectories to build N ``bags of motion patterns'' corresponding to N views .
Trajectory-aligned descriptors are then calculated on the extracted trajectories in order to build N ``bags of motion patterns'' corresponding to N views .
135 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:11,9,10:para-freeword 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved

Clustering : The clustering step is to convert a ``bag of motion patterns'' from dataset to a ``bag of quantized motion patterns'' . 
Clustering : The clustering step converts the ``bag of motion patterns'' from the dataset into a ``bag of quantized motion patterns'' . 
136 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7:5:paraphrase 8:6:bigrammar-det 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:13:preserved 15:14:bigrammar-prep 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved :12:mogrammar-det

A quantized motion pattern can be considered as a representative of several similar motion patterns . 
A quantized motion pattern can be considered to be representative of several similar motion patterns . 
137 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8:7,8:para-freeword 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

A standard clustering method ( e.g. k-means ) can be applied over all the motion patterns . 
A standard clustering method ( e.g. k-means ) can be applied to all the motion patterns . 
138 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-prep 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Quantized motion patterns are then defined as the centers of the learned clusters . 
Quantized motion patterns are then defined as the centers of the learned clusters . 
139 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The number of the clusters is the size of ``bag of quantized motion patterns'' .
The number of clusters is the size of the ``bag of quantized motion patterns'' .
140 0:0:preserved 1:1:preserved 2:2:preserved 3:5:preserved 4:3:preserved 5:4:preserved 6:8:preserved 7:6:preserved 8:7:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Quantization and Fusion : To represent an action with captured motion patterns , we map each  motion pattern  to a certain `` quantized motion pattern '' through the matching process . 
Quantization and Fusion : To represent an action with captured motion patterns , we map each  motion pattern  to a certain `` quantized motion pattern '' through the matching process . 
141 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

Afterwards , the histogram of the quantized motion patterns is generated to represent action on a corresponding view . 
Afterwards , a histogram of the quantized motion patterns is generated to represent the action in the corresponding view . 
142 0:0:preserved 1:1:preserved 2:2:bigrammar-det 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:bigrammar-prep 15:16:bigrammar-det 16:17:preserved 17:18:preserved 18:19:preserved :13:mogrammar-det

After that , the histograms generated from all views are concatenated to form a larger feature vector as input to classifiers . 
After that , the histograms generated from all views are concatenated to form a larger feature vector as input to the classifiers . 
143 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:21:preserved 21:22:preserved :20:bigrammar-others

Since each individual feature vector has the same meaning , the feature fusion can guarantee the effectiveness to represent action .
Since each individual feature vector has the same meaning , the feature fusion can guarantee the effectiveness of the action representation .
144 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15,16,17,18,19:15,16,17,18,19,20:para-colocation 20:21:preserved

Training and Testing : After the final feature representations are generated , we separate them into two histogram databases for training and testing phases . 
Training and Testing : After the final feature representations are generated , we separate them into two histogram databases for the training and testing phases . 
145 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :20:bigrammar-others

We use a machine learning method such as Support Vector Machine ( SVM ) for classification . 
We use a machine learning method such as Support Vector Machine ( SVM ) to make the classification . 
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14,15,16:paraphrase 15:17:preserved 16:18:preserved

In practice , we use the precomputed-kernel technique with the histogram intersection kernel for this process . 
In practice , we use the precomputed-kernel technique with the histogram intersection kernel for this process . 
147 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Besides , we perform the one-vs-all strategy for multi-class classification .
In addition , we employ the one-vs-all strategy for multi-class classification .
148 0:0,1:para-freeword 1:2:preserved 2:3:preserved 3:4:paraphrase 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved

Our proposed trajectory-based approach is compared with the state-of-the-art methods for human action recognition on depth data . 
We compared our trajectory-based approach with state-of-the-art methods for recognizing human actions with depth data . 
149 0,1,4,7,2,3,5:0,2,1,3,4:para-freeword 6:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11,12,13:10,9,11:para-colocation 14:12:bigrammar-prep 15:13:preserved 16:14:preserved 17:15:preserved

Actually , our approach does not count skeleton extraction , which is used as an important factor in some works , such as </CITE> . 
Our approach does not use skeleton extraction , which is an important part of some methods , such as </CITE> . 
150 24,0,1:20:para-freeword 2:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 6:4:paraphrase 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12,13,14,15,16,17,18,19:10,11,12,13,14,15:para-freeword 20:16:preserved 21:17:preserved 22:18:preserved 23:19:preserved

In fact , extracting skeleton exactly is still an completely unsolved problem , due to the challenges , such as cluttered background , hardware quality , camera motion , so on .
In fact , extracting a skeleton exactly is still an unsolved problem because of challenges such as cluttered backgrounds , poor hardware quality , and camera motion .
151 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9,10,11:11,10:paraphrase 12,17,31,28:27:para-freeword 13,14,15:12,13:para-freeword 16:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:bigrammar-nnum 22:19:preserved 23,24:20,21,22:paraphrase 25:23,24:para-freeword 26:25:preserved 27:26:preserved 29,30::unaligned :4:mogrammar-det

As mentioned in section </Ref> , to support the aim , we decompose each 3D action to a set of 2D actions and leverage the trajectory-based approach to effectively capture the discriminative motion patterns . 
As mentioned in section </Ref> , we decompose each 3D action into a set of 2D actions and leverage the trajectory-based approach to capture the discriminative motion patterns . 
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 6,7,8,9,10,5:5:paraphrase 11:6:preserved 12:7:preserved 13:8:preserved 14:9:preserved 15:10:preserved 16:11:bigrammar-prep 17:12:preserved 18:13:preserved 19:14:preserved 20:15:preserved 21:16:preserved 22:17:preserved 23:18:preserved 24:19:preserved 25:20:preserved 26:21:preserved 27,28,29:23,22:paraphrase 31:25:preserved 32:26:preserved 33:27:preserved 34:28:preserved :24:mogrammar-det

In this section , we provide a description of our proposed method to obtain 2D videos from various views . 
In this section , we describe our method to obtain 2D videos from various views . 
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8:5:para-freeword 9:6:preserved 10,11:7:paraphrase 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved 16:12:preserved 17:13:preserved 18:14:preserved 19:15:preserved

In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. 
In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. 
158 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

As mentioned in section </CITE> , we decompose each 3D action into a set of 2D actions and leverage the trajectory-based approach to capture the discriminative motion patterns . 
As mentioned in section </CITE> , we decompose each 3D action into a set of 2D actions and leverage the trajectory-based approach to capture the discriminative motion patterns . 
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

In this section , we describe our method to obtain 2D videos from various views . 
In this section , we describe our method to obtain 2D videos from various views . 
160 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. </CITE> , which has been demonstrated state-of-the-art at action recognition . 
In addition , we briefly present the dense trajectory-based feature proposed by H. Wang et al. </CITE> , which has state-of-the-art performance in action recognition . 
161 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20,21,22:20,21:paraphrase 23:22:bigrammar-prep 24:23:preserved 25:24:preserved 26:25:preserved

Related parts , such as : dense sampling , tracking , and feature descriptors are also referred to .
Related aspects including dense sampling , tracking , and feature descriptors are also referred to .
162 0:0:preserved 1,2,3,4,5:1,2:para-freeword 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved

Point </Eq> is the projection of point </Eq> along the view direction </Eq> onto the view plane </Eq> , which has state-of-the-art performance in action recognition . 
Point </Eq> is the projection of point </Eq> along the view direction </Eq> onto the view plane </Eq> , which has state-of-the-art performance in action recognition . 
163 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Our proposed method to obtain discriminative motion patterns for human action recognition on depth video is as follow . 
Our method to obtain discriminative motion patterns for recognizing human actions in depth video is as follows . 
168 0:0:preserved 1,2:1:paraphrase 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9,10,11:9,8,10:para-colocation 12:11:bigrammar-prep 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:bigrammar-nnum 18:17:preserved

At first , 2D motion videos are formed from the sequence of depth maps , as illustrated in figure </fig> . 
First , 2D motion videos are formed from a sequence of depth maps , as illustrated in Figure </fig> . 
169 0,1:0:para-freeword 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:bigrammar-det 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:typo 19:18:preserved 20:19:preserved

At this step , to obtain a 2D motion video from a view direction </Eq> , corresponding to a view plane </Eq> , in each depth map </Eq> , each point </Eq> is projected to </Eq> on the view plane </Eq> see in figure </Fig> by    :
In this step , to obtain a 2D motion video from a view </Eq> , corresponding to a view plane </Eq> , in each depth map </Eq> , each point </Eq> is projected to </Eq> on the view plane </Eq> ( see Figure </Fig> ) as follows    :
170 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 14,13:13:paraphrase 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved 31:30:preserved 32:31:preserved 33:32:preserved 34:33:preserved 35:34:preserved 36:35:preserved 37:36:preserved 38:37:preserved 39:38:preserved 40:39:preserved 42,43,45,41:40,42,44,45,46,41:para-freeword 44:43:preserved 46:47:preserved

where , And the intensity value </Eq> at the projected point </Eq> is calculated by    :
where , The intensity value </Eq> at the projected point </Eq> is calculated as    :
171 0:0:preserved 1:1:preserved 2,3:2:para-freeword 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:bigrammar-prep 15:14:preserved

So , given a set of points </Eq> , we have a projection </Eq> . 
Thus , given a set of points </Eq> , we have a projection </Eq> . 
172 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Therefore , a set of the projections obtained from a given sequence of M depth maps under a view direction </Eq> is formed to a corresponding 2D motion video </Eq> . 
Therefore , a set of the projections obtained from a given sequence of M depth maps for the view direction </Eq> is formed into a corresponding 2D motion video </Eq> . 
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:bigrammar-prep 17:17:bigrammar-det 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:bigrammar-prep 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

Each 2D video can be regarded as a 2D motion representation of corresponding action in depth video .
Each 2D video can be regarded as a 2D motion representation of the corresponding action in the depth video .
174 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved :12:mogrammar-det :16:mogrammar-det

In particular , we choose three 2D motion representations to present action on three view directions    : front , side , and top in 3D space , corresponding to three view planes , respectively    : </Eq> , </Eq> and </Eq> . 
In particular , we use three 2D motion representations of action in three view directions    : the front , side , and top in 3D space , corresponding to three view planes    : </Eq> , </Eq> and </Eq> . 
175 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:paraphrase 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-prep 11,10:10:paraphrase 12:11:bigrammar-prep 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:17,16:bigrammar-det 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32,33,40:38:para-freeword 34:32:preserved 35:33:preserved 36:34:preserved 37:35:preserved 38:36:preserved 39:37:preserved

With these view directions , the corresponding projections are respectively    :
The corresponding projections in these view directions are    :
176 0,4,9,1,2,3,5,6,7,8:3,0,1,2,4,5,6,7:para-freeword 10:8:preserved

And the corresponding intensity values in the three projections are , respectively    :
The corresponding intensity values of the three projections are    :
177 0,1:0:para-freeword 2:1:preserved 3:2:preserved 4:3:preserved 5:4:bigrammar-prep 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 12,10,11:9:paraphrase

Essentially there are points we can observe from a certain view , but from other views in is impossible . 
Essentially , there are points we can observe from a certain view , but not from other views . 
182 0:0:preserved 1:2,1:typo 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 16,17,18,15,14,13:14,15,16,17:para-freeword 19:18:preserved

Indeed , considering an example as shown in figure </Fig> , two points </Eq> have the corresponding projections </Eq> along a view direction </Eq>    :
Indeed , considering the example shown in Figure </Fig> , two points </Eq> have the corresponding projections </Eq> along the view direction </Eq>    :
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-det 4:4:preserved 6,5:5:bigrammar-prep 7:6:preserved 8:7:typo 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:bigrammar-det 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved

However , if we observe two points </Eq> , </Eq> along a view direction </Eq> , their projection is only point </Eq>  :
However , if we observe </Eq> and </Eq> along the view direction </Eq> , their projection is only </Eq>  :
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8,9:5,6,7:para-freeword 10:8:preserved 11:9:bigrammar-det 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20,21:18:paraphrase 22:19:preserved

In such cases , we cannot observe points ( i.e. , point </Eq> ) hidden by other point ( i.e. point </Eq> ) along a certain view direction ( i.e. , view direction </Eq> ) . 
In such cases , we cannot observe points ( i.e. , point </Eq> ) hidden by others ( i.e. , point </Eq> ) along a certain view direction ( i.e. , view direction </Eq> ) . 
185 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16,17:16:paraphrase 18:17:preserved 19:18,19:typo 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

Therefore , the corresponding intensity value </Eq> at the projected point </Eq> is calculated by  :
Therefore , the corresponding intensity value </Eq> at the projected point </Eq> is  :
186 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12,13,14:12:paraphrase 15:13:preserved

Where </Eq> are parameters of the plane </Eq> . And </Eq> is the coordinate of </Eq> .
Here , </Eq> are parameters of the plane </Eq> , and </Eq> is the coordinate of </Eq> .
187 0,9,16,8:0,1,9,10,17:para-freeword 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved

Points </Eq> </Eq> are respectively the projection of points </Eq> , </Eq> along a view direction </Eq> onto a view plane </Eq> .  
Points </Eq> and </Eq> are respectively the projections of </Eq> and </Eq> along the view direction </Eq> onto the view plane </Eq> .  
188 0:0:preserved 1,2:1,2,3:para-freeword 3:4:preserved 4:5:preserved 5:6:preserved 6:7:bigrammar-nnum 7:8:preserved 8,9:9:paraphrase 10:10:para-freeword 11:11:preserved 12:12:preserved 13:13:bigrammar-det 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:bigrammar-det 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Point </Eq> is the projection of points </Eq> , </Eq> along a view direction </Eq> onto the view plane </Eq> . 
</Eq> is the projection of </Eq> and </Eq> along the view direction </Eq> onto the view plane </Eq> . 
189 0,1:0:paraphrase 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6,7:5:paraphrase 8:6:typo 9:7:preserved 10:8:preserved 11:9:bigrammar-det 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved

In this case , the intensity value of point </Eq> is calculated by distance from point </Eq> to the view plane </Eq> .
In this case , the intensity value of </Eq> is calculated using the distance from </Eq> to </Eq> .
190 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9:8:paraphrase 10:9:preserved 11:10:preserved 12,15,19,20,21,18,17,16,14,13:11,12,13,14,15,16,17:para-freeword 22:18:preserved

Considering equations , they are obviously the distance equations between a point and a plane . 
Equations are obviously distance equations between a point and a plane . 
191 0,1,3,2:0:paraphrase 4:1:preserved 5:2:preserved 6::mogrammar-det 7:3:preserved 8:4:preserved 9:5:preserved 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved

Therefore , equation can be presented by  :
Therefore , equation can be represented as  :
192 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5,6:paraphrase 7:7:preserved

Where , </Eq> is the distance between point </Eq> and plane </Eq> .
Here , </Eq> is the distance between the point </Eq> and the plane </Eq> .
193 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved :7:mogrammar-det :11:mogrammar-det

In addition , figure </Fig> shows that is greater than . 
In addition , Figure </Fig> shows that is greater than . 
194 0:0:preserved 1:1:preserved 2:2:preserved 3:3:typo 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The evaluation has provided an important conclusion presented by the following equation  :
The evaluation leads to an important conclusion in the form of the following equation  :
195 0:0:preserved 1:1:preserved 2,3:2,3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7,8,9,10,11:7,9,10,11,12,13,8:para-freeword 12:14:preserved

More generally , if there are N points belong to a line parallel to the view direction </Eq> , the intensity value at the projected point on plane </Eq> is calculated by  :
More generally , if there are N points on a line parallel to the view direction </Eq> , the intensity value at the projected point on plane </Eq> is  :
196 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10,11,12:8,9,10,11:para-freeword 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 30,31,29:28:paraphrase 32:29:preserved

Trajectories provide a compact representation of motion information in video . 
Trajectories provide a compact representation of motion information in video . 
202 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Trajectories from intensity videos can be used for multimedia event detection ( MED ) , video mining , action classification , and so on . 
Trajectories from intensity videos can be used for multimedia event detection ( MED ) , video mining , action classification , and so on . 
203 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Trajectory extraction crucially depends on both processes : sampling and tracking . 
Trajectory extraction crucially depends on the sampling and tracking processes . 
204 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,7,6,8,9,10:5,6,7,8,9:para-freeword 11:10:preserved

Some methods , such as </CITE> , used KLT tracker </CITE> or </CITE> matched SIFT descriptors between consecutive frames to obtain feature trajectories . 
Some methods , such as </CITE> , use KLT tracker </CITE> or matched SIFT descriptors </CITE> between consecutive frames to obtain feature trajectories . 
205 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-vtense 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:15:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

Recently , the dense trajectory-based motion feature proposed by </CITE> has achieved the state of the art performance on MED systems , such as segment-based system </CITE> on TRECVID MED 2010 , 2011 or AXES </CITE> , and BBNVISER </CITE> on TRECVID MED 2012 . 
Recently , the dense trajectory-based motion feature proposed by </CITE> has achieved high levels of performances on MED systems , including the segment-based system </CITE> on TRECVID MED 2010 and 2011 , AXES </CITE> , and BBNVISER </CITE> on TRECVID MED 2012 . 
206 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12,14,13,15,16,17:12,13,14,15:paraphrase 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22,23:20:para-freeword 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved 28:26:preserved 29:27:preserved 30:28:preserved 31,33:29,31:para-freeword 32:30:preserved 34:32:preserved 35:33:preserved 36:34:preserved 37:35:preserved 38:36:preserved 39:37:preserved 40:38:preserved 41:39:preserved 42:40:preserved 43:41:preserved 44:42:preserved :21:mogrammar-det

The sampling is performed at multiple scales with a factor of </Eq> . 
The sampling is performed at multiple scales with a factor of </Eq> . 
208 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Then , tracking is performed to form trajectories . 
Tracking is then performed to form trajectories . 
209 0,1,2:0,2:para-freeword 3:1:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved

At each scale , in frame t , each point </Eq> is tracked to point </Eq> in next frame </Eq> by  : where </Eq> denotes the dense optical flow field , </Eq> is the kernel of median filtering , and </Eq> is the rounded position of </Eq> . 
At each scale , in frame t , each point </Eq> is tracked to point </Eq> in the next frame </Eq> by using  : where </Eq> denotes the dense optical flow field , </Eq> is the kernel of median filtering , and </Eq> is the rounded position of </Eq> . 
210 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21,22:paraphrase 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:40:preserved 39:41:preserved 40:42:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved 45:47:preserved 46:48:preserved 47:49:preserved :17:mogrammar-det

The algorithm of </CITE> adopts dense optical flow . 
The algorithm presented in </CITE> uses dense optical flows . 
211 0:0:preserved 1,2,4,5,6:1,2,3,5,6,7:para-freeword 3:4:preserved 7:8:paraphrase 8:9:preserved

And to avoid a drifting problem , a suitable value of trajectory length is set to 15 frames . 
To avoid drifting , it sets a suitable trajectory length of 15 frames . 
212 0,1:0:para-freeword 2:1:preserved 3::mogrammar-det 4,5:2:paraphrase 6:3:preserved 7,9,13,14,15,8,10,11,12:4,5,6,7,8,9,10:para-freeword 16:11:preserved 17:12:preserved 18:13:preserved

Besides trajectories with sudden changes are removed .
It also removes trajectories with sudden changes .
213 0,5,6:0,1,2:para-freeword 1:3:preserved 2:4:preserved 3:5:preserved 4:6:preserved 7:7:preserved

After extracting trajectories , two kinds of descriptors : a trajectory shape descriptor and a trajectory-aligned descriptor , can be adopted . 
Once the trajectories have been extracted , two kinds of descriptor , i.e. , a trajectory shape descriptor and a trajectory-aligned descriptor , can be used . 
214 0,1,8:0,1,3,4,5,12,13:para-freeword 2:2:preserved 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:bigrammar-nnum 9:14:preserved 10:15:preserved 11:16:preserved 12:17:preserved 13:18:preserved 14:19:preserved 15:20:preserved 16:21:preserved 17:22:preserved 18:23:preserved 19:24:preserved 20:25:paraphrase 21:26:preserved

In our experiments , we only use trajectory-aligned descriptors including the HOG </CITE> , the Histogram of Optical Flow ( HOF ) </CITE> , and the Motion Boundary Histogram ( MBH ) </CITE> . 
In our experiments , we only used trajectory-aligned descriptors , including the HOG </CITE> , the Histogram of Optical Flow ( HOF ) </CITE> , and the Motion Boundary Histogram ( MBH ) </CITE> . 
215 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8,9:typo 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved

HOG captures local appearance information , while HOF and MBH encode local motion patterns . 
HOG captures local appearance information , while HOF and MBH encode local motion patterns . 
216 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The descriptors are computed within a space-time volume </Eq> spatial pixels and </Eq> temporal frames ) around the trajectory . 
The descriptors are computed within a space-time volume </Eq> spatial pixels and </Eq> temporal frames ) around the trajectory . 
217 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

This volume is divided into a 3D grid ( spatially </Eq> grid and temporally </Eq> segments ) . 
This volume is divided into a 3D grid ( spatially into </Eq> grid and temporally into </Eq> segments ) . 
218 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved :10,15:mogrammar-prep

The default settings of these parameters are </Eq> = 32 pixels , </Eq> = 15 frames , </Eq> = 2 , and </Eq> = 3 .
The default settings of these parameters are </Eq> = 32 pixels , </Eq> = 15 frames , </Eq> = 2 , and </Eq> = 3 .
219 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

According to authors in </CITE> , all the three descriptors have shown the effectiveness for action recognition on intensity video . 
According to the authors of </CITE> , all three descriptors are capable of recognizing actions in intensity video . 
220 0:0:preserved 1:1:preserved 2:3:preserved 3:4:bigrammar-prep 4:5:preserved 5:6:preserved 6:7:preserved 7::mogrammar-det 8:8:preserved 9:9:preserved 10,11,12,13,14,15,16:10,11,12,13,14:para-freeword 17:15:bigrammar-prep 18:16:preserved 19:17:preserved 20:18:preserved :2:mogrammar-det

The experimental settings for these descriptors are based on an empirical study showed in </CITE> . 
The experimental settings for these descriptors are based on an empirical study </CITE> . 
221 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10,11,12,13:9,10,11:para-freeword 14:12:preserved 15:13:preserved

In our work , we also conduct experiments on all the three descriptors to wholly evaluate their effectiveness on depth video .
We also conducted experiments on all the three descriptors to evaluate their effectiveness on depth video .
222 0,1,2,3,4:0:para-freeword 5:1:preserved 6:2:bigrammar-vtense 7:3:preserved 8:4:preserved 9:5:preserved 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14,15:10:paraphrase 16:11:preserved 17:12:preserved 18:13:preserved 19:14:preserved 20:15:preserved 21:16:preserved

This section presents the experimental results related to our proposed approach on MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
This section presents the experimental results of our approach for the MSR Action 3D dataset , MSR Gesture 3D dataset , and 3D Action Pairs dataset . 
228 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6,7:6:paraphrase 8:7:preserved 10,9:8:paraphrase 11:9:bigrammar-prep 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved :10:mogrammar-det

Our aim is to clear up the following issues  : 
Our aim is to clear up the following issues  : 
229 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

(1) Recognition accuracy in case of single view; 
(1) Recognition accuracy in case of single view; 
230 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

(2) The role of compensating information from multiple views; 
(2) The role of compensating information from multiple views; 
231 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

(3) Comparison with the state-of-the-art approaches . 
(3) Comparison with state-of-the-art approaches . 
232 0:0:preserved 1:1:preserved 2:2:preserved 3::mogrammar-det 4:3:preserved 5:4:preserved 6:5:preserved

For analysis , we concentrate on MSR Action 3D dataset to explain experimental results . For MSR Gesture 3D dataset and 3D Action Pairs dataset , we only show the final results . 
We will concentrate on the MSR Action 3D dataset when explaining the experimental results and show only the final results for the MSR Gesture 3D dataset and 3D Action Pairs dataset . 
233 0,1,2,15,25,26,27,28,29,30,31,16,18,17,19,20,21,22,24,23,14:17,20,21,26,14,15,16,18,19,22,23,24,25,28,27,29,30:para-freeword 3:0:preserved 4:2,1:bigrammar-vtense 5:3:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10,11:9,10:paraphrase 12:12:preserved 13:13:preserved 32:31:preserved :4:unaligned :11:mogrammar-det

All experimental results are reported under the settings mentioned in section . 
All experimental results are reported for the settings described in section . 
234 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:paraphrase 9:9:preserved 10:10:preserved 11:11:preserved

In comparison with the state-of-the-art approaches , our reported result is calculated on concatenating action representations from the combinations of three views  : front , side and top . 
Unlike the state-of-the-art approaches , our reported results are for concatenating action representations from combinations of three views  : front , side and top . 
235 0,1,2:0:para-freeword 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:bigrammar-nnum 10,11,12:8,9:paraphrase 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17::mogrammar-det 18:14:preserved 19:15:preserved 20:16:preserved 21:17:preserved 22:18:preserved 23:19:preserved 24:20:preserved 25:21:preserved 26:22:preserved 27:23:preserved 28:24:preserved

All the results are compared in terms of recognition accuracy . 
All the results are compared in terms of recognition accuracy . 
236 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The best performance is highlighted in bold .
The best performance is highlighted in bold .
237 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

In this section , we provide the detail of parameters used in each step of our 3D action recognition framework .
Here , we detail the parameters used in each step of our 3D action recognition framework .
241 0,1,2:0:paraphrase 3:1:preserved 4:2:preserved 5,6,7,8,9:3,4,5:para-freeword 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved 16:12:preserved 17:13:preserved 18:14:preserved 19:15:preserved 20:16:preserved

Projection : Our aim at this step is to select the best views to effectively capture most discriminative motion patterns . 
Projection : Our aim in this step is to select the best views to capture the most discriminative motion patterns . 
242 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-prep 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 14,13:13:paraphrase 15:14:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved :15:mogrammar-det

In our experiments , we conduct projections on views : front , side , and top , which have been demonstrated effective in works </CITE> . 
We make projections from front , side , and top views  : these views have been demonstrated to be effective in other studies </CITE> . 
243 6,5,4,3,2,1,0,7,8,9,10,11,12,13,14,15,16,17:2,0,1,3,4,5,6,7,8,9,10,11,12,13:para-freeword 18:14:preserved 19:15:preserved 20:16:preserved 21:19,17,18:paraphrase 22:20:preserved 23:21,22:paraphrase 24:23:preserved 25:24:preserved

Results obtained from the projections are three 2D motion videos .
The projections are then used to make three 2D motion videos .
244 0,1,2,3,4:1,0:paraphrase 5:2,3,4,5,6:paraphrase 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved

Feature Extraction : We use the application available online to extract dense trajectories and calculate aligned-descriptors ( i.e. , MBH , HOG and HOF ) for each 2D motion video . 
Feature Extraction : We use an application available online to extract trajectory - aligned descriptors ( i.e. , MBH , HOG and HOF ) for each 2D motion video . 
245 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-det 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12,14,15,13:11,12,13,14:para-freeword 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved

Experimental results reported in section attach to the MBH descriptor . 
The experimental results reported in section are for the MBH descriptor . 
246 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5,6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved

The HOG , HOF descriptors will be mentioned in the section .
The HOG and HOF descriptors are described in section .
247 0:0:preserved 1:1:preserved 2,5,6,7:2,5,6:para-freeword 3:3:preserved 4:4:preserved 8:7:preserved 9::mogrammar-det 10:8:preserved 11:9:preserved

Clustering : Purpose of this step is to learn a visual vocabulary or codebook . 
Clustering : The purpose of this step is to learn a visual vocabulary or codebook . 
248 0:0:preserved 1:1:preserved 2:2,3:bigrammar-det 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

Corresponding to three views : front , side and top , we create three codebooks ( i.e. three bags of quantized motion patterns ) . 
We create three codebooks ( i.e. three bags of quantized motion patterns ) corresponding to the front , side , and top views . 
249 0:13:preserved 1:14:preserved 2,11,4:0,15:para-freeword 3:22:preserved 5:16:preserved 6:17:preserved 7:18:preserved 8:20:preserved 9:21:preserved 10:19:preserved 12:1:preserved 13:2:preserved 14:3:preserved 15:4:preserved 16:5:preserved 17:6:preserved 18:7:preserved 19:8:preserved 20:9:preserved 21:10:preserved 22:11:preserved 23:12:preserved 24:23:preserved

In addition , due to purpose of a stable and unified framework on all benchmark datasets , we cluster extracted motion features to 2000 codewords ( i.e. 2000 quantized motion patterns ) for each codebook . 
In addition , to ensure a unified framework on all benchmark datasets , we cluster the extracted motion features around 2000 codewords ( i.e. 2000 quantized motion patterns ) for each codebook . 
250 0:0:preserved 1:1:preserved 2:2:preserved 3,5,6,8,9:4:para-freeword 4:3:preserved 7:5:preserved 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved 16:12:preserved 17:13:preserved 18:14:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:bigrammar-prep 23:20:preserved 24:21:preserved 25:22:preserved 26:23:preserved 27:24:preserved 28:25:preserved 29:26:preserved 30:27:preserved 31:28:preserved 32:29:preserved 33:30:preserved 34:31:preserved 35:32:preserved :15:mogrammar-det

The k-means algorithm with Euclidean distance is applied at this step .
The k-means algorithm with the Euclidean distance is used in this step .
251 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7,8:8,9:paraphrase 9:10:preserved 10:11:preserved 11:12:preserved :4:mogrammar-det

Quantization and Fusion : Two popular strategies used to quantize extracted features are hard-assignment and soft-assignment . 
Quantization and Fusion : Two popular strategies for quantizing extracted features are hard-assignment and soft-assignment . 
252 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8,9:7,8:paraphrase 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved

To guarantee the efficiency of our framework , we apply the hard-assignment strategy to quantize dense trajectory motion features extracted at step Feature Extraction . 
To guarantee the efficiency of our framework , we use the hard-assignment strategy to quantize the dense trajectory motion features extracted in step Feature Extraction . 
253 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:paraphrase 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:bigrammar-prep 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :15:mogrammar-det

With this strategy , each feature vector can be assigned to a codeword using Euclidean distance or rejected as an outlier . 
With this strategy , each feature vector can be assigned to a codeword or be rejected as an outlier by using the Euclidean distance . 
254 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:20:preserved 14:22:preserved 15:23:preserved 16:13:preserved 17:15,14:para-passact 18:16:preserved 19:17:preserved 20:18:preserved 21:24:preserved :19:mogrammar-prep :21:mogrammar-det

Results of the quantization step is to generatee 2D motion representations corresponding to selected views .
The results of the quantization step are 2D motion representations corresponding to the selected views .
255 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 6,7,5:6:paraphrase 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:13:preserved 14:14:preserved 15:15:preserved :12:mogrammar-det

After that , these 2D motion representations are concatenated to form final motion representation for corresponding depth video . 
After that , these 2D motion representations are concatenated to form the final motion representation for the corresponding depth video . 
256 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved :11,16:mogrammar-det

The final representations are then separated into two histogram databases for training and testing phases .
The final representations are then separated into two histogram databases for the training and testing phases .
257 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved :11:mogrammar-det

Training and Testing : Most recent works used SVM classifier refer to the libSVM library </CITE> published online by author . 
Training and Testing : Most of the recent methods using SVM classifiers refer to the libSVM library </CITE> published online . 
258 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,6:4,5,6,8,7:para-colocation 7:9:bigrammar-vtense 8:10:preserved 9:11:bigrammar-nnum 10:12:preserved 11:13:preserved 12::unaligned 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18,19,20:20:para-freeword :14:mogrammar-det

In our framework , we use the libSVM library with histogram intersection kernel  :
In our framework , we use the libSVM library with the histogram intersection kernel  :
259 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :10:mogrammar-det

The one-vs-all strategy is used for classifiers in both phrases of training and testing . 
The one-vs-all strategy is used for classifiers in both training and testing . 
260 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10,11:9:para-freeword 12:10:preserved 13:11:preserved 14:12:preserved

Predicted value of each action is defined as the maximum score obtained from all the classifiers . 
The predicted value of each action is defined as the maximum score obtained from all the classifiers . 
261 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved

This score shows that a human action is confused with another or not .
This score indicates whether a human action is confused with another or not .
262 0:0:preserved 1:1:preserved 2,3:2,3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

This dataset </CITE> contains 20 actions , as shown in figure </Fig> . 
This dataset </CITE> contains 20 actions , as shown in Figure </Fig> . 
268 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:typo 11:11:preserved 12:12:preserved

Actions are performed by ten subjects for two or three times in the context of game console interaction .
Actions were performed two or three times by ten subjects in the context of game console interaction .
269 0:0:preserved 1:1:bigrammar-vtense 2:2:preserved 3:7:preserved 4:8:preserved 5:9:preserved 6::mogrammar-prep 7:3:preserved 8:4:preserved 9:5:preserved 10:6:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved

In total , there are 567 sequences of depth maps . 
In total , there were 567 sequences of depth maps . 
270 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-vtense 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The depth maps are shot at frame rate of 15 fps . 
The depth maps were shot at a frame rate of 15 fps . 
271 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-vtense 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :6:mogrammar-det

The size of the depth map is </Eq> to ensure processing efficiency .
The size of the depth map was </Eq> to ensure processing efficiency .
272 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The three action subsets used in the experiments .
The three action subsets used in the experiments .
273 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

In order to conduct a fair comparison , we use the same experimental settings as </CITE> . 
In order to conduct a fair comparison , we used the same experimental settings as in </CITE> . 
274 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-vtense 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved :15:mogrammar-prep

In the settings , the dataset is divided into three action subsets . 
In these settings , the dataset is divided into three action subsets . 
275 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Each subset has 8 actions . 
Each subset has eight actions . 
276 0:0:preserved 1:1:preserved 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved

The two subsets AS1 and AS2 present that grouped actions have similar movements . 
The AS1 and AS2 subsets are such that grouped actions have similar movements . 
277 0:0:preserved 1,6:5,6:para-freeword 2:4:preserved 3:1:preserved 4:2:preserved 5:3:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The subset AS3 groups complex actions together . 
The AS3 subset groups complex actions together . 
278 0:0:preserved 1:2:preserved 2:1:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

For instance , action hammer seems to be confused with action forward punch in AS1 or similar movements between action hand catch and side boxing in AS2 . 
For instance , the hammer action seems to be confused with the forward punch action in AS1 , and the hand catch and side boxing actions are similar movements in AS2 . 
279 0:0:preserved 1:1:preserved 2:2:preserved 4,3:4,5:para-colocation 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 11,12,10:12,13,14:para-colocation 13:15:preserved 14:16:preserved 15:17:paraphrase 16:27:preserved 17:28:preserved 18,19:19,22,25,26:para-freeword 20:20:preserved 21:21:preserved 22:18:preserved 23:23:preserved 24:24:preserved 25:29:preserved 26:30:preserved 27:31:preserved :3:mogrammar-det :11:mogrammar-det

As for each subset , we select half of the subjects as training and the rest as testing ( i.e. cross subject test ) .
For each subset , we selected half of the subjects for training and the rest for testing ( i.e. , a cross subjects test ) .
280 0::mogrammar-prep 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:bigrammar-vtense 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:bigrammar-prep 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:bigrammar-prep 17:16:preserved 18:17:preserved 19:18:preserved 20:21:preserved 21:19,20,22:bigrammar-nnum 22:23:preserved 23:24:preserved 24:25:preserved

In this part , we evaluate the dense trajectory-based approach for action recognition under observing actions from single views . 
In this part , we evaluate the dense trajectory-based approach on single views . 
285 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10,11,12,13,14,15,16:10:para-freeword 17:11:preserved 18:12:preserved 19:13:preserved

A straightforward view is front view . 
A straightforward view is the front . 
286 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:4:para-freeword 6:6:preserved

In order to obtain action presentation on front view from depth video , a simple way is to consider depth value as intensity value . 
A simple way to obtain an action representation of the front view from the depth video is to consider the depth value as an intensity value . 
287 0,1,5,6,12:7,8,9:para-freeword 2:3:preserved 3:4:preserved 4:6:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:14:preserved 11:15:preserved 13:0:preserved 14:1:preserved 15:2:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:24:preserved 23:25:preserved 24:26:preserved :5:mogrammar-det :13:mogrammar-det :19:mogrammar-det :23:mogrammar-det

Table </tab> shows three confusion matrices corresponding to evaluations on three action subsets of MSR Action 3D dataset . 
Table </tab> shows three confusion matrices corresponding to evaluations on three action subsets of the MSR Action 3D dataset . 
288 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :14:mogrammar-det

Considering results reported in table </tab> , two subsets AS1 , AS2 contain many confused actions as mentioned in the dataset description . For example , hammer (a03) and forward punch (a05) in AS1 , or side-boxing (a12) and hand catch (a04) in AS2 . 
The results reported in the table </tab> indicate that AS1 , AS2 subsets contain many confusable actions , e.g. , hammer (a03) and forward punch (a05) in AS1 , or side-boxing (a12) and hand catch (a04) in AS2 , as is mentioned in the dataset description . 
289 0,7,23,24,22:0,7,8,18,38,40,4:para-freeword 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:10:preserved 8:12:preserved 9:9:preserved 10:17:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:bigrammar-wform 15:16:preserved 16:39:preserved 17:41:preserved 18:26:preserved 19:43:preserved 20:44:preserved 21:45:preserved 25:19:preserved 26:20:preserved 27:21:preserved 28:22:preserved 29:23:preserved 30:24:preserved 31:25:preserved 32:36:preserved 33:27:preserved 34:28:preserved 35:29:preserved 36:30:preserved 37:31:preserved 38:32:preserved 39:33:preserved 40:34:preserved 41:35:preserved 42:42:preserved 43:37:preserved 44:46:preserved

The main cause is due to similar movements of actions in the same view direction . 
The main cause is the similar movements of the actions in the same view direction . 
290 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 5,4:4:paraphrase 6:5:preserved 7:6:preserved 8:7:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved :8:mogrammar-det

That is reason why we need compensating motion information from other views ( e.g. , side view and top view ) .
That is why we need compensating motion information from other views ( e.g. , the side view and/or top view ) .
291 0:0:preserved 1:1:preserved 2,3:2:paraphrase 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:15:preserved 16:16:preserved 17:17:paraphrase 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved :14:mogrammar-det

This section presents our experiments on action recognition using information from multiple views . 
This section presents our experiments on action recognition using information from multiple views . 
296 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Action representations in a depth sequence is fused by concatenating the feature vectors computed from corresponding views .
The action representations in a depth sequence are fused by concatenating the feature vectors computed from the corresponding views .
297 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:bigrammar-wform 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved :16:mogrammar-det

We report the experimental results on three action subsets and the average of the three subsets . 
We report the experimental results on three action subsets and the average of the three subsets . 
298 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Figure </Fig> shows a comparison between the intensity representations from front , side and top and their fused representation .
Figure </Fig> compares intensity representations from the front , side and top and their fused representation .
299 0:0:preserved 1:1:preserved 2,3,4,5:2:paraphrase 6:6:preserved 7:3:preserved 8:4:preserved 9:5:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved

The average recognition accuracy of the fusion , which is 96.67% accuracy is better than the individual intensity representations on the three action subsets . 
The average recognition accuracy of the fusion ( 96.67%) is better than those of the individual intensity representations on the three action subsets . 
300 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8,10,11,9:7,8,12,13:para-freeword 12:9:preserved 13:10:preserved 14:11:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved

These results demonstrate the effectiveness of leveraging depth information from multiple views .
These results demonstrate the effectiveness of leveraging depth information from multiple views .
301 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Results in figure </Fig> show the role of views to our approach . 
The results in Figure </Fig> show the role of views in our approach . 
305 0,2:0,1,3:para-colocation 1:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-prep 10:11:preserved 11:12:preserved 12:13:preserved

Experimental results confirm that action representations from front view achieve the best accuracy . 
They confirm that action representations from the front view achieve the best accuracy . 
306 0,1:0:para-colocation 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved :6:mogrammar-det

Obviously , the presentation from front view is an indispensable component for combination . 
Obviously , a front view representation is an indispensable component of a combination . 
307 0:0:preserved 1:1:preserved 2,3,4:2,5:para-freeword 5:3:preserved 6:4:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:bigrammar-prep 12:12:preserved 13:13:preserved :11:mogrammar-det

Therefore , for the rest , we perform experiments on view combinations with front view . We creaate additional combinations front and side , front and top . 
Therefore , in what follows , all view combinations that we describe will include a front view , i.e. , front and side or front and top . 
308 0:0:preserved 1:1:preserved 2,3,4,7,8,9,12,16,17,18,19,27:2,3,4,6,9,11,12,13,14,18,19:para-freeword 5:5:preserved 6:10:preserved 10:7:preserved 11:8:preserved 13:15:preserved 14:16:preserved 15:27:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:paraphrase 24:24:preserved 25:25:preserved 26:26:preserved :17:unaligned

Figure </Fig> shows the performance of the view combinations . 
Figure </Fig> shows the performance of the view combinations . 
309 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Interestingly , the achieved performance ( 96.95% ) from the combination of front and top beats the performance based on combining all the three views ( 96.67\% ) as well as the combination of front and side ( 93.94\% ) in terms of average accuracy . 
Interestingly , t the front and top combination ( 96.95\% ) beats combining all three views ( 96.67\% ) as well as front and side combination ( 93.94\% ) in terms of average accuracy . 
310 0:0:preserved 1:1:preserved 2:3:preserved 3,4,8,9,16,17,18,19,31,33:2:para-freeword 5:8:preserved 6:9:preserved 7:10:preserved 10:7:preserved 11::unaligned 12:4:preserved 13:5:preserved 14:6:preserved 15:11:preserved 20:12:preserved 21:13:preserved 22::mogrammar-det 23:14:preserved 24:15:preserved 25:16:preserved 26:17:preserved 27:18:preserved 28:19:preserved 29:20:preserved 30:21:preserved 32:25:preserved 34:22:preserved 35:23:preserved 36:24:preserved 37:26:preserved 38:27:preserved 39:28:preserved 40:29:preserved 41:30:preserved 42:31:preserved 43:32:preserved 44:33:preserved 45:34:preserved

In addition , based on experimental results presented in figure </Fig> indicates two interesting points .
In addition , the results in this figure </Fig> indicate two interesting points .
311 0:0:preserved 1:1:preserved 2:2:preserved 3,4,5:3:para-freeword 6:4:preserved 7,8:6,5:para-freeword 9:7:preserved 10:8:preserved 11:9:bigrammar-vtense 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved

Firstly , compensating information from various views can cause unexpected risks , due to erroneous information from certain views . 
Firstly , compensating information from various views can cause unexpected risks , due to erroneous information from certain views . 
312 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Indeed , consider two actions  : high arm wave and two hand wave
Indeed , consider two actions  : high arm wave and two hand wave . 
313 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved :13:bigrammar-others

although both contain ``wave arm'' movement , we easily recognize them from front and top views due to number of performed movements . 
Although both contain the ``wave arm'' movement , we can easily distinguish them from the number of performed movements apparent in the front and top views . 
314 0:0:typo 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:10:preserved 9:11:paraphrase 10:12:preserved 11:13:preserved 12:22:preserved 13:23:preserved 14:24:preserved 15:25:preserved 16,17:9,14,19,20,21:para-freeword 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:26:preserved :3:mogrammar-det

However , if we observe the two actions from side view , a half of body is hidden ( see figure </Fig> ) . 
However , if we observe the two actions from the side view , half of the body is hidden ( see Figure </Fig> ) . 
315 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:9,15:bigrammar-det 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:typo 21:22:preserved 22:23:preserved 23:24:preserved

Therefore we confuse movements performed on the two actions . In this case , merging information from side view into the combination of front and top view causes to decrease the performance of the recognition system .
In this case , it becomes easy to confuse movements of the two actions , and merging information from the side view into the front and top view combination actually causes the performance of the recognition system to decrease .
316 0,1,4,5:4,5,6,14,25,29,34,37:para-freeword 2:8:preserved 3:9:preserved 6:11:preserved 7:12:preserved 8:13:preserved 9::unaligned 10:0:preserved 11:1:preserved 12:2:preserved 13:3:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:19:preserved 21:28:preserved 22:10:preserved 23:24:preserved 24:15:preserved 25:26:preserved 26:27:preserved 27:30:preserved 28:7:preserved 29:38:preserved 30:23:preserved 31:32:preserved 32:33:preserved 33:31:preserved 34:35:preserved 35:36:preserved 36:39:preserved

Secondly , the experimental results have provided a good choice to decrease computational cost but still ensures a convincing performance . 
Secondly , the experimental results indicate a good way to decrease computational cost but still ensure convincing performance . 
317 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5:paraphrase 7:6:preserved 8:7:preserved 9:8:paraphrase 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:bigrammar-vtense 17::mogrammar-det 18:16:preserved 19:17:preserved 20:18:preserved

Looking at figure </Fig> , we can see that the performances of two combinations , i.e. , ( front & top ) and ( front & side & top ) , are comparable . 
Looking at figure </Fig> , we can see that the performances of two combinations , i.e. , ( front & top ) and ( front & side & top ) , are comparable . 
318 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved

In some cases , such as in action subsets 2 , 3 , and average , combination of front and top provide better performances . 
In some cases , such as in action subsets 2 , 3 , and average , the front and top combination has better performance . 
319 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16,17,18,19,20:20,16,17,18,19:para-colocation 21:21:paraphrase 22:22:preserved 23:23:bigrammar-wform 24:24:preserved

Obviously , if we eliminate unnecessary views , we can improve the efficiency of our system but still achieve competitive results .
Obviously , if we eliminate unnecessary views , we can improve the efficiency of our system but still achieve competitive results .
320 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

These interesting points confirm that information combination from multiple views is better than only from single views . 
These points confirm that information from multiple views is better than information from only single views . 
321 0:0:preserved 1,2:1:para-colocation 3:2:preserved 4:3:preserved 5,6:4:para-colocation 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12,13,14:10,11,12,13:para-freeword 15:14:preserved 16:15:preserved 17:16:preserved

In addition , the points can lead to looking for optimal solution of combining views . 
In addition , they suggest that there is an optimal way of combining views . 
322 0:0:preserved 1:1:preserved 2:2:preserved 3,4,5,6,7,8,9:3,4,5,6,7,8:para-freeword 10:9:preserved 11:10:paraphrase 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved

This is a promising challenge to overcome and build an effective and efficient recognition system .
These are promising results for building an effective and efficient recognition system .
323 0:0:bigrammar-nnum 1:1:bigrammar-wform 2::mogrammar-det 3:2:preserved 4,5,6,8,7:3,4,5:para-freeword 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved

Table </tab> shows evaluation results of our proposed approach compared to the state-of-the-art approaches on three action subsets of MSR Action 3D dataset ( seeing table </tab> ) . 
Table </tab> compares the results of our approach with those of the state-of-the-art approaches on three action subsets of the MSR Action 3D dataset ( see Table </tab> ) . 
328 0:0:preserved 1:1:preserved 2,3,7,9,10:2,8,9,3,10:para-freeword 4:4:preserved 5:5:preserved 6:6:preserved 8:7:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:bigrammar-wform 25:26:typo 26:27:preserved 27:28:preserved 28:29:preserved :19:mogrammar-det

The compared approaches use various feature representations , such as silhouette features </CITE> , skeletal joint features like </CITE> , local occupancy patterns </CITE> , normal orientation features </CITE> , and cuboid similarity features </CITE> . 
The compared approaches use various feature representations , such as silhouette features </CITE> , skeletal joint features like </CITE> , local occupancy patterns </CITE> , normal orientation features </CITE> , and cuboid similarity features </CITE> . 
329 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

Under the same setting ( i.e. cross subjects test ) , the result table </tab> indicates that our approach achieves the highest accuracy .
For the same setting ( i.e. , a cross subjects test ) , the table </tab> indicates that our approach achieves the highest accuracy .
330 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:para-freeword 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12,13:14:para-colocation 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved :7:mogrammar-det

The Gesture3D dataset </CITE> is a hand gesture dataset of depth sequences captured by a depth camera . 
The Gesture3D dataset </CITE> is a hand gesture dataset of depth sequences captured by a depth camera . 
336 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

This dataset contains a set of 12 gestures defined by American Sign Language ( ASL ) as shown in figure </Fig> . 
This dataset contains a set of 12 gestures defined by American Sign Language ( ASL ) ( see Figure </Fig> ) . 
337 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16,17,18,19:16,17,18,20:para-freeword 20:19:preserved 21:21:preserved

In this dataset , ten subjects perform each gesture two or three times . 
In this dataset , ten subjects performed each gesture two or three times . 
338 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

In total , the dataset contains 333 depth sequences . 
In total , the dataset contains 333 depth sequences . 
339 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The main challenge in this dataset is about self-occlusion . 
The main challenge here is self-occlusion . 
340 0:0:preserved 1:1:preserved 2:2:preserved 3,4,5,7:3:para-freeword 6:4:preserved 8:5:preserved 9:6:preserved

We follow the experimental settings in </CITE> ( i.e. , the leave-one-subject-out cross-validation ) to evaluate our approach . 
We used the experimental settings in </CITE> ( i.e. , the leave-one-subject-out cross-validation ) to evaluate our approach . 
341 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

The results are described in table </tab> , where our approach outperforms all previous approaches .
The results are in Table </tab> , from which it is clear that our approach outperformed the previous approaches .
342 0:0:preserved 1:1:preserved 2,3:2:paraphrase 4:3:preserved 5:4:typo 6:5:preserved 7:6:preserved 8:7,8,9,10,11,12:para-freeword 9:13:preserved 10:14:preserved 11:15:bigrammar-vtense 12:16:paraphrase 13:17:preserved 14:18:preserved 15:19:preserved

The 3D Action Pairs dataset </CITE> is a new type of action dataset . 
The 3D Action Pairs dataset </CITE> is a new type of action dataset . 
347 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The dataset contains pairs of actions , such that within each pair , the motion and the shape cues are similar , but their correlations vary . 
The dataset contains pairs of actions , such that within each pair , the motion and shape cues are similar , but their correlations vary . 
348 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16::mogrammar-det 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

It is useful to evaluate how well the approaches capture the prominent cues jointly in depth sequences . 
It is useful for evaluating how well the approaches capture the prominent cues jointly in depth sequences . 
349 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-prep 4:4:bigrammar-wform 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

There are six pairs of actions see figure </Fig> . 
There are six pairs of actions ( see figure </Fig> ) . 
350 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6,7:7,8:preserved 8:9:preserved 9:11,10,6:para-freeword

Each action is performed three times by ten subjects . 
Each action was performed three times by ten subjects . 
351 0:0:preserved 1:1:preserved 2:2:bigrammar-vtense 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Actions from the first five subjects are used for testing , and the rest for training .
Actions from the first five subjects were used for testing , the rest for training .
352 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved

We compare our performance with the HON4D approach </CITE> , which achieved the best performance on this dataset up to date . 
We compared our method with the HON4D approach </CITE> , which so far has had the best performance on this dataset . 
353 0:0:preserved 1:1:bigrammar-vtense 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,18,19,20:11,12,13,14:para-freeword 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 21:21:preserved

We summarize results in table </tab> , and show the confusion matrices in table </tab> . 
Table </tab> summarizes the results , and Table </tab> shows the confusion matrices . 
354 2,1,0,3,4,8,12,13:4,3,2,0,7,9:para-freeword 5:1:preserved 6:5:preserved 7:6:preserved 9:10:preserved 10:11:preserved 11:12:preserved 14:8:preserved 15:13:preserved

It is clear that our approach significantly outperforms the state-of-the-art approach for which suffered from confusion appeared within action pairs .
It is clear that our approach significantly outperformed the state-of-the-art approach , which suffered from confusion within the action pairs .
355 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-vtense 8:8:preserved 9:9:preserved 10:10:preserved 11,16:11:para-freeword 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 17:16:preserved 18:18:preserved 19:19:preserved 20:20:preserved :17:mogrammar-det

According to </CITE> , MBH is the best feature descriptor for dense trajectories on intensity videos .
According to </CITE> , MBH is the best feature descriptor for dense trajectories on intensity videos .
361 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Therefore , in previous experiments , we only use MBH descriptor to represent motion information . 
Therefore , in the previous experiments , we only used the MBH descriptor to represent the motion information . 
362 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:bigrammar-vtense 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:16:preserved 14:17:preserved 15:18:preserved :3:mogrammar-det :10,15:mogrammar-det

Due to the difference between depth data and intensity data , we conduct other experiments to investigate the impact of feature descriptors with replacing MBH with HOG and HOF .
Due to the difference between the depth data and intensity data , we conducted other experiments to investigate the impact of feature descriptors by replacing MBH with HOG and HOF .
363 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:bigrammar-vtense 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:bigrammar-prep 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved :5:mogrammar-det

We report the average recognition accuracies of the approach using different descriptors from three views ( i.e. front , side , and top ) in table </tab> . 
Table </tab> lists the average recognition accuracies of the approach using different descriptors from three views ( i.e. front , side , and top ) . 
364 0,1,24,25:0,2:para-freeword 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 26:1:preserved 27:25:preserved

Experimental results verify that the MBH descriptor is still the best trajectory-aligned descriptor in comparison with the HOG , HOF descriptors on the experimental datasets . 
The experimental results verify that the MBH descriptor is still the best trajectory-aligned descriptor on the experimental datasets . 
365 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13,14,15,17,18,19,20,25:18:para-freeword 16::unaligned 21:14:preserved 22:15:preserved 23:16:preserved 24:17:preserved

However , using HOG or HOF also gives competitive performances .
However , HOG and HOF do give competitive performance .
366 0:0:preserved 1:1:preserved 2,4,6,7:3,5,6:para-freeword 3:2:preserved 5:4:preserved 8:7:preserved 9:8:bigrammar-nnum 10:9:preserved

In addition , extracting HOG , HOF is less expensive than extracting MBH . 
In addition , extracting HOG or HOF is less expensive than extracting MBH . 
367 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

These advantages provide a promising way for building effective and efficient systems .
These advantages are promising for building effective and efficient systems .
368 0:0:preserved 1:1:preserved 2,4,5,3:2,3:para-freeword 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved

We proposed a novel approach to effectively exploit discriminative motion patterns for human action recognition using depth sequences in this work . 
We proposed a novel approach to effectively exploit discriminative motion patterns for human action recognition using depth sequences . 
373 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18,19,20,21:18:para-freeword

The motion patterns based on trajectories jointly encodes local motion and appearance cues .
The motion patterns based on trajectories jointly encode local motion and appearance cues .
374 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-vtense 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

In order to deal with confused actions due to similar movements , compensating information from different view directions is proposed . 
Compensating information from different view directions is used to deal with actions that can be confused due to their having similar movements . 
375 0,1,11:12,13,14:para-freeword 2:8:preserved 3:9:preserved 4:10:preserved 5:15:preserved 6:11:preserved 7:16:preserved 8:17:preserved 9,10:20,18,19,21:para-colocation 12:0:preserved 13:1:preserved 14:2:preserved 15:3:preserved 16:4:preserved 17:5:preserved 18:6:preserved 19:7:paraphrase 20:22:preserved

In addition , we also provide an analysis about the role of single views in merging information with aim to obtain the best combination . 
We conducted an analysis of the role of single views in merging information with the aim of obtaining the best combination of views . 
376 0,1,2,3,4:0,21,22:para-freeword 5:1:paraphrase 6:2:preserved 7:3:preserved 8:4:bigrammar-prep 9:5:preserved 10:6:preserved 11:7:preserved 12:8:preserved 13:9:preserved 14:10:preserved 15:11:preserved 16:12:preserved 17:13:preserved 18:15:preserved 19:16:bigrammar-prep 20:17:bigrammar-wform 21:18:preserved 22:19:preserved 23:20:preserved 24:23:preserved :14:mogrammar-det

We have evaluated our proposed approach extensively on three challenging benchmark datasets and shown that it significantly outperforms the state-of-the-art .
In addition , we extensively evaluated our approach on three challenging benchmark datasets and found that it significantly outperformed state-of-the-art methods .
377 0:3,0,1,2:para-freeword 1,2:5:bigrammar-vtense 4,3,5:6,7:para-colocation 6:4:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:paraphrase 14:15:preserved 15:16:preserved 16:17:preserved 17:18:bigrammar-vtense 18,19:19,20:para-colocation 20:21:preserved

Our motion pattern-based approach with compensating information from separate motion representations shows promising results . 
Our motion pattern-based approach with compensating information from separate motion representations shows promising results . 
378 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

This also suggests the importance of discriminative motion patterns for human action recognition on depth sequences . 
Our study also suggests the importance of discriminative motion patterns for recognizing human actions in depth sequences . 
379 0:0,1:para-colocation 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10,11,12:12,11,13:para-colocation 13:14:bigrammar-prep 14:15:preserved 15:16:preserved 16:17:preserved

Therefore , exploiting depth-based motion trajectories can be beneficial for action recognition systems using depth cameras . 
Therefore , depth-based motion trajectories can be beneficial for action recognition systems using depth cameras . 
380 0:0:preserved 1:1:preserved 2,3,4,5:2,3,4:para-colocation 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved

This is also an interesting idea for our future work .
This is an interesting idea for us to pursue in our future work .
381 0:0:preserved 1:1:preserved 2:6,7,8,9:paraphrase 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved

