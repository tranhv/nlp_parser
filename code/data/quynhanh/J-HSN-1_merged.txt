While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .
While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .
3 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,6,7,8:4,5,6:para-colocation 9,10:7,11:paraphrase 11:8:preserved 12:9:preserved 13:10:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:25:preserved 27:27:preserved 28:26:paraphrase 29:28:bigrammar-prep 30:29:preserved 31:30:preserved 32:31:preserved

In this paper , we investigate the effects of word segmentation methods on SMT , by comparing evaluation results of translation outputs while varying word segmentation methods .
In this paper , we investigate the effects of word segmentation methods on SMT , by comparing the evaluation results of the translation outputs , while varying word segmentation methods .
4 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:22:preserved 21:23:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved :17:mogrammar-det :21:mogrammar-det

Additionally , meta-evaluations of evaluation metrics are also provided to investigate validity of the metrics .
Additionally , meta-evaluations of evaluation metrics are also provided to investigate the validity of the metrics .
5 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved :11:mogrammar-det

The experiments revealed that supervised morphological analyzers were competitive , and considerably better than an unsupervised analyzer and a heuristic segmentation method .
The experimental results confirmed that supervised morphological analyzers were competitive with , and performed considerably better than an unsupervised analyzer and a heuristic segmentation method .
6 0:0:preserved 1:1,2:paraphrase 2:3:paraphrase 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:11:preserved 10:12,13:paraphrase 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved :10:mogrammar-prep

However , a character-based segmentation has achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on corpus sizes and domains .
However , a character-based segmentation achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on the corpus sizes and domains .
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5:bigrammar-vtense 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved :22:mogrammar-det

For this result we discuss the problem of the comparability of evaluation metrics and the possibility of better word segmentation than popular supervised morphological analyzers .
In conclusion , we discuss the problem of the comparability of evaluation metrics , and consider ways of improving word segmentation more than popular supervised morphological analyzers .
8 0,1,2:0,1:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14,15,16,17:15,16,17,18:paraphrase 18:19:preserved 19:20:preserved 20:22,21:paraphrase 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved

Several natural languages like Chinese and Japanese do not have to put spaces between words in their written forms .
Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .
12 0:0:preserved 1,2:1:paraphrase 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7,8:8,9:preserved 9,10,11:10:paraphrase 12:11:preserved 13:12:preserved 14:13:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

In order to process such languages , we need to tokenize each sentence .
In order to process such languages , we need to tokenize each sentence .
13 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

This process is called word segmentation .
This process is called word segmentation .
14 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Since the process is fundamental and indispensable , we need to explore how word segmentation affects Natural Language Processing applications .
Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .
15 0:0:preserved 1,2:1,2:paraphrase 3:3:preserved 4:5,4,6:paraphrase 5:8:preserved 6:11:preserved 7:7:preserved 8:17:preserved 9,10:13,14,15,16:paraphrase 11,12,13,14,15,16,17,18,19:18,19,20,21,22,23,24,25,26:preserved

Therefore , we investigate how Japanese word segmentation affects on SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .
Thus , we investigate how Japanese word segmentation affects SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .
18 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9::mogrammar-prep 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved

The word segmentation methods includes both standard Japanese morphological analyzers and several heuristic methods .
The word segmentation methods include both standard Japanese morphological analyzers and several heuristic methods .
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-inter 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

We also examine an unsupervised morphological analyzer and its results .
In addition , we examine an unsupervised morphological analyzer , and its results .
20 0:3:preserved 1:0,1:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved

In addition , we focus on the meta-evaluation of the current evaluation metrics and find whether the metrics are consistent or not , when we vary word segmentation methods .
We focus on the meta-evaluation of the current evaluation metrics and determine whether the metrics are consistent or not , when we vary word segmentation methods .
21 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:paraphrase 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved 25:22:preserved 26:23:preserved 27:24:preserved 28:25:preserved 29:26:preserved

Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .
Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on the outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .
26 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved :14:mogrammar-det

They acquired the 2 .3 score improvement from the worst to the best combinations .
They acquired a 2 .3 score improvement in comparing the worst to best combinations .
27 0:0:preserved 1:1:preserved 2:2:bigrammar-det 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:paraphrase 8:9:preserved 9:10:preserved 10:11:preserved 11::mogrammar-det 12:12:preserved 13:13:preserved 14:14:preserved

Wang et al.
Wang et al.
30 0:0:preserved 1:1:preserved 2:2:preserved

( 2010 ) suggested a new short unit word segmentation standard in Chinese which defines a more frequent string subset as a word .
( 2010 ) suggested a new short unit word segmentation standard in Chinese , which defines a more frequent string subset as a word .
31 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

For instance , They separated one word " 全球化 globalization " into two words " 全球 global " and " 化 -lization " .
For instance , one word “全球化 globalization” was separated into two words “全球 global” and “化 -lization” .
32 0:0:preserved 1:1:preserved 2:2:preserved 4:7,8:paraphrase 5:3:preserved 6:4:preserved 8,9:5,6:preserved 11:9:preserved 12:10:preserved 13:11:preserved 15,16,18,20,21:12,13,14,16,15:preserved

By this standard , they obtained 1 .0 BLEU score improvement within Chinese-Japanese SMT .
By this standard , they obtained 1 .0 BLEU score improvement within Chinese-Japanese SMT .
33 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .
However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .
36 0:0:paraphrase 1:1:preserved 2:2:paraphrase 3,4,5:3,4,6,7,5:paraphrase 6:8:paraphrase 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved

In addition , comparison of morphological analyzers are necessary because different analyzers produce different outputs to SMT .
In addition , comparing morphological analyzers is necessary , because different analyzers produce different outputs to SMT .
37 0:0:preserved 1:1:preserved 2:2:preserved 3,4:3:paraphrase 5:4:preserved 6:5:preserved 7:6:bigrammar-others 8:7:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Therefore , we conduct several translation tasks between English and Japanese .
We therefore conduct several translation tasks between English and Japanese .
38 0:1:preserved 2:0:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved

We measure the qualities of Japanese morphological analyzers and compare them with other word segmentation methods .
We measure the qualities of Japanese morphological analyzers and compare them with other word segmentation methods .
39 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

We also investigate consistencies of evaluation metrics by comparing results .
We also investigate consistencies of evaluation metrics by comparing the results .
40 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved :9:mogrammar-det

This work aims to empirically compare representative word segmentation methods in terms of SMT quality .
This work aims at empirically comparing representative word segmentation methods in terms of the SMT quality .
45 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-inter 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved :13:mogrammar-det

The following experiments are designed in order to answer these questions :
The following experiments are designed in order to answer these questions :
46 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affect SMT evaluation metrics , depending on corpus sizes and domains .
- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affects the SMT evaluation metrics , depending on the corpus sizes and domains .
49 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-inter 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved :21:mogrammar-det :28:mogrammar-det

- Whether or not SMT evaluation metrics provide a consistent measure while varying word segmentation methods .
- Whether or not SMT evaluation metrics provide a consistent measure , while varying word segmentation methods .
52 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved

We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .
We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .
55 0:0:preserved 1:1,2:spelling 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:15:preserved 13:16:preserved 14:17:bigrammar-det 15:18:preserved 16:19:preserved 17:21,22,23:paraphrase 18:24:paraphrase 19,20,21,22,23,24,25,26:25,26,27,28,29,30,31,32:preserved :14:mogrammar-det

As shown in Table 1 , the following word segmentation methods output delimiters ( " | " represents a delimiter ) for a given input character sequence .
As shown in Table 1 , the following word segmentation methods output delimiters ( “|” represents a delimiter ) for a given input character sequence .
59 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14,16,15:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved

The most popular method for Japanese word segmentation is to apply a morphological analyzer to obtain morpheme-based segmentation .
The most popular method for Japanese word segmentation is to apply a morphological analyzer to obtain morpheme-based segmentation .
63 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

It is , however , not clear which analyzer works better for the SMT task than the other analyzers .
It is ; however , unclear as to which analyzer works better for the SMT task .
64 0:0:preserved 1:1:preserved 2:4:preserved 3:3:preserved 5,6:5:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 19:16:preserved :6,7:mogrammar-prep

Therefore , we use four representative morphological analyzers and compare them :
Therefore , we use four representative morphological analyzers and compare them :
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

- KyTea predicts word segmentation delimiters by pointwise prediction ( Neubig et al. , 2011 ) , using linear Support Vector Machine or logistic regression .
- KyTea predicts word segmentation delimiters by pointwise prediction ( Neubig et al. , 2011 ) , using linear Support Vector Machine or logistic regression .
68 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

- MeCab regards word segmentation as a sequence labeling problem .
- MeCab regards word segmentation as a sequence labeling problem .
71 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

It uses Conditional Random Field for learning ( Kudo et al. , 2004 ) .
It uses Conditional Random Field for learning ( Kudo et al. , 2004 ) .
72 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

- JUMAN also regards word segmentation as a sequence labeling , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .
- JUMAN also regards word segmentation as a sequence labeling problem , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .
75 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9,10:paraphrase 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved

The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98\% for news text .
The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98% for news texts .
78 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-nnum 21:21:preserved

On the other hand , the unsupervised method latticelm achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news text , while the method does not have any answers of word definitions .
On the other hand , the unsupervised method , latticelm , achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news texts , while the method does not have any answers for word definitions .
79 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:bigrammar-nnum 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:bigrammar-prep 35:37:preserved 36:38:preserved 37:39:preserved

Therefore , it is not possible to compare such a result with the supervised results . Even though , it is fair to compare it with SMT contribution point of view .
Therefore , it is not possible to compare its result with the supervised results , even though it is fair to compare it from the SMT contribution point-of-view .
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9:8:paraphrase 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 16:15:preserved 17:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:bigrammar-prep 26:25:preserved 27:26:preserved 30,29,28:27:spelling 31:28:preserved :24:mogrammar-det

Furthermore , their policies about word segmentation definitions are very much different .
Furthermore , their policies concerning word segmentation definitions vary significantly .
83 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:paraphrase 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10,11:8,9:paraphrase 12:10:preserved

While MeCab can change its definitions by external lexicons and JUMAN has its own internal standard , KyTea is based on the short unit standard of Balanced Corpus of Contemporary Written Japanese , which is considered one of the shortest definitions of Japanese words .
While MeCab can change its definitions by external lexicons , and JUMAN has its own internal standard , KyTea is based on the short unit standard of the Balanced Corpus of Contemporary Written Japanese , which is considered to have one of the shortest definitions of Japanese words .
84 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37,38,39:paraphrase 36,37,38,39,40,41,42,43:40,41,42,43,44,45,46,47:preserved :27:mogrammar-det

For example , if we are given a string " 見れば( if someone see ) " , MeCab separates it into two words " 見れ | ば " and JUMAN keep the same string , but KyTea outputs it as three words " 見 | れ | ば " where every character is a word .
For example , if we are given a string , “見れば( if someone sees )” , MeCab separates it into two words , “見れ | ば” and JUMAN retains the same string , but KyTea outputs it as three words , “見 | れ | ば” where every character is a word .
85 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 10,9:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-inter 14,15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 24,23,26,27,25:23,25,24:preserved 28:26:preserved 29:27:preserved 30:28:paraphrase 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:preserved 37:35:preserved 38:36:preserved 39:37:preserved 40:38:preserved 41:39:preserved 43,42:41:preserved 44:42:preserved 45:43:preserved 46:44:preserved 47:45:preserved 49:46:preserved 50:47:preserved 51:48:preserved 52:49:preserved 53:50:preserved 54:51:preserved 55:52:preserved

In the case of latticelm , as it has no supervised definition of words , it uses the expectation maximized length of words for every word depending on training data .
For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .
86 0,2,1,3:0:paraphrase 4:1:preserved 5:2:preserved 6:3:paraphrase 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved 25:22:preserved 26:24:preserved 27:25:preserved 28:27:preserved 29:28:preserved 30:29:preserved :26:mogrammar-det

We also investigate such morphological analysis accuracy and word definition problems in our experiments .
In our experiments , we further investigate such morphological analysis accuracies and word definition problems .
89 0,2,3,4,5:4,6,7,8,9:preserved 1:5:paraphrase 6:10:bigrammar-nnum 7,8,9,10:11,12,13,14:preserved 11,12,13:0,1,2:preserved 14:15:preserved

Chang et al.
Chang et al.
94 0:0:preserved 1:1:preserved 2:2:preserved

( 2008 ) suggested that word segmentation consistency and granularity can be important factors for SMT .
( 2008 ) suggested that word segmentation consistency and granularity can be important factors for SMT .
95 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Therefore , we introduce two heuristic methods for Japanese word segmentation .
Therefore , we introduce two heuristic methods for Japanese word segmentation .
98 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

One is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .
One method is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .
101 0:1,0:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved

CAT puts a word boundary when character categories change .
CAT places a word boundary when character categories change .
102 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Character categories in Japanese include : Kanji ( Chinese character ) , Hiragana , Katakana , Latin alphabet , numeral digit , multi-byte alphabet , multi-byte digit , and other tokens .
Character categories in Japanese include : Kanji ( Chinese character ) , Hiragana , Katakana , Latin alphabet , numeral digit , multi-byte alphabet , multi-byte digit , and other tokens .
103 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

The CHAR method considers every Unicode character as a word as proposed by Xu et al.
The CHAR method considers every Unicode character as a word , as proposed by Xu et al.
104 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved

( 2004 ) .
( 2004 ) .
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

We use two news corpora : Reuters corpora ( REUTERS ) and Japanese-English News Article Alignment Data ( JENAAD ) ( Utiyama and Isahara , 2003 ) .
We use two news corpora : Reuters corpora ( REUTERS ) and Japanese-English News Article Alignment Data ( JENAAD ) ( Utiyama and Isahara , 2003 ) .
111 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

Another corpus we use in the experiments is a Wikipedia corpus , Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .
Another corpus we use in the experiments is a Wikipedia corpus : the Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :12:mogrammar-det

From these corpora , we prepared three data sets as explained below .
From these corpora , we prepared three data sets , as explained below .
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved

In the case of REUTERS , we have used all 56 ,282 sentences .
For REUTERS , we used all 56 ,282 sentences .
117 0,1,2,3:0:paraphrase 4:1:preserved 5:2:preserved 6:3:preserved 7,8:4:bigrammar-vtense 9,10,11,12:5,6,7,8:preserved

Then , we split the data into three parts : the first 1 ,000 as the test , the next 500 as the development , and the rest 55 ,282 as the training data .
Then , we split the data into three parts : the first 1 ,000 as the test , the next 500 as the development , and the remaining 55 ,282 as the training data .
118 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:paraphrase 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved

In this data , we have combined JENAAD and REUTERS news corpora to get one news corpus .
For this data , we combined the JENAAD and REUTERS news corpora to acquire one news corpus .
123 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:paraphrase 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

We have used all 56 ,282 and 150 ,000 sentences respectively .
We used all 56 ,282 and 150 ,000 sentences , respectively .
124 0:0:preserved 1,2:1:bigrammar-vtense 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:10:preserved 11:11:preserved

For each corpus , we divide it into the first 1 ,000 , the next 500 , and the rest for test , development , and training .
For each corpus , we divided the sentences into the first 1 ,000 for testing , the next 500 for development , and the remaining for training .
125 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-vtense 6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 20,21:19,13,14:paraphrase 22:21:preserved 23:20:preserved 25:22:preserved 26:26,23,24,25:paraphrase 27:27:preserved

We have gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively , in total .
In total , we gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively .
126 0:3,0,1,2:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 23:22:preserved

Firstly , since the WIKIPEDIA corpus is a multi-category XML dataset , we have sorted them by the DOCID in the ascending order and by the document categories LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .
Since the WIKIPEDIA corpus is a multi-category XML dataset , we sorted them by the DOCID in ascending order , and by the document categories : LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .
131 2:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13,14:11:bigrammar-vtense 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20::mogrammar-det 21,22:17,18:preserved 23:20:preserved 24:21:preserved 25:22:preserved 26:23:preserved 27:24:preserved 28:26:preserved 29:27:preserved 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:preserved 37:35:preserved 38:36:preserved 39:37:preserved 40:38:preserved 41:39:preserved 42:40:preserved 43:41:preserved 44:42:preserved 45:43:preserved 46:44:preserved 47:45:preserved 48:46:preserved 49:47:preserved 50:48:preserved 51:49:preserved 52:50:preserved 53:51:preserved 54:52:preserved 55:53:preserved 56:54:preserved

Secondly , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .
Next , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .
132 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Thirdly , sentence pairs that include a character " | " in English or Japanese are removed because it caused a problem with Moses .
Then , sentence pairs that include a character “|” in English or Japanese were removed , because it caused a problem with Moses .
133 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15,16:13,14:bigrammar-vtense 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved

Finally , we obtained 477 ,012 sentence pairs in total .
Finally , we obtained 477 ,012 sentence pairs in total .
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

In order to adjust the balance of the domains , we have sampled the data twice : First we extract the first line for every 477 lines .
In order to adjust the balance of the domains , we sampled the data twice : First , we extracted the first line for every 477 lines .
135 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12:11:bigrammar-vtense 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:18:preserved 19:19:bigrammar-vtense 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

After this , we have merged the remaining 476 ,012 lines and from this extract the first line for every 952 lines .
Then , we merged the remaining 476 ,012 lines , and from this extract , we extracted the first line for every 952 lines .
136 0,1:0:paraphrase 2:1:preserved 3:2:preserved 4,5:3:bigrammar-vtense 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:17,14,15,16:paraphrase 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved

Finally , we have obtained 1 ,000 test , 500 development , and 475 ,512 training data .
Finally , we obtained 1 ,000 test , 500 development , and 475 ,512 training data .
137 0:0:preserved 1:1:preserved 2:2:preserved 3,4:3:bigrammar-vtense 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved

We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .
We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .
143 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved 49:49:preserved 50:50:preserved 51:51:preserved 52:52:preserved 53:53:preserved 54:54:preserved 55:55:preserved

Currently , the most popular way to evaluate Statistical Machine Translation is to use word-based evaluation metrics such as BLEU and RIBES .
Currently , the most popular way to evaluate SMT is to use word-based evaluation metrics , such as BLEU and RIBES .
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10:8:para-freeword 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved

However , these word-based evaluation metrics have a problem on independency of word segmentation evaluations .
However , these word-based evaluation metrics have a problem on independency of word segmentation evaluations .
147 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

If we do not have segmented reference and test data , we cannot evaluate outputs by word-based evaluation metrics .
If we do not have segmented reference and test data , we cannot evaluate the outputs by word-based evaluation metrics .
150 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :14:mogrammar-det

For example , in the case of English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .
For example , for English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .
151 0:0:preserved 1:1:preserved 2:2:preserved 3,6,4,5:3:bigrammar-prep 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved

On the other hand , in the case of Japanese-English translations , we must tokenize test data to evaluate the outputs .
On the other hand , for Japanese-English translations , we must tokenize test data to evaluate the outputs .
152 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8:5:bigrammar-prep 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved

As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is hard to independently evaluate the effects of word segmentation on training data .
As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .
153 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:20,19:paraphrase 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved

It is also possible to detokenize SMT outputs first , and then tokenize them by the shared word segmentation .
It is also possible to detokenize SMT outputs first , and then tokenize them by the shared word segmentation .
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

However , our preliminary experiments showed that the results obtained with this method were not independent from word segmentation of training data .
However , our preliminary experiments indicated that the results obtained with this method were not independent from word segmentation of the training data .
157 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:21:preserved 21:22:preserved 22:23:preserved :20:mogrammar-det

And the best results were obtained when we use the same word segmentation as the training data .
The best results were obtained when we used the same word segmentation as the training data .
158 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:bigrammar-vtense 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved

Hence , this problem remains if we keep our word-based evaluations .
Hence , if we keep our word-based evaluations , this problem remains .
159 0:0:preserved 1:1:preserved 2,3,4:9,10,11:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:12:preserved

In order to manage such a problem , we use one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .
In order to manage this issue , we used one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .
162 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,6:5,4:paraphrase 7:6:preserved 8:7:preserved 9:8:bigrammar-vtense 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved

As this method evaluates the character-level information , outputs are not required to be segmented and it is free from word segmentation variations .
As this method evaluates the character-level information , outputs are not required to be segmented , and it is free from word segmentation variations .
163 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

We have conducted English and Japanese machine translation in both directions by the following steps :
We have conducted English and Japanese machine translation in both directions , following the steps below :
168 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11::mogrammar-prep 12:13:preserved 13:12:preserved 14:14:preserved 15:16,15:paraphrase

1Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .
1 .Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .
171 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved

2Run Japanese word segmentation methods and a normalization script which was introduced by the NTCIR-9 PATMT task .
2 .Run Japanese word segmentation methods and a normalization script , which was introduced by the NTCIR-9 PATMT task .
174 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved

3Tokenize and lowercase English text by Moses' tokenizer and lowercase scripts .
3 .Tokenize and lowercase English texts by Moses' tokenizer and lowercase scripts .
177 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:bigrammar-nnum 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

4Create language models from target languages' training data , with SRILM 1 .5 .12 .
4 .Create language models from target languages' training data , with SRILM 1 .5 .12 .
180 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

5Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .
5 .Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .
183 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

6Decode source test data with Moses ( 2010-08-13 ) .
6 .Decode source test data with Moses ( 2010-08-13 ) .
186 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved

7Compute evaluation scores of the outputs .
7 .Compute evaluation scores of the outputs .
189 0:1,0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved

We used Enju 2 .4 .2 ( Miyao and Tsujii , 2005 ) and Head Finalization ( Isozaki et al. , 2010b ) to preprocess English data .
We used Enju 2 .4 .2 ( Miyao and Tsujii , 2005 ) and Head Finalization ( Isozaki et al. , 2010b ) to preprocess English data .
193 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

This method enabled more accurate translations within English-Japanese translations than the conventional settings .
This method enabled more accurate translations within English-Japanese translations than with the conventional settings .
194 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :10:mogrammar-prep

We have applied the following Head Finalization rules from ( Su-doh et al. , 2011 ) :
We have applied the following Head Finalization rules from ( Su-doh et al. , 2011 ) :
195 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

- Reverse each phrase 's word orders when the phrase does not end with a head .
- Reverse each phrase 's word orders when the phrase does not end with a head .
198 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

- Exclude coordination from reversing
- Exclude coordination from reversing
201 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

- Convert plural nouns to singular forms
- Convert plural nouns to singular forms
204 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

- Remove articles " a " , " an " , and " the "
- Remove articles “a” , “an” , and “the”
207 0:0:preserved 1:1:preserved 2:2:preserved 4,3,5:3:preserved 6:4:preserved 7,8,9:5:preserved 11:7:preserved 12,13,14:8:preserved

- Insert pseudo-particles _va0 , _va1 , and _va2 .
- Insert pseudo-particles _va0 , _va1 , and _va2 .
210 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

For the pseudo-particles , we use the following insertion rules ( arg1 and arg2 are swapped when the head verb 's voice is passive ) :
For the pseudo-particles , we use the following insertion rules ( arg1 and arg2 are swapped when the head verb 's voice is passive ) :
213 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

- Add _va0 after the arg1 entry of the sentence head verb
- Add _va0 after the arg1 entry of the sentence head verb
216 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

- Add _va1 after arg1 entries of other verbs
- Add _va1 after arg1 entries of other verbs
219 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

- Add _va2 after arg2 entries of all verbs
- Add _va2 after arg2 entries of all verbs
222 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Table 2 and Table 3 show the English-Japanese and Japanese-English evaluation results .
Table 2 and Table 3 show the English-Japanese and Japanese-English evaluation results .
229 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The best scores in each evaluation metrics are highlighted for each data set .
The best scores in each evaluation metrics are highlighted for each data set .
230 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

All evaluation metrics have been used in both directions between English and Japanese , to measure consistency and sufficiency of the metrics in the language pair .
All evaluation metrics have been used in both directions between English and Japanese , to measure the consistency and sufficiency of the metrics in the language pair .
232 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved :16:mogrammar-det

In this case , the evaluation scores created by BLEU and RIBES are not comparative due to the differences of Japanese word definitions between the outputs of word segmentation methods .
In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .
236 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:bigrammar-prep 20:22:preserved 21:23:preserved 22:24:preserved 23:25:bigrammar-prep 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved :21:mogrammar-det

Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost same while small changes have been introduced due to statistical errors and the differences in the methods how to treat space characters .
Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost the same , while small changes have been introduced , due to statistical errors and the differences in the methods in how to treat space characters .
237 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:17:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved 34,35,33,36,37:37,38,39,40,41:preserved :16:mogrammar-det :36:mogrammar-prep

We found that the three supervised morphological analyzers KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .
We found that the three supervised morphological analyzers : KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .
240 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved

For instance , on REUTERS in Table 2 , BLEU scores were ranged from 27 .88 to 29 .53 , while latticelm was 15 .28 and CAT was 22 .10 .
For instance , on REUTERS in Table 2 , BLEU scores ranged from 27 .88 to 29 .53 , while for latticelm , the score was 15 .28 and for CAT , the score was 22 .10 .
241 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12:11:paraphrase 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:21:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:30:preserved 27,28,29:34,35,36:preserved :20:mogrammar-prep :29:mogrammar-prep

The unsupervised morphological analyzer latticelm and one of heuristic methods CAT were worse than our expectations .
The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .
244 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:13:preserved 11:15:paraphrase 12,13:16,17:preserved 14::mogrammar-det 15:18:preserved 16:19:preserved

These two were the worst or the second worst results in all settings .
These two results were the worst , in all of the settings .
245 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 8,6::unaligned 9:2:preserved 10:7:preserved 11:8:preserved 12:11:preserved 13:12:preserved :9:mogrammar-prep :10:mogrammar-det

The results of CHAR were counterintuitive and yet to be discussed .
The results of CHAR were counterintuitive and are yet to be discussed .
248 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

It was relatively much better than the supervised morphological analyzers in BLEU .
The results were better than the results for the supervised morphological analyzers in BLEU .
249 0,1:0,1,2:paraphrase 4:3:preserved 5:4:preserved 6:8:bigrammar-det 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved :7:mogrammar-prep

Besides , it was almost competitive in RIBES and BLUE in Characters .
It was almost competitive in RIBES and BLUE in Characters .
252 2:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved

For example , CHAR achieved the best 38 .42 score in BLEU on REUTERS , but the second best KyTea was 29 .53 .
CHAR achieved the best score in BLEU on REUTERS ( 38 .42 ) , but the second-best was KyTea ( 29 .53 ) .
253 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:10:preserved 8:11:preserved 11,10,12,13,9:5,6,7,8,4:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17,18:16:spelling 19:18:preserved 20:17:preserved 21:20:preserved 22:21:preserved 23:23:preserved

In the case of BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .
For BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .
254 1,2,3,0:0:paraphrase 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved 25:22:preserved

In this case , the evaluation scores are lower than English-Japanese translations in general .
For Japanese-English translations , the evaluation scores were generally lower than for English-Japanese translations .
259 0,1,2:0,1,2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-vtense 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12,13:8:paraphrase 14:14:preserved :11:mogrammar-prep

It is because Japanese-English translations are conducted without Head-Finalization .
This is because Japanese-English translations are conducted without Head-Finalization .
260 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Again , the supervised morphological analyzers KyTea , MeCab , and JUMAN were competitive .
Again , the supervised morphological analyzers KyTea , MeCab , and JUMAN were competitive .
263 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

All supervised analyzers were better than the unsupervised and the both heuristic methods .
All supervised analyzers performed better than the unsupervised and the both heuristic methods .
264 0:0:preserved 1:1:preserved 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

On the other hand , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT were competitive to the supervised analyzers in RIBES .
Conversely , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT performed competitively with the supervised analyzers in RIBES .
267 0,1,3,2:0:paraphrase 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16,17,18:13,14,15:paraphrase 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved

For example , latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .
latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .
268 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved

In this case , CHAR was not competitive to the supervised analyzers in total .
In this case , CHAR was not competitive to the supervised analyzers in total .
271 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

The results were the worst scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS . The only one exception was in the case of the best 56 .55 BLEU in Characters on REUTERS .
The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .
272 0:0:preserved 1:1:preserved 2:4:preserved 3:5:preserved 4:6:paraphrase 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 15,18:18,19:preserved 21::unaligned 23:20:preserved 24:21:preserved 25:22:preserved 26:23:preserved 27:24:preserved 28:25:preserved 29:26:preserved 30:27:preserved 31:28:preserved 32:29:preserved 33:30:preserved :17:mogrammar-prep

We found the results of the supervised morphological analyzers are better in both English-Japanese and Japanese-English experiments .
We found that the results of the supervised morphological analyzers were better in both English-Japanese and Japanese-English experiments .
277 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-vtense 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved

And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .
Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .
278 0:0:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:paraphrase 17:18:preserved 18:19:preserved 19:20:bigrammar-prep 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved :34:mogrammar-prep

This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions in most cases .
This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions , in most cases .
279 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT were very much worse than the supervised morphological analyzers .
On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT performed much poorer than the supervised morphological analyzers .
282 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17,18,20:17,19:paraphrase 19:18:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

The experiments demonstrated an unexpected result for CHAR .
The experiments demonstrated an unexpected result for CHAR .
285 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

It was good at English-Japanese but not at Japanese-English translations .
It excelled with English-Japanese translations , but not with Japanese-English translations .
286 0:0:preserved 1,2,3,7:1,2,8:paraphrase 4:3:preserved 5:6:preserved 6:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved

We consider the possible reasons for this result :
We consider the possible reasons for this result in the following list :
287 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

- The Head Finalization of English-Japanese lead better phrase alignments .
- The Head Finalization of English-Japanese lead better phrase alignments .
290 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

- Since CHAR treat a character as a word , the best combination of its phrase alignments were the best suited for the SMT decoding .
- Since CHAR treats a character as a word , the best combination of its phrase alignments were the best suited for the SMT decoding .
293 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-inter 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

On the other hand , we observed the following issues from our error analysis :
On the other hand , we observed the following issues from our error analysis :
296 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

- Uncommon named entities were almost wrongly translated .
- Uncommon named entities were almost wrongly translated .
299 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

( For example , チェコ Czech was produced instead of チェコスロバキア Czechoslovakia . )
( For example , チェコ Czech was produced instead of チェコスロバキア Czechoslovakia . )
300 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

- Long sentences were translated worse than the other word segmentation outputs .
- Long sentences were not translated as well as other word segmentation outputs .
303 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5,6:4,6,7,8:paraphrase 7::mogrammar-det 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved

The reasons of the CHAR results are yet to be analyzed in details .
The reasons for the CHAR results are yet to be analyzed in detail .
306 0:0:preserved 1:1:preserved 2:2:bigrammar-prep 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-nnum 13:13:preserved

However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .
However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .
307 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

We are planning to conduct further investigation in future .
We are planning to conduct further investigations in the future .
308 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-nnum 7:7:preserved 8:9:preserved 9:10:preserved :8:mogrammar-det

The current evaluation metrics we pursued in this paper were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation , since they did not produce consistent scores as explained below :
The current evaluation metrics that we pursued in this paper were insufficient to discuss the relative advantages and disadvantages of word segmentation in detail , since the scores that were produced were inconsistent , as explained below :
314 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10,11:11:paraphrase 12:12:preserved 13:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:preserved 23:20:preserved 24:21:preserved 25:24:preserved 26:25:preserved 27,28,29,30,31,32:32,31,30,29,28,27,26:paraphrase 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved

- There were many contradictory figures among evaluation metrics .
- There were many contradictory figures among evaluation metrics .
317 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

There was a case that BLEU was high , while other metrics were low .
There was a case that BLEU was high , while other metrics were low .
318 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Moreover , there is also a case that RIBES and BLEU in Characters were incompatible with each other .
Moreover , there is also a case in which RIBES and BLEU in Characters were incompatible with each other .
319 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:paraphrase 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved

For example , on WIKIPEDIA in Table 2 , while CHAR was relatively the highest and greatly better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .
For example , on WIKIPEDIA in Table 2 , while CHAR was the highest , and performed better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .
320 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 13:12:preserved 14:13:preserved 15:15:preserved 16:16:paraphrase 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved

- If we compare every column in a row , there were tendencies that the best and the worst corpora were the same for every evaluation metrics .
- If we compare every column in a row , there were tendencies that the best and the worst corpora were the same for every evaluation metrics .
323 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved

In Table 2 , REUTERS was the best and WIKIPEDIA was the worst in terms of BLEU , but also JENAAD+REUTERS was the best and WIKIPEDIA was the worst in terms of RIBES .
In Table 2 , REUTERS was the best and WIKIPEDIA was the worst in terms of BLEU , but also JENAAD+REUTERS was the best and WIKIPEDIA was the worst in terms of RIBES .
324 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved

- Even when we compare every row in a column , there were no tendencies .
- Even when we compare every row in a column , there were no tendencies .
327 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

For instance , in terms of BLEU in Characters , CHAR , JUMAN , and MeCab achieved the best scores in Table 3 .
For instance , in terms of BLEU in Characters , CHAR , JUMAN , and MeCab achieved the best scores in Table 3 .
328 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved

This work focused on how the difference of word segmentation affects SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
334 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-nnum 7:7:bigrammar-prep 8:8:preserved 9:9:preserved 10:10:bigrammar-vtense 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

In summary , we found that the representative morphological analyzers were competitive and much better than both unsupervised analyzer and one of our heuristic methods .
In summary , we found that the representative morphological analyzers were competitive and much better than both the unsupervised analyzer and one of our heuristic methods .
337 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :17:mogrammar-det

After all , a heuristic word segmentation method CHAR achieved relatively good word-based BLEU scores and competitive character-based BLEU results , compared to the supervised analyzers .
Nevertheless , a heuristic word segmentation method CHAR achieved relatively good word-based BLEU scores and competitive character-based BLEU results , compared to the supervised analyzers .
338 0,1:0:paraphrase 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

Additionally , as we could not always obtain consistent scores from the current evaluation metrics , they were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation .
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , the data was insufficient for discussing the relative advantages and disadvantages of word segmentation , with accuracy .
339 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16,17:para-freeword 17:18:bigrammar-inter 18,19:19:paraphrase 20,21:20,21:paraphrase 23:31,32:paraphrase 24::mogrammar-prep 25:22:preserved 26:23:preserved 27:24:preserved 28:25:preserved 29:26:preserved 30:27:preserved 31:28:preserved 32:29:preserved 33:33:preserved

We also suggested it is possible to implement more optimized word segmentation on SMT .
We have also suggested that it is possible to implement more optimized word segmentation on SMT .
340 0:0:preserved 1:2:preserved 2:1,3,4:bigrammar-vtense 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved

