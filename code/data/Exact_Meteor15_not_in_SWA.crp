0
This paper analyzes the effects of structural variation of sentences on parsing performances .
This paper analyzes the effect of the structural variation of sentences on parsing performance .
1
The target parsers are adapted to the sentences for these constructions extracted from fiction and query texts .
The target parsers are adapted to sentences of these constructions extracted from fiction and query texts .
2
Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
3
However , there are some cases where we cannot achieve as high parsing accuracies as parsing the Penn Treebank just by re-training or adaptation .
However , there are some cases where we cannot achieve such high parsing accuracy as parsing the Penn Treebank ( PTB ) merely by re-training or adaptation .
4
For example , the parsing accuracy for the Brown corpus is significantly lower than for the WSJ portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .
For example , the parsing accuracy for the Brown corpus is significantly lower than that for the Wall Street Journal ( WSJ ) portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .
5
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracies of a part-of-speech tagger for them .
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracy of their part-of-speech ( POS ) tagger .
6
Their main focus was on adapting parsing models trained with a specific genre of text ( in most cases Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
The main focus of these works is on adapting parsing models trained with a specific genre of text ( in most cases the Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
7
They collected question sentences from TREC 9-12 competitions , and annotated these sentences with POSs and CCG lexical categories .
In this work , question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories .
8
Although the publicly available implementation of each parser also has an option to restrict the output to be a projective dependency tree , we used the non-projective version because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .
Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree , we used the non-projective versions because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .
9
For the evaluation of the output from each of the MST and Malt parser , we used the labeled attachment accuracy excluding the punctuations .
To evaluate the output from each of the parsers , we used the labeled attachment accuracy excluding punctuation .
10
The Enju parser \CITE is a deep parser based on the HPSG formalism .
The Enju parser \CITE is a deep parser based on the HPSG ( Head Driven Phrase Structure Grammar ) formalism .
11
This section explains how we collected the treebanks of imperatives and questions , which were used in the experiments in Section \REF .
This section explains how we collected the treebanks of imperatives and questions used in the experiments in Section \REF .
12
While the Wall Street Journal ( WSJ ) treebank has extensively been used for parsing experiments , we use the treebank of the Brown Corpus in our experiments .
Although the WSJ treebank has been used extensively for parsing experiments , we used the treebank of the Brown Corpus in our experiments .
13
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " represents wh-questions , while " SQ " denotes yes / no questions .
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " denotes wh-questions , while " SQ " denotes yes / no questions .
14
We extracted those sentences annotated with these phrase labels .
All sentences annotated with these phrase labels were extracted .
15
Imperatives and questions appear not only at the top level but also appear as embedded clauses .
Imperatives and questions appear not only at the top level but also as embedded clauses .
16
Extracted sentences are post-processed so that they have natural sentence forms : first characters are capitalized , and question marks or periods are added when appropriate .
Extracted sentences were post-processed to fit the natural sentence form ; that is , with first characters capitalized and question marks or periods added as appropriate .
17
We will not use this data in the experiments .
This data was not used in the experiments .
18
As we will describe below , we additionally use QuestionBank in experiments .
As described below , we also used QuestionBank in the experiments .
19
However , an advantage of using the Brown treebank is that it includes annotations of function tags and empty categories . Therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \CITE , which relies on function tags and empty categories .
The advantage , however , of using the Brown treebank is that it includes annotations of function tags and empty categories , and therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \CITE , which relies on function tags and empty categories .
20
Hence , we will show experimental results on Enju only with the Brown data .
Hence , we show experimental results for Enju only with the Brown data .
21
We extracted 3 ,859 sentences that are annotated with " SBARQ " or " SQ " .
We extracted 3 ,859 sentences annotated with " SBARQ " or " SQ " .
22
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged not with " . " but with " ? " ( 2 ,051 sentences ) , and phrase labels annotated as POS ( one sentence ) .
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged with " ? " instead of " . " ( 2 ,051 sentences ) , and phrase labels annotated as the POS ( one sentence ) .
23
By observing the effects of parser or tagger adaptation to each domain , we would like to see the difficulties in parsing imperative and question sentences .
By observing the effect of the parser or tagger adaptation in each domain , we can identify the difficulties in parsing imperative and question sentences .
24
We also examined the portability of sentence construction properties between two similar domains : questions in Brown and QuestionBank .
We also examined the portability of sentence construction properties between two similar domains : questions in Brown and in QuestionBank .
25
We made experimental datasets for five domains : Wall Street Journal ( WSJ ) , Brown overall sentences , Brown imperatives , Brown questions , and QuestionBank questions .
We created experimental datasets for five domains : WSJ , Brown overall , Brown imperatives , Brown questions , and QuestionBank questions .
26
- Divided into three parts for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .
- Divided into three parts , for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .
27
In order to adapt each parser or POS tagger to a target domain , we trained the parser on combined training data for the target domain and for the original parser .
To adapt each parser and the POS tagger to a target domain , we trained the parser using combined training data for the target domain and the original parser .
28
For a domain which contains only small training data , we replicated the training data for certain times and just utilized the concatenated replicas for training .
For a domain containing only a small amount of training data , we replicated the training data a certain number of times and utilized the concatenated replicas for training .
29
- For Brown overall , we trained the model with the combined training data for the target domain and for the original model .
- For Brown overall , we trained the model with the combined training data for the target domain and the original model .
30
For Brown imperatives / questions and QuestionBank , we replicated the training data for certain times and utilized the concatenated replicas and WSJ training data for training .
For Brown imperatives / questions and QuestionBank , we replicated the training data a certain number of times and utilized the concatenated replicas and WSJ training data for training .
31
For POS tagger , the number of replicas of training data was determined among 1 , 2 , 4 , 8 , 16 , 32 , 64 , and 128 , by testing these numbers on development test sets in three of ten datasets of cross validation .
For the POS tagger , the number of replicas of training data was determined as either 1 , 2 , 4 , 8 , 16 , 32 , 64 , or 128 , by testing these numbers on the development test sets in three of the ten datasets for cross validation .
32
- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and for the original model .
- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and the original model .
33
The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .
The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .
34
This graph shows that for both types of sentences , first 300 training sentences improved the accuracy rapidly , and after that , the effect of adding training corpus declined .
This graph shows that for both types of sentences , the first 300 training sentences greatly improved the accuracy , but thereafter , the effect of adding training data declined .
35
In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\% in Table \REF ) , it would not seem to be enough only to prepare much more training data .
To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\% in Table \REF ) , preparing much more training data does not appear to be enough .
36
We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .
Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .
37
In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , " VB \SPEC " .
From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , " VB \SPEC " .
38
These two types of errors would respectively come from the following differences in sentence constructions between WSJ declarative and the Brown imperative sentences .
These two types of errors arise from the following differences in sentence constructions between the WSJ declarative and Brown imperative sentences .
39
Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .
First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .
40
The WSJ tagger was trained on the domain mainly consisting of declarative sentences , and the training was based on N-gram sequences of words or POSs . The tagger therefore preferred to give noun phrase-derived tags to the beginning of a sentence .
Since The WSJ tagger was trained on a domain consisting mainly of declarative sentences , with the training based on N-gram sequences of words or POSs , preference was given to noun phrase-derived tags at the beginning of a sentence .
41
Secondly , main verbs in imperative sentences take base forms while main verbs in declarative sentences take the forms according to tense .
Second , the main verb in an imperative sentence takes a base form , whereas the main verb in a declarative sentence takes a form based on tense .
42
The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .
A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .
43
The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .
Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .
44
After adapting the tagger to Brown imperatives , the N-gram model of tagger would have learned that the first word in a sentence tends to be a verb , and the main verb tends to take base form ( VB ) .
After adapting the tagger to Brown imperatives , the N-gram model of the tagger would have learned that the first word in a sentence tends to be a verb , and that the main verb tends to take the base form ( VB ) .
45
Table \REF shows that the above two types of errors did decrease to some extent . However , we can also observe that not a few mis-tags to verbs were still left after the adaptation .
Table \REF shows that after adaptation the above two types of errors decreased to some extent , although a few mistags of verbs still remained .
46
When we observe each of the left errors around VB , we found that several errors still occurred even in simple imperative sentences such as " VB \SPEC NN " for " Charge " in " Charge something for it . " , and that some errors tended to occur after to-infinitive phrase or conjunction , such as " VB \SPEC NN " for " subtract " in " To find estimated net farm income , subtract . . . "
By investigating the remaining errors associated with VB , we found that several errors still occurred even in simple imperative sentences such as " VB \SPEC NN " for " Charge " in " Charge something for it " , and that some errors tended to occur after a to-infinitive phrase or conjunction , such as " VB \SPEC NN " for " subtract " in " To find the estimated net farm income , subtract . . . " .
47
The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .
The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .
48
We also analyzed the errors in Brown questions and QuestionBank , and again found that the WSJ tagger seems to make many errors due to the fact that the tagger was trained on a corpus mainly consisting of declarative sentences .
We also analyzed the errors in Brown questions and QuestionBank , and again found that many errors were due to the fact that the WSJ tagger was trained on a corpus consisting mainly of declarative sentences .
49
After the adaptation , while some of the errors such as special usage of wh-words , i.e. , " WDT \SPEC WP " , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .
After the adaptation , although some of the errors such as the special use of wh-words , i.e. , " WDT \SPEC WP " , were corrected , other kinds or errors related to the global change in sentence structure still remained .
50
In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
51
Table \REF shows the parsing accuracies of MST( first order ) , MST( second order ) , Malt , and Enju parser for WSJ , Brown overall , Brown imperatives and Brown questions .
Table \REF gives the parsing accuracy of MST ( first order ) , MST ( second order ) , Malt , and the Enju parser for WSJ , Brown overall , Brown imperatives , and Brown questions .
52
Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
53
When we adapted the parser model ( see fifth column in Table \REF ) , the parser could give two to four points higher accuracies for each of the Brown domains than the WSJ parser .
After adaptation ( see " Adapted " column in Table \REF ) , the parser achieved two to four percent higher accuracy for each of the Brown domains compared to the WSJ parser .
54
For the QuestionBank , 25 to 35 points accuracy improvements were observed .
For QuestionBank , 25 to 35 percent improvement in accuracy was observed .
55
This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .
This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .
56
In Figure \REF , when we focus on the QuestionBank where we could use much more training data than Brown questions , the parser accuracies were approaching the accuracies of WSJ parser for WSJ or exceeded the accuracy .
In Figure \REF , the parser accuracy for QuestionBank , for which we could use much more training data than for Brown questions , approaches or even exceeds that of the WSJ parser for WSJ .
57
However , we have no more training data for Brown imperatives and questions . We should prepare more training data or explore approaches to enable us to sufficiently adapt parsers with small training data .
However , as there is no more training data for Brown imperatives and questions , we need to either prepare more training data or explore approaches that enable the parsers to be adapted with small amounts of training data .
58
In order to capture the outline of the adaptation effects , we observed error reduction for the Malt parser .
To capture an overview of the adaptation effects , we observed the error reduction in the Malt parser .
59
Since ROOT dependencies , that is , heads of sentences would be critical to construction of sentences , we mainly focus on that type of errors .
Since ROOT dependencies , that is , heads of sentences , are critical to the construction of sentences , we focus mainly on this type of error .
60
For Brown imperatives and questions , we could observe that the reduction of ROOT dependency was prominent .
For Brown imperatives and questions , the reduction in ROOT dependencies was prominent .
61
When we focus on this type of errors , we could find that the WSJ parser could often make mistakes in parsing sentences which began or ended with the names of persons who were talk to .
On investigation , we found that the WSJ parser often made mistakes in parsing sentences which began or ended with the name of the person being addressed .
62
For example in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser regarded the person name " Zion " as ROOT , and the main verb " See " as modifiers of the name .
For example , in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser mistook the name " Zion " to be ROOT , and the main verb " See " to be a modifier of the name .
63
We could also often find that the WSJ parser could often make mistakes in parsing sentences containing quotation , exclamation , and question marks , such as " " Hang on " !! " " or " Why did you kill it " ? ? " or " " " " .
We also found that the WSJ parser often made mistakes in parsing sentences containing quotation , exclamation , or question marks , such as " " Hang on " !! " " or " Why did you kill it " ? ? " or " " " " .
64
We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .
A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .
65
On the other hand , we also observed some still unsolved errors . We would show the two kinds of major errors among them .
On the other hand , we also observed some unsolved errors , of which we discuss two .
66
First , Brown imperatives and questions , include many conversation sentences , and therefore rather flexible constructions could be observed especially for imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
First , Brown imperatives and questions , include many colloquial sentences , which have rather flexible constructions , especially imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
67
Second , when the different constructions of sentences were in one sentence , such as , the case where to-infinitive phrases or subordinate clauses precede imperatives and questions , the parser would often be confused .
Second , having different sentence constructions within a single sentence , such as , where a to-infinitive phrase or subordinate clause precedes an imperative or question , often confused the parser .
68
For example , for the imperative sentence " To find estimated net farm income , subtract estimated annual farming expenditures . . . " , both of the WSJ and adapted parsers regarded " find " as ROOT , because the parsers regarded the words following " find " as a that-clause complement for the " find " , like " To find [ ( that ) estimated net farm income , subtract estimated annual farming . . .] " .
For example , for the imperative sentence , " To find the estimated net farm income , subtract the estimated annual farming expenditure . . . " , both the WSJ and adapted parsers regarded " find " as ROOT , because the parsers regarded the words following " find " as a that-clause complementing " find " , as in " To find [ ( that ) the estimated net farm income , subtract the estimated annual farming . . .] " .
69
The parser would parse such complex sentences without partition into each construction , and therefore it would sometimes be confused .
These complex sentences were parsed without being partitioned into separate constructions , and as a result the parser sometimes became confused .
70
Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .
As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .
71
However , we would be able to expect that sentence constructions would be basically common and portable between two domains , which would contribute to give correct boundary for phrases and therefore the correct dependencies in phrases would be introduced by the adaptation .
However , we would expect that sentence constructions are basically common and portable between two domains , which would provide a correct boundary for phrases and therefore , the correct dependencies in phrases would be introduced by the adaptation .
72
the difference from Table \REF was that the parsers and the tagger were adapted to another question domain .
These results differ from those in Table \REF in that the parsers and the tagger have been adapted to another question domain .
73
The table shows that the parsers adapted to Brown questions improved the parsing accuracies for QuestionBank , while the parsers adapted to QuestionBank decreased .
The table shows that the parsers adapted to the Brown questions improved their parsing accuracy with QuestionBank , whereas the parsers adapted to QuestionBank decreased in accuracy .
74
With Brown questions , we could learn wh-questions which QuestionBank mainly contain , while with QuestionBank , we could not we could not learn yes-no questions which more than half of Brown corpus contain .
Using Brown questions , many wh-questions were learnt , which is what QuestionBank mainly contains . On the other hand , despite yes-no questions constituting more than half the Brown corpus , these were not learnt using QuestionBank for training .
75
This type of problem would not be noticed so much when we were working mainly on declarative sentences .
This type of problem was not so obvious when we were working mainly with declarative sentences .
76
This observation holds both for POS tagging and syntactic parsing , and itindicates that we need fundamental improvement of parsers , such as re-constructing feature designs or changing parsing models .
This observation holds both for POS tagging and syntactic parsing , and indicates that the parsers need to be fundamentally improved , such as re-constructing feature designs or changing parsing models .
77
Following the present work , future work should include investigating parsing frameworks that are robust for sentences with various sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives , questions , and more .
Following on from this study , future work includes investigating parsing frameworks that are robust for sentences with different sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives and questions , among others .
78
While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .
While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .
79
However , a character-based segmentation has achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on corpus sizes and domains .
However , a character-based segmentation achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on the corpus sizes and domains .
80
For this result we discuss the problem of the comparability of evaluation metrics and the possibility of better word segmentation than popular supervised morphological analyzers .
In conclusion , we discuss the problem of the comparability of evaluation metrics , and consider ways of improving word segmentation more than popular supervised morphological analyzers .
81
Several natural languages like Chinese and Japanese do not have to put spaces between words in their written forms .
Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .
82
Since the process is fundamental and indispensable , we need to explore how word segmentation affects Natural Language Processing applications .
Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .
83
For instance , They separated one word " 全球化 globalization " into two words " 全球 global " and " 化 -lization " .
For instance , one word “全球化 globalization” was separated into two words “全球 global” and “化 -lization” .
84
Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .
However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .
85
We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .
We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .
86
It is , however , not clear which analyzer works better for the SMT task than the other analyzers .
It is ; however , unclear as to which analyzer works better for the SMT task .
87
- JUMAN also regards word segmentation as a sequence labeling , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .
- JUMAN also regards word segmentation as a sequence labeling problem , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .
88
On the other hand , the unsupervised method latticelm achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news text , while the method does not have any answers of word definitions .
On the other hand , the unsupervised method , latticelm , achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news texts , while the method does not have any answers for word definitions .
89
Therefore , it is not possible to compare such a result with the supervised results . Even though , it is fair to compare it with SMT contribution point of view .
Therefore , it is not possible to compare its result with the supervised results , even though it is fair to compare it from the SMT contribution point-of-view .
90
While MeCab can change its definitions by external lexicons and JUMAN has its own internal standard , KyTea is based on the short unit standard of Balanced Corpus of Contemporary Written Japanese , which is considered one of the shortest definitions of Japanese words .
While MeCab can change its definitions by external lexicons , and JUMAN has its own internal standard , KyTea is based on the short unit standard of the Balanced Corpus of Contemporary Written Japanese , which is considered to have one of the shortest definitions of Japanese words .
91
For example , if we are given a string " 見れば( if someone see ) " , MeCab separates it into two words " 見れ | ば " and JUMAN keep the same string , but KyTea outputs it as three words " 見 | れ | ば " where every character is a word .
For example , if we are given a string , “見れば( if someone sees )” , MeCab separates it into two words , “見れ | ば” and JUMAN retains the same string , but KyTea outputs it as three words , “見 | れ | ば” where every character is a word .
92
In the case of latticelm , as it has no supervised definition of words , it uses the expectation maximized length of words for every word depending on training data .
For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .
93
We also investigate such morphological analysis accuracy and word definition problems in our experiments .
In our experiments , we further investigate such morphological analysis accuracies and word definition problems .
94
One is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .
One method is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .
95
In the case of REUTERS , we have used all 56 ,282 sentences .
For REUTERS , we used all 56 ,282 sentences .
96
In this data , we have combined JENAAD and REUTERS news corpora to get one news corpus .
For this data , we combined the JENAAD and REUTERS news corpora to acquire one news corpus .
97
We have used all 56 ,282 and 150 ,000 sentences respectively .
We used all 56 ,282 and 150 ,000 sentences , respectively .
98
For each corpus , we divide it into the first 1 ,000 , the next 500 , and the rest for test , development , and training .
For each corpus , we divided the sentences into the first 1 ,000 for testing , the next 500 for development , and the remaining for training .
99
We have gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively , in total .
In total , we gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively .
100
Firstly , since the WIKIPEDIA corpus is a multi-category XML dataset , we have sorted them by the DOCID in the ascending order and by the document categories LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .
Since the WIKIPEDIA corpus is a multi-category XML dataset , we sorted them by the DOCID in ascending order , and by the document categories : LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .
101
Thirdly , sentence pairs that include a character " | " in English or Japanese are removed because it caused a problem with Moses .
Then , sentence pairs that include a character “|” in English or Japanese were removed , because it caused a problem with Moses .
102
In order to adjust the balance of the domains , we have sampled the data twice : First we extract the first line for every 477 lines .
In order to adjust the balance of the domains , we sampled the data twice : First , we extracted the first line for every 477 lines .
103
After this , we have merged the remaining 476 ,012 lines and from this extract the first line for every 952 lines .
Then , we merged the remaining 476 ,012 lines , and from this extract , we extracted the first line for every 952 lines .
104
Finally , we have obtained 1 ,000 test , 500 development , and 475 ,512 training data .
Finally , we obtained 1 ,000 test , 500 development , and 475 ,512 training data .
105
Hence , this problem remains if we keep our word-based evaluations .
Hence , if we keep our word-based evaluations , this problem remains .
106
- Remove articles " a " , " an " , and " the "
- Remove articles “a” , “an” , and “the”
107
In this case , the evaluation scores created by BLEU and RIBES are not comparative due to the differences of Japanese word definitions between the outputs of word segmentation methods .
In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .
108
Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost same while small changes have been introduced due to statistical errors and the differences in the methods how to treat space characters .
Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost the same , while small changes have been introduced , due to statistical errors and the differences in the methods in how to treat space characters .
109
For instance , on REUTERS in Table 2 , BLEU scores were ranged from 27 .88 to 29 .53 , while latticelm was 15 .28 and CAT was 22 .10 .
For instance , on REUTERS in Table 2 , BLEU scores ranged from 27 .88 to 29 .53 , while for latticelm , the score was 15 .28 and for CAT , the score was 22 .10 .
110
The unsupervised morphological analyzer latticelm and one of heuristic methods CAT were worse than our expectations .
The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .
111
These two were the worst or the second worst results in all settings .
These two results were the worst , in all of the settings .
112
It was relatively much better than the supervised morphological analyzers in BLEU .
The results were better than the results for the supervised morphological analyzers in BLEU .
113
For example , CHAR achieved the best 38 .42 score in BLEU on REUTERS , but the second best KyTea was 29 .53 .
CHAR achieved the best score in BLEU on REUTERS ( 38 .42 ) , but the second-best was KyTea ( 29 .53 ) .
114
The results were the worst scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS . The only one exception was in the case of the best 56 .55 BLEU in Characters on REUTERS .
The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .
115
And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .
Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .
116
It was good at English-Japanese but not at Japanese-English translations .
It excelled with English-Japanese translations , but not with Japanese-English translations .
117
We consider the possible reasons for this result :
We consider the possible reasons for this result in the following list :
118
The current evaluation metrics we pursued in this paper were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation , since they did not produce consistent scores as explained below :
The current evaluation metrics that we pursued in this paper were insufficient to discuss the relative advantages and disadvantages of word segmentation in detail , since the scores that were produced were inconsistent , as explained below :
119
For example , on WIKIPEDIA in Table 2 , while CHAR was relatively the highest and greatly better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .
For example , on WIKIPEDIA in Table 2 , while CHAR was the highest , and performed better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .
120
This work focused on how the difference of word segmentation affects SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .
121
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , they were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation .
Additionally , as we could not always obtain consistent scores from the current evaluation metrics , the data was insufficient for discussing the relative advantages and disadvantages of word segmentation , with accuracy .
122
We also suggested it is possible to implement more optimized word segmentation on SMT .
We have also suggested that it is possible to implement more optimized word segmentation on SMT .
123
However , problems with their approaches are the disregard of the interdependencies of word senses , and the limited applicability to those word senses for which training instances are served .
However , the problem with these approaches , is that they disregard the interdependencies of word senses , and that it is limited in its applicability to the word senses for which training instances are served .**[<-This sentence is a bit confusing]
124
In experiments , we show the appropriateness of considering the sense dependencies , as well as the advantage of the combination of fine- and coarse-grained tag sets .
In experiments , we display the appropriateness of considering the sense dependencies , as well as the advantage of [having ? Using ?] the combination of fine- and coarse-grained tag sets .
125
It is considered to be an intermediate , but necessary step toward many NLP applications including machine translation and information extraction , which require the knowledge of word senses to achieve better performance .
It is considered to be an intermediate , but necessary step for many NLP applications , including machine translation and information extraction , which[what does " which " refer to ? Machine translation ? Information extraction , or both ? Clarify] require the knowledge of word senses to perform better .
126
In order to resolve this problem , several semi-supervised approaches have been explored in recent years .
In recent years in order to resolve this problem , several semi-supervised approaches have been explored .
127
Some researchers have addressed directly the scarcity of the training data , and explored the methods to obtain more tagged instances , by the co-training and self-training .
Some researchers have addressed the scarcity of the training data directly , and have explored the methods to obtain more tagged instances , by co-training and self-training .
128
As a result , the resulting sense assignment may not semantically consistent over the sentence .
In turn , the resulting sense assignment may not be semantically consistent over the sentence .
129
We focus on the use of the interdependency of word senses , so that we can directly address the issue of semantic ambiguity of a whole sentence arose from the interaction of each word 's sense ambiguity .
We focus on using the interdependency of word senses , so that we can directly address the issue of semantic ambiguity in a whole sentence that arose from the interaction of each word 's sense ambiguity . **[ <- this part is confusing .]
130
Specifically , we assume that there exist strong sense dependencies between a syntactic head and its dependents in the dependency tree , rather than between neighboring words in the sentence .
Specifically , we assume that are strong sense dependencies between a syntactic head , and its dependents in the dependency tree , rather than between neighboring words of a sentence .
131
We solve WSD as a labeling problem to a sentence described as a dependency tree , where the vertices correspond to words and the edges correspond to the sense dependencies .
We solve WSD as a labeling problem to a sentence described as a dependency tree , where the vertices correspond to the words , and the edges correspond to the sense dependencies .
132
In Section 5 , 6 , and 7 , we present our experimental setup and results , and an in-depth analysis on the contribution of the sense dependency features .
In Section 5 , 6 , and 7 , we present our experimental setup , the results , and an in-depth analysis on the contribution of the sense dependency features .
133
It also serves as an ontology , in which various kinds of meta data , relations among words and senses , and well-organized hierarchical classification of word senses are defined .
WordNet also serves as an ontology of various kinds of meta data , relations among words and senses , and a well-organized hierarchical classification of word senses that are defined .
134
In the WordNet , nouns and verbs are organized in hierarchical structures with IS-A ( hypernym-hyponym ) relationships among words , as shown in Figure 1 .
As shown in Figure 1 , in WordNet , nouns and verbs are organized into hierarchical structures with IS-A ( hypernym-hyponym ) relationships among words , .
135
All nouns and verbs except some top-level concepts are classified into primitive groups called supersenses , which we describe later .
All nouns and verbs , with the exception of some top-level concepts , are classified into primitive groups called supersenses , which we will describe later .
136
The synset {group#1 , grouping#1} is a broad semantic category that governs the supersense noun group .
The synset {group#1 , grouping#1} is a broad semantic category that governs the supersense group noun .group .
137
Hence , we can expect them to act as a good smoothing feature for WSD , which would make up for the sparseness of features associated with finer-grained senses .
Hence , we can expect them to act as a good smoothing feature for WSD , which would make up for the problem of the sparseness of features , commonly associated with finer-grained senses .
138
It is considered to be a good feature that reflects the sense frequency information when sufficient training data is available for every sense .
When sufficient training data is available for every sense this method is considered to be a good feature that reflects the sense frequency information .
139
On the other hand , the traditional approach to the supervised WSD is to solve an independent classification problem for each word .
On the contrary , the traditional approach to supervised WSD is to solve an independent classification problem for each word .
140
However , the dependencies they considered are rather simple ones between the adjacent words , and between either WordNet synsets or supersenses .
The dependencies that they considered , however , are rather simple ones between the adjacent words , and between either WordNet synsets or supersenses .
141
Note additionally that they do not mention how and how much they contribute to the improvement of supervised WSD .
Note additionally , that they do not mention the means or the quality of contribution in improving supervised WSD .
142
One interesting model related is the exponential family model proposed by , which captures the occurrences and co-occurrences of words and senses in a joint probability distribution .
The exponential family model proposed by , captures the occurrences and co-occurrences of words and senses in a joint probability distribution .
143
Although they focused on the use of the co-occurrences of word senses rather than the dependencies , they clarified the contribution of sense co-occurrences to the supervised WSD .
Although they focused on the use of the co-occurrences of word senses rather than that of dependencies , they clarified the contribution of sense co-occurrences to the supervised WSD .
144
Also , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not explicitly examined thus far .
Furthermore , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not been explicitly examined thus far .
145
This problem may even be magnified when we consider the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .
This problem may even be magnified , when taking into consideration the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .
146
In this case , the inter-annotator agreements are turned out to reach around 90% .
In this case , the inter-annotator agreements have reached nearly90% .
147
For this reason , we use as our sense inventory the WordNet supersenses as well as the synsets .
For this reason , we use the WordNet supersenses , as well as the synsets as our sense inventory .
148
This approach has been taken in several hierarchical WSD methods , but never combined with the sense dependencies as we use .
This approach has been taken in several hierarchical WSD methods , but has never been combined with the sense dependencies in a way that have used them .
149
For this reason , for the synset-based model , we convert the outputted dependency tree into a tree of content words , as exemplified on the right-hand side of Figure 2 .
Thus , for the synset-based model , we convert the outputted dependency tree into a tree of content words , as exemplified on the right-hand side of Figure 2 .
150
Then , on the right-hand side of Figure 2 , we can see that the dependency between confidence and bank is now described as a direct edge .
Then , on the right-hand side of Figure 2 , the dependency between confidence and bank is now described as a direct edge .
151
Thus , by the compaction of the trees , our model can capture more useful dependencies among word senses .
By the compaction of the trees , therefore , our model can capture more useful dependencies among word senses .
152
The inclusion of removed words and dependency relation labels are performed in the same manner as in the synset-based model , and the tree on the right hand side of Figure 2 in this case remains unchanged because the sentence does not contain any adjectives nor adverbs .
The inclusion of removed words and dependency relation labels are performed in the same manner as in the synset-based model ; the tree on the right hand side of Figure 2 in this case remains unchanged , because the sentence does not contain any adjectives nor adverbs .
153
For the linear-chain models , we do not need to parse a sentence .
For the linear-chain models , parsing a sentence is unnecessary .
154
Next , as the same reason for the tree-structured case , we remove from the graph those words that we do not need to disambiguate , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .
Next , as the same reason as for the tree-structured case , we remove those words that we do not need to disambiguate from the graph , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .
155
Here , we focus on three words destroy , confidence , and bank in Sentence ( i ) , and for simplicity consider only two major senses for each word as described in Table 3 , so that the number of possible sense assignments is in this case / MATH .
Here , we focus on three words : destroy , confidence , and bank in Sentence ( I ) . For simplicity , we consider only two major senses for each word as described in Table 3 , so that the number of possible sense assignments is / MATH .
156
The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) but not related to natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .
The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) , but is unrelated to a natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .
157
In order to see whether the sense dependency features are certainly effective or not , we include as vertex features the word forms , lemmas , and parts of speech of both the parent and the child words in the dependency tree .
In order to see the efficiency of sense dependency features , we include as vertex features the word forms , lemmas , and parts of speech of both the parent and the child words in the dependency tree .
158
Using this contextual information and the set of vertex labels / MATH , we construct a set of features on a vertex / MATH by / MATH .
Using this contextual information , and the set of vertex labels / MATH , we construct a set of features on a vertex / MATH by / MATH .
159
However , on the other hand , our model cannot output any answers to multi-word expressions that have no directly corresponding WordNet synsets , because we treat expression as one unit in the process of WSD .
However , our model cannot output any answers to multi-word expressions that have no directly corresponding WordNet synsets , because we treat expression as one unit in the process of WSD .
160
For example , the multi-word expression tear-filled is treated as one instance but not tagged with any WordNet synsets in the converted corpus , while in the original corpus it is tagged with two WordNet synsets for tear and filled .
For example , the multi-word expression tear-filled is treated as one instance , but are untagged with any WordNet synsets in the converted corpus , while in the original corpus it[define " it " ] is tagged with two WordNet synsets for tear and filled .
161
As they noted , in the WordNet , there is semantically inconsistent labeling of supersenses such that top level synsets are tagged as the supersense noun .Tops rather than the specific supersense they govern .
As noted , in the WordNet , the labeling of supersensesis semantically inconsistent , and top level synsets are tagged as the supersense noun .Tops[ ? ?] rather than the specific supersense they govern .
162
For this reason , we adopted the modification of noun supersenses in the same way as , substituting noun .Tops labels with more specific supersense labels when possible , and left some general nouns with noun .TopsoteNouns which are left with noun .Tops are : entity , thing , anything , something , nothing , object , living thing , organism , benthos , heterotroph , life , and biont . .
For this reason , we adopted the modification of noun supersenses in the same way as , substituting noun .Tops labels with more specific supersense labels when possible , and left some general nouns with noun .TopsoteNouns , which are left with noun .Tops are : entity , thing , anything , something , nothing , object , living thing , organism , benthos , heterotroph , life , and biont . .
163
We ignore the adjective and adverb instances in the evaluation .
We ignore the adjective and adverb instances in the evaluation .**[This section is a bit confusing . Maybe break up the longer sentences to clarify]
164
Table 6 is the list of models we use for the evaluation , where FS and SR correspond to the first sense and sense ranking features respectively , and non-dependency denotes models that do not incorporate sense dependency features ( i.e.
Table 6 is the list of models that we use for the evaluation , where FS and SR correspond to the first sense and sense ranking features respectively , and non-dependency denotes models that do not incorporate sense dependency features ( i.e.
165
In this section , each figure shows the mean recall ( equivalent to the precisions ) averaged over the five trials of the cross validation , the " Diff . " rows show the differences between the dependency models and the non-dependency models , and / MATH and / MATH denote the statistical significance of / MATH and / MATH respectively .
Each figure displays the mean recall ( equivalent to the precisions ) averaged over the five trials of the cross validation , the " Diff . " rows show the differences between the dependency models and the non-dependency models , and / MATH and / MATH denote the statistical significance of / MATH and / MATH respectively .
166
We can see from Table 7 that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .
From Table 7 , it can be seen that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .
167
These improvements seem small in terms of figures ; However , considering for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model only by 0 .37% on SEM , the further improvement of 0 .21% is considerable because this means our dependency model could handle 57% more instances over the first sense baseline .
These improvements seem insignificant in figures ; however , considering that for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model by only 0 .37% on SEM , the further improvement of 0 .21% is substantial , because it indicates that our dependency model could handle 57% more instances over the first sense baseline .
168
However , by the incorporation of the sense dependencies , the improvements with the sense ranking features are even smaller , and the deteriorations without them are even larger than in the tree-structured case .
Nonetheless , by the incorporation of the sense dependencies , the improvements with the sense ranking features was even less , and the deteriorations without them[define " them " ] were even more than in the tree-structured case .
169
These results suggest that although both the dependency trees and the linear chains capture useful dependencies of word senses , the dependencies on the tree structures capture more important information .
Thus , although both the dependency trees and the linear chains capture useful dependencies of word senses , the dependencies on the tree structures capture more important information .
170
Whereas Tree-WS-SR and Tree-WS use all four sense labels for the edge features ( / MATH ) , Tree-WS-SR' and Tree-WS' only use the synset labels ( / MATH ) , so that we can see the contribution of the coarse-grained sense labels .
Whereas Tree-WS-SR and Tree-WS use all four sense labels for the edge features ( / MATH ) , Tree-WS-SR' and Tree-WS' only use the synset labels ( / MATH ) . Thus , we can see the contribution of the coarse-grained sense labels .
171
These results suggest that even though the granularity of the supersenses is sufficient for many NLP tasks , they are too coarse-grained to capture enough information for WSD models ; Therefore , even for the supersense-based disambiguation , we can improve the performance by considering finer-grained senses .
Thus , even though the granularity of the supersenses is sufficient for many NLP tasks , they are too coarse-grained to capture enough information for WSD models ; therefore , for the supersense-based disambiguation , we can improve the performance by considering finer-grained senses .
172
However , considering that all systems in Table 12 except for Simil-Prime utilize other sense-annotated corpora in addition to SemCor , such as the Senseval data sets or example sentences in the WordNet , and our model cannot handle multi-word expressions that do not exist in the WordNet as noted in Section 5 .1 , we can conclude that the performance of our T-CRF model is comparable to that of state-of-the-art WSD systems .
However , taking into consideration that all systems in Table 12 except for Simil-Prime utilize other sense-annotated corpora in addition to SemCor , such as the Senseval data sets or example sentences in the WordNet , and that our model cannot handle multi-word expressions that do not exist in the WordNet as noted in Section 5 .1 , we can conclude that the performance of our T-CRF model is comparable to that of state-of-the-art WSD systems . **[This is a long sentence- shorten .]
173
We call a feature either with a positive lambda or with an alpha larger than 1 as an excitatory feature , while that either with a negative lambda or an alpha smaller than 1 as an inhibitory feature .
We call a feature either with a positive lambda or with an alpha larger than 1 as an excitatory feature , and those features with either with a negative lambda or an alpha smaller than 1 as an inhibitory feature .
174
But , fortunatelly , the first six senses belong to different supersenses , and our dependency model succeeded in outputting the correct sense take#4 ( SS :verb .contact , take physically ) by making use of the strong dependency SS :verb .contact-SS :noun .substance ( / MATH ) , given dust#1 belongs to noun .substance .
But fortunately , the first six senses belong to different supersenses , and our dependency model succeeded in outputting the correct sense take#4 ( SS :verb .contact , take physically ) by making use of the strong dependency SS :verb .contact-SS :noun .substance ( / MATH ) , given dust#1 belongs to noun .substance .
175
Here , the correct sense for nail is nail#2 ( noun .artifact , a thin pointed piece of metal ) and that for level is level#5 ( noun .artifact , indicator of the horizontal ) .
Here , the correct sense for nail is nail#2 ( noun .artifact , a thin pointed piece of metal ) , and that for level is level#5 ( noun .artifact , indicator of the horizontal ) .
176
Despite the small improvements in terms of overall figures , these improvements indeed correspond to 25%-57% improvements over the first sense baseline .
Despite the small improvements in overall figures , these improvements indeed correspond to 25%-57% improvements over the first sense baseline .
177
However , our experiments on the other hand showed that even when combined with the coarse-grained tag sets , the sense dependency features do not improve the performance unless combined with proper sense frequency information , due to the data sparseness problem .
However , our experiments showed that even when combined with the coarse-grained tag sets , the sense dependency features do not improve the performance , unless combined with proper sense frequency information . This is due to the data sparseness problem .
178
Although our model was based on a simple framework and trained only on the SemCor corpus , the results we gained were promising , suggesting that our model still has a great potential for improvement .
Although our model was based on a simple framework , and was trained only on the SemCor corpus , the results that we gained were promising . They suggested that our model still has a great potential for improvement .
179
Generating short summary videos for rushes is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .
Generating short summary videos for rushes is a challenging task due to the difficulty in eliminating redundancy and determining the important objects and events to be placed in the summary .
180
This makes approaches using one keyframe for shot representation failed in doing clustering .
This makes approaches using one keyframe for a shot representation fail when trying to form a cluster .
181
In this paper , we introduce two approaches to these problems .
,We introduce two approaches to solve these problems .
182
Video summarization is a significant research that helps to meet these needs by developing a condensed version of a full length digital video with the most important contents \CITE .
Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \CITE .
183
Summary videos can help users to browse and navigate large video archives efficiently and effectively .
Summary videos can help users more efficiently and effectively browse and navigate through large video archives .
184
Generating summary videos for BBC rushes \CITE is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .
Generating summary videos for BBC rushes \CITE is a challenging task due to the difficulty with redundancy elimination and determining the most important objects and events to be placed in the summary .
185
High recall , i.e many objects and events ( called scenes ) are included in the summary , usually reduce the number of frames for each scene .
High recall , i.e. many objects and events ( called scenes ) included in the summary , usually reduces the number of frames for each scene .
186
On the contrary , smooth presentation of events consumes a lot number of frames , that decrease the recall .
On the other hand , a smooth presentation of these events would consume a large number of frames , which would decrease the recall .
187
Video segmentation : This step decomposes the original video into segments , such shots or sub-shots .
Video segmentation : This step breaks down the original video into segments , such as shots or sub-shots .
188
In the other case , assume that we have selected appropriate segments , the total length of these segments are usually larger than that of the final summary .
In the other case , assuming that we have selected the appropriate segments , the total length of these segments is usually larger than that of the final summary .
189
The question is how to determine the important part of the selected segment such that it conveys information of the scene as much as possible .
The question is how to determine the most important parts of the selected segments so that they convey as much of the information of the scene as possible .
190
The first approach represents each segment by one key-frame and groups similar segments by doing clustering on these key-frames .
The first approach represents each segment by using one key-frame and groups similar segments by clustering them on these key-frames .
191
From the definition , all rushes are unedited; therefore it must consist of hard cut only .
By definition , all rushes are unedited; therefore they must consist of hard cuts only .
192
The shot boundary detection algorithm in \CITE is used to determine shot boundary and partition the input video into shots .
The shot boundary detection algorithm in \CITE is used to determine the shot boundary and to partition the input video into shots .
193
The \MATH distance used to compute the distance of frame sequence until the sum of the sorted value of lower eight is larger than a threshold \MATH .
The \MATH distance used to compute the distance of the frame sequence until the sum of the sorted value of the lower eight is larger than the threshold \MATH . //[distance / length?]
194
From the properties of single color image , a dominant color in its global histogram is large .
From the properties of a single color image , the dominant color in its global histogram is large .
195
The clapper boards have many types , such as scale , rotation , and illumination changes .
There are many types of clapper boards , such as scale , rotation , and illumination changes .
196
If a result of the NDK algorithm returns a match between a keyframe with a query then we define the sub-shot is a clapper board sub-shot .
If the result of the NDK algorithm returns a match from a keyframe with a query then the sub-shot is defined as a clapper board sub-shot .
197
However , rushes videos containing of repetitive story , such as retake scenes , are unedited .
However , rushes videos containing a repetitive story , such as a retake of scenes , are unedited .
198
To do the clustering on keyframes , three different features , including mean , variance , and skewness , are extracted from local color histogram .
To do clustering on keyframes , three different features , including the mean , variance , and skew , are extracted from the local color histogram .
199
The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of summary is 2\% of the original video ) , less repetitive of content , and must have many objects and events as possible .
The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of a summary is 2\% of the original video ) , less repetitive content , and must have as many objects and events as possible .
200
Third , merge consecutive sub-shots in each cluster into shots and compute the priority of each shot based on priority of shot weighted duration and shot weighted average motion magnitude using the following equation : \MATH</p>
Third , merge the consecutive sub-shots in each cluster into shots and compute the priority of each shot based on the priority of the shot weighted duration and shot weighted average motion magnitude using the following equation : \MATH</p>
201
In order to reduce the computation time , we only extract a subset of frames from the original video by sampling at a five frame interval ( i.e extract frames 0th , 5th , 10th , and so on ) .
In order to reduce the computation time , we only extract a subset of the frames from the original video by sampling it at a five frame interval ( i.e. extract frames 0 , 5th , 10th , and so on ) .
202
If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered as junk and all fragments of the cluster containing junk fragment are eliminated .
If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered junk and all the fragments of the cluster containing the junk fragment are eliminated .
203
This portion covers a duration twice as much as the fragment quota by selecting frames with sampling rate of 2 frames .
This portion covers a duration twice the size of the fragment quota by selecting the frames with a sampling rate of two frames .
204
The system NII-2 achieves higher recall ( IN ) than the system NII-1 since NII-1 only uses one keyframe for each sub-shot and has shorter duration ( DU ) for summary videos .
The NII-2system achieves a higher recall ( IN ) than the NII-1 system because NII-1 only uses one keyframe for each sub-shot and has a shorter duration ( DU ) for summary videos .
205
The clapper board detection process using NDK consumes around half of processing time of NII-1 but performance is low due to large variations of clapper boards in videos ( see Figure \REF ) .
The clapper board detection process using NDK consumes around half of the processing time of NII-1, but its performance is low due to the large variations in clapper boards in the videos ( see Figure \REF ) .
206
The 14 systems listed in this table have IN score larger than the median ( 0.45 ); and other scores such as RE and TE larger than half of maximum score ( 2.5 ) .
The 14 systems listed in this table have an IN score that is above the median ( 0.45 ); and other scores, such as RE and TE, are larger than half of the maximum score ( 2.5 ) .
207
Using all frames of one segment instead of using one keyframe as proposed in NII-2 is one of the efforts toward this direction .
Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .
208
In the first approach, NII-1, redundancy elimination is done by doing clustering on the set of keyframes extracted from sub-shots .
In the first approach, NII-1, clustering the set of keyframes extracted from the sub-shots helps to eliminate redundancy .
209
This approach achieves good performance in usability score but low performance in recall .
This approach has a good usability score but is not very good at recall .
210
This approach achieves good performance in recall and reasonable performance in usability score .
This approach is good for recall and has a reasonably good usability score .
211
Searching persons is one of the essential tasks required by users for image and video search engines .
Searching for images of people is one of the essential tasks required by users for image and video search engines .
212
In this paper , we propose a method to effectively retrieve relevant faces for one person by learning visual consistency from results retrieved from text correlation based search engines .
We propose a method to effectively retrieve relevant faces for one person by learning visual consistency from results retrieved from text correlation based search engines .
213
This problem is challenging because ( i ) no any label is provided leading to be difficult to use supervised-based ranking methods .
This problem is challenging because ( i ) there is no label provided making it difficult to use supervised-based ranking methods .
214
With the rapid growing of digital technology , large image and video databases are available easier than ever to users .
With the rapid growth of digital technology , large image and video databases are more available than ever to users .
215
Usually , most of current search engines use text associated with images or videos as a significant clue to return the results .
Usually , most current search engines use the texts associated with images or videos as significant clues for returning results .
216
However , since it is not necessary faces and names appear simultaneously and are aligned ( as shown in Figure \REF ) , the main drawback of this approach is existence of many irrelevant results that makes the retrieval performance very low .
However , other un-queried faces and names appear simultaneously and are aligned ( as shown in Figure \REF ) , which significantly lowers retrieval performance .
217
In this paper , we propose a method to solve the mentioned problem .
We propose a method to solve the above-mentioned problem .
218
It is necessary to use the second stage to improve the rank list .
A second stage is necessary to improve this rank list .
219
Furthermore , with recent studies \CITE SVM classifiers can provide probability outputs that are suitable for ranking .
Furthermore , recent studies suggest that \CITE SVM classifiers provide probability outputs that are suitable for ranking .
220
-We propose a general framework to boost the face retrieval performance from the results retrieved from text correlation based search engines by learning visual consistency .
-We propose a general framework to boost the face retrieval performance from results retrieved from text correlation-based search engines by the learning of visual consistency .
221
There are several approaches proposed for general object classification rather than for face retrieval .
There are several more proposed approaches for general object classification than for those for face retrieval .
222
Compared to the problem of face retrieval based recognition , the problem of object classification is easier since classification of different object types such as airplane and non-airplane only needs to handle inter-variations between different categories while discriminating personA and personB requires to handle both intra-variations and inter-variations of the same category .
Compared to the problem of face-based recognition , the problem of object classification is easier since classification of different object types such as airplane and non-airplane only needs to handle inter-variations between different categories , while discriminating between person-A and person-B requires handling of both intra-variations and inter-variations of the same category .
223
Working closely to our problem , in \cite{Ozkan06CVPR} , a graph based approach was proposed \CITEin which a graph is formed by faces as nodes and weights of edges linked between nodes are the similarity of faces .
A graph-based approach was proposed by \CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
224
By assuming that the number of faces of the queried person are larger than that of other persons , and these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph whose solution is available .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph with an available solution . //[Do graphs have solutions ? They just provide information .]
225
Step 3 used to find initial ranks for faces is described in \REF .
Step 3 used to find initial ranks for faces described in \REF .
226
Since it is not guaranteed top \MATH and bottom \MATH of faces in the rank list are correctly correspondent to faces of the queried person \MATH and faces of non person \MATH as shown in Figure \REF , selecting randomly subsets to train weak classifiers and then combining these classifiers might help to reduce risk of using noisy training sets .
Since it is not guaranteed that the top \MATH and bottom \MATH of faces in the rank list correctly correspond to the faces of the queried person-\MATH and faces of non person-\MATH as shown in Figure \REF , randomly selecting subsets to train weak classifiers , and then combining these classifiers might help reduce the risk of using noisy training sets .
227
We introduce here two common outliers detection methods including distance-based outliers detection( DBO ) \CITE and local outliers factor based method ( LOF ) \CITE .
We introduce two common outlier detection methods , distance-based outlier detection ( DBO ) \CITE and local outlier factor-based method ( LOF ) \CITE .
228
In our experiments , the distance between two objects is Euclidean distance between two faces and is computed in the eigen-subspace ( described in section \REF ) .
In our experiments , the distance between two objects is the Euclidean distance between two faces and is computed in the eigen-subspace ( described in section \REF ) .
229
-Step 1 : For each data object \MATH compute \MATH ( the distance to the \MATH nearest neighbor ) and \MATH ( all points in a \MATH sphere ) .
-Step 1 : For each data object \MATH compute the \MATH ( the distance to the \MATH nearest neighbor ) and \MATH ( all points in a \MATH sphere ) .
230
-Step 3 : Compute local reachability density of data object \MATH as inverse of the average reachability distance based on the \MATH ( minimum number of data objects ) nearest neighbors of data object \MATH .
-Step 3 : Compute local reachability density of data object \MATH as inverse of the average reachability distance based on the \MATH ( minimum number of data objects ) of the nearest neighbors to data object \MATH .
231
-Step 4 : Compute LOF of data object \MATH as average of the ratios of the local reachability density of data object \MATH and local reachability density of \MATH nearest neighbors .
-Step 4 : Compute LOF of data object \MATH as the average of the ratios of the local reachability density of data object \MATH and local reachability density of \MATH of nearest neighbors .
232
After eliminating faces whose facial features are poorly detected by a rectification process and faces whose associated names are not extracted properly from corresponding captions , 30 , 281 faces were kept .
After eliminating faces whose facial features were poorly detected by a rectification process and faces whose associated names were not extracted properly from the corresponding captions , 30 , 281 faces were kept .
233
We selected sixteen celebrities who are government leaders such as George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key persons such as John Paul II ( the Former Pope ) , Kofi Annan and Hans Blix ( UN ) . These persons are selected since their appearances are highly frequent in the dataset \CITE .
We selected sixteen government leaders including George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals such as John Paul II ( the Former Pope ) and Kofi Annan and Hans Blix ( UN ) since their images appeared frequently in the dataset \CITE .
234
In total , 3 , 907 faces are retrieved in which 2 , 094 faces are relevant .
In total , 3 , 907 faces were retrieved in which 2 , 094 faces were relevant .
235
The number of eigenfaces was selected so that 97% of the total energy are retained \CITE .
A number of eigenfaces was selected so that 97% of the total energy was retained \CITE . //[What is that number ? ]
236
In addition , to evaluate performance of multiple queries , we used mean average precision that is the mean of average precisions computed from queries .
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .
237
In the baseline method , faces are sorted by the time that the associated news article is published .
In the baseline method , faces were sorted by the time the associated news article was published .
238
The subsets \MATH and \MATH are generated by randomly selecting with replacement 70% samples of \MATH amd \MATH .
The subsets \MATH and \MATH were generated by randomly selecting with replacement 70% samples of \MATH and \MATH .[�gWith replacement�h does not make sense here . I am not sure what you want to say .]
239
Figure \REF shows performance of single classifiers and ensemble classifiers .
Figure \REF shows the performance of single and ensemble classifiers .
240
In addition , the performance of the ranking process is improved when using the ensemble classifier .
In addition , the performance of the ranking process improved when the ensemble classifier was used .
241
As shown in Figure \REF , the performance does not change so much after 5 iterations .
As shown in Figure \REF , the performance did not change much after five iterations .
242
The performance of different methods shown in Figure \REF indicates that our proposed method outperforms the distance-based outliers detection method and has comparable performance with the supervised method using 5% annotation data .
The performance of different methods shown in Figure \REF indicates that our proposed method outperformed the distance-based outlier detection method and performed comparable to the supervised method using 5% annotation data .
243
As shown in Figure \REF , \REF , \REF , our proposed method produces better results in terms of average precision in which relevant faces are put on the top of the returned list .
As shown in Figures \REF , \REF , \REF , our proposed method produced better results in terms of average precision in which relevant faces were put at the top of the returned list .
244
By combining multiple weak classifiers in a bagging framework , the final strong classifier is constructed and produce good results .
By combining multiple weak classifiers in a bagging framework , we constructed the final strong classifier , which produced good results .
245
Human face processing techniques for broadcast video including face detection , tracking and recognition have long been a topic that attracts much research interest due to its crucial value in various applications including video structuring , indexing , retrieval , summarization , etc.
Human face processing techniques for broadcast video , including face detection , tracking , and recognition , have long been a topic that has attracted a lot of research interest due to its crucial value in various applications , such as in video structuring , indexing , retrieval , and summarization .
246
The main reason is human face provides rich information for people 's appearance such as a government leader in a news video , a pitcher in a sport video or a hero in a movie , and is the basis for interpreting facts .
The main reason for this is that the human face provides rich information for people 's appearances , such as for a government leader in a news video , a pitcher in a sports video or a hero in a movie , and is the basis for interpreting facts .
247
Face detection which is the task of localizing faces in an input image is fundamental for any face processing system .
Face detection , which is the task of localizing faces in an input image , is a fundamental part of any face processing system .
248
- Robustness : it should be capable of handling appearance variations of pose changes , size , illuminations , occlusions , complex background , facial expressions , low resolutions , etc.
- Robustness : it should be capable of handling appearance variations , such as pose changes , size , illuminations , occlusions , complex backgrounds , facial expressions , and low resolutions .
249
- Fastness : it should be fast for real-time processing which is an important factor in processing large video archives .
- Quickness : it should be fast in order to perform real-time processing , which is an important factor in processing large video archives .
250
The number of patterns extracted from one 320x240 frame image is large , approximately 160 ,000 in which only a small number of patterns containing face .
The number of patterns extracted from a 320 x 240 frame image is large , approximately 160 ,000 , in which only a small number of patterns contain a face .
251
- Classification : the extracted features is passed through a classifier which is trained beforehand to classify the input pattern associated with these features as a face or a non-face .
- Classification : the extracted features are passed through a classifier that has been previously trained to classify the input pattern associated with these features as a face or a non-face . //[trained / programmed ?]
252
Since the number of processed patterns is large while the vast majority of them are non-face , a single classifier based systems such as neural network \CITE and support vector machines \CITE are usually slow .
Since the vast majority of processed patterns are non-face , the single classifier based systems , such as the neural network \CITE and the support vector machines \CITE , are usually slow .
253
In this structure , fast and simple classifiers are used as filters at the earliest stages to quickly reject a large number of non-face patterns and slower yet more accurate classifiers are then used for classifying face-like patterns .
In this structure , fast and simple classifiers are used as filters in the earliest stages to quickly reject a large number of the non-face patterns and then slower but more accurate classifiers are used for classifying the face-like patterns .
254
In this way , the complexity of classifiers can be adapted corresponding to the increasing difficulty in the input patterns .
In this way , the complexity of classifiers can be adapted to correspond to the increasing difficulty with the input patterns .
255
Face patterns are manually collected in images containing faces and then are scaled to the same size and normalized to a canonical pose which eyes , mouth and nose are aligned .
Face patterns are manually collected from images containing faces and then are scaled to the same size and normalized to a canonical pose in which the eyes , mouth , and nose are aligned .
256
Then these face patterns can be used to generate other artificial faces by randomly rotating the images ( about their center points ) up to 10 degree , scaling between 90% and 110% , translating up to half a pixel , and mirroring to enlarge the number of positive samples \CITE .
Then these face patterns can be used to generate other artificial faces by randomly rotating the images ( about their center points ) by up to 10 degrees , scaling them between 90 and 110% , translating them up to half a pixel , and mirroring them to enlarge the number of positive samples \CITE .
257
- Learning method selection : Basically , in the ideal case with proper settings , advanced learning methods such as neural network , support vector machines and AdaBoost produce similar performance .
- Learning method selection : Basically , in an ideal situation with the proper settings , the advanced learning methods , such as the neural network , support vector machines , and AdaBoost , can perform similarly .
258
Therefore , it is preferable to use support vector machines since the number of parameters is only two if using RBF kernel and many tools are available .
Therefore , it is preferable to use support vector machines because only two parameters are necessary if a RBF kernel is used and many tools are available .
259
Face tracking is the process of locating a moving face or several ones in time using a camera , as illustrated in Figure 1 .
Face tracking is the process of locating a moving face or several of them over a period of time using a camera , as illustrated in Fig. 1 .
260
Face tracker then analyses subsequent video frames and outputs the location of the initialized face within these frames by estimating the motion parameters of the moving face .
The face tracker then analyzes the subsequent video frames and outputs the location of the initialized face within these frames by estimating the motion parameters of the moving face .
261
Different from face detection , the outcome of which is the position and scale of one single face in one single frame , face tracking enables the information acquisition of multiple consecutive faces within consecutive video frames .
This is different from face detection , the outcome of which is the position and scale of one single face in one single frame ; face tracking enables the information acquisition of multiple consecutive faces within consecutive video frames .
262
Although frame-based face detection techniques have demonstrated success on real images , the current ability on detecting faces from video is still primitive .
Although frame-based face detection techniques have been successfully demonstrated on real images , the current ability for detecting faces from video is still primitive .
263
The detector responses can decrease due to different reasons including occlusions , lighting conditions and face pose .
The quality of the detector responses can decrease due to different reasons including occlusions , lighting conditions , and face poses .
264
It is therefore important to incorporate the temporal information in a video sequence to provide more complete video segments displaying the person of interest , which is always named as face tracking .
It is therefore important to incorporate the temporal information in a video sequence to provide more complete video segments displaying the person of interest , which is always named as / already called ? face tracking .
265
One of the main applications of face tracking is person retrieval from broadcast video , for example : intelligent fast-forwards " , where the video jumps to the next scene containing a certain person / actor ; or retrieval of different TV interventions , e.g. interviews , shows , etc. , of a given person in a video or a large collection of TV broadcast videos .
One of the main applications for face tracking is in the person retrieval from broadcast video , for example : " Intelligent fast-forwards�E, where the video jumps to the next scene containing a certain person / actor ; or retrieval of different TV interventions , e.g. interviews , shows , etc. , of a given person in a video or a large collection of TV broadcast videos .
266
In [5] , a person retrieval system for feature-length movie video is proposed using straightforward face tracking .
In [5] , the person retrieval system for a feature-length movie video is proposed using straightforward face tracking .
267
At run time a user outlines a face in a frame of the video , and the face tracks within the movie are then ranked according to the similarity to the outlined query face in the manner of Google .
At run time a user outlines a face in a video frame , and the face tracks within the movie are then ranked according to their similarity to the outlined query face in the same way as Google .
268
Face tracking also finds applications in the area of face-name association , the objective of which is to label television or movie footage with the identity of the person present in each frame of the video .
Face tracking is also used in the area of face-name association , the objective of which is to label television or movie footage with the identity of the person present in each frame of the video .
269
This system uses a face tracker similar with [5] to extract a few hundred tracks of a particular character each in a single shot .
This system uses a face tracker similar to the one in [5] that can extract a few hundred tracks of each particular character in a single shot .
270
Based on the temporal information obtained from the face tracker , textual information for TV and movie footage including subtitles and transcripts is employed to assign the character 's name to each face track .
Based on the temporal information obtained from the face tracker , the textual information for TV and the movie footage including the subtitles and transcripts is employed to assign the character 's name to each face track .
271
Can the system run in real time ? Similar with many other processing tools for broadcast video , speed is not the most critical issue because offline processing is permitted in most cases of video structuring and indexing .
Can the system run in real time ? Similar to many other processing tools for broadcast video , speed is not the most critical issue because offline processing is permitted in most video structuring and indexing cases .
272
Can the system cope with varying illumination , facial expression , scale , pose , camerawork , occlusion and large head motion ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who are moving from indoor to outdoor environment .
Can the system cope with varying illuminations , facial expressions , scales , poses , camerawork , occlusion , and large head motions ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who is moving from an indoor to an outdoor environment .
273
Small face scale always leads to low resolution and will reject most face trackers designed by computer vision researchers .
A smaller face scale always leads to a lower resolution and will reject most face trackers designed by computer vision researchers .
274
Pose variation , i.e. head rotations including pitch , roll and yaw , is another influencing factor , which can cause disappearance of part of the face .
Pose variations , i.e. head rotations including the pitch , roll , and yaw , is another influencing factor , which can cause disappearances of parts of faces .
275
Moreover , the task of face tracking becomes even more difficult when the head are moving fast relative to the frame rate so that the tracker fails to arrive in time " .
Moreover , the task of face tracking becomes even more difficult when the head is moving fast relative to the frame rate , so that the tracker fails to �arrive in time�E.
276
Lowering the threshold of the face detector reduces false rejections but increases the number of false detections , and vice versa .
Lowering the threshold of the face detector reduces the number of false rejections , but increases the number of false detections , and vice versa .
277
For each tracked face , three steps are involved that are initialization , tracking and a stopping procedure , as illustrated in Figure 2 .
For each tracked face , three steps are involved , which are the initialization , tracking , and stopping procedures , as illustrated in Fig. 2 .
278
The exploitation of color is one of the common choices in order to be invariant to facial expression , scale and pose change [4 , 9] .
The exploitation of color is one of the more common choices in order to be invariant to facial expressions , scale , and pose changes [4 , 9] .
279
Most facial-feature-based face trackers [6 , 10] are only tested by using non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .
Most facial-feature-based face trackers [6 , 10] have been tested using only non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .
280
Another example is proposed by Li et al [9] , which uses a multi-view face detector to detect and track faces of different poses .
Another example was proposed by Li et al. [9] , which uses a multi-view face detector to detect and track faces from different poses .
281
It is based on the idea that head can be considered as the object of interest instead of face because face is not always present in the tracking process .
It is based on the idea that a head can be considered an object of interest instead of a face , because the face is not always present in the tracking process .
282
Based on the assumption that face can be considered as a planar object , the corresponding motion model can be a 2D transformation , e.g. affine transformation or homography , of an image of the face , e.g. the initial frame [3 , 6] .
Based on the assumption that a face can be considered a planar object , the corresponding motion model can be a 2D transformation , e.g. affine transformation or homography , of an image of the face , e.g. the initial frame [3 , 6] .
283
Some system try to model face in this sense , and the image of deformable faces can be covered with a mesh , i.e. a sophisticated geometry and texture face model [2 , 7] .
Some systems try to model faces in this sense , and the image of deformed face can be covered with a mesh , i.e. a sophisticated geometry and texture face model [2 , 7] .
284
Generally if the quality of the video is high , more sophisticated motion model is used , more accurate result the face tracker generates .
Generally if the quality of the video is high , a more sophisticated motion model is used , and then the face tracker generates a more accurate result .
285
For instance , a sophisticated geometry and texture model might suffer from false face detections and drifting less than a simple 2D transformation model .
For instance , a sophisticated geometry and texture model might suffer from false face detections and a level of drifting [less than / that is worse than ?] a simple 2D transformation model .
286
But note that most 3D-based and mesh-based face trackers require relatively clear appearance , high resolution , and limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
However , it must be noted that most 3D-based and mesh-based face trackers require a relatively clear appearance , high resolution , and a limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .
287
Besides , most mesh-based trackers and top-down trackers are considered to be able to avoid drifting .
In addition , most mesh-based and top-down trackers are assumed to be able to avoid drifting .
288
However , while most attempts have been made on face tracking for videos with high quality by computer vision researchers , only a limited number of face trackers are designed for broadcast video .
However , while most of the attempts have been on the face tracking for high-quality videos by computer vision researchers , only a limited number of face trackers are designed for broadcast video .
289
We propose a method to retrieve relevant faces for one person by learning the visual consistency among results retrieved from text-correlation-based search engines .
We propose a method for retrieving relevant faces of one person by learning the visual consistency among results retrieved from text-correlation-based search engines .
290
This score is used to form a ranked list , in which faces having high density scores are considered relevant and are put at the top of the list .
This score is used to form a ranked list , in which faces with high-density scores are considered relevant and are put at the top .
291
A graph-based approach was proposed by \CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
A graph-based approach was proposed by Ozkan and Duygu \CITE , in which a graph is formed from faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .
292
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and therefore can be solved by taking an available solution .[It might be unclear as to what " available solution " you are talking about .
Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and can therefore , be solved by taking an available solution . //It might be unclear as to what " available solution " you are talking about . You might want to give more detail here .
293
You might want to give more detail here .] Although , experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of relevant faces of the queried person and it is easy to extend for the ranking problem .
Although experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of the relevant faces of the queried person and it is easy to extend for the ranking problem .
294
Furthermore , they perform hard categorization on input images that is [It is not clear if " hard categorization " is inapplicable or if the " input images " are inapplicable .]in applicable for re-ranking .
Furthermore , they are used for performing hard categorization on input images that are inapplicable for re-ranking . //It is not clear if " hard categorization " is inapplicable or if the " input images " are inapplicable .
295
This leads the number of collected images is reduced .
This leads to the reduction in the number of collected images .
296
However , different from the methods in \cite{xx} , we used an unsupervised method to select training samples automaticallyCITE .
However , we used an unsupervised method to select training samples automatically , which is different from the methods proposed by Fergus et al. and Li et al. \CITE .
297
This unsupervised method is different from the one in \CITE in its way of modeling the distribution of relevant images .
This unsupervised method is different from the one by Ozkan and Dugyu \CITE in the modeling of the distribution of relevant images .
298
-Step 4 : Improve this ranked list by Rank-By-Bagging-ProbSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet . You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
-Step 4 : Improve this ranked list using rank-by-bagging-probSVM . //I found not hits for " rank-by-bagging-probSVM " on the Internet. You might want to double check to see if this is a standard term . The same is true for " rank-by-local-density score " . If this is your own term , you might want to specify this at some point .
299
However , to get ideal clustering result is impossible , since these faces are high dimensional data and the clusters are in different shapes , sizes and densities .
However , to obtain ideal clustering results is impossible since these faces are high dimensional data and the clusters are in different shapes , sizes , and densities .
300
Instead , in \cite{xx} , a graph based approach was proposed CITEin which the nodes are faces and edge weights are the similarities between two faces .
Instead , a graph-based approach was proposed by Ozkan and Dugyu \CITE in which the nodes are faces and edge weights are the similarities between two faces .
301
We use the idea of density-based clustering described in \CITE to solve this problem .
We use the idea of density-based clustering described by Ester et al. and Breunig et al. \CITE to solve this problem . //idea / concept?
302
One limitation of the local density score based ranking is it could not handle the case that faces of another person have strong association in \MATH-neighbor set ( for example , many duplicates ) .
One limitation of the local density score based ranking is it cannot handle faces of another person strongly associated in the \MATH-neighbor set ( for example , many duplicates ) .
303
Since the maximum value of \MATH is \MATH where \MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :
Since the maximum value of \MATH is \MATH , where \MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :
304
Only frontal faces were considered since current frontal face detection systems \CITE can work in real time and have accuracies exceeding 95\% .
Only the front of faces were considered since current frontal face detection systems \CITE work in real time and have accuracies exceeding 95\% .
305
The eye detector , built with the same approach as in \CITE , had an accuracy of more than 95\% .
The eye detector , built with the same approach as that of Viola and jones \CITE , had an accuracy of more than 95\% .
306
Precision and recall only evaluate the quality of an unordered set of retrieved faces .
Precision and recall are only used to evaluate the quality of an unordered set of retrieved faces .
307
Since we do not know the number of returned faces from text based search engines , we used another input value \MATH defined as the fraction of neighbors and estimated \MATH by the formula : \MATH , where \MATH is the number of returned faces .
Since we do not know the number of returned faces from text-based search engines , we used another input value \MATH , defined as the fraction of neighbors , and estimated \MATH by the formula : \MATH , where \MATH is the number of returned faces .
308
However , UEL-LDS improves the performance significantly even when the performance of LDS is poor .
However , UEL-LDS improves significantly even when the performance of LDS is poor .
309
Figure \REF shows an examples of top 50 faces ranked by the methods TBL , DBO , DSG and LDS .
Figure \REF shows an examples of the top 50 faces ranked using the TBL , DBO , DSG , and LDS methods .
310
We conducted another experiment to show the effectiveness of our approach in which learned models can be used to annotate new faces of other databases .
We conducted another experiment to show the effectiveness of our approach in which learned models are used to annotate new faces of other databases .
311
For each name in the list , we used it as the query to obtain top 500 images from Google Image Search Engine .
We used each name in the list as a query to obtain the top 500 images from the Google Image Search Engine ( GoogleSE ) .
312
There were 4 ,103 faces ( including false positives - non-faces were detected as faces ) detected from 7 ,500 returned images .
There were 4 ,103 faces ( including false positives - non-faces detected as faces ) detected from 7 ,500 returned images .
313
On average , the accuracy of the Google Search Engine ( GoogleSE ) is 57 .08\% .
On average , the accuracy of the GoogleSE is 57 .08\% .
314
We evaluated the performance by calculating the precision at top 20 returned faces , which is popular for image search engines ; and recall and precision on all detected faces of the test set .
We evaluated the performance by calculating the precision of the top 20 returned faces , which is common for image search engines and recall and precision on all detected faces of the test set .
315
The precision at top 20 of SVM-SUP-05 is poorer than that of UEL-LDS is due to small number of training samples .
The precision of the top 20 of SVM-SUP-05 is poorer than that of UEL-LDS due to the small number of training samples .
316
In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely relevant or irrelevant faces .
In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely to be relevant or irrelevant faces , respectively .
317
Our approach is beneficial in the case multiple faces residing in the returned image as shown in Figure \REF .
Our approach is beneficial when there are several faces in a returned image , as shown in Figure \REF .
318
In this paper , we present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing .
We present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing . //detecting / determining?
319
By the formulation that each frame is considered as a word and shot boundaries are treated as boundaries of text segments ( e .g topics ) .
This is possible by assuming that each frame is a word and then the shot boundaries are treated as text segment boundaries ( e.g. topics ) .
320
Text segmentation based approaches that have been well studied in natural language processing can be adopted .
The text segmentation based approaches in natural language processing can be used .
321
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as continuous frames from a single camera at a time .
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .
322
By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
323
A fade is usually a change in brightness with one or several solid black frames in between , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
A fade is usually a change in brightness with one or several solid black frames in between the key frames , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
324
Since these approaches use threshold-based models for detection , their advantage is fast speed .
Since these approaches use threshold-based models for detection , their advantage is they are fast .
325
Recent works \CITE use machine learning methods for making decision and show impressive results on test videos of TRECVID \CITE which is a de-facto benchmark for evaluation of various techniques in shot boundary detection .
Recent works \CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .
326
In this study , we propose a new approach inspired from natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem of text segmentation .
In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .
327
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of labels such as
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of the following labels :
328
Given a video , the shot boundary detection process is carried out through two main stages .
The shot boundary detection process for a given video is carried out through two main stages .
329
We use the following six labels to label frames in video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , POST -GRAD ( post-frame of a GRADUAL transition ) .
We use the following six labels to label frames in a video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , and POST -GRAD ( post-frame of a GRADUAL transition ) .
330
To label a frame in video , firstly we extract features for that frame and then use a classifier , that has been trained by annotated frames in advance , to classify it into one of six categories mentioned above .
To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .
331
We use two typical features that are color moments , edge direction histogram for representing visual information of each frame .
We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .
332
By this way , we can have a unified framework for shot boundary detection and consequently avoid to have special treatments for different shot boundary types as described in many works participated the TRECVID benchmark \CITE .
In this way , we can have a unified framework for the shot boundary detection and consequently avoid having to give special treatment to the different shot boundary types as described in many of the works that participated the TRECVID benchmark \CITE .
333
where \MATH is the d-dimensional vector of an observation example , \MATH is a class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
where \MATH is the d-dimensional vector of an observation example , \MATH is the class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
334
To handle the case of multi-class classification , there are two common approaches .
There are two common approaches for handling multi-class classification .
335
To learn this classifier , we manually annotate frames in the training data .
To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?
336
A gradual transition usually has the pattern " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' and a cut transition usually has the pattern " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " ' .
A gradual transition usually has a " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' pattern and a cut transition usually has a " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " 'pattern .
337
Since the classifier occasionally produce false predictions due to variations caused by photo flashes , rapid camera movement and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many truth shot boundaries .
Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .
338
where \MATH is the \MATH-th element of the feature vectors \MATH respectively , \MATH is the number of dimensions .
where \MATH is the \MATH-th element of the feature vectors \MATH , respectively , and \MATH is the number of dimensions .
339
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take the \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
340
The results that were evaluated by a tool provided by TRECVID with standard measurement such as precision , recall and F1 score clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
The results that were evaluated by a tool provided by TRECVID with a standard measurements , such as the precision , recall , and F1 score , clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
341
As shown in Figure \REF , the best performance is obtained with the sampling rate of \MATH .
As shown in Figure \REF , the best performance was obtained at a sampling rate of \MATH .
342
In Table \REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
343
The first one is GCM , the second one is EOH and the last one GCM+EOH is combination of distances using GCM and distances using EOH .
The first one is GCM , the second one is EOH , and the last one GCM+EOH is a combination of the distances using GCM and the distances using EOH .
344
Many previous shot boundary detectors usually divided the system into sub-systems in which special treatments were proposed to handle different types of shot transitions .
Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .
345
Therefore , it is difficult to generalize for new test sets .
Therefore , it is generalization is difficult for new test sets .
346
Different from these approaches , in this paper , we have proposed a unified and general framework for shot boundary detection using a text segmentation based approach .
We have proposed a unified and general framework for shot boundary detection that uses a text segmentation based approach .
347
Recently , boosting is used widely in object detection applications because of its impressive performance in both speed and accuracy .
Recently , boosting has come to be used widely in object detection applications because of its impressive performance in both speed and accuracy .
348
This paper describes a novel method for efficiently learning weak classifiers using entropy measures , called Ent-Boost .
We have developed Ent-Boost , a novel method for efficiently learning weak classifiers using entropy measures . //method / boosting scheme?
349
Experiments have shown that strong classifiers learned by Ent-Boost can achieve good performance , and have compact storage space .
Experiments showed that strong classifiers learned by Ent-Boost can achieve good performance and be stored compactly . //[be stored compactly / achieve compact storage?]
350
Results on building a robust face detector are also reported .
The results of building a robust face detector using Ent-Boost showed the boosting scheme to be effective .
351
Boosting [3] and its variants [4] ,[5] ,[6] ,[7] ,[8] ,[9] ,[10] have recently gained a lot of attentions from researchers because of its excellent performance .
Boosting [3] and its variants [4] ? [10] have recently gained much attention from researchers because of their excellent performance .
352
In regards to face detection , for example , the methods described in works [4] ,[5] ,[10] represent the state of the art in terms of both high accuracy and running speed .
In regard to face detection , for example , the methods described in [4] , [5] , and [10] are state-of-the-art in terms of both accuracy and running speed .
353
Generally , for efficient computation , the dimension of the input space of weak classifiers is reduced to much lower than that of the strong classifier .
Generally , for efficient computation , the dimensions of the input space of weak classifiers are reduced be to much smaller than those of the strong classifier[s?] .
354
Typically , most current works [5] ,[17] ,[6] ,[8] ,[10] split the data into \MATH bins that are equal width which suffers from following limitations :
Typically , most current works [5] , [6] , [8] , [10] , [17] split the data into \MATH bins that are equal in width . This method suffers from the following limitations : //[works / systems?]
355
Normally , it has been done by trials and errors [6] ,[17] - a tedious task .
Normally , it has been done by trial and error [6] , [17] ? a tedious task .
356
In the training cascade of classifiers [6] ,[17] , when the complexity of the training data changes over time , using the same number of bins for training every layers is not optimal .
In the training cascade of classifiers [6] , [17] , when the complexity of the training data changes over time , using the same number of bins for training every layer is not optimal .
357
Furthermore it might increase computation and training time , waste storage space which is critical in applications with limited resources , for example , face detection on mobile phones .
Furthermore , it might lengthen computation and training time and waste storage space , which is critical in applications with limited resources , for example , face detection on mobile phones .
358
Meanwhile choosing a small number of bins might not well approximate the real densities of the data distribution and thus influence selection of the best weak classifier .
Choosing a small number of bins , however , might not accurately approximate the real densities of the data distribution and could influence the selection of the best weak classifier .
359
It is therefore necessary to have a deterministic method to choose this number of bins automatically and optimally .
A deterministic method is therefore needed to automatically and optimally choose the number of bins .
360
Furthermore , many studies have been shown that discretization process might help to improve performance in induction tasks [18] , it can also work with a weighted data distribution ; therefore , it is most appropriate for boosting-based methods .
Furthermore , many studies have shown that the discretization process might help to improve performance in induction tasks [18] and it can also work with a weighted data distribution . Therefore , it is most appropriate for boosting-based methods .
361
Besides learning weak classifiers , selecting the best weak classifier in the large weak classifier set in each round of boosting is also important .
Besides learning weak classifiers , selecting the best weak classifier in the large set of weak classifiers in each round of boosting is also important .
362
Adopting [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples .
Following the method used in [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples . // [used / proposed?]
363
Originally , Discrete AdaBoost proposed by Freund and Schapire [16] is a learning method of combining weak classifiers to a strong classier .
Originally , Discrete AdaBoost , proposed by Freund and Schapire [16] , was a learning method of combining weak classifiers to form a strong classier .
364
Given a training set \MATH where \MATH and \MATH , a weak classifier \MATH has the form \MATH .
Given a training set \MATH , where \MATH and \MATH , a weak classifier \MATH has the form \MATH .
365
Therefore , in many applications [4] ,[5] ,[7] , it is simplified by associating to one feature \MATH .
Therefore , in many applications [4] , [5] , [7] , it is simplified by associating with one feature \MATH .
366
This method also proposes designing weak classifiers that partition the input space into subspaces so that its predictions are unique in each subspace .
Do you mean that the creators of this system proposed this?] designing weak classifiers that partition the input space into subspaces so that the predictions are unique in each subspace .
367
In such face detection systems as [5] ,[6] ,[17] ,[8] , weak classifiers are usually associated with one feature .
In such face detection systems as [those described in?] [5] , [6] , [8] , and [17] , weak classifiers are usually associated with one feature .
368
With a very large number of available features , hundreds of thousands , there are a lot of choices to choose one weak classifier for each round of boosting .
With a very large number of available features ? hundreds of thousands ? [there are many candidates from which to / many choices must be made to?] select one weak classifier for each round of boosting .
369
Many measurements have been proposed ; for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] and , recently , Jensen-Shannon divergence [8] and mutual information [9] ( cf . Table 1 .
Many measurements have been proposed , for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] , and recently , Jensen-Shannon divergence [8] and mutual information [9] ( Table 1 ) .
370
However , it will be shown in section 5 , these performances are affected seriously if different subspace splitting methods are used .
However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .
371
However , instead of using equal-width binning method like Real AdaBoost [6] ,[17] which is hard to know the suitable number of bins in advance , we use entropy-based discretization method [19] to split the input space into subspaces .
However , instead of using the equal-width binning method used in Real AdaBoost [6] , [17] which has a hard time predicting the suitable number of bins in advance , we use the entropy-based discretization method [19] to split the input space into subspaces .
372
This subspace splitting process is totally automatically in which the stopping criteria of splitting process is determined through using Minimum Description Length Principles ( MDLP ) ( see the next section ) .
This subspace splitting process is totally automatic ; the stopping criteria of the splitting process are determined using minimum description length principles ( MDLP ) . This process will be described in greater detail in the next section .
373
A higher arity can make the understanding of an attribute more difficult , while a very low arity may affect predictive accuracy negatively .
A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .
374
Given set S and a potential binary partition , \MATH , specified on S by the given cut-point \MATH , a stopping criteria is used to decide whether or not this partition should be accepted .
Given set S and a potential binary partition \MATH , specified on S by the given cut-point \MATH , a stopping criteria is used to decide whether or not this partition should be accepted .
375
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to send before partition is more than the length of the message required to send after partition .
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to be sent before the partition is more than the length of the message required to be sent after the partition .
376
Haar wavelet feature that has been widely used in many face detection systems [4] ,[6] ,[14] is used in our experiments .
Haar wavelet features , which have been used in many face detection systems [4] , [6] , [14] , were used in our experiments .
377
The feature value is defined as the difference of sum of the pixels within rectangles ( cf . Figure 3 ) .
The feature value was defined as the difference of the sum of the pixels within rectangles ( Figure 3 ) .
378
It was a cascade of Ent-Boost based classifiers that were trained similar to [4] .
It was a cascade of Ent-Boost-based classifiers that were trained [through a process similar to that used in] [4] .
379
Performances of AdaBoost-based face detector [4] and Ent-Boost based face detector on MIT+CMU test set [1] shown in Table 2 has confirmed the effectiveness of our proposed boosting scheme .
The performances of the AdaBoost-based face detector [4] and our Ent-Boost-based face detector on the MIT+CMU test set [1] confirmed the effectiveness of our proposed boosting scheme ( Table 2 ) .
380
Furthermore , it overcomes the main limitation of Real AdaBoost which is hard to determine the suitable number of bins for subspace splitting .
Furthermore , this new boosting scheme overcomes the main limitation of Real AdaBoost , which is difficulty in determining the suitable number of bins for subspace splitting .
381
This paper describes an efficient feature selection method which quickly selects a small subset out of a given huge feature set for building robust object detection systems .
This paper describes an efficient feature selection method which that quickly selects a small subset out of a given huge feature set ; the proposed method for will be useful for building robust object detection systems .
382
As a result , the selected feature set only contains highly informative and non-redundant features which when combined together , significantly improve classification performance .
As a result , the selected feature set only contains only highly informative and non-redundant features , which significantly improve classification performance when combined together , significantly improve classification performance .
383
It is significant due to the following three reasons .
Improving the method of accomplishing this task is important due to the following three reasons .
384
First , there are many ways to represent a target object , leading to a huge feature set .
First , there are many ways can be used to represent a target object , and this variety leadsleading to a huge feature set .
385
Furthermore , less complex model is easier to understand and verify .
Furthermore , less complex models is are easier to understand and verify .
386
The filter-based approach is independent of any induction algorithm while the wrapper-based approach is associated with a specific induction algorithm to evaluate the goodness of the selected feature subset .
The filter-based approach is independent of any induction algorithm , while but the wrapper-based approach is associated with a specific induction algorithm to evaluate the quality of the selected feature subset . //[goodness / quality / appropriateness?<--If " goodness " is the word you would usually use in your field for this , it is fine , but I would suggest a different word choice otherwise . " Goodness " seems vague , so in what sense do you mean " good " ?]
387
However , the mutual relationships between features is often not taken into account , leading selected features might be highly redundant and less informative because two features with high individual predict power when combined together might not bring significant performance improvement compared with two features which one of them has low predictive power but is useful when combined with others .
However , because the mutual relationships between features is are often not taken into account , leading the selected features might be highly redundant and less informative because two features with high individual predictive power , when combined together , might not bring significant performance improvement . Combining compared with two features of which one of them has low predictive power but is useful when combined with others would thus be more effective for improving performance .
388
Since wrapper-based feature selection methods use machine learning algorithms as a black box in selection process , they can suffer from over-fitting in situations of small training sets .
Since wrapper-based feature selection methods use machine learning algorithms as a black box in the selection process , they can suffer from over-fitting in situations of when applied to small training sets . //[when used with / when applied to?]
389
Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands features , using wrapper-based methods is obviously inefficient because of very high computation cost .
Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands of features , so using wrapper-based methods is obviously inefficient because of the very high computation costs they incur .
390
For example , in the state of the art face detection system [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by AdaBoost has taken several weeks .
For example , in the state- of- the- art face detection system in [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by using AdaBoost has takentook several weeks . //[by using / generated by?]
391
Consequently , conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of above approaches for handling large scale feature sets .
Consequently , feature selection methods based on conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of the above approaches for handling large scale feature sets .
392
It does not select a feature similar to already selected ones , even if it is individual powerful , as selecting it might not increase much information about the target class [7] .
It doesThese methods do not select a feature similar to ones already selected ones , even if itthe feature is individually powerful , as because selecting it might not do much to increase much information about the target class [7] .
393
In [9] , Kwak and Choi used Parzen windows based density estimation method in which many parameters such as kernel function and window width are complicated to determine .
In [9] , Kwak and Choi used a Parzen windows -based density estimation method in which many parameters such as kernel function and window width are complicated to determine .
394
It is better if multiple thresholds are used to discretize data .
Using multiple thresholds to discretize data is better than using a binary approach .
395
Experiments show that the proposed method can well handle huge feature sets for face detection such as Haar wavelets [1] and Gabor wavelets [12] , significantly reduce the training time while maintaining high classification performance .
Experiments show that the proposed method can well capably handle huge feature sets of data such as Haar wavelets [1] and Gabor wavelets [12] for face detection , significantly reducinge the training time while maintaining high classification performance .
396
To measure relevance of a feature , the entropy-based measure which quantifies the uncertainty of random variables is normally used .
To measure the relevance of a feature , an entropy-based measure , which quantifies the uncertainty of random variables , is normally used .
397
In the first step , the most relevant feature F1 which has the highest mutual information is selected .
In the first step , the most relevant feature F1 , which has the highest largest amount of mutual information , is selected .
398
However , in the second step , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .
However , iIn the second step , however , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .
399
Therefore , F2 is selected so that maximizing :\MATH .
Therefore , F2 is selected so that maximizingas to maximize the information it can add :\MATH .
400
In order to simply estimate mutual information , the easiest way is features are discretized in binary values by specifying thresholds [8 , 7] .
To simply estimate mutual information , the easiest way is to discretize features are discretized in binary values by specifying thresholds [8 , 7] .
401
Suppose that we are given a set of instances S , a feature A and a cut-point T ( a cutpoint is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold ) .
Suppose that we are given a set of instances S , a feature A , and a cut-point T . ( A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold . ) .
402
MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \MATH Where \MATH ,where \MATH , \MATH , \MATH is the number of classes in \MATH , \MATH , \MATH .
MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \MATH wWhere \MATH ,where \MATH , \MATH , and \MATH is are the numbers of classes in \MATH , \MATH , and \MATH , respectively .
403
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
404
Gabor wavelet features have also often been used in face recognition systems [12] and are defined as : \MATH where \MATH and \MATH define the orientation and scale of the Gabor kernels respectively , \MATH , and the wave vector \MATH , is defined as : \MATH where \MATH , \MATH \MATH .
Gabor wavelet features have also often been used often in face recognition systems [12] and are defined as : \MATH , where \MATH and \MATH define the orientation and scale of the Gabor kernels respectively , \MATH , and the wave vector \MATH , is defined as : \MATH where \MATH , \MATH and \MATH .
405
Similar to [12] , Gabor kernels at five scales \MATH and eight orientations \MATH were used .
Similar to [12] , Gabor kernels at five scales , \MATH , and eight orientations , \MATH , were used .
406
In order to show effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods that are forward feature selection ( FFS ) [16] and CMI-basedmethod using binary features ( CMIBinary ) [8 , 7] on the data set and feature setsmentioned above .
To prove the effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods ?that are forward feature selection ( FFS ) [16] and a CMI-based methods using binary features ( CMI-Binary ) [8 , 7] ? on the data set and feature sets mentioned described above .
407
We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results when not only reducing significantly the training time of AdaBoost-based face detection system [1] ( about 100 times ) but also maintaining comparable performance .
We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results , when not only reducing significantly the training time of the AdaBoost-based face detection systems [1] by ( about 100 times , ) but also maintaining comparable performance .
408
It indicates that , the proposed method CMI-Multi outperforms the others while FFS and CMI-Binary have comparable performance .
The figureIt indicates that , the proposed method , CMI-Multi , outperforms the others while the performances of FFS and CMI-Binary have were comparable performanceto one another .
409
The similar result is also shown when tested on Gabor wavelet features .
The A similar result is was also shown when the three feature selection methods were tested on Gabor wavelet features .
410
The estimation of mutual information is simplified by using MDLP based discretization method .
The estimation of mutual information is simplified by using an MDLP- based discretization method .
411
Experiments on two popular feature sets such as Haar wavelets and Gabor wavelets have demonstrated the effectiveness of the proposed method .
Experiments on two popular feature sets have demonstrated the effectiveness of the proposed method . //[Please note : I am not sure which of the following you mean .--> one composed of such as Haar wavelets and the other composed of Gabor wavelets / ? Haar wavelets and Gabor wavelets ?]
412
A multi-stage approach --- which is fast , robust and easy to train --- for a face-detection system is proposed .
A multi-stage approach that is fast , robust , and easy to train is proposed for a face-detection system .
413
First , a new stage is added to detect face candidate regions more quickly by using a larger window size and larger moving step size .
First , a new stage has been added to detect face candidate regions more quickly by using a larger window size and larger moving step size .
414
Second , SVM classifiers are used instead of AdaBoost classifiers in the last stage , and Haar wavelet features selected by the previous stage are reused for the SVM classifier robustly and efficiently .
Second , support vector machine ( SVM ) classifiers are used instead of AdaBoost classifiers in the last stage , and Haar wavelet features selected by the previous stage are reused for the SVM classifier robustly and efficiently .
415
The proposed multi-stage-based system is shown to run faster than the original AdaBoost-based system while maintaining comparable accuracy .
The proposed multi-stage-based system has been shown to run faster than the original AdaBoost-based system while maintaining comparable accuracy .
416
Recently , with advances in machine learning research , Neural Network [5] ,[6] , Support Vector Machines ( SVM ) [7] ,[8] ,[9] and AdaBoost [1] ,[10] ,[11] ,[12] ,[13] are typical choices for building robust face detectors .
Recently , with advances in machine learning research , neural networks [5] , [6] , support vector machines ( SVM ) [7] , [8] , [9] and AdaBoost [1] , [10] , [11] , [12] , [13] are typical choices for building robust face detectors .
417
Generally , to classify an input pattern of intensities as a face or non-face , features must be extracted and normalized before passing to a classifier [14] .
Generally , to classify an input pattern of intensities as a face or non-face , features must be extracted and normalized before passing [the image / the pattern / the results?] to a classifier [14] .
418
There are many kinds of features that have been used ranging from simple features such as intensity values [7] ,[5] and eigenspace [15] to complex features such as wavelets [16] ,[1] ,[12] , edge orientation histograms [17] ,[18] and Bayesian discriminating features ( BDF ) [19] .
Many kinds of features have been used , ranging from simple ones such as intensity values [7] , [5] and eigenspace [15] to complex ones such as wavelets [16] , [1] , [12] , edge orientation histograms [17] , [18] , and Bayesian discriminating features ( BDF ) [19] .
419
In a typical face detector which is scale-free and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .
In a typical face detector that is scale- and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .
420
To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers is proposed [8] ,[1] ,[9] ,[20] ,[21] ,[11] .
To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers has been proposed [8] , [1] , [9] , [20] , [21] , [11] .
421
In particular , fast and simple classifiers are used as filters at the earliest stages to quickly reject a large number of non-face patterns and a slower yet more accurate classifier is then used for classifying face-like patterns .
In particular , fast and simple classifiers are [recommended to be?] used as filters at the earliest stages to quickly reject a large number of non-face patterns and a slower yet more accurate classifier is then recommended to be used for classifying face-like patterns .
422
By this way , the complexity of classifiers is adapted corresponding to the difficulty in the input patterns .
In this way , the complexity of classifiers can be adapted corresponding to the difficulty in the input patterns . / / [is / can be?]
423
In [8] , non linear SVM classifiers using pixel-based features are arranged into a sequence with increasing number of support vectors , or in [9] , linear SVM classifiers trained at different resolutions are used for rejection and a reduced set of principle component analysis ( PCA )-based features are used with the non linear SVM at the classification stage in order to reduce computation time .
In [8] , nonlinear SVM classifiers using pixel-based features were arranged into a sequence by increasing the number of support vectors , while in [9] , linear SVM classifiers trained at different resolutions were used for rejection and a reduced set of principle component analysis ( PCA )-based features were used with the nonlinear SVM at the classification stage in order to reduce computation time .
424
In [1] , AdaBoost based classifiers are arranged in a degeneration decision tree or a cascade .
In [1] , AdaBoost-based classifiers were arranged in a degeneration decision tree or a cascade .
425
-Firstly , the cascaded structure of simple-to-complex classifiers reduces computation time dramatically ( as mentioned above ) .
-The cascaded structure of simple-to-complex classifiers reduces computation time dramatically .
426
-Thirdly , Haar-wavelet features used for all stages are informative [22] and evaluated extremely fast due to the introduction of the integral image .
-Haar wavelet features used for all stages are informative [22] and can be evaluated extremely quickly due to the introduction of the integral image .
427
With the first several layers in our experiment ( cf. Figure 1 ) , using some 800 weak classifiers , more than \MATH of non-face patterns are rejected .
With the first several layers in our experiment ( cf . Figure 1 ) , using some 800 weak classifiers , more than \MATH of non-face patterns were rejected .
428
However , turning the later layers into robustly classifying a smaller number of remaining patterns , it requires a lot more , e.g. , 5 ,660 , weak classifiers , thus making the training task much more complicated .
However , enabling the later layers to robustly classify a smaller number of remaining patterns requires many more weak classifiers ( around 5 ,660 ) , thus making the training task much more complicated .
429
Firstly , it requires a long training time because the training time is proportional to the number of features in the input feature set ( which is normally hundreds of thousands ) and the number of training samples ( which is generally tens of thousands ) .
It requires a long time because the training time is proportional to the number of features in the input feature set ( which is normally hundreds of thousands ) and the number of training samples ( which is generally tens of thousands ) .
430
Secondly , AdaBoost-based classifiers are constructed by adding features after each round of boosting , so several training parameters must be tuned manually while training .
Another thing that complicates the training process is that AdaBoost-based classifiers are constructed by adding features after each round of boosting , so several training parameters must be tuned manually while training .
431
-First , to detect the face candidate regions , a new stage ( using a larger window size and a larger moving step size ) is added .
-First , to detect the face candidate regions , a new stage ( using a larger window size and a larger moving step size ) has been added .
432
Second , how to efficiently reuse the features selected by AdaBoost in the previous stage , for the SVM classifiers of the last stage , is investigated .
Second , we have investigated how to efficiently reuse the features selected by AdaBoost in the previous stage for the SVM classifiers of the last stage .
433
Furthermore , it is unnecessary to re-evaluate these features because they have been previously evaluated .
Furthermore , these features do not need to be re-evaluated because they have already been evaluated .
434
( ii ) By using SVM classifiers with powerful generalization , using too many features in the cascade is avoided , therefore importantly training time is saved and over-fitting is avoided .
( ii ) By using SVM classifiers with powerful generalization , using too many features in the cascade is avoided , with the important results of saving training time and avoiding over-fitting .
435
Third , the training time of AdaBoost classifiers is shortened by using simple sampling techniques to reduce the number of features in the feature set .
Third , the training time of AdaBoost classifiers has been shortened by using simple sampling techniques to reduce the number of features in the feature set .
436
Experiments will show that for rejection , using a full feature set and a sampled feature set gives the comparable performance .
Experiments showed that for rejection , the performance gained by using a sampled feature set was comparable to that of a full feature set .
437
Along with using several SVM classifiers instead of many AdaBoost classifiers in later layers , the total training time is reduced significantly .
Along with using several SVM classifiers instead of many AdaBoost classifiers in later layers , the total training time has been significantly reduced .
438
There have been several studies working on how to handle the drawbacks of Viola and Jones' system .
Several studies have worked on addressing the drawbacks of Viola and Jones' system .
439
It is therefore very time consuming because all weak classifiers must be trained every time one feature is selected .
This process is very time consuming because all weak classifiers must be trained every time one feature is selected .
440
With their new proposal , weak classifiers are trained only once and features are selected by the direct feature selection method that directly maximizes the learning objective of the output classifier .
With the new proposal of Wu et al. , weak classifiers are trained only once and features are selected by the direct feature selection method , which directly maximizes the learning objective of the output classifier .
441
Xiao et al. [20] and Huang et al. [11] propose the boosting chain structure in which subsequent layers utilize historical information of previous layers .
Xiao et al. [20] and Huang et al. [11] proposed a boosting chain structure in which subsequent layers utilize the historical information of the previous layers .
442
Studies based on RealBoost [26] , such as [12] ,[10] ,[27] ,[11] , introduced new kinds of weak classifiers that are stronger than binary weak classifiers .
Studies based on RealBoost [26] , such as [12] , [10] , [27] , and [11] , introduced new kinds of weak classifiers that are stronger than binary weak classifiers .
443
Small number of bins might not well approximate the real distribution while large number of bins might cause over-fitting , increase computation time and waste storage space .
A small number of bins might not accurately approximate the real distribution , while a large number of bins might cause over-fitting , increase computation time , and waste storage space .
444
Actually , our system can benefit from this approach when building the rejection stage and thus also reduce the training time much more .
However , our system can benefit from this approach when building the rejection stage and can thus reduce the training time even further .
445
In our experiments , only 100 features are used and hence it is faster than using any pixel-based SVM classifiers [8] ,[9] .
In our experiments , only 100 features were used , making classification faster than it would have been using pixel-based SVM classifiers [8] , [9] .
446
The same feature set as proposed in [1] is used ( cf. Figure 4 ) .
The same feature set proposed in [1] was used ( cf . Figure 4 ) .
447
Each feature is parameterized by four parameters : the position within the window \MATH , width \MATH and height \MATH ( cf. Figure 5 ) .
Each feature is parameterized by four parameters : the position within the window \MATH , the width \MATH , and the height \MATH ( cf . Figure 5 ) .
448
By using integral image definition [1] , these rectangle feature values can be computed extremely quickly .
By using integral image definition [1] , the feature values of these rectangles can be computed extremely quickly .
449
Each weak classifier \MATH is associated with a feature \MATH and a threshold \MATH such that the number of incorrect classified examples corresponding to this weak classifier is minimized : \MATH , where polarity \MATH indicates the direction of the inequality sign .
Each weak classifier \MATH is associated with a feature \MATH and a threshold \MATH such that the number of incorrectly classified examples corresponding to the weak classifier is minimized : \MATH , where polarity \MATH indicates the direction of the inequality sign .
450
The error of each weak classifier is measured with respect to the set of weights over each example of the training set \MATH , where \MATH and \MATH are the weight and the label of the training example \MATH , respectively .
The error of each weak classifier is measured with respect to the set of weights over each example of the training set \MATH , where \MATH and \MATH are the respective weight and label of the training example \MATH .
451
1 .	<section label= " SVM Classifier " >
1 .	<section label= " SVM Classifier " >
452
To compare the performance of classifiers , we have implemented a fully cascade of classifiers trained by AdaBoost , similar to that used by Viola and Jones [1] .
To compare the performance of classifiers , we implemented a full cascade of classifiers trained by AdaBoost , similar to that used by Viola and Jones [1] .
453
The minimum of the detection rate is \MATH , the maximum of the false positive rate is \MATH and the maximum of the number of features in each layer is 200 .
The minimum of the detection rate was \MATH , the maximum of the false positive rate was \MATH , and the maximum of the number of features in each layer was 200 .
454
In the other hand , a feature set is parameterized by \MATH .
A feature set , on the other hand , is parameterized by \MATH .
455
The classifier 's threshold is changed to meet the detection rate of \MATH .
The classifier 's threshold was changed to meet the detection rate of \MATH .
456
Rejection performance is evaluated through the false positive rate on a validation test set which contains 500 ,000 non-face patterns .
Rejection performance was evaluated through the false positive rate on a validation test set that contains 500 ,000 non-face patterns .
457
Our another experiment has shown that , for similar performance , the AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than that trained on the full feature set .
Another experiment we conducted showed that , for similar performance , an AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than one trained on the full feature set . / / [Do you need a reference here , or is this still talking about the experiments you report in this paper?]
458
Since a 36x36 face sample contains a lot of background outside the 24x24 face region while the classifier is required to be fast and to keep all possible face regions , training parameters are set as follows : the minimum detection rate of \MATH and maximum of false positive rate of \MATH .
Since a 36x36 face sample contains a large proportion of background outside the 24x24 face region and the classifier is required to be fast and to keep all possible face regions , a minimum detection rate of \MATH and a maximum of false positive rate of \MATH were set as the training parameters .
459
In our experiments , after reaching 50 features , the classifier 's performance does not significantly increase anymore , so the maximum number of features for each layer is set to 50 .
In our experiments , after reaching 50 features , the classifier 's performance did not significantly increase , so the maximum number of features for each layer is set to 50 .
460
Two main issues surrounding the reuse of features selected by AdaBoost are : ( i ) which layer whose features will be reused for SVM is the best? and ( ii ) How many features should be used?
Two main issues surrounding the reuse of features selected by AdaBoost are : ( i ) which layer�fs features should be reused for SVM and ( ii ) how many features should be used .
461
For comparison of the performance of SVM classifiers , 2 ,450 face patterns and 7 ,500 non-face patterns which are separated from the training set ( section 6 .1 ) were used .
For comparison of the performance of SVM classifiers , 2 ,450 face patterns and 7 ,500 non-face patterns that were separated from the training set ( section 6 .1 ) were used .
462
The parameter \MATH is set to \MATH .
The parameter \MATH was set to \MATH .
463
To determine how many features is robust enough , we used the 200-feature set selected in layer 17 to generate different subsets of features with different number of features .
To determine the number of features is that would be sufficiently robust , we used the 200-feature set selected in layer 17 to generate different subsets of features with different numbers of features .
464
Basically , the speed of a SVM classifier is proportional to the number of features used , so the greater number of features used , the slower the classifier will be .
Basically , the speed of a SVM classifier is proportional to the number of features used , so the greater the number of features used , the slower the classifier will be .
465
The SVM classifier using 25 features run fastest while the SVM classifier using 200 features run slowest .
The SVM classifier using 25 features ran the fastest , while the SVM classifier using 200 features was the slowest .
466
The speeds of SVM classifiers using 100 , 125 and 175 features are not importantly different because their difference in terms of number of features and number of support vectors is inconsiderable .
The speeds of SVM classifiers using 100 , 125 , and 175 features were not importantly different because their difference in terms of number of features and number of support vectors were not large enough to have a significant impact .
467
The parameter \MATH is set to \MATH .
The parameter \MATH was set to \MATH .
468
These parameters are found by using cross-validation test .
These parameters were found by using a cross-validation test .
469
In the first stage , the cascaded 36x36 classifiers consist of three layers , making a total number of features used of 120 .
In the first stage , the cascaded 36x36 classifiers consist of three layers , making for a total of 120 features .
470
Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and , thus , can save significant training time ( which is approximate 27 times in total ) .
Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and can thus save significant training time ; the training time needed using the new system is approximately 27 times shorter / approximately 27 rounds of training are needed in the new system . / / <--I think that the first choice here is your intended meaning , but please check carefully .
471
The first row presents the number of features of each layer , and the second row shows the fraction of the remaining patterns after each layer processing .
The first row presents the number of features of each layer and the second row shows the fraction of the remaining patterns after each layer were processed . / / [fraction / percentage?<--Here and after , you use " percentage " in the graph , so you may want to keep the same term here .]
472
All these statistics are extracted from running the classifiers on the MIT+CMU test set .
All these statistics were extracted by running the classifiers on the MIT+CMU test set .
473
If the first 24x24 layer classifier is added to the cascade of 36x36 classifiers , this combination rejects 85 .91\% of analyzed patterns compared to \MATH of using only the first layer of the single cascade 24x24 classifiers .
When the first 24x24 layer classifier was added to the cascade of 36x36 classifiers , this combination rejected 85 .91\% of analyzed patterns compared to \MATH of using only the first layer of the single cascade of 24x24 classifiers .
474
Furthermore , the rejection of this very large number of patterns is done extremely quickly , only using \MATH of processing time .
Furthermore , the rejection of this very large number of patterns was done extremely quickly , only using \MATH of the total processing time . / / [the total / the standard?]
475
Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers do not affect so much in final performance because it might also be rejected by 24x24 classifiers in later layers .
Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers did not severely affect the final performance because they might also be rejected by 24x24 classifiers in later layers .
476
The cascaded structure of AdaBoost-based classifiers in two first stages allows to best adapt to various complexities of input patterns ,while non linear SVM classifiers at the final stage are robust enough to achieve good results .
The cascaded structure of AdaBoost-based classifiers in the two first stages allows the system to best adapt to various complexities of input patterns , while nonlinear SVM classifiers at the final stage are robust enough to achieve good results .
477
Extensive experiments demonstrated that a significant computation time is devoted to potential face regions because almost all non-face patterns are rejected quickly by the two first stages , and only a very small number of face-like patterns is processed by the slow SVM classifiers .
Extensive experiments demonstrated that a significant computation time is devoted to potential face regions because almost all non-face patterns are rejected quickly by the two first stages , and only a very small number of face-like patterns are processed by the slow SVM classifiers . / / [are / need to be?]
478
to improve the retrieval performance of image search engines that use textual information for indexing , it is necessary to utilize visual information .
It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing .
479
Most of the state of the art methods for learning the visual consistency usually learn one specific classifier for each query for re-ranking the returned images .
Most state-of-the-art methods of learning visual consistency usually learn one specific classifier for each query to re-rank the returned images .
480
The drawback of these methods is it requires computational cost and processing time that are unsuitable for handling a large number of queries .
The main drawback with these methods is that they require computational cost and processing time that are unsuitable for handling a large number of queries .
481
The generic classifier is built automatically and independent with existing ranking algorithms of input search engines .
The generic classifier is built automatically and is independent of existing ranking algorithms for input search engines .
482
Most of existing image search engines usually use text information for judging relevancy , resulting low precision performance .
Most existing image-search engines usually use text information to determine relevance , resulting in poor precision .
483
To improve the retrieval performance , it is necessary to use visual information of images for re-ranking .
To improve the accuracy of retrieval , it is necessary to use visual information from images to re-rank them .
484
However , content-based image understanding is a challenging and unsolved problem .
However , understanding content-based images remains a challenging and unsolved problem .
485
One popular approach \CITE combining both text and visual information is to use text information to quickly retrieve a set of candidates and then do post-processing (i . e . re-rank) on this set to improve the precision .
One popular approach \CITE combining both text and visual information has been to use text information to quickly retrieve a set of candidates and then do post-processing ( i . e . , re-ranking ) on this set to improve precision .
486
There are two ways for post-processing : The first way \CITE is to build a ranker or a classifier specific to the given query using the returned images .
There are two ways of doing post-processing : The first \CITE has been to build a ranker or a classifier specific to the given query using the returned images .
487
This way is more scalable and can be used for practical applications such as meta search engines .
This is more scalable and can be used for practical applications such as meta-search engines .
488
By addressing these problems , Our contribution is two-fold :
Our contribution by addressing these problems is two-fold :
489
In this framework , We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not .
We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not in this framework .
490
experimental results show that the relevance classifier that is independent with underlying ranking algorithm of existing search engines can significantly boost the performance .
The experimental results demonstrated that the relevance classifier that is independent of the underlying ranking algorithms of existing search engines could significantly boost performance .
491
Specifically , We detect and group faces of persons appearing in video programs in face tracks in which each face track contains of the faces of one person .
We specifically detected and grouped faces of people appearing in video programs in face tracks in which each face track contained the faces of one person .
492
To distinguish face tracks of different persons , we assume that if multiple faces are detected at different locations in one frame , they are of different persons (cf . Figure \REF) .
To distinguish the face tracks of different people , we assumed that if multiple faces were detected at different locations in one frame , they would be of different people ( cf .
493
To enlarge the number of such face tracks , We use video programs of multiple genres and channels .
We used video programs from multiple genres and channels to increase the number of such face tracks .
494
From these faces , We can artificially generate face sets similar to the sets returned by search engines given person names .
We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .
495
Since we know the relevance of these faces to the artificial sets , the labels of each face can be easily generated and no human intervention is needed for this process .
Since we knew the relevance of these faces to the artificial sets , the labels for each face could be easily generated and no human intervention was needed in this process .
496
Note that the label of faces in our approach is not identity of that face . It is the relevance between the face and the associated query .
Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .
497
Collecting training sets from such external sources as video archives is easy and efficient because : firstly , a large number of videos can be easy to obtain .
Collecting training sets from such external sources as video archives is easy and efficient because , first , a large number of videos can be easily obtained .
498
In addition , using temporal information , faces of one person appearing in consecutive frames can be automatically grouped with high accuracy .
In addition , the faces of one person appearing in consecutive frames can be automatically grouped with a high degree of accuracy using temporal information .
499
given a query described by text , for example , 'airplane' or 'George Bush' , finding relevant images with high precision is essential for image search engines .
It is essential for image-search engines to find relevant images with a high degree of precision given queries described by text , e.g. , 'airplane' or 'George Bush' .
500
The idea is to rely on the visual consistency among these images to learn visual classifiers that measure the relevancy between an image and the input query .
The idea is to rely on the visual consistency between these images to learn visual classifiers that measure the relevance between an image and the input query .
501
Work such as \CITE extend topics models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or Hierarchical Dirichlet Process to learn generative model based classifiers .
Work \CITE has extended to topics on models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or the Hierarchical Dirichlet Process to learn generative model-based classifiers .
502
In addition , how to select the best topic associated with the input query for identifying target label is still challenging \CITE .
In addition , how the best topic is selected associated with the input query to identify the target label is still a difficult issue \CITE .
503
This method makes the training data cleaner that leads to performance improvement .
This method made the training data cleaner and led to improved performance .
504
In \CITE , A multiple instance learning framework is used to learn category models from images associated with keywords \CITE .
A multiple-instance learning framework has been used to learn category models from images associated with keywords \CITE .
505
The work mentioned above are for re-ranking images containing general objects .
These researchers re-ranked images containing general objects .
506
For re-ranking faces , work described in \CITE use Gaussian mixture models to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
Gaussian mixture models have been used for re-ranking faces to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
507
In \CITE , Discriminative approach based models such as SVM and linear discriminant analysis are used instead of Gaussian mixture models \CITE .
Discriminative-approach-based models such as SVM and linear discriminant analysis have been used instead of Gaussian mixture models \CITE .
508
In \CITE , A densest graph based method is used for finding the face group relevant to the query \CITE .
A densest-graph-based method has been used for finding the face group relevant to the query \CITE .
509
As for these approaches , One specific classifier is built for each query .
One specific classifier is built for each query in these approaches .
510
Therefore , to handle a large number of queries , many classifiers must be built which are not suitable in practice .
Therefore , many classifiers must be built , which are not suitable in practice , to handle a large number of queries .
511
As for specific classifiers , Each image is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , for example , 'airplane' .
Each image for specific classifiers is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , e.g. , 'airplane' .
512
In generic classifier , Each image is classified as relevant or irrelevant to the query .
Each image in a generic classifier is classified as relevant or irrelevant to the query .
513
This method works well for objects such as car , flag , but fails to handle faces .
This method works well for objects such as cars and flags , but fails to handle faces .
514
To build the specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by the query-independent feature such as pixel intensity around facial features such as eyes , nose , and mouth \CITE .
To build a specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by a query-independent feature such as pixel intensity around facial features such as the eyes , nose , and mouth \CITE .
515
Meanwhile , to build the generic classifier which is independent with any \textit{'personX'} , each face is represented by the query-dependent feature .
Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .
516
In \CITE , the Query-dependent features using textual information are proposed \CITE .
Query-dependent features using textual information has been proposed \CITE .
517
In \CITE , Each image \CITE is represented as a set of visual words .
Each image in \CITE is represented as a set of visual words .
518
Since this method is suitable for general objects rather than faces , we proposed another method described below for extracting query-dependent features to train the generic classifier .
Since this method is suitable for general objects rather than faces , we propose another method of extracting query-dependent features to train the generic classifier that is described below .
519
To be able to model the relevance between a face and the given query , We assume that there is visual consistency among faces returned by search engines for that query .
We assumed that there would be visual consistency between faces returned by search engines for a query to be able to model the relevance between a face and that given query .
520
Given a threshold \MATH , for each point \MATH , we examine number of points \MATH so that \MATH , where \MATH is the distance ( e .g . Euclidean distance ) between \MATH and \MATH in the feature space .
Given threshold \MATH , for each point \MATH , we examine the number of points \MATH so that \MATH , where \MATH is the distance ( e.g. , Euclidean distance ) between \MATH and \MATH in the feature space .
521
Points with larger values for \MATH have more sparse neighborhoods and are likely outliers than points belonging to dense clusters which usually have lower values of \MATH .
Points with larger values for \MATH have sparser neighborhoods and are more likely outliers than points belonging to dense clusters , which usually have lower values for \MATH .
522
In our framework , Each face is an sample , and non-outliers / outliers mean faces relevant / irrelevant to the query ( i .e . target person ) .
Each face in our framework is a sample , and non-outliers / outliers mean faces are relevant / irrelevant to the query ( i.e. , target person ) .
523
In order to train the relevance classifier using supervised learning methods such as SVM , it requires a sufficient number of training samples .
It requires a sufficient number of training samples to train the relevance classifier using supervised learning methods such as SVM .
524
To collect training samples , The simplest way \CITE is we pick many names , and pass them to search engines .
The simplest way \CITE of collecting training samples is to pick many names , and pass them to search engines .
525
As a result , this method can stimulate face sets returned by search engines using many names mentioned above .
As a result , this method can be used to stimulate face sets returned by search engines using many names as mentioned above .
526
-If there are more than one face appearing in different locations in one frame , they likely belong to different persons .
-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .
527
If we have large video archives , using these heuristics we can collect a sufficient number of training samples for learning the relevance classifier .
If we have large video archives , we can collect a sufficient number of training samples to learn the relevance classifier by using these heuristics .
528
For each channel , We scanned all face tracks extracted from the videos broadcast by this channel , and picked face tracks extracted from keyframes that several faces were detected at different locations .
We scanned all face tracks for each channel extracted from the videos broadcast by this channel , and picked face tracks extracted from key frames where several faces were detected at different locations .
529
To guarantee selected face tracks representing different persons , for one channel , only face tracks of one shot was picked .
To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .
530
Using these face tracks , We generated 133 labeled sets described in Section \REF and used them for training the relevance classifier .
We generated the 133 labeled sets described in Section \REF using these face tracks and used them to train the relevance classifier .
531
On average , The accuracy was \MATH .
The accuracy was \MATH on average .
532
For each query , We crawled a maximum of 500 images from URLs returned by Google .
We crawled a maximum of 500 images from URLs returned by Google for each query .
533
On average , The accuracy was \MATH .
The accuracy was \MATH on average .
534
The datasets , Yahoo News Images and Google Images as shown in Figure \REF , were used for testing .
The datasets for Yahoo News Images and Google Images , as shown in Figure \REF , were used for testing .
535
To group faces belonging to one person in one video shot , We simply used a similar technique described in \CITE .
We simply used a similar technique to that described in \CITE to group faces belonging to one person in one video shot .
536
Specifically , for each face , 9 facial feature points were detected , and 4 more facial feature points were inferred from these 9 points .
Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .
537
In total , There were 13 feature points from which features are extracted .
There were a total of 13 feature points from which features were extracted .
538
To evaluate ranked lists in which both recall and precision are taken into account , the average precision is usually used .
Average precision is usually used to evaluate ranked lists in which both recall and precision are taken into account .
539
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
The interpolated precision , \MATH , at a certain recall level , \MATH , is defined as the highest precision found for any recall level \MATH :
540
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .
In addition , we used the mean average precision to evaluate the performance of multiple queries , which is the mean of average precisions computed from queries .
541
In this experiment , We compare the MAP performance of the following systems testing on YahooNews Images :
We compared the performance of the Maximum A-Posteriori ( MAP ) algorithm in seven systems in this experiment by testing it on YahooNews Images :
542
-Mensink[15]-GaussianModels : This method proposed by Mensink et al . \CITE models the returned faces by using two Gaussians , one for the faces relevant to the target person and one for the remaining faces .
-Mensink[15]-GaussianModels : This method proposed by Mensink et al. \CITE modeled the returned faces by using two Gaussians , the first for the faces relevant to the target person and the second for the remaining faces .
543
The Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are the state of the art methods that learn a specific classifier for each query .
Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are state-of-the-art that learn a specific classifier for each query .
544
The method Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then is used for new queries .
Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then used for new queries .
545
As for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID , the curves show the correlation between the performance and the number of features .
The curves plot the correlation between performance and the number of features for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID .
546
-The performance of the system using the training data generated artificially by our method is comparable with that of the system using the training data returned by search engines .
-The performance of the system using training data generated artificially with our method was comparable to that of the system using training data returned by search engines .
547
-Our proposed method DistScore-TrainTRECVID has comparable performance to the state of the art method in specific classifier-based approach Mensink[15]-Friends .
-The method of DistScore-TrainTRECVID we propose performed comparably to the state-of-the-art method in the specific classifier-based approach of Mensink[15]-Friends .
548
It outperforms the method using only visual information Mensink[15]-GaussianModels .
It outperformed the method where only visual information was used , i.e. , Mensink[15]-GaussianModels .
549
The result of DistScore-TrainTRECVID on YahooNews Images set and Google Images set indicates that the relevance classifier of our proposed method is able to generalize well on different queries and independent with underlying ranking algorithms of search engines .
The results for DistScore-TrainTRECVID on the YahooNews Images set and Google Images set indicate that the relevance classifier with our proposed method was able to generalize well on different queries and was independent of underlying ranking algorithms used in search engines .
550
Figure \REF shows an example of re-ranking result of top-30 faces for the query John Paul that is one of the most difficult cases of the YahooNews Images set .
Figure \REF shows an example of re-ranking results for the top-30 faces for the query John Paul , which is one of the most difficult cases in the YahooNews Images set .
551
Our query-dependent feature is based on nearest neighbors of the images in the returned image set that usually have complexity of \MATH , where \MATH is the total number of images in the set .
Our query-dependent feature was based on the nearest neighbors of images in the returned image set that usually have a complexity of \MATH , where \MATH is the total number of images in the set .
552
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and SASH \CITE can speed up the nearest neighbor search significantly .
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and a Self Adaptive Set of Histograms ( SASH ) \CITE could significantly speed up the nearest neighbor search .
553
For example , as described in \CITE , the complexity of fast lookup of $k$ approximate nearest neighbors is \MATH \CITE .
For example , the complexity of the fast lookup of $k$ approximate nearest neighbors is \MATH \CITE .
554
To train the generic classifier , We propose a simple unsupervised method to obtain a large number of labeled faces from video archives .
We propose a simple unsupervised method to train the generic classifier to obtain a large number of labeled faces from video archives .
555
Experiments shown that although our method is unsupervised and independent with underlying algorithms of existing search engines but successfully learns visual consistency among returned faces to boosts retrieval performance .
Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .
556
In the current digital environment , the mathematical content being published on the Web is increasing day by day . While more and more mathematical contents being available on the Web , retrieving mathematical contents becomes an important issue for many users .
The mathematical content being published on the Web is increasing day by day , and retrieving mathematical content has become an important issue for many users .
557
Teach-ers , students , researchers do need to gain access to mathematical resources for teaching , studying , or obtaining updated information for research and development .
Teachers , students , and researchers need better access to mathematical resources for teaching , studying , and obtaining information for research and development .
558
Internet search engines are able to detect some particular keywords in mathematical formula but they mostly fail to recognize mathematical symbols and constructs such as integral sym-bols , square root symbols , fractions , or matrices .
Internet search engines can detect particular keywords in mathematical formulas but they mostly fail at recognizing mathematical symbols and constructs such as integral and square root symbols , fractions , and matrices .
559
Furthermore , these systems do not take into account the semantics of mathematical formulas revealed by surrounding natural language text , like the name of the formula and its variables' descrip-tion .
Furthermore , these systems do not take into account the semantics of mathematical formulas as revealed by the surrounding natural language text , e.g. , the formula�fs name or the description of its variables .
560
But full mathematical search is still not available .
But even this site does not provide a full mathematical search .
561
The Wolfram Functions Site [7] contains large mathe-matical formulas and also provides a semantics search for mathematical formulas .
The Wolfram Functions Site [7] contains a large number of mathematical formulas and also provides a semantic search for them .
562
This site and some recent works done by Adeel et al. [2] and Yokoi and Aizawa [1] propose similarity search methods based on MathML but these works do not make use of the semantics of the formulas' surrounding text , which is considered to be important information sources .
This site and some recent work done by Adeel et al. [2] and Yokoi and Aizawa [1] employ similarity search methods based on MathML but they do not make use of the semantics of the formulas' surrounding text . //[ ? ? propose is unclear in the sense of a website .]
563
It also provides opportunities to make mathematical better understandable and usable for di erent groups of people with disabilities .
It also provides opportunities to make mathematics better understandable and usable for people with disabilities .
564
The remainder of this paper is organized as follow : In section 2 , we present an overview of the proposed framework .
The remainder of this paper is organized as follows : we present an overview of our framework in section 2 .
565
Mathematical formulas on the Web has many di erent formats , some of them are LaTeX , and the Mathematical Markup Language ( MathML ) [6] .
Mathematical formulas on the Web have many different formats , e.g. , LaTeX and Mathematical Markup Language ( MathML ) [6] .
566
In this paper , we use the presentation MathML format for mathematical formulas .
In this paper , we shall use the MathML format for mathematical formulas .
567
At this point , we use some heuristics to provide an adequate solution for matching mathematical formulas with their names .
We used heuristics to ensure adequate matching of mathematical formulas with their names .
568
In the rst case , users can use the extracted keywords for search , for example : " sin " , " Pythagorean " or " trigonometric functions " .
In a text content search , users search with extracted keywords , e.g. , " sin " , " Pythagorean " or " trigonometric functions " .
569
In the second case , users can input the mathematical formulas directly , for example : \MATH .
In a formula content search , users directly input the formulas , for example : \MATH .
570
Else , it just looks for mathematical formulas which are similar to the input ( including formulas with similar structure ) .
If nothing matching is found , it looks for mathematical formulas which are similar to the input ( including formulas with a similar structure ) .
571
In our work , we manually consider formulas with the same semantic meaning are relevant .
We consider that formulas with the same semantic meaning are relevant . //[The original is unclear the rewrite seems to be what you mean .]
572
Table 1 shows top 5 of the searching results for the query \MATH .
Table 1 shows the top 5 search results for the query " sin( a + b ) " .
573
As can be seen from the table , when the system associates the formulas with their names , it can provide more useful information to the user .
As can be seen , when the system associates the formulas with their names , it can provide more useful information to the user .
574
In this paper , we presented a new framework for mathematical search where links between formulas and their names are automatically detected from the target documents and then utilized in the search .
We presented a new framework for mathematical searches where links between formulas and their names are automatically detected in the target documents and then utilized in the search .
575
The experimental results have shown how helpful this information provides to the users of mathematical search .
The experimental results showed how helpful this information is to mathematical search users .
576
Using formula 's name is one way of taking into account the semantic meaning of the formula , we are considering other information such as formula 's description and variable 's description .
Using a formula 's name is only one way of taking into account the semantic meaning of the formula ; we are considering other information such as the formula 's description and its variable 's description .
577
Experimental results on the Wolfram Function Site show that our approach achieves an improvement against the prior rule-based system .
Experimental results on the Wolfram Function Site show that our approach is an improvement over prior rule-based systems . // <Note : It seems that where were two prior systems that were compared . If not , you can go back to using a prior system .> .
578
The direct application of this is enabling semantic searches for mathematical expressions by understanding the intent of the searcher and the contextual meaning of mathematical terms improve search accuracy .
The direct application of this idea enables semantic searches for mathematical expressions whereby the system 's �eunderstanding ' of the intent of the searcher and the contextual meaning of mathematical terms improves search accuracy .
579
- The third problem is that new notations tend to be introduced and used as and when needed so a mechanism is required for referring to mathematical concepts outside of the base collection , allowing them to be represented .
- The third problem is that new notations tend to be introduced and used when needed so a mechanism is required for referring to mathematical concepts outside of the base collection .
580
The aim of this paper is to introduce a method for automatic mathematics semantic enrichment that capable of analyze and disambiguate mathematical terms .
The aim of this paper is to describe a method of automatic semantic enrichment for mathematics that is capable of analyzing and disambiguating mathematical terms .
581
- Due to the diversity of mathematical expressions , SnuggleTeX is still to be considered experimental and has difficulty processing complicated mathematical symbols and expressions .
- Due to the diversity of mathematical expressions , SnuggleTeX is still considered experimental and has difficulty processing complicated mathematical symbols and expressions .
582
In this paper , we propose an approach that automatically learn the semantics inference from a presentation from parallel markup data .
In this paper , we propose an approach that automatically learns semantic inferences in a presentation from parallel markup data . // <The original has too many from to be logically clear . The rewrite is a guess . > .
583
The probability distribution will be automatically learned from data that have both Presentation and Content MathML markup , that is the parallel markup MathML data .
The probability distribution is automatically learned from both Presentation and Content MathML markup data , that is , parallel markup MathML data .
584
Since both quantity and quality of mathematical expressions are continuing to grow and expand through time , we believe that our system will cover most of real life mathematical expressions .
The quantity and quality of mathematical expressions are continuing to grow , and we believe that our system will be able to cover most mathematical expressions .
585
- Second , mathematics knowledge such as symbol 's meanings or structural relations is automatically learned while training , therefor it is not required mathematics experts nor human effort and it is also easier to update the system given more data .
- Second , mathematics knowledge such as a symbol 's meanings or structural relations is automatically learned while training ; therefore , the system requires no human effort or expertise , and it is easier to update with more data .
586
In our experiments , we performed a 10-folds cross validation on mathematical expressions from 6 categories of the Wolfram Functions Site to evaluate the effectiveness of our proposed learning method .
We performed a ten-fold cross validation on mathematical expressions from six categories of the Wolfram Functions Site to evaluate the effectiveness of our learning method .
587
We set up another experiment to confirm the correlation between system performance and training set size and saw that increasing the size of training data actually boost the system performance .
We performed another experiment to assess the correlation between the system 's performance and training set size and found that increasing the size of the training data boosted the system 's performance .
588
The remainder of this paper is organized as follows : In Section 2 , we give a brief overview of the background and related work for semantic enrichment of mathematical expressions , while in Section 3 we present our proposed method .
The remainder of this paper is organized as follows : In Section 2 , we give a brief overview of the background and related work on semantic enrichment of mathematical expressions .
589
We then describe the experimental setup and results in Section 4 .
We present our method in Section 3 and describe the experimental setup and results in Section 4 .
590
For scientific documents , \TeX{} has been used to encode mathematical formulas .
\TeX{} has been used to encode mathematical formulas in scientific documents .
591
MathML has two types of encoding , content-based encoding which is called Content MathML , dealing with the meaning of formulas , and presentation-based encoding which is called Presentation MathML , dealing with the display of formulas .
MathML has two types of encoding , content-based encoding , called Content MathML , dealing with the meaning of formulas , and presentation-based encoding , called Presentation MathML , dealing with the display of formulas .
592
For understanding mathematical expressions , Grigole et al. \CITE proposed an approach based on the surrounding text of mathematical expressions .
Grigole et al. \CITE proposed an approach to understanding mathematical expressions based on the text surrounding the mathematical expressions .
593
The similarity scores obtained were weighted , summed up , and normalized by the length of the considered context .
The similarity scores obtained are weighted , summed up , and normalized by the length of the considered context .
594
The assigned interpretation is the Term Cluster with the highest similarity score .
The Term Cluster with the highest similarity score is assigned as the interpretation .
595
The approach was evaluated on 451 manually annotated mathematical expressions and the best result was 68.26 $ F_{0.5} $ score .
The approach was evaluated on 451 manually annotated mathematical expressions , and the best result was an F_{0.5} $ score of 68.26 $ .
596
To deal with the meanings of mathematical formulas , Nghiem et al. \CITE proposed an approach for extracting the names or descriptions of the formulas using natural language text surrounding them .
To deal with the meanings of mathematical formulas , Nghiem et al. \CITE proposed an approach for extracting names or descriptions of formulas by using the natural language text surrounding them .
597
- Extracting Rules : given a dataset contains MathML parallel markup expressions , extract the rules for translation .
- Rule Extraction : This module is given a dataset containing MathML parallel markup expressions , and it extracts translation rules from it . // <The original is ungrammatical and unclear . The rewrite is a guess .> .
598
- Generating Content MathML : given a mathematical expressions in Presentation MathML markup , and a set of rules , generate Content MathML expressions to enrich the Presentation MathML expressions .
- Content MathML Generation : This module is given mathematical expressions in Presentation MathML markup and a set of rules , and it generates Content MathML expressions to enrich the Presentation MathML expressions .
599
By investigating the data from the Wolfram Function Site , we noticed that there are elements that have no specific meaning , they are used for displaying purpose only and most of them are layout schemata .
After investigating data on the Wolfram Function Site , we noticed that there are elements that have no specific meaning ; they are used for display purposes only and most of them are layout schemata .
600
Another example are the pairs of parentheses , it is used to indicate that the expressions in the parentheses go together , while its structure already encoded that information .
Another example is pairs of parentheses ; these are used to indicate that the expressions in the parentheses go together , despite that their structure already encodes that information . // <The original is unclear . The rewrite is a guess . > .
601
As a result , in this preprocessing step , these elements are removed .
This preprocessing step removes these elements .
602
In this step , we also removed mathematical expressions with error markups such as expressions that have no Content markup .
We also remove mathematical expressions with error markups such as expressions that have no Content markup .
603
For simplification , expressions with more than 200 content nodes also be removed .
For simplification , expressions with more than 200 content nodes are also removed .
604
These rules are applied to break the large Presentation MathML tree into smaller sub-trees while maintaining the structure of output Content MathML trees .
These rules are used to break up a large Presentation MathML tree into smaller sub-trees while maintaining the structure of the output Content MathML trees .
605
Each rule in fragment rule set is associated with its probability , that is the frequent that rule happened in the training data .
Each rule in the fragment rule set is associated with a probability , that is , the frequency at which a rule occurs in the training data .
606
If the sub-trees can not be broken any longer , we extract another rules , which we called " translation rules " , at that point .
Once the sub-trees cannot be broken down further , we start to extract other rules , which we call " translation rules " .
607
The reason for this is that there is infinite number and we could never present every number in the rule .
The reason for this is that there is an infinite number of rules . // <The rewrite is a guess .> .
608
The experiments were carried out using the datasets from the Wolfram Function site .
The experiments were carried out using datasets from the Wolfram Function site .
609
Training and testing were performed using 10-fold cross-validation ; for each category , the original corpus is partitioned into 10 subsets .
Training and testing were performed using ten-fold cross-validation ; for each category , the original corpus was partitioned into ten subsets .
610
Of the 10 subsets , a single subset is retained as the validation data for testing the model , and the remaining subsets are used as training data .
Of the ten subsets , a single subset was retained as the validation data for testing the model , and the remaining subsets were used as training data .
611
The cross-validation process is then repeated 10 times , with each of the 10 subsets used exactly once as the validation data .
The cross-validation process was repeated ten times , with each of the ten subsets used exactly once as the validation data .
612
Currently we have 20 papers from ACL archive , all of the math expressions in these papers are annotated manually with both Presentation Markup and Content Markup .
Currently , we have 20 papers from the ACL archive , and we manually annotated all of the math expressions in these papers with both Presentation Markup and Content Markup . // The original is somewhat vague . The rewrite is a guess . Use it if it is correct . > .
613
- Translation Error Rate \CITE : translation error rate is an error metric for machine translation that measures the number of edits required to change a system output into one of the references .
- the Translation Error Rate \CITE : the translation error rate is an error metric for machine translation that measures the number of edits required to change a system output into one of the references .
614
For the data in Wolfram Function site , it appeared that SnuggleTeX is not applicable to this data since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .
It appeared that SnuggleTeX was not applicable to the data from the Wolfram Function site since it uses ASCII MathML but the site does not .
615
For ACL-ARC data , the experimental results from our side-by-side comparison show that our system significantly outperforms SnuggleTeX in terms of Tree Edit Distance Rate .
For ACL-ARC data , the experimental results show that our system significantly outperforms SnuggleTeX in terms of the Tree Edit Distance Rate .
616
In this paper , we discussed the problem of the semantic enrichment of mathematical expressions .
We discussed the problem of semantic enrichment of mathematical expressions .
617
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expression to a Content MathML expression has the significant improvement over a prior system .
Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expressions to Content MathML expressions is a significant improvement over prior systems .
618
By combining the automatic extraction of fragment rules and translation rules , our approach has shown promising results .
Our approach combining automatic extraction of fragment rules and translation rules has shown promising results .
619
In future work , we should also consider expanding it to cover all mathematical notations .
In the future , we should consider expanding it to cover all mathematical notations .
620
Recent research shows a major part of difficult cases in event extraction for the biomedical domain are related to coreference .
Current research has shown that major difficulties in event extraction cases for the biomedical domain are related to coreference .
621
However , the shared task results showed that transferring coreference resolution methods developed for other domains to the biological domain was not straight forward , which is supposed to be caused by the domain differences in coreference phenomena .
However , the shared task results indicated that transferring coreference resolution methods developed for other domains to the biological domain was not straightforward , due to the domain differences in the coreference phenomena .
622
We compared our system with the top four systems in the BioNLP-ST 2011 , and surprisingly we found that the minimal configuration has outperformed the best system in the BioNLP-ST 2011 .
We compared our system with the top four systems in the BioNLP-ST 2011 ; surprisingly , we found that the minimal configuration had outperformed the best system in the BioNLP-ST 2011 .
623
Analysis of the experimental results showed that semantic classification using protein information has contributed to an increase in performance ( 2.3 % on the test data , and 4 .0% on the development data , in F-score ) .
Analysis of the experimental results revealed that semantic classification , using protein information , had contributed to an increase in performance by 2.3 % on the test data , and 4 .0% on the development data , in F-score .
624
Since such information is difficult to be transferred across different domains , we need to continue seeking for methods to exploit and use it in coreference resolution .
Since it is difficult to transfer domain-specific information across different domains , we need to continue to seek methods to exploit and use it in coreference resolution .
625
While named entity recognition ( NER ) and relation or event extraction are regarded as standard tasks of biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is more and more recognized as an important component of IE for a higher performance .
While named entity recognition ( NER ) and relation / event extraction are regarded as standard tasks for biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is being recognized more and more as an important component of IE to achieve a higher performance .
626
Without coreference resolution , the performance of IE is often substantially limited due to an abundance of coreference relations in natural language text , i.e. , information pieces written in text with involvement of a coreference relation are hard to be captured [ 9 , 14 ] .
Without coreference resolution , oftentimes , the IE performance issubstantially limited , due to the abundance of coreference relations in natural language text ; information pieces written in text with the involvement of a coreference relation are hard to be captured [ 9 , 14 ] .
627
There have been several attempts for coreference resolution , particularly for newswire texts [ 7 , 8 , 22 , 23 , 28 , 30 ] .
There have been several attempts for coreference resolution ; in particular , they have been for newswire texts [ 7 , 8 , 22 , 23 , 28 , 30 ] .
628
It is also one of the lessons from BioNLP Shared Task ( BioNLP-ST , hereafter ) 2009 that coreference relations in biomedical text substantially hinder the progress of fine-grained IE [ 10 ] .
Coreference resolution is also one of the lessons from the BioNLP Shared Task ( BioNLP-ST , hereafter ) 2009 , in which it was communicated that coreference relations in biomedical text substantially hinder the progress of fine-grained IE [ 10 ] .
629
This task definition focuses on a specific type of entities , i.e. Protein .
This task definition focuses on protein , as a specific type of entity .
630
In the figure , protein names are highlighted in bold face , P4 - P10 , and targeted anaphoric expressions of the shared task , e.g. pronouns and definite noun phrases , are T29 , and T32 , of which the antecedents are indicated by arrows if found in the text .
In the figure , protein names P4 - P10 are highlighted in boldface ; the targeted anaphoric expressions of the shared task ( pronouns and definite noun phrases ) are T29 , and T32 , for which the antecedents are indicated by arrows , if found in the text .
631
Without knowing this coreference relation , it becomes hard to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is localization of p65 ( out of nucleus ) according to the framework of BioNLP-ST .
Without knowing this coreference relation , it becomes difficult to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is a localization of p65 ( out of nucleus ) , according to the framework of BioNLP-ST .
632
A new term is introduced in the BioNLP-ST is antecedent protein , which indicates the protein mention contained in the antecedent expression , e.g. p65 in T28 .
A new term introduced in the BioNLP-ST is antecedent protein , which indicates the protein mention contained in the antecedent expression , e.g. , p65 in T28 .
633
There are other coreferential expressions which are ignored in the context of this COREF task such as this complex and the NF-kappa B transcription factor complex ( Figure 1 ) , since we only focus on the antecedent expressions that contain and point to protein mentions .
There are other coreferential expressions , which are ignored in the context of this COREF task , such as : this complex and the NF-kappa B transcription factor complex ( Figure 1 ) , since our focus is on the antecedent expressions that contain and point to protein mentions .
634
However , the external coreference tool achieves much lower results on biological texts than that on news texts , from 66 .38% down to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .
However , the external coreference tool " s performance drops for biological texts than for news texts , from 66 .38% to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .
635
Below are examples of the coreference types .
Examples of the coreference types are outlined below :
636
- " . . . ,the phosphorylation status of [ TRAF2 ] had significant effects on the ability of [ the protein ] to bind to CD40 , " ( DNP )
- " [ . . . ] the phosphorylation status of [ TRAF2 ] had significant effects on the ability of [ the protein ] to bind to CD40 , " ( DNP )
637
- " Subnuclear fractionation reveals that there are [ two ATF1 isoforms ] [ which ] appear to differ with respect to DNA binding activity , " ( RELAT )
- " Subnuclear fractionation reveals that there are [ two ATF1 isoforms , which ] appear to differ with respect to DNA binding activity , " ( RELAT )
638
The analysis results in also showed that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type are 27 .5% F-score and 10 .1 F-score respectively , which are far less than that for relative pronoun ( the RELAT type ) 66 .2 % F-score .
An analysis of the results indicated that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type was 27 .5% F-score and 10 .1 F-score , respectively ; the scores were much lower than the F-score for relative pronouns ( the RELAT type ) , which yielded a 66 .2 % F-score .
639
Thus , it can be inferred that definite noun phrases and pronouns are more difficult to be resolved than relative pronouns .
Thus , it can be inferred that it is more difficult to resolve definite noun phrases and pronouns than relative pronouns .
640
In this paper , we compare the contributions of different features in coreference resolution , two simple types of domain-portable information : discourse preference and number-agreement , and domain-specific information which can be considered as more difficult to be transferred across different domains .
In this paper , we compare the contributions of different features in coreference resolution ; two simple types of domain-portable information : discourse preference and number-agreement , is compared , as well as domain-specific information , which is considered to be more difficult to be transferred across different domains .
641
As we needed to get an insight into the problem , we took a rule-based approach , analyzing the training data of BioNLP-ST 2011 Coref task .
In order to acquire insight into the problem , we took a rule-based approach , analyzing the training data of BioNLP-ST 2011 Coref task .
642
We used Genia Sentence Splitter and Enju Parser [ 15 ] for the purposes , respectively .
We used the Genia Sentence Splitter and Enju Parser [ 15 ] for sentence segmentation and syntactic parsing , respectively .
643
( Enju parser comes with a default tokenizer and part-of-speech tagger for biological text . ) Row 1 in the example Table 1 shows three sentences outputted from Genia Sentence Splitter , and noun phrases outputted from Enju Parser for the sentence S3 .
( Enju parser comes with a default tokenizer and part-of-speech tagger for biological text . ) Row 1 in the example of Table 1 shows three sentences as the output from the Genia Sentence Splitter , and noun phrases as the output from the Enju Parser for the sentence , S3 .
644
For the set of markables , noun phrases , which do not include subordinate clause , are collected as analyzed by a syntactic parser , Enju in our case .
For the set of markables , noun phrases , which do not include a subordinate clause , are collected as they are analyzed by a syntactic parser ( in our case , Enju ) .
645
In the sentence S3 , three noun phrases recognized by the NX and NP tags of Enju output , role , role for c-Myc in apoptosis , and this role for c-Myc in apoptosis ( Step 0 results ) share the same head word role , thus only the longest one this role for c-Myc in apoptosis is selected .
In the sentence S3 , three noun phrases recognized by the NX and NP tags of the Enju output , role , role for c-Myc in apoptosis , and this role for c-Myc in apoptosis ( Step 0 results ) share the same head-word role ; thus , only the longest noun phrase , this role for c-Myc in apoptosis , is selected .
646
Step 2 - Anaphor selection : determines candidate anaphoric expressions , which are basically pronouns and definite noun phrases ( a minority of anaphors are indefinite noun phrases or entity names , which act as appositions . )
Step 2 - Anaphor selection : Candidate anaphoric expressions , which are basically pronouns and definite noun phrases , are determined . A minority of anaphors are indefinite noun phrases or entity names , which act as appositions .
647
Syntactic filters are used to filter out pleonastic its , or pronouns such as he , she , which are not expected to refer to proteins .
Syntactic filters are used to filter out pleonastic its , or pronouns , like : he , she , which are not expected to refer to proteins .
648
In practice , for definite noun phrase type of anaphors , this is done using a list of possible head words of protein references , and for pronouns , their context words are used .
In practice , for definite noun phrase type of anaphors , this is accomplished , by using a list of possible head-words of protein references ; for pronouns , their context words are used .
649
The idea behind this is that some types of syntactic relations imply the impossibility of coreference relations between its argument noun phrases and the inclusive expressions of these noun phrases .
The idea behind this filter is that some types of syntactic relations imply the impossibility of coreference relations between its argument noun phrases and the inclusive expressions of these noun phrases .
650
-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate which is not number conflict with anaphor is selected .
-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate , which does not conflict in number with the anaphor , is selected .
651
The rules are implemented using different features of expressions such as syntactic types of expression , head noun , semantic types , etc. , in a similar way to [ 22 ] .
The rules are implemented using different features of expressions , such as syntactic types of expressions , head noun , semantic types , etc. , in a similar way to [ 22 ] .
652
In this step , we want to filter out those pronouns and definite noun phrases that are not target of this task , comprised of two types : non-anaphoric expressions , and anaphoric expressions which do not point to proteins .
In this step , we want to filter out those pronouns and definite noun phrases that are not a target of this task . The expressions are comprised of two types : non-anaphoric expressions , and anaphoric expressions , which do not point to proteins .
653
Non-anaphoric expressions includes first and second person pronouns such as I , we , you , . . . , and pleonastic it .
Non-anaphoric expressions include first and second-person pronouns such as I , we , you , and pleonastic it .
654
First and second person pronouns are easily to be recognized by the part-of-speech tags , thus we use part-of-speech information for the filtering .
First and second-person pronouns are easily recognized by the part-of-speech tags ; thus , we use part-of-speech information for the filtering .
655
Basically all expressions detected in the initial expression set are antecedent candidate , except for anaphoric pronouns .
Basically , all of the expressions detected in the initial expression set are an antecedent candidate , with the exception of anaphoric pronouns .
656
This is exactly what our system does .
Our system accomplishes this task .
657
However , a disadvantage of this method is when the parser makes mistake on finding the correct arguments , coreference also fails , as in the example " . . .of transcription factor NF-kappa B also encodes a p70 I kappa B protein , I kappa B gamma , which is identical to the C-terminal 607 amino acids of . . . "
However , a disadvantage to using this method is that when the parser makes a mistake on finding the correct arguments , the coreference also fails . This is exemplified in the following : " . . .of transcription factor NF-kappa B also encodes a p70 I kappa B protein , I kappa B gamma , which is identical to the C-terminal 607 amino acids of . . . "
658
The rules are applied in a succession order one after another until the inequality occurs , or end of the rule list is reached .
The rules are applied in a successive order , one after another , until the inequality occurs , or until the end-of-the-rule list is reached .
659
The default rule of the procedure prefers the closer antecedent candidate .
The default rule of the procedure , is in the preference of the closer antecedent candidate .
660
In this work , we only focus on the Protein type , ignoring other possible semantic types , so we do not take the structure of taxonomy into account .
In this work , we only focus on the Protein type , ignoring other possible semantic types , so the structure of the taxonomy is not taken into account .
661
Focusing on specific entity types , i.e. Protein type , helps us to find a proper method for determining the likelihood , and how to encode the likelihood in coreference resolution .
Focusing on specific entity types , i.e. , Protein type , enables us to find a proper method for determining the likelihood , and method for encoding the likelihood in coreference resolution .
662
Another case is when the head noun is either protein or gene , and has a protein mention as its premodifier , such as the Tax protein .
In another case , when the head noun is either protein or gene , and has a protein mention as its premodifier , such as the Tax protein .
663
In particular , we check the noun phrase whose determiner is its or their ; if the noun phrase contains a protein key word then the inclusive pronoun is classified into the Protein semantic type .
In particular , we check the noun phrase in which the determiner is its or their ; if the noun phrase contains a protein key word , then the inclusive pronoun is classified into the Protein semantic type .
664
In other words , their antecedents do not necessarily exist in the textual context ; in particular in biomedical scientific papers , many definite noun phrases do not have antecedents since the referred concepts can be anything understood by experts in the domain .
In other words , their antecedents do not necessarily exist in the textual context ; in particular , in biomedical scientific papers , many definite noun phrases do not have antecedents , since the referenced concepts can include any concept that is understood by subject matter experts in the domain .
665
Knowing their semantic type helps to filter out irrelevant candidate antecedents , increasing chance to pick up the right antecedent or the precision of antecedent prediction .
Knowing their semantic type helps to filter out irrelevant candidate antecedents , thereby increasing the chance of picking up the right antecedent , and increasing the precision of antecedent prediction .
666
The word list used to filter out anaphoric definite noun phrases in step 2 contains the following words : protein , gene , factor , molecule , element ,family , inhibitor , and receptor .
The word list used to filter out anaphoric definite noun phrases in step 2 contains the following words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .
667
Besides , premodifiers of definite noun phrases are also limited to numbers and popular premodifiers of proteins such as nuclear , transcription .
Premodifiers of definite noun phrases are also limited to numbers and popular premodifiers of proteins , such as nuclear , and transcription .
668
Note that RB-MIN with minimal configuration already outperforms the best result by the UU team , with up to 7 .1% higher in Fscore .
Note that the results from RB-MIN with minimal configuration , already surpasses the best results obtained by the UU team , with up to 7 .1% higher performance in F-score .
669
Breaking down the system performance by types of anaphors gives us an insight into what have been solved by our methods , and what needs more improvement effort .
Breaking down the system performance by the different types of anaphors provides us with insight into what has been accomplished / solved by our methods , and also provides us with improvement opportunities .
670
However , it should be noted that our antecedent prediction for the RELAT type is based completely on the output of Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See section Discussions ) .
However , it should be noted that our antecedent prediction for the RELAT type is based solely on the output of the Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See Discussions section ) .
671
The analysis results are given in section Discussions .
The analyses of the results are provided in the section entitled Discussions .
672
The combination of rule 1 , 2 and 3 resulted in 62 .4% fscore ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contribute to the increasement of 4 points Fscore on the development set , and 2 .3 points Fscore on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .
The combination of rule 1 , 2 and 3 resulted in a 62 .4% F-score ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contributes to a 4-point F-score increase in the development set , and 2 .3-point F-score increase on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .
673
( Coreference examples in this paper are represented as below : gold anaphoric and antecedent expressions are bracketed , antecedents before anaphors ; gold protein mentions are underlined ; and incorrect response antecedents are in italics . )
( Coreference examples in this paper are represented in the following manner : gold anaphoric and antecedent expressions are bracketed , antecedents before anaphors ; gold protein mentions are underlined ; and incorrect response antecedents are in italics . )
674
Although studies and a dominant negative form of its heterodimeric binding partner have been crossed out because of disagreement in numbers , and violation of abandoned syntactic constraints correspondingly , the system would return the incorrect antecedent apoptosis instead of c-Myc .
Although studies and a dominant negative form of its heterodimeric binding partner have been crossed out because of disagreement in numbers , and violation of abandoned syntactic constraints , correspondingly , the system would return the incorrect antecedent apoptosis instead of c-Myc .
675
To classify anaphors into protein or non-protein reference , our system employs a head-word based classfier for definite noun phrases , DEFNP-ANA-SEM , and a context-based classifier for pronouns , PRO-ANA-SEM ( Section Methods ) .
To classify anaphors into protein or non-protein reference , our system employs a head-word based classifier for definite noun phrases , DEFNP-ANA-SEM , and a context-based classifier for pronouns , PRO-ANA-SEM ( Section Methods ) .
676
This is because the semantic filter is the only way to filter out definite noun phrase anaphors .
This decrease is due to the fact that the semantic filter is the only way to filter out definite noun phrase anaphors .
677
This is an encouraging sign to seek for a systematic method to exploit and include such contextual information in coreference resolution .
This gain is a good indication for seeking a systematic method to develop and include such contextual information in coreference resolution .
678
Below are the examples showing the effectiveness of semantic information from the context of pronouns .
Examples showing the effectiveness of semantic information from the context of pronouns is provided below :
679
In all the above examples , the appearance of words such as binding , transactivation , DNA target sequence in the noun phrases of which the anaphor plays a role as a determiner , is contextual indicator for the protein type .
In all the examples above , the appearance of words such as binding , transactivation , DNA target sequence in the noun phrases for which the anaphor plays a role as a determiner , is a contextual indicator for the protein type .
680
However , we found in the data several coreferential expressions violating this constraint .
However , in the data , we found several coreferential expressions that violate this constraint .
681
For instance , the anaphor and antecedent in the following :
The anaphor and antecedent in the following is an instance of this violation :
682
The following example shows a coordination-structured antecedent AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 that was failed to be detected by the parser .
The following example shows a coordination-structured antecedent AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 that failed to be detected by the parser .
683
This subproblem is often thought as an easy task in coreference resolution systems , however , indeed it is an important subtask which strongly affects the performance of coreference system .
This sub-problem is often regarded as an easy task in coreference resolution systems ; however , in actuality , it is an important subtask , which strongly affects the performance of coreference system .
684
However , from another perspective , the perspective of coreference data creation , we should revise the markable annotations , for the sake of automatic and robust markable detection .
However , from the perspective of coreference data creation , revision of the markable annotations would aid in automatic and robust markable detection .
685
As for the future , more effort should be spent on automating the semantic classification for coreference expressions using context .
For future opportunities , more effort should be spent on automating the semantic classification for coreference expressions , using context .
686
The success of corpus-based methods has made syntactically annotated corpora important resources for natural language processing .
Syntactically annotated corpora have become important resources for natural language processing , due in part to the success of corpus-based methods .
687
This is also a concern of Vietnamese Treebank ( VTB ) , the first and the only publicly available syntactically annotated corpus so far for the Vietnamese language .
This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language .
688
Then , by combining and splitting the inconsistent annotations detected , we could observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation .
Then , by combining and splitting the inconsistent annotations that were detected , we are able to observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation .
689
This performance is far lower than the state-of-the-art performance reported for Berkeley Parser on English Penn Treebank , 90 .3% in F-score ( Petrov et al ., 2006 ) .
This score is far lower than the state-of-the-art performance reported for the Berkeley Parser on the English Penn Treebank , which reported 90 .3% in F-score ( Petrov et al ., 2006 ) .
690
There are two possible reasons for this .
There are two possible reasons to explain this outcome .
691
First , the quality of VTB is not good enough to build a good a parser , including the quality of the annotation scheme , the annotation guidelines , and the annotation process .
One reason for this outcome is the quality of VTB , including the quality of the annotation scheme , the annotation guidelines , and the annotation process .
692
Second , parsing Vietnamese is a diffcult problem by its own , and we need to seek new solutions to the problem .
The second reason is the difficulty of parsing Vietnamese ; we need to seek new solutions to address this problem .
693
This paper focuses on the word segmentation issues since the most basic unit of a treebank is word ( Di Sciullo and Edwin , 1987 ) , and defining `` What are words ? '' is the first problem that a treebank has to solve ( Xia , 2000b ,a ; Sornlertlamvanich et al ., 1997 , 1999 ) .
This paper focuses on the word segmentation , since the most basic unit of a treebank are words ( Di Sciullo and Edwin , 1987 ) , and defining `` words '' is the first step ( Xia , 2000b ,a ; Sornlertlamvanich et al ., 1997 , 1999 ) .
694
For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) ; '' Word segmentation is expected to break down the sentence at the boundaries of these words , not to split `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .
For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words , `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) '' . Word segmentation is expected to break down the sentence at the boundaries of these words , instead of splitting `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .
695
In such context , the extracted words are more appropriate for building a dictionary than for corpus-based language processing , which are out of the focus of this paper .
In such a context , the extracted words are more appropriate for building a dictionary , rather than for corpus-based language processing , which are outside of the scope of this paper .
696
Establishing a gold standard for Vietnamese word segmentation faces some diffcuties coming from the characteristics of the language .
Because of the discussed characteristics of the language , there are challenges in establishing a gold standard for Vietnamese word segmentation .
697
Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus on how to segment a sentence into words .
Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus as to the methodology for segmenting a sentence into words .
698
The disagreement is not only because of the different functions of blank spaces as mentioned above , but also because Vietnamese is not an inflectional language like English or Japanese , where morphological forms can be useful clues for word segmentation .
The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation .
699
While the similar problems also happen with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more diffcult because the modern Vietnamese writing system is based on Latin characters , which represents the pronunciation but not the meaning of words .
While similar problems also occur with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more difficult , because the modern Vietnamese writing system is based on Latin characters , which represent the pronunciation , but not the meaning of words .
700
This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language like English to a language that has not yet intensively studied .
This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language , like English , to a language that has not yet been intensively studied .
701
VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al ., a ) , which can be divided into three groups : single , compound , and special `` words '' .
VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al ., a ) , which can be divided up into three groups : single , compound , and special `` words '' .
702
The terminology tokens refers to text spans separated with each other by blank spaces .
The terminology tokens refers to text spans that are separated from each other by blank spaces .
703
Besides , we added a translation for each token when possible , so that the readers unfamiliar with Vietnamese can have an intuitive idea of how the compound words are formed .
Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed .
704
A special type of words in Vietnamese is classifer noun , denoted by the part-of-speech Nc in Table 2 .
A classifier noun , denoted by the part-of-speech Nc in Table 2 , is a special type of word in Vietnamese .
705
The analysis results revealed several types of inconsistent annotations , which are also
The analysis revealed several types of inconsistent annotations , which are also
706
Our analysis is based on two types of inconsistency : variation and structural inconsistency , whose definitions and details are given below .
Our analysis is based on two types of inconsistencies : variation and structural inconsistency , which are defined below .
707
Structural inconsistency : happens when different sequences have similar structures , thus should be splitted in the same way , but are segmented in different ways in the corpus .
Structural inconsistency : happens when different sequences have similar structures , and thus should be split in the same way , but are segmented in different ways in the corpus .
708
For example , `` con gái/girl '' and `` con trai/boy '' have similar structures , a combination of a classifier noun and a common noun Nc + N , so when `` con gái/girl '' is splitted and `` con trai/boy '' is not , it is considered as a structural inconsistency of Nc .
For example , `` con gái/girl '' and `` con trai/boy '' have similar structures : a combination of a classifier noun and a common noun , Nc + N , so when `` con gái/girl '' is split , and `` con trai/boy '' is not , it is considered as a structural inconsistency of Nc .
709
It is likely that structural inconsistency in word segmentation level makes the higher levels of processing , POS tagging and bracketing , become more complicated .
It is likely that structural inconsistency at the word segmentation level complicates the higher levels of processing , including POS tagging and bracketing .
710
The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in VTB treebank , following the definition of variation inconsistency above .
The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in the VTB , following the definition for variation inconsistency , above .
711
The precision of our method is high enough so that so we can use the extracted variations to study the insights of word segmentation problem .
The precision for our method is high , so we can use the extracted variations to provide insights on the word segmentation problem .
712
There are totally 54 patterns of POS sequence , of which top 10 confusing patterns , a long with their counts of 2-gram variations , and examples are shown in Table 4 .
There are a total of 54 patterns of POS sequences . The top 10 confusing patterns , their counts of 2-gram variations , and examples are depicted in Table 4 .
713
This problem seems to have caused a lot of trouble for the annotators of VTB .
This scenario has proven to be problematic for the annotators of VTB .
714
Furthermore , observing the POS patterns in Table 5 and Table 6 , we can see the potential of structural inconsistency , in particular for closed-set POS tags .
Furthermore , by observing the POS patterns in Table 5 and Table 6 , we can see the potential for structural inconsistency , particularly for closed-set POS tags .
715
First , we collected all affxes and classifier nouns in the VTB corpus . Then , extracted 2-grams containing these affxes or classifier nouns , which also are the structural inconsistencies .
We collected all affixes and classifier nouns in the VTB corpus , and then extracted 2-grams containing these affixes or classifier nouns , which are also structural inconsistencies .
716
Note that even though the sequence " con trai " is always splitted into two words throughout the corpus , it can still be an inconsistency if we consider similar structures such as " con gái " .
Even though the sequence , " con trai " is always split into two words throughout the corpus , it can still be an inconsistency , if we consider similar structures such as " con gái " .
717
In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent if we consider the higher analysis levels , POS tagging .
In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent , if we consider the higher analysis levels , POS tagging .
718
For example a classifier noun , e.g. , " con " in " con trai ( boy ) " , or " con gái ( girl ) " , can also be a simple word which means " I ( first person pronoun used by a child when talking to his / her parents ) " , or part of a complex noun " con cái ( children ) " .
For example a classifier noun , e.g. , " con " in " con trai ( boy ) " , or " con gái ( girl ) " , can also be a simple word , which means " I ( first person pronoun used by a child when talking to his / her parents ) " , or part of a complex noun " con cái ( children ) " .
719
Examing the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like percentage % in " 30% " , is splitted or combined with " 30 " .
By examining the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like a percentage ( % ) in " 30% " , is split or combined with " 30 " .
720
Checking structural inconsistency of these special characters including percentage% , hyphen - , and so on , we found quite a significant amount of inconsistent annotations .
By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations .
721
For example , the character % in " 30% " is splitted but is combined with the number in " 50 % " , which is considered as a structural inconsistency .
For example , the character , % , in " 30% " is split , but is combined with a number in " 50 % " , which is considered to be a structural inconsistency .
722
Note that although it can be argued that whether " N% " can be splitted into two words or combined in one word is dependent on the blank space in between N and " % " .
Note that it can be argued that splitting " N% " into two words or combined in one word is dependent on the blank space in-between N and " % " .
723
To improve the quality of VTB corpus , we extracted the probably problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency .
To improve the quality of the VTB corpus , we extracted the problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency .
724
For example , hyphens in date expressions like " 5-4-1975 " , which means the date " April the fifth , 1975 , " are combined with the numbers .
For example , hyphens in date expressions like " 5-4-1975 " , which refers to the date , " the fifth of April , 1975 , " are combined with the numbers .
725
or " , as in " 2-3 gi░ sáng " meaning " around 2 or 3 o’clock in the morning " , we decided to separate it from the surrounding numbers .
or " , as in " 2-3 gi░ sáng " , meaning " around 2 or 3 o’clock in the morning " , we decided to separate it from the surrounding numbers .
726
Text classification is defined as a task of determining for an input document the most suitable topic from the predefined topics .
Text classification is defined as a task of determining the most suitable topic from the predefined topics , for an input document .
727
The difference is that we performed for document level , not for sentence level .
The difference is that we performed the task at the document level , instead of at the sentence level .
728
A	phrase-based SMT system for English-Vietnamese translation was implemented .
A	phrase-based SMT system for English-Vietnamese translation was implemented .
729
There are two important conclusions can be drawn from these tables : ( 1 ) Quality of the treebank strongly affects the applications since our BASE model and most of other enhanced models improved the performance of TC and SMT systems ; ( 2 ) " Splitting " seems to be a good solution for word segmentation of controversial cases , including the split of variations , affxes , and classifier nouns .
There are two important conclusions that can be drawn from these tables : ( 1 ) The quality of the treebank strongly affects the applications , since our BASE model and most of the other enhanced models improved the performance of TC and SMT systems ; ( 2 ) " Splitting " seems to be a good solution for word segmentation for controversial cases , including the split of variations , affxes , and classifier nouns .
730
Except for STRUCT_NC , all the modifications to the original VTB corpus increase the performance of WS .
With the exception of STRUCT_NC , all of the modifications to the original VTB corpus increase the performance of WS .
731
Additionally , because our automatic methods for inconsistency detection could not cover all types of inconsistency in word segmentation annotation , further improvement of corpus quality is demanded .
Additionally , because our automatic methods for inconsistency detection could not cover all of the types of inconsistencies in word segmentation annotation , further improvement of corpus quality is demanded .
732
Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining affxes with their head nouns resulted in slightly better results for WS , TC , and does not change the performance of SMT .
Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining affxes with their head nouns resulted in slightly better results for WS and TC , and did not change the performance of SMT .
733
Another intention of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining .
Another part of the scope of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining .
734
Our experimental results showed that manual modification done for annotation of spe-cial characters and most of other word segmentation criteria significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , comparing with the use of the original VTB corpus .
Our experimental results showed that manual modification , done for annotation of special characters , and most other word segmentation criteria , significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , in comparison with the use of the original VTB corpus .
735
Although there are several approaches proposed to cope with this problem , their extremely high computational cost limits their scalability on largescale video datasets that may contain millions faces of hundreds characters .
Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .
736
Instead of using all faces in face-tracks to compute their similarity , our approach select representative faces for each face-track .
Instead of using all the faces in the face tracks to compute their similarity , our approach selects representative faces for each face track .
737
Their scales have not been considered in literature ever .
scales that have never been considered in the literature .
738
Since most of the news is related to human , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
739
On the other hand , efficiency is also an issue of such a face retrieval system beside its accuracy since scales of available datasets are getting larger rapidly , for instance , exceeding thousands hours of videos with millions faces of hundreds character .
Besides accuracy , efficiency is also an issue in such a face retrieval system because the scales of available datasets are rapidly getting larger , for instance , exceeding thousands of hours of videos with millions of faces of hundreds of characters .
740
Generally , there are two principle steps in a face retrieval system .
Generally , a face retrieval system consists of two principal steps .
741
While conventional approaches consider single face images as the basic units for extracting and matching \CITE , recently proposed approaches sifted towards sets of face images called face-tracks .
Whereas conventional approaches consider single face images as the basic units in extracting and matching \CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .
742
Face images in a face-track may present the corresponding character under different viewpoints and facial expressions ( as shown in Figure 1 ) .
The face images in a face track may present the corresponding character from different viewpoints and with different facial expressions ( as shown in Figure 1 ) .
743
By exploiting the plenteous information from multiple exemplar faces in face-tracks , face-track based approaches are expected to achieve more robust and stable performance.
By exploiting the plenteous information from the multiple exemplar faces in the face tracks , face track-based approaches are expected to achieve a more robust and stable performance.
744
Once all face-tracks in video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .
Once all the face tracks in the video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .
745
There are several approaches introduced to deal with this problem \CITE .
Several approaches have been introduced to deal with this problem \CITE .
746
In these works , image set has been modeled in different way , such as distributions \CITE , subspaces \CITE , convex geometric region in feature space \CITE , or more general manifolds \CITE .
Using these approaches , the image set has been modeled in different ways , including as distributions \CITE , subspaces \CITE , a convex geometric region in a feature space \CITE , or more general manifolds \CITE .
747
Although these approaches shown promising results on benchmark datasets , they require high computational costs to characterize the representation of face-tracks , such as computing the convex geometric region in \CITE , the probability in \CITE , and the eigenvectors in \CITE .
Although these approaches have shown promising results in benchmark datasets , they require high computational costs to characterize the representation of face tracks , such as computing the convex geometric region in \CITE , the probability in \CITE , and the eigenvectors in \CITE .
748
Working toward solving the above problems , our contributions in this paper is three-fold.
This paper provides a threefold contribution toward solving the above problems , .
749
, We introduce an approach for this purpose .
For this purpose , we introduce an approach .
750
, In constrast to the approach in \CITE , which is failed to deal with specific problems of news video caused by sudden illumination change and partial occlusion , our approach is incorporated techniques to overcome the problems .
Our approach incorporates techniques to overcome specific problems with news video caused by sudden illumination change and partial occlusion , in contrast to the approach in \CITE , which failed to deal with , these problems .
751
We introduce an approach which significantly reduces the computational cost for face-track matching while maintaining a competitive performance compare to those of the state-of-the-art approaches .
We introduce an approach that significantly reduces the computational cost for face-track matching while maintaining a competitive performance with state-of-the-art approaches .
752
Based on the observation that face-tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all faces in a face-track for learning the variation of faces .
Based on the observation that face tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all the faces in a face track for learning the variation of faces .
753
The size of the set is much smaller than the size of original face-track .
The size of the set is much smaller than that of the original face track .
754
Then , the mean face of sampled faces in the set is computed .
The , mean face of the sampled faces in the set is then computed .
755
The second dataset is observed from NHK News7 channel in 11 years .
The second dataset includes 1.2 million faces of 111 individuals observed in the NHK News 7 program over 11 years .
756
In this dataset , 1.2 millions faces of 111 individuals are provided .
, .
757
Section 5 presents our experimental settings , .
Section 5 presents our experimental settings , and Section 6 provides our .
758
Then , in the second step , detected faces of the same character will be grouped by using either clustering approaches \CITE or tracking approaches \CITE .
, In the second step , the detected faces of the same character are grouped by using either clustering \CITE or tracking approaches \CITE .
759
Limitations of this approach includes the expensive computational cost for constructing and clustering high dimensional representation feature vectors; and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure no group contains faces of multiple characters and groups are not over-fragmented.
The limitations of this approach include its high computational cost for constructing and clustering high-dimensional representation feature vectors and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure that no group contains faces of multiple characters and that groups are not over-fragmented.
760
There are two major categories of approaches target to employ multiple-exemplar of faces in face-tracks ( i.e. , sets of face images ) for robust face matching and recognition .
There are two major categories of approaches to using multiple exemplars of faces in face tracks ( i.e. , sets of face images ) for robust face matching and recognition .
761
Given a face-track as an input retrieval query , the online stage of our system starts by using our proposed face-track matching algorithm ( described in Section 5 ) to estimate the similarity between a query face-track and each face-track in the retrieved set containing all face-tracks extracted from the dataset in the offline stage .
Given a face track as an input retrieval query , the online stage of our system starts by using our proposed face-track matching algorithm ( described in Section 5 ) to estimate the similarity between a query face track and each face track in the retrieved set containing all face tracks extracted from the dataset in the offline stage .
762
A common strategy of existing approaches for face-track extraction consists of detecting faces in frames and grouping detected faces of the same character .
A common strategy in the existing approaches to face-track extraction consists in detecting faces in frames and grouping detected faces of the same character .
763
Its problems as it is applied to news video and our proposed solutions to overcome the problems is then presented.
We then present the problems with this approach as applied to news video and our proposed solutions.
764
To group detected faces into face-tracks , connections between faces belonging to the same character in different frames should be established .
To group detected faces into face tracks , connections should be established between faces belonging to the same character in different frames .
765
in \CITE propose to use KLT tracker for this purpose .
in \CITE proposed the use of a KLT tracker for this purpose .
766
To successfully connect actual faces of the same character in different frames , track points generated for the first face should be tracked and retained inside the latter faces for a sufficient number of shared points between faces .
To successfully connect actual faces of the same character in different frames , the track points generated for the first face should be tracked and retained inside the latter faces for a sufficient number of shared points between faces .
767
Firstly , \CITE , our approach does not compare all possible pairs of faces in a shot for face grouping as in \CITE .
First , unlike in \CITE , our approach does not compare all possible pairs of faces in a shot for face grouping\CITE; .
768
, They can not enrich information of its corresponding face-track , but may add noise .
Thus , the faces cannot enrich the information on its corresponding face track , but may only add noise .
769
To indetify flash-frames , we measures the brightness of frames in the video shot .
To identify flash frames , we measure the brightness of the frames in the video shot .
770
If the brightness of a frame significantly increases compared with those of its neighbors , the frame is declared as a flash-frame and is skipped for processing .
If the brightness of a frame is significantly increased compared with its neighbors , the frame is declared a flash frame and skipped in processing .
771
This helps us to save computational cost as well as to avoid tracking errors caused by transition effects between shots .
This helps us to save on computational cost and avoid tracking errors caused by transition effects between shots .
772
If there are faces detected , each face is checked against all existing facetracks formed in the previous frames to find out which facetrack it belongs to .
If there are faces detected , each face is checked against all the existing face tracks formed in the previous frames to find out to which face track the face belongs .
773
Checking between a face and a facetrack is based on enumerating points shared by both the face and the last appeared face of the face-track .
The checking between a face and a face track is based on enumerating the points shared by both the face and the last face that appeared on the face track .
774
A face which can not be grouped into any face-track is treated as an initial face of a new face-track .
A face that cannot be grouped into any face track is treated as the initial face of a new face track .
775
We can ensure that there are always track points for all faces appear in the shot .
We can ensure that there are track points for all faces that appear in the shot .
776
We remove all points which are inside the last appeared face of the face-track but not inside the current face , and vice versa .
We remove all points that are inside the last face that appeared on the face track but are not inside the current face , and vice versa .
777
However , instead of using all faces in a face-track , we propose to subsample the faces .
However , instead of using all the faces in a face track , we propose taking a subsample of faces .
778
By doing that , the require computational cost can be reduced while a sufficient amount of information is kept for improving accuracy .
In doing so , the required computational cost can be reduced while keeping the amount of information sufficient to improve accuracy .
779
Given a specific value of k , which indicates the expected size of the sub-sampled set of a face-track , the approach starts by dividing each face-track into k parts following its temporal order .
Given a specific value of k , which indicates the expected size of the subsampled set of a face track , the approach starts by dividing each face track into k parts according to the temporal order of appearances .
780
Let denote mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} are two mean faces of two face-track A and B , respectively , with N imposes the number of dimension of the feature space .
Let mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} denote the mean faces of face tracks A and B , respectively , with N representing the number of dimensions of the feature space .
781
An illustration of our k-Faces , is shown in Figure 4 .
Figure 4 illustrates our k-Faces , with the following .
782
Its pseudo-code is presented as follows .
pseudo-code: .
783
Clearly , the higher value of k is selected , the more faces in each face-track are selected to compute the representative face of the face track .
Clearly , the higher the value of k selected , the more faces in each face track selected to compute the representative face and the .
784
By using k as a predefined parameter , k-Faces provides flexibility for users in balancing their expected accuracy and the cost which they can afford ( or time they can wait for the result ).
By using k as a predefined parameter , k-Faces provides users with flexibility in balancing the accuracy they expect and the cost they can afford ( or the time they can spend waiting for the result ).
785
Besides that , since k-Faces averages multiple faces for a representative face of a face-track , the effects of noisy or outliers faces on estimating the similarity of face-tracks will be substantially reduced.
Besides , because k-Faces averages multiple faces for the representative face of a face track , the effects of noisy or outlier faces on estimating the similarity of face tracks will be substantially reduced.
786
Evaluation of the proposed approach for face-track matching is given in the second part.
face-track matching.
787
A face detector based on Viola-Jones approach \CITE was used for detecting near frontal faces in every frame of these video sequences .
A face detector based on the Viola-Jones approach \CITE is used to detect near frontal faces in every frame of the video sequences .
788
A face-track of one character appearing in a video shot is annotated by indexes of the frames which the first face and the last face of that character occur .
Each face track of a character appearing in a video shot is annotated by indexes of the frames in which the first face and the last face of that character occur .
789
Note that if a character moves out of the frame then moves in again , annotators will divide the appearance of that character into two independent face-tracks in our ground-truth .
Note that if a character moves out of the frame and then moves back into it again , annotators will divide the appearance of that character into two independent face tracks in ground-truth annotation .
790
The number of frames , faces , and face tracks are shown in Table 1 .
Table 1 shows the number of frames , faces , and face tracks .
791
in handling problem caused by partial occlusion and appearance of character in the middle of a shot .
in handling problems caused by partial occlusion and the appearance of a character in the middle of a shot .
792
Thus , there is no clue to re-group face of that person after such full occlusions .
After such full occlusions , there is no clue to regrouping the face of that person .
793
To handle this problem , using only tracker is not enough .
, Using only a tracker is not enough to handle this problem .
794
However , we observe that fully occlusion is rarely happened in news video since characters reported in the news are recorded with care , especially with important and well-known character .
Nevertheless , we observe that full occlusion rarely happens in news video because the characters featured in the news are recorded with care , especially the important and well-known ones .
795
However , our complexity is somehow linear to total number of face , because we consequently enlarge face-tracks following temporal order by checking new faces with only one last appeared face of each face-track .
However , our complexity is somehow linear to the total number of faces , because we consequently enlarge face tracks according to the temporal order by checking new faces with only the last face that appeared on each face track .
796
If this number is getting larger , the gap in speed between our approach and the approach by Everingham et al .
If the number of faces increases , the gap in speed between our approach and that by Everingham et al .
797
Because all presented problems here , such as those due to flash , occlusion , and in-the-middle face appearance , are practically observed , overcoming them is vital for practical application .
Because all the problems presented here , such as those due to flash , occlusion , and in-the-middle face appearance , are practically observed , overcoming them is vital for the practical application of our approach .
798
Face-tracks in videos of the datasets are extracted by using our proposed approach for face-track extraction ( see section 4.2 ) .
Face tracks are extracted from videos of the datasets by using our proposed approach to face-track extraction ( see section 4.2 ) .
799
This dataset is observed from NHKNews7 channel in 11 years .
This dataset consists of observations from the NHK News 7 program over 11 years .
800
, It is obvious that our datasets are extremely higher than datasets , such as MoBo and Honda / UCSD , on all statistical terms , including the number of videos , characters , and average length of face-track .
Based on the results , it is obvious that our datasets are superior over the other datasets , such as MoBo and Honda / UCSD , on all statistical terms , including number of videos , number of characters , and average face-track length .
801
Compared to Youtube Faces dataset , although ours have less number of character ( or subjects ) , we provide much more face-tracks ( or video shots ) per character , .
Compared to the YouTube Faces dataset , , we provide much more face tracks ( or video shots ) per character , although our datasets have smaller numbers of characters ( or subjects ) .
802
Statistical information of our datasets is given in the Figure 5 .
Figure 5 presents statistical information on our datasets .
803
A feature vector of a face is extracted by computing descriptors of the local appearance of the face around each of the located facial features .
The feature vector of a face is extracted by computing the descriptors of the local appearance of the face around each of the located facial features .
804
Then , appearance descriptors are computed around each facial feature .
Then , the appearance descriptors around each facial feature are computed .
805
They then use the maximum distance , the minimum distance , or the mean distance of the computed pair-wise distances as the similarity measurement between two face-tracks .
The maximum distance , the minimum distance , or the mean distance of the computed pair-wise distances is the used as the similarity measurement between two face tracks .
806
All of these approaches had been shown their robustness on benchmark datasets , such as MoBo , HondaUCSD , and Youtube Faces .
All of these approaches have shown their robustness in benchmark datasets , such as MoBo , HondaUCSD , and YouTube Faces .
807
Besides evaluating k-Faces with different values of k as well as different types of distance ( e.g. , Euclidean , L1 , cosine ) , we try another criterion to select k representative faces in a face-track .
Besides evaluating k-Faces with different values of k and different types of distance ( e.g. , Euclidean , L1 , and cosine ) , we try another criterion for selecting k representative faces in a face track .
808
In the original way , we proposed to select these faces by partitioning the face-track following temporal order and selecting the middle face of each partition .
In the original way , we proposed selecting these faces by partitioning the face track according to the temporal order and choosing the middle face of each partition .
809
However , an yet another criterion can be applied to select these representative faces is based on clustering .
However , another criterion that is based on clustering can be applied in selecting these representative faces .
810
We evaluate performance of a face-track matching approach by computing the average precision on the rank list returned by the approach .
We evaluate the performance of a face-track matching approach by computing the average precision of the rank list that it returned .
811
In particular , for each dataset , each face-track is alternatively picked out as a query facetrack , while the remaining face-tracks are used as the retrieved database .
In particular , in each dataset , a face track is alternatively picked out as a query face track , while the remaining face tracks are used as the retrieved database .
812
, Average precision of the returned ranked list is computed , given a query .
Given a query , the average precision of the returned ranked list is computed , .
813
Finally , the mean of all average precision ( MAP ) from all query is reported as the overall evaluation metric for the approach on the database.
Finally , the mean of all average precision ( MAP ) values for all queries is reported as the overall evaluation metric for the approach with the given database.
814
Let denote r as a rank in the returned face-track list , Pre( r ) as is the precision at the rank r of the list , Nl as the length of the list , Nhit as the total number of face-tracks matched with the query face-track q , and I sMatched( k ) as a binary function returning 1 if the face-track at rank r is matched with q ( based on ground-truth annotations ) , zero otherwise .
Let r denote a rank in the returned face-track list , Pre( r ) the precision at rank r of the list , Nl the length of the list , Nhit the total number of face tracks matched with the query face track q , and I sMatched( k ) a binary function returning 1 if the face track at rank r is matched with q ( based on ground-truth annotations ) and , zero otherwise .
815
Meanwhile , , the best MAP is 60.99% , and the worst MAP is 42.75% on NHKNews7 dataset .
Meanwhile , in the NHKNews7 dataset , the best MAP is 60.99% , and the worst is 42.75% .
816
Secondly , videos in NHKNews7 are recorded during a long time ( i.e. , 11 years ) .
Second , the videos in NHKNews7 were recorded over a long time ( i.e. , 11 years ) .
817
Thus , besides facial variations caused by enviromental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) in each face-track , face-tracks of a character themself also contain biological variation of the character during time .
Thus , besides facial variations in each face track caused by the environmental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) , the face tracks of the character themselves also reflect the biological variations of the character over time; .
818
At the same time , it eliminates the influence of pairs containing identical faces , which can help to instantly determine they are belong to the same character .
At the same time , it eliminates the influence of pairs containing identical faces , which can help to instantly determine that the faces belong to the same character .
819
Since the average length of face-tracks on NHKNews7 is longer ( i.e. , each face-track contains more sample faces of a character ) , there is more chance that two face-tracks of the same character contain identical faces.
Because the average length of face tracks on NHKNews7 is longer ( i.e. , each face track contains more sample faces of a character ) , there is a greater chance that two face tracks of the same character contain identical faces.
820
Since k-Faces.KMeans always use all faces in a facetrack for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
Because k-Faces.KMeans always uses all the faces in a face track for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
821
The higher k is set , the more representative faces of each facetrack are selected .
The higher the k set , the more representative faces of each face track selected .
822
Meanwhile , its disadvantage is the expensive computational cost to perform clustering faces on a high dimensional feature space ( i.e. , 1937 dimensions ) .
However , its disadvantage is the high computational cost of clustering faces on a high-dimensional feature space ( i.e. , 1,937 dimensions ) .
823
Since keep increasing k does not help to obtain imporant accuracy improvement but expensive computational cost , we select k = 20 to investigate the trade-off between accuracy and computational costs of k-Faces approaches compared to others .
Because further increasing k does not help improve accuracy but increases the computational cost , we select k = 20 for investigating the trade-off between the accuracy and computational cost of k-Faces approaches in comparison to others .
824
We report MAP and processing time of each approach in the Table 5 .
Table 5 shows the MAP and processing time of each approach .
825
Processing time is separated into two parts , corresponding to preprocessing time and matching time .
Processing time is divided into two parts , preprocessing and matching .
826
Preprocessing time presents time required for preprocessing face-tracks before matching .
The preprocessing time refers to the time required to preprocess face tracks before matching .
827
This suggest that , selecting presentative faces based on tempo .
This suggests that in terms of both accuracy and efficiency , selecting representative faces based on temporal sampling is better than that based on clustering .
828
ral sampling is better than one based on clustering , in both terms of accuracy and efficiency.
,
829
The gap with pair:min is 2.89% difference in MAP .
The difference in MAP between our approach and pair:min is 2.89% .
830
This is due to the fact that face-tracks on NHKNews7 dataset is larger than those on Trecvid dataset .
The reason for this is the fact that the face tracks in the NHKNews7 dataset are larger than those in the Trecvid dataset .
831
Thirdly , we prepare , evaluate state-of-the-art face retreival approaches , and publish real-world face-track datasets whose scale have not been considered in literature ever.
Third , we prepare datasets , evaluate state-of-the-art face retrieval approaches , and publish real-world face-track datasets of such scales that have never been considered in the literature.
832
As a result , the exponential growth of image repositories creates the urgent needs for searching images . Because of its importance and wide applications , image search has attracted more interest in recent years .
The resulting exponential growth of image repositories , however , has created an urgent need for effective ways of searching images .
833
In a typical scenario of image search , users supply a query item which is usually represented by a region cropped from an image .
Moreover , image search has gained interest in recent years because of its importance and wide range of applications . In a typical scenario , users supply a query item , which is usually a region cropped from an image .
834
Several extensive works have been conducted with great interest on improving search performance \CITE .
Extensive studies have been conducted with an eye to improving the performance of this sort of search \CITE .
835
A normal user without prior knowledge about the retrieved database has no choice but search by trial-and-error .
A normal user without prior knowledge about a database has no choice but to search it by trial-and-error .
836
Second , even if a candidate item is known , enumerating its occurrences in the database is not trivial because it is subject to many variations such as viewpoint and scale changes , rotation or occlusion .
Second , even if a candidate item is known , enumerating its occurrences in the database is a not trivial task because it is subject to many variations in viewpoint , scale , rotation , occlusion , etc.
837
Furthermore , scanning over all regions in images of the database will inevitably be prohibitive , if not infeasible for practical purposes .
Furthermore , the cost of scanning all regions of the images of the database will inevitably be prohibitive for practical purposes .
838
In this paper , we employ state-of-the-art techniques such as SIFT and Bag-of-Words ( BoW ) model to handle matching regions under variations .
In this paper , we employ state-of-the-art techniques such as SIFT and the Bag-of-Words ( BoW ) model to handle the task of matching regions with the above variations .
839
Our main focus is an efficient approach for quantifying occurences of candidate items over the database to generate recommendations .
Our main focus is to devise an efficient approach for quantifying occurrences of candidate items in the database in order to generate recommendations .
840
The efficiency advantages of our approach come from various methodologies .
The advantage in efficiency comes from our use of various methodologies .
841
By applying the approach instead of other naive sampling approach such as sliding windows , the number of items ( i.e. regions ) that need to be processed in each image dramatically reduces .
By applying this approach instead of a naive sampling approach such as sliding windows , we were able to dramatically reduce the number of items ( i.e. regions ) that need to be processed in each image .
842
Given two sets of regions , one contains regions of candidate items in the initial query image and the other contains regions of items in images of the database .
Given two sets of regions , one containing regions of candidate items in the initial query image and the other containing regions of items in images of the database , our task is to find occurrences of all candidate items in the database .
843
Finding occurences of all candidate items in the database can be equally treated as finding pairs of matched regions , knowing a pair is formed by a region in one of the sets with a region in the other .
This task can be equivalently treated as finding pairs of matched regions , knowing that a pair is formed by a region in one of the sets with a region in the other .
844
So , if top region pairs with sufficient high similarity scores are found , we can enumerate occurences of the items .
So , if the top region pairs are found with sufficiently high similarity scores , we can enumerate the occurrences of the items .
845
Based on these insights , we make an yet another efficiency boost by formulating the problem as an optimization problem which can be solved by applying a branh-and-bound algorithm .
Based on this insight , we can boost efficiency yet again by formulating the problem as an optimization problem that can be solved by applying a branch-and-bound algorithm .
846
In order to do that , we introduce a novel representation based on hierarchical structure to describe a set of region pairs and a corresponding function bounding the similarity scores of pairs over such a set .
In order to do that , we introduce a novel representation based on a hierarchical structure describing a set of region pairs and a corresponding function bounding the similarity scores of pairs over such a set .
847
However , by doing that , extra costs for mining unnecessary items , which appear in the database but the initial query image , arise accordingly .
However , doing that incurs the extra cost of mining unnecessary items that appear in the database , but not in the initial query image .
848
VQS requires an initial text query for suggestion formulation and its suggestions are both keywords and images .
VQS requires an initial text query for formulating the suggestion , and its suggestions are both keywords and images .
849
VQS proposes to help users to overcome query ambiguity formulation by precisely expressing search intents , assuming relevant items are always available .
VQS proposes to help users to overcome their tendency to formulate ambiguous queries by precisely expressing search intents , assuming the relevant items are always available .
850
However , ours is differentiated in the way we represent sets of region pairs , instead of sets of regions only .
However , ours is differentiated from the other studies in that we represent sets of region pairs , instead of only sets of regions .
851
Although coordinate intervals as in ESS ( or ESR ) can be extended to represent set of region pairs , such criterion may suffer the branch-and-bound algorithm from curse-of-dimensionality problem since the number of dimension required is at least doubled .
Although coordinate intervals as in ESS ( or ESR ) can be extended to represent sets of region pairs , such a criterion in the context of the branch-and-bound algorithm may suffer from the curse-of-dimensionality problem since the number of dimensions required at least doubles .
852
The framework of Recommend-Me consists of 4 main steps towards formulating final recommendations for users .
The framework of Recommend-Me consists of four main steps .
853
More importantly , human users are often get attracted by object-like items .
More importantly , users are often attracted by object-like items .
854
Then , it performs a greedy algorithm which iteratively merges the two most similar regions together until the whole image becomes a single region .
Then , it performs a greedy search < ? ?> algorithm that iteratively merges the two most similar regions together until the whole image becomes a single region .
855
In this step , we perform our proposed approach , explained in Section 3 , to find top \MATH ( an expected number of returned region pairs ) of such pairs in the pool .
In this step , we use the approach explained in Section 3 to find the top \MATH ( the expected number of returned region pairs ) of such pairs in the pool .
856
Given \MATH region pairs returned in Step 2 and assuming each region pair in \MATH pairs is formed by a candidate item and its corresponding match , we now can enumerate the number of occurences of the items .
Given \MATH region pairs returned in Step 2 and assuming each region pair in the \MATH pairs is formed by a candidate item and its corresponding match , we can enumerate the number of occurrences of the items .
857
They are perceived as the same item by human being .
These regions would be perceived as the same item by users .
858
With a similarity function \MATH , we have to solve the following optimization problem in order to find the region pair \MATH with the highest similarity score .
By using the similarity function \MATH , we have to solve the following optimization problem in order to find the region pair \MATH with the highest similarity score .
859
Once \MATH is found , we can obtain the other top region pairs by continuing the search processs with the remaining search spaces , in which found top pairs eliminated .
Once \MATH is found , we can obtain the other top region pairs by continuing the search process with the remaining search space , in which the found top pairs have been eliminated .
860
Given such structures , we show in the following how a branch-and-bound algorithm applied to our problem .
Given such structures , we show in what follows how the branch-and-bound algorithm can be used to solve our problem .
861
An illustration of a branching step is given in Figure \REF .
The branching step is illustrated in Figure \REF .
862
We then rely on NHI to define \MATH bounding the values of \MATH , with : \MATH
We will then rely on NHI to define \MATH bounding the values of \MATH , with : \MATH
863
The algorithm examines next the set having highest bounding value \MATH .
The algorithm examines the set having the highest bounding value \MATH . //<" next " is unclear . The rewrite is a guess .>
864
To obtain more than one region pair , we simply continue the loop in the Algorithm 1 until the expected number of region pairs \MATH have been reached .
To obtain more than one region pair , we simply repeat the loop in Algorithm 1 until the expected number of region pairs \MATH is reached .
865
As a result , we have the constraint ( b ) satisfied .
As a result , constraint ( b ) is satisfied .
866
The new hierarchical structure therefore satisfy both the constraints ( a ) and ( b ) .
The new hierarchical structure therefore satisfies both constraints .
867
An illustration is presented in Figure X .
Figure X is an illustration of this organization .
868
Given multiple hierarchical structures returned from the first stage , we use their root nodes as initial elements to construct an yet another hierarchical structure over them by divisive clustering .
If multiple hierarchical structures are returned by the first stage , we use their root nodes as the initial elements to construct an yet another hierarchical structure over them by divisive clustering . //<the rewrite is a guess .>
869
At last , by unifying results of both stages , we have a unique hierarchical structure over the set of regions of multiple images , which satisfies the both constraints .
Finally , by unifying the results of both stages , we have a unique hierarchical structure over the set of regions of multiple images , which satisfies both constraints .
870
Note that , because the hierarchical for regions of images in the database is independent of query , we construct it only one time .
Note that because the hierarchy of the regions of images in the database is independent of the query , we construct it only one time .
871
We call such recommendations as hit recommendations . Thus , a good recommendation system should accurately provide such hit recommendation to users .
We call such recommendations" hit recommendations " , and a good recommendation system should accurately provide them to users .
872
Based on those insights , we evaluate Recommend-Me system using two evaluation metrics : precision on introducing recommendation and rank of the first hit recommendation on the list .
Based on these insights , we evaluated the Recommend-Me system using two evaluation metrics : precision in pesenting recommendations and rank of the first hit recommendation on the list .
873
Given an initial query image with ground-truth annotation indicates bounding box of an item known existed in the database , Recommend-Me is determined to precisely introduce recommendation if at least one of its recommendation is a hit recommendation .
Given an initial query image with ground-truth annotations indicating the bounding box of an item known to existin the database , Recommend-Me will provide a recommendation if at least one of its recommendations is a hit recommendation .
874
We apply an approach used in Pascal VOC challenge to clarify whether a recommendation is a hit recommendation .
We used an approach from the Pascal VOC challenge to clarify whether a recommendation is a hit recommendation or not .
875
To evaluate the efficiency of Recommend-Me on finding \MATH region pairs with highest similarity scores , we compute the number of evaluation for the quality bounding function in the branch-and-bound algorithm .
To evaluate the efficiency of Recommend-Me in finding \MATH region pairs with the highest similarity scores , we computed the number of evaluations for the quality bounding function in the branch-and-bound algorithm .
876
This number is then divided by the size of all possible region pairs formed by regions in the initial query image and regions in images of the database .
This number was then divided by the size of all possible region pairs formed by regions in the initial query image and regions in images of the database . //< " the size of all possible region pairs " is unclear to me . Do you mean " the average size of all possible region pairs " or " the sizes of all possible region pairs " ?>
877
Visual words in images are located by dense grid sampling and Different-of-Gaussian( DoG ) detector .
Visual words in the images were located by using dense grid sampling and a Different-of-Gaussian ( DoG ) detector .
878
%to cluster points on a set of random images .% Additionally , the set of interest points obtained by DoG in the query image is used to remove regions without any of such points inside .
%to cluster points on a set of random images .% Additionally , the set of points of interest obtained by the DoG in the query image was used to remove regions without any such points inside .
879
In this experiment , we used two color channels which are RGB and Hue , since regions generated on those channels can cover 99 .72\% area of the annotated item regions in our dataset .
In this experiment , we used two color channels , RGB and Hue , since the regions generated on those channels can cover 99 .72\% of the area of the annotated item regions in our dataset .
880
A virtual root node is created to compose two color-dependent binary trees into one unique binary tree for each image .
A virtual root node was created to compose two color-dependent binary trees into one unique binary tree for each image .
881
Bron-Kerbosch algorithm is then applied to find all maximal cliques in the graph .
The Bron-Kerbosch algorithm was then applied to find all maximal cliques in the graph .
882
The reported numbers of precision , rank of the first hit recommendation as well as efficiency improvement are averaged as we perform Recommend-Me with 375 different initial query images and an individual value of \MATH .
The reported precision , rank of the first hit recommendation , and efficiency improvement are averages of Recommend-Me on 375 different initial query images and an individual value of \MATH .
883
By increasing \MATH , we obtain more region pairs with sufficient hight similarity scores .
By increasing \MATH , we can obtain more region pairs with sufficiently high similarity scores .
884
In this paper , we introduced a new system , named Recommend-Me , for visual query suggestion .
We described oursystem , named Recommend-Me , for making visual query suggestions .
