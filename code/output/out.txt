--------------------------{1=>"The target parsers are adapted to sentences of these constructions extracted from fiction and query texts .\n", 2=>"The target parsers are adapted to the sentences for these constructions extracted from fiction and query texts .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}
nil_first --> [7]nil_second --> [6, 8]--------------------------{1=>"Analysis of the experimental results illustrates the need to handle different sentence constructions through fundamental improvement of the parsers such as re-construction of feature designs .\n", 2=>"The analysis of the experimental results will illustrate the necessity for handling various sentence constructions by fundamental improvement of parsers such as re-construction of feature designs .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#7#5#stem -1#8#6#exact -1#9,10#7,8#para -1#11#9#stem -1#12#10#para -1#13#11#exact -1#14#12#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#0#17#lc -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}
nil_first --> [13]nil_second --> [6, 15]--------------------------{1=>"Recent research on parsing technologies has achieved high parsing accuracy in the same domain as the training data , but once we move to unfamiliar domains , the performance decreases to unignorable levels .\n", 2=>"Recent research on parsing technologies has achieved high parsing accuracies on the same domains as the training data , but once we move to unfamiliar domains , the performances decrease at unignorable levels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10,11,12#10,11,12#para -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#stem -1#29#29#stem -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}
nil_first --> [30]nil_second --> [30]--------------------------{1=>"Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .\n", 2=>"Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .\n", 3=>"-1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#28#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}
nil_first --> [0, 1, 28]nil_second --> [0, 1, 8]--------------------------{1=>"However , there are some cases where we cannot achieve such high parsing accuracy as parsing the Penn Treebank ( PTB ) merely by re-training or adaptation .\n", 2=>"However , there are some cases where we cannot achieve as high parsing accuracies as parsing the Penn Treebank just by re-training or adaptation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#22#syn -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact \n"}
nil_first --> [10, 19, 20, 21]nil_second --> [10]--------------------------{1=>"For example , the parsing accuracy for the Brown corpus is significantly lower than that for the Wall Street Journal ( WSJ ) portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .\n", 2=>"For example , the parsing accuracy for the Brown corpus is significantly lower than for the WSJ portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#21#exact -1#17#23#exact -1#18#24#exact -1#19#25#exact -1#20#26#exact -1#21#27#exact -1#22#28#exact -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact -1#28#34#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#32#38#exact -1#33#39#exact -1#34#40#exact -1#35#41#exact -1#36#42#exact -1#37#43#exact -1#38#44#exact \n"}
nil_first --> [14, 17, 18, 19, 20, 22]nil_second --> []--------------------------{1=>"This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions that have not been extensively studied in recent parsing research : imperatives and questions .\n", 2=>"This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions which were not extensively studied in the recent parsing research : imperatives and questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#21#20,21#para -1#20#22#syn -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}
nil_first --> [19]nil_second --> [19, 25]--------------------------{1=>"In these constructions , words in certain syntactic positions disappear or the order of the words changes .\n", 2=>"In these constructions , words in some syntactic positions disappear or the orders of words change .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14,15#para -1#15#16#stem -1#16#17#exact \n"}
nil_first --> [6]nil_second --> [6]--------------------------{1=>"To do so , we first prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .\n", 2=>"In order to do so , we prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11,12#10,11,12,13#para -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}
nil_first --> [5]nil_second --> [0, 1, 13, 14]--------------------------{1=>"In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracy of their part-of-speech ( POS ) tagger .\n", 2=>"In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracies of a part-of-speech tagger for them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22,23,24#22,23,24,25#para -1#26#26#stem -1#27#27#exact -1#31,32#28#para -1#29#29#exact -1#30#33#exact -1#33#34#exact \n"}
nil_first --> [30, 31, 32]nil_second --> [25, 28]--------------------------{1=>"Since domain adaptation is an extensive research area in parsing research \\CITE , many ideas have been proposed , including un- or semi-supervised approaches \\CITE and supervised approaches \\CITE .\n", 2=>"Since domain adaptation has been an extensive research area in parsing research \\CITE , a lot of ideas have been proposed , including un- / semi-supervised approaches \\CITE and supervised approaches \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact \n"}
nil_first --> [13, 21]nil_second --> [14, 15, 16, 24]--------------------------{1=>"The main focus of these works is on adapting parsing models trained with a specific genre of text ( in most cases the Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .\n", 2=>"Their main focus was on adapting parsing models trained with a specific genre of text ( in most cases Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#13#3#exact -1#3#5,6#para -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#26#16#exact -1#27#17#exact -1#15#18#exact -1#16,17,18#19,20,21,22#para -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#14#31#exact -1#28#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact \n"}
nil_first --> [4, 30]nil_second --> []--------------------------{1=>"The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression .\n", 2=>"A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions .\n", 3=>"-1#10#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#stem -1#9#7#exact -1#11#8,9#para -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#syn -1#18#16#exact -1#19#17#stem -1#20#18#exact \n"}
nil_first --> []nil_second --> [0, 6, 8]--------------------------{1=>"Compared to domain adaptation , structural types of sentences have received little attention to date .\n", 2=>"Compared to domain adaptation , structural types of sentences have gained little attention to date .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}
nil_first --> [10]nil_second --> [10]--------------------------{1=>"This work highlighted the low accuracy of state-of-the-art parsers on questions , and proposed a supervised parser adaptation by manually creating a treebank of questions .\n", 2=>"The work pointed out low accuracy of state-of-the-art parsers on questions , and proposed supervised parser adaptation by manually creating a treebank of questions .\n", 3=>"-1#1#1#exact -1#0#3#lc -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#20#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}
nil_first --> [0, 2, 21]nil_second --> [2, 3]--------------------------{1=>"QuestionBank was used for the supervised training of an LFG parser , resulting in a significant improvement in parsing accuracy .\n", 2=>"QuestionBank was used for the supervised training of an LFG parser , and achieved a significant improvement in parsing accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#17#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}
nil_first --> [12, 17]nil_second --> [12, 13]--------------------------{1=>"In this work , question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories .\n", 2=>"They collected question sentences from TREC 9-12 competitions , and annotated these sentences with POSs and CCG lexical categories .\n", 3=>"-1#8#3#exact -1#2#4#exact -1#3#5#exact -1#1#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#9#12#exact -1#10#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}
nil_first --> [0, 1, 2, 6, 15]nil_second --> [0, 11, 12, 14]--------------------------{1=>"The authors reported a significant improvement in CCG parsing without phrase structure annotations .\n", 2=>"They reported a significant improvement in CCG parsing without phrase structure annotations .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"Although QuestionBank and the resource of \\CITE claim to be corpora of questions , they are biased because the sentences come from QA queries .\n", 2=>"Although QuestionBank and the resource of \\CITE are claimed to be a corpus of questions , they are biased because the sentences come from QA queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10#7,8,9#para -1#12#10#syn -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}
nil_first --> []nil_second --> [7, 11]--------------------------{1=>"For example , such queries rarely include yes / no questions or tag questions .\n", 2=>"For example , such queries rarely include yes / no questions and tag questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}
nil_first --> [11]nil_second --> [11]--------------------------{1=>"For our study , sentences were collected from the Brown corpus , which includes a wider range of types of questions and imperatives .\n", 2=>"In our work , sentences are collected from the Brown corpus , which includes a wider range of types of questions and imperatives .\n", 3=>"-1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}
nil_first --> [0]nil_second --> [0]--------------------------{1=>"In the experiments , we also used QuestionBank for comparison .\n", 2=>"In the experiments , we will additionally use QuestionBank for comparison .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#para -1#7#6#stem -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact \n"}
nil_first --> []nil_second --> [5]--------------------------{1=>"We used the tagger in \\CITE .\n", 2=>"We use a tagger in \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact \n"}
nil_first --> [2]nil_second --> [2]--------------------------{1=>"The MST and Malt parsers are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \\CITE and transition-based algorithm \\CITE , respectively .\n", 2=>"The MST parser and Malt parser are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \\CITE and transition-based algorithm \\CITE respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#8#4#exact -1#6#5#exact -1#7#6#exact -1#2#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#25#exact -1#26#26#exact \n"}
nil_first --> [24]nil_second --> [5]--------------------------{1=>"Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree , we used the non-projective versions because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .\n", 2=>"Although the publicly available implementation of each parser also has an option to restrict the output to be a projective dependency tree , we used the non-projective version because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#stem -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact \n"}
nil_first --> []nil_second --> [17]--------------------------{1=>"We used the pennconverter \\CITE to convert a PTB-style treebank into dependency trees .\n", 2=>"We used pennconverter \\CITE to convert a PTB-style treebank to dependency trees .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}
nil_first --> [2, 10]nil_second --> [9]--------------------------{1=>"To evaluate the output from each of the parsers , we used the labeled attachment accuracy excluding punctuation .\n", 2=>"For the evaluation of the output from each of the MST and Malt parser , we used the labeled attachment accuracy excluding the punctuations .\n", 3=>"-1#2,3,4#1,2#para -1#5#3#exact -1#6,7,8,9#4,5,6#para -1#1#7#exact -1#13#8#stem -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#17#12#exact -1#18#13#exact -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#23#17#stem -1#24#18#exact \n"}
nil_first --> [0]nil_second --> [0, 10, 11, 12, 22]--------------------------{1=>"The Enju parser \\CITE is a deep parser based on the HPSG ( Head Driven Phrase Structure Grammar ) formalism .\n", 2=>"The Enju parser \\CITE is a deep parser based on the HPSG formalism .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#19#exact -1#13#20#exact \n"}
nil_first --> [12, 13, 14, 15, 16, 17, 18]nil_second --> []--------------------------{1=>"It produces an analysis of a sentence including the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .\n", 2=>"It produces an analysis of a sentence that includes the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23,24,25,26#22,23,24#para -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}
nil_first --> [25]nil_second --> []--------------------------{1=>"We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank .\n", 2=>"We used a toolkit distributed with the Enju parser for training the parser with a PTB-style treebank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#11#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}
nil_first --> [11]nil_second --> [2]--------------------------{1=>"The toolkit initially converts a PTB-style treebank into an HPSG treebank and then trains the parser on this .\n", 2=>"The toolkit initially converts the PTB-style treebank into an HPSG treebank and then trains the parser on it .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact \n"}
nil_first --> [4, 17]nil_second --> [4, 17]--------------------------{1=>"As evaluation metrics for the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .\n", 2=>"As the evaluation metrics of the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}
nil_first --> [3]nil_second --> [1, 4]--------------------------{1=>"This section explains how we collected the treebanks of imperatives and questions used in the experiments in Section \\REF .\n", 2=>"This section explains how we collected the treebanks of imperatives and questions , which were used in the experiments in Section \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact \n"}
nil_first --> []nil_second --> [12, 13, 14]--------------------------{1=>"The Penn Treebank 3 contains treebanks of several genres of texts .\n", 2=>"Penn Treebank 3 contains treebanks of several genres of texts .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}
nil_first --> [0]nil_second --> []--------------------------{1=>"Although the WSJ treebank has been used extensively for parsing experiments , we used the treebank of the Brown Corpus in our experiments .\n", 2=>"While the Wall Street Journal ( WSJ ) treebank has extensively been used for parsing experiments , we use the treebank of the Brown Corpus in our experiments .\n", 3=>"-1#1#1#exact -1#6#2#exact -1#8#3#exact -1#9#4#exact -1#11#5#exact -1#12#6#exact -1#10#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#17#12#exact -1#18#13#stem -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#22#17#exact -1#23#18#exact -1#24#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#28#23#exact \n"}
nil_first --> [0]nil_second --> [0, 2, 3, 4, 5, 7]--------------------------{1=>"As the Brown Corpus portion includes texts of literary works , it is expected to contain inherently a larger number of imperatives and questions than the WSJ portion .\n", 2=>"Because the Brown Corpus portion includes texts of literary works , it is expected that it inherently contains a larger number of imperatives and questions than the WSJ portion .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13#11,12,13,14#para -1#17#15#stem -1#16#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}
nil_first --> [0]nil_second --> [0, 14, 15]--------------------------{1=>"The Brown Corpus portion of the Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .\n", 2=>"The Brown Corpus portion of Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#16#5#exact -1#17#6#exact -1#18#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#5#18#exact -1#6#19#exact -1#19#20#exact -1#20#21#exact \n"}
nil_first --> [17]nil_second --> []--------------------------{1=>"Interrogative sentences are annotated with the phrase label \" SBARQ \" or \" SQ \" , where \" SBARQ \" denotes wh-questions , while \" SQ \" denotes yes / no questions .\n", 2=>"Interrogative sentences are annotated with the phrase label \" SBARQ \" or \" SQ \" , where \" SBARQ \" represents wh-questions , while \" SQ \" denotes yes / no questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#27#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}
nil_first --> [27]nil_second --> [20]--------------------------{1=>"All sentences annotated with these phrase labels were extracted .\n", 2=>"We extracted those sentences annotated with these phrase labels .\n", 3=>"-1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#1#8#exact -1#9#9#exact \n"}
nil_first --> [0, 7]nil_second --> [0, 2]--------------------------{1=>"Imperatives and questions appear not only at the top level but also as embedded clauses .\n", 2=>"Imperatives and questions appear not only at the top level but also appear as embedded clauses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact \n"}
nil_first --> []nil_second --> [12]--------------------------{1=>"However , is these were embedded in another imperative or question , we only extracted the outermost one .\n", 2=>"When they are embedded in another imperative or question , we only extracted the outermost one .\n", 3=>"-1#9#1#exact -1#2#3,4#para -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}
nil_first --> [0, 2, 11]nil_second --> [0, 1]--------------------------{1=>"Extracted sentences were post-processed to fit the natural sentence form ; that is , with first characters capitalized and question marks or periods added as appropriate .\n", 2=>"Extracted sentences are post-processed so that they have natural sentence forms : first characters are capitalized , and question marks or periods are added when appropriate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#25#5#para -1#8#6,7#para -1#9#8#exact -1#10#9#stem -1#5#11#exact -1#21,22#12#para -1#16#13#exact -1#12#15#exact -1#13#16#exact -1#15#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#23#23#exact -1#26#26#exact \n"}
nil_first --> [4, 10, 14, 22, 24, 25]nil_second --> [4, 6, 7, 11, 14, 24]--------------------------{1=>"The numbers of sentences for each section are given in Table \\REF .\n", 2=>"The number of sentences for each section is shown in Table \\REF .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}
nil_first --> [8]nil_second --> [8]--------------------------{1=>"Although we also applied a similar method to the WSJ portion , we only obtained 115 imperatives and 432 questions .\n", 2=>"Although we also applied a similar method to the WSJ portion , we could obtain only 115 imperatives and 432 questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#15#13#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}
nil_first --> [14]nil_second --> [13, 14]--------------------------{1=>"This data was not used in the experiments .\n", 2=>"We will not use this data in the experiments .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#2#3#exact -1#3#4#stem -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact \n"}
nil_first --> [2]nil_second --> [0, 1]--------------------------{1=>"As described below , we also used QuestionBank in the experiments .\n", 2=>"As we will describe below , we additionally use QuestionBank in experiments .\n", 3=>"-1#0#0#exact -1#3#1#stem -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#para -1#8#6#stem -1#9#7#exact -1#10#8#exact -1#11#9,10#para -1#12#11#exact \n"}
nil_first --> []nil_second --> [1, 2]--------------------------{1=>"The advantage , however , of using the Brown treebank is that it includes annotations of function tags and empty categories , and therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \\CITE , which relies on function tags and empty categories .\n", 2=>"However , an advantage of using the Brown treebank is that it includes annotations of function tags and empty categories . Therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \\CITE , which relies on function tags and empty categories .\n", 3=>"-1#26#0#lc -1#3#1#exact -1#22#2#exact -1#0#3#lc -1#1#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#34#21#exact -1#40#22#exact -1#21#23#lc -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#41#42,43#para -1#42#44#exact -1#43#45#exact \n"}
nil_first --> [24, 28, 36]nil_second --> [2, 20]--------------------------{1=>"Hence , we show experimental results for Enju only with the Brown data .\n", 2=>"Hence , we will show experimental results on Enju only with the Brown data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}
nil_first --> [6]nil_second --> [3, 7]--------------------------{1=>"QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .\n", 2=>"QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}
nil_first --> []nil_second --> [8]--------------------------{1=>"We extracted 3 ,859 sentences annotated with \" SBARQ \" or \" SQ \" .\n", 2=>"We extracted 3 ,859 sentences that are annotated with \" SBARQ \" or \" SQ \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact \n"}
nil_first --> []nil_second --> [5, 6]--------------------------{1=>"During the experiments , we found several annotation errors that caused fatal errors in the treebank conversion .\n", 2=>"During experiments , we found several annotation errors that caused fatal errors of treebank conversion .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}
nil_first --> [1, 13, 14]nil_second --> [12]--------------------------{1=>"We manually corrected the annotations of twelve sentences .\n", 2=>"We therefore corrected annotations of twelve sentences manually .\n", 3=>"-1#0#0#exact -1#7#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#8#exact \n"}
nil_first --> [3]nil_second --> [1]--------------------------{1=>"We intend making these corrections publicly available .\n", 2=>"We plan to make these corrections publicly available .\n", 3=>"-1#0,1#0,1#para -1#3#2#stem -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact \n"}
nil_first --> []nil_second --> [2]--------------------------{1=>"We also found and corrected obvious inconsistencies in the corpus : character \" ' \" replaced by \" $<$ \" ( 737 sentences ) , token \" ? \" tagged with \" ? \" instead of \" . \" ( 2 ,051 sentences ) , and phrase labels annotated as the POS ( one sentence ) .\n", 2=>"We also found and corrected obvious inconsistencies in the corpus : character \" ' \" replaced by \" $<$ \" ( 737 sentences ) , token \" ? \" tagged not with \" . \" but with \" ? \" ( 2 ,051 sentences ) , and phrase labels annotated as POS ( one sentence ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#30#exact -1#32#31#exact -1#38#32#exact -1#39#33#exact -1#30#34#para -1#34#36#exact -1#33#37#exact -1#37#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#54#54#exact -1#55#55#exact -1#56#56#exact \n"}
nil_first --> [35, 50]nil_second --> [35, 36]--------------------------{1=>"We examined the performance of the three parsers and the POS tagger with Brown imperatives and questions , and QuestionBank questions .\n", 2=>"We examined performances of the three parsers and the POS tagger for Brown imperatives and questions , and QuestionBank questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#8#2#exact -1#2#3#stem -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}
nil_first --> [9, 12]nil_second --> [11]--------------------------{1=>"By observing the effect of the parser or tagger adaptation in each domain , we can identify the difficulties in parsing imperative and question sentences .\n", 2=>"By observing the effects of parser or tagger adaptation to each domain , we would like to see the difficulties in parsing imperative and question sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#18#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#17#15,16#para -1#19,20#17,18,19#para -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}
nil_first --> []nil_second --> [14, 15, 16]--------------------------{1=>"We also examined the portability of sentence construction properties between two similar domains : questions in Brown and in QuestionBank .\n", 2=>"We also examined the portability of sentence construction properties between two similar domains : questions in Brown and QuestionBank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact \n"}
nil_first --> [18]nil_second --> []--------------------------{1=>"We created experimental datasets for five domains : WSJ , Brown overall , Brown imperatives , Brown questions , and QuestionBank questions .\n", 2=>"We made experimental datasets for five domains : Wall Street Journal ( WSJ ) , Brown overall sentences , Brown imperatives , Brown questions , and QuestionBank questions .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#12#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#18#12#exact -1#19#13#exact -1#20#14#exact -1#21#15#exact -1#22#16#exact -1#23#17#exact -1#24#18#exact -1#25#19#exact -1#26#20#exact -1#27#21#exact -1#28#22#exact \n"}
nil_first --> []nil_second --> [8, 9, 10, 11, 13, 17]--------------------------{1=>"- Divided into three parts , for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .\n", 2=>"- Divided into three parts for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#17#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#23#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#28#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#35#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact \n"}
nil_first --> [36]nil_second --> []--------------------------{1=>"To adapt each parser and the POS tagger to a target domain , we trained the parser using combined training data for the target domain and the original parser .\n", 2=>"In order to adapt each parser or POS tagger to a target domain , we trained the parser on combined training data for the target domain and for the original parser .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#26#4#exact -1#16#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#23#15#exact -1#17#16#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#28#22#exact -1#24#23#exact -1#25#24#exact -1#29#26,27#para -1#30#28#exact -1#31#29#exact \n"}
nil_first --> [17, 25]nil_second --> [0, 1, 6, 18, 27]--------------------------{1=>"For a domain containing only a small amount of training data , we replicated the training data a certain number of times and utilized the concatenated replicas for training .\n", 2=>"For a domain which contains only small training data , we replicated the training data for certain times and just utilized the concatenated replicas for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5,6#para -1#7#8,9#para -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17,18#para -1#17#20,21#para -1#18#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact \n"}
nil_first --> [7, 19]nil_second --> [15, 19]--------------------------{1=>"- For Brown overall , we trained the model with the combined training data for the target domain and the original model .\n", 2=>"- For Brown overall , we trained the model with the combined training data for the target domain and for the original model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}
nil_first --> []nil_second --> [19]--------------------------{1=>"For Brown imperatives / questions and QuestionBank , we replicated the training data a certain number of times and utilized the concatenated replicas and WSJ training data for training .\n", 2=>"For Brown imperatives / questions and QuestionBank , we replicated the training data for certain times and utilized the concatenated replicas and WSJ training data for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13,14#para -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact \n"}
nil_first --> [15]nil_second --> [13]--------------------------{1=>"For the POS tagger , the number of replicas of training data was determined as either 1 , 2 , 4 , 8 , 16 , 32 , 64 , or 128 , by testing these numbers on the development test sets in three of the ten datasets for cross validation .\n", 2=>"For POS tagger , the number of replicas of training data was determined among 1 , 2 , 4 , 8 , 16 , 32 , 64 , and 128 , by testing these numbers on development test sets in three of ten datasets of cross validation .\n", 3=>"-1#0#0#exact -1#4#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#5,6#5,6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38,39#para -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45,46#para -1#43#47#exact -1#45#49#exact -1#46#50#exact -1#47#51#exact \n"}
nil_first --> [14, 15, 30, 48]nil_second --> [13, 28, 44]--------------------------{1=>"- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and the original model .\n", 2=>"- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and for the original model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}
nil_first --> []nil_second --> [20]--------------------------{1=>"For Brown imperatives and questions , we replicated the training data ten times and utilized the concatenated replicas and WSJ training data for training .\n", 2=>"For Brown imperatives and questions , we replicated the training data for ten times and utilized the concatenated replicas and WSJ training data for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}
nil_first --> []nil_second --> [11]--------------------------{1=>"- We used the toolkit in the Enju parser \\CITE .\n", 2=>"- We used a toolkit in the Enju parser \\CITE\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact \n"}
nil_first --> [6]nil_second --> [3]--------------------------{1=>"Table \\REF gives the POS tagging accuracy for the target domains .\n", 2=>"Table \\REF shows the POS tagging accuracies for the target domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}
nil_first --> [2]nil_second --> [2]--------------------------{1=>"When we applied the WSJ tagger to other domains , the tagging accuracy basically decreased .\n", 2=>"When we applied WSJ tagger to other domains , the tagging accuracy more or less decreased .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#9#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#15#14#exact -1#16#15#exact \n"}
nil_first --> [10, 13]nil_second --> [12, 13, 14]--------------------------{1=>"For Brown overall , compared with the WSJ , the accuracy did not decrease much .\n", 2=>"Among them , for Brown overall sentences , the accuracy did not decrease much from WSJ .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#2#3#exact -1#8#6#exact -1#15#7#exact -1#7#8#exact -1#9#9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#16#15#exact \n"}
nil_first --> [4, 5]nil_second --> [0, 1, 6, 14]--------------------------{1=>"The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .\n", 2=>"The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .\n", 3=>"-1#15#0#lc -1#16#1#exact -1#18#2#exact -1#19#3#exact -1#20#4#exact -1#4#5#exact -1#21#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#3#16#exact -1#6#17#stem -1#22#18#exact -1#23#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#29,30#25#para -1#33#27#exact -1#24#28#exact -1#34#29#exact -1#35#30#exact -1#36#31#exact -1#37#32#exact -1#38#33#exact -1#39#34#exact \n"}
nil_first --> [14, 15, 23, 24, 26]nil_second --> [0, 1, 2, 5, 14, 17, 28, 31, 32]--------------------------{1=>"Figure \\REF shows the POS tagging accuracy for the target domains for varying sizes of the target training data .\n", 2=>"Figure \\REF shows the POS tagging accuracy for the target domains given by changing the size of the target training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#syn -1#14,15#13#para -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact \n"}
nil_first --> [11]nil_second --> [11, 12]--------------------------{1=>"This graph shows that for both types of sentences , the first 300 training sentences greatly improved the accuracy , but thereafter , the effect of adding training data declined .\n", 2=>"This graph shows that for both types of sentences , first 300 training sentences improved the accuracy rapidly , and after that , the effect of adding training corpus declined .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#15#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#23#17#exact -1#16#18#exact -1#18#19#exact -1#22#22#exact -1#24,25#23,24,25#para -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact \n"}
nil_first --> [15, 20, 21, 28]nil_second --> [17, 19, 20, 21, 28]--------------------------{1=>"To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\\% in Table \\REF ) , preparing much more training data does not appear to be enough .\n", 2=>"In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\\% in Table \\REF ) , it would not seem to be enough only to prepare much more training data .\n", 3=>"-1#2#0#lc -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#30#20#stem -1#31#21#exact -1#32#22#exact -1#33#23#exact -1#34#24#exact -1#23,24,25#25,26,27,28#para -1#26#29#exact -1#27#30#exact -1#35#31#exact \n"}
nil_first --> [1, 10]nil_second --> [0, 1, 3, 21, 22, 28, 29]--------------------------{1=>"Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .\n", 2=>"We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .\n", 3=>"-1#0#2#lc -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#11#10#exact -1#12#11#exact -1#13,14#12,13#para -1#15#14#exact -1#16#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#24#22#exact -1#26#23,24#para -1#28#25#para -1#27#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#29#32#para -1#30#33#exact -1#35#34#exact \n"}
nil_first --> [0, 1, 16, 21, 31]nil_second --> [1, 9, 10, 20, 22, 23, 25]--------------------------{1=>"Tables \\REF , \\REF , and \\REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank , respectively .\n", 2=>"Table \\REF , \\REF , and \\REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank respectively .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#31#exact -1#31#32#exact \n"}
nil_first --> [30]nil_second --> []--------------------------{1=>"From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , \" VB \\SPEC \" .\n", 2=>"In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , \" VB \\SPEC \" .\n", 3=>"-1#1#1#exact -1#3#3#exact -1#4#4#exact -1#6#5#syn -1#7#6#exact -1#8#7#exact -1#9#8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact \n"}
nil_first --> [0, 2, 19, 20]nil_second --> [0, 2, 5, 20, 21, 22]--------------------------{1=>"We then analyzed why each of these errors had occurred .\n", 2=>"We then analyzed why each of such errors had occurred .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}
nil_first --> [6]nil_second --> [6]--------------------------{1=>"These two types of errors arise from the following differences in sentence constructions between the WSJ declarative and Brown imperative sentences .\n", 2=>"These two types of errors would respectively come from the following differences in sentence constructions between WSJ declarative and the Brown imperative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#19#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact \n"}
nil_first --> [5]nil_second --> [5, 6, 7]--------------------------{1=>"First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .\n", 2=>"Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#3#exact -1#3#4#stem -1#4#5#exact -1#5,6#6,7#para -1#7#9#exact -1#8#10#stem -1#10#14#exact -1#11#15#stem -1#12#16#exact -1#13,14#17,18#para -1#15#20#exact -1#16#21#stem -1#17#22#exact \n"}
nil_first --> [2, 8, 11, 12, 13, 19]nil_second --> [9]--------------------------{1=>"Since The WSJ tagger was trained on a domain consisting mainly of declarative sentences , with the training based on N-gram sequences of words or POSs , preference was given to noun phrase-derived tags at the beginning of a sentence .\n", 2=>"The WSJ tagger was trained on the domain mainly consisting of declarative sentences , and the training was based on N-gram sequences of words or POSs . The tagger therefore preferred to give noun phrase-derived tags to the beginning of a sentence .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#40#7#exact -1#7#8#exact -1#9#9#exact -1#8#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#6#16#exact -1#16#17#exact -1#17,18,19#18,19#para -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#30#27#stem -1#32#29#syn -1#31#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#37,38#34,35,36,37#para -1#41#38,39#para -1#42#40#exact \n"}
nil_first --> [0, 15, 26, 28]nil_second --> [14, 15, 26, 27, 28, 29, 36, 39]--------------------------{1=>"Second , the main verb in an imperative sentence takes a base form , whereas the main verb in a declarative sentence takes a form based on tense .\n", 2=>"Secondly , main verbs in imperative sentences take base forms while main verbs in declarative sentences take the forms according to tense .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#17#2#exact -1#2#3#exact -1#3#4#stem -1#4#5#exact -1#5#7#exact -1#6#8#stem -1#7#9#stem -1#8#10,11#para -1#9#12#stem -1#11#15,16#para -1#12#17#stem -1#13#18#exact -1#14#20#exact -1#15#21#stem -1#16#22#stem -1#18#24#stem -1#21#27#exact -1#22#28#exact \n"}
nil_first --> [6, 13, 14, 19, 23, 25, 26]nil_second --> [10, 19, 20]--------------------------{1=>"A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .\n", 2=>"The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .\n", 3=>"-1#1#1#exact -1#2#2#para -1#15#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#16#7#exact -1#6#8#exact -1#7#9#exact -1#13#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#28#16#exact -1#21#17#exact -1#14#18#stem -1#17#21#exact -1#18#22#stem -1#19#23#exact -1#20#24#syn -1#30#25#exact -1#22#26#exact -1#23#27#stem -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#stem -1#0#34#lc -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#44#exact \n"}
nil_first --> [0, 19, 20, 32, 33, 42, 43]nil_second --> [29]--------------------------{1=>"Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .\n", 2=>"The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#5#para -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#9#11,12#para -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact \n"}
nil_first --> [0, 4, 10]nil_second --> [8]--------------------------{1=>"After adapting the tagger to Brown imperatives , the N-gram model of the tagger would have learned that the first word in a sentence tends to be a verb , and that the main verb tends to take the base form ( VB ) .\n", 2=>"After adapting the tagger to Brown imperatives , the N-gram model of tagger would have learned that the first word in a sentence tends to be a verb , and the main verb tends to take base form ( VB ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#17#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#30#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#31#32,33#para -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact \n"}
nil_first --> [31, 38]nil_second --> []--------------------------{1=>"Table \\REF shows that after adaptation the above two types of errors decreased to some extent , although a few mistags of verbs still remained .\n", 2=>"Table \\REF shows that the above two types of errors did decrease to some extent . However , we can also observe that not a few mis-tags to verbs were still left after the adaptation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#32#4#exact -1#34#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#11#12#stem -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#17#16#exact -1#24#18#exact -1#25#19#exact -1#28#22#exact -1#30#23#exact -1#15#25#exact \n"}
nil_first --> [17, 20, 21, 24]nil_second --> [10, 16, 18, 19, 20, 21, 22, 23, 26, 27, 29, 31, 33]--------------------------{1=>"By investigating the remaining errors associated with VB , we found that several errors still occurred even in simple imperative sentences such as \" VB \\SPEC NN \" for \" Charge \" in \" Charge something for it \" , and that some errors tended to occur after a to-infinitive phrase or conjunction , such as \" VB \\SPEC NN \" for \" subtract \" in \" To find the estimated net farm income , subtract . . . \" .\n", 2=>"When we observe each of the left errors around VB , we found that several errors still occurred even in simple imperative sentences such as \" VB \\SPEC NN \" for \" Charge \" in \" Charge something for it . \" , and that some errors tended to occur after to-infinitive phrase or conjunction , such as \" VB \\SPEC NN \" for \" subtract \" in \" To find estimated net farm income , subtract . . . \"\n", 3=>"-1#5#2#exact -1#6#3#syn -1#7#4#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact -1#47#44#exact -1#48#45#exact -1#49#46#exact -1#50#47#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact -1#57#55#exact -1#58#56#exact -1#59#57#exact -1#60#58#exact -1#61#59#exact -1#62#60#exact -1#63#61#exact -1#64#62#exact -1#65#63#exact -1#66#64#exact -1#67#65#exact -1#68#66#exact -1#69#67#exact -1#70#68#exact -1#71#69,70#para -1#72#71#exact -1#73#72#exact -1#74#73#exact -1#75#74#exact -1#76#75#exact -1#77#76#exact -1#78#77#exact -1#79#78#exact -1#80#79#exact -1#40#80#exact \n"}
nil_first --> [0, 1, 5, 6, 48]nil_second --> [0, 1, 2, 3, 4, 8]--------------------------{1=>"The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .\n", 2=>"The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5,6#3,4#para -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#15#13#exact -1#16#14#exact -1#3,4#15#para -1#17#16#exact -1#33#17#exact -1#23#18#exact -1#22#19#exact -1#24#20#exact -1#25#21#exact -1#26,27#22,23#para -1#28#24#exact -1#29#25#exact -1#30#27#exact -1#31#28#exact -1#21#30#para -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact \n"}
nil_first --> [12, 26, 29]nil_second --> [14, 18, 19, 20, 32]--------------------------{1=>"We also analyzed the errors in Brown questions and QuestionBank , and again found that many errors were due to the fact that the WSJ tagger was trained on a corpus consisting mainly of declarative sentences .\n", 2=>"We also analyzed the errors in Brown questions and QuestionBank , and again found that the WSJ tagger seems to make many errors due to the fact that the tagger was trained on a corpus mainly consisting of declarative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#21#15#exact -1#22#16#exact -1#23,24,25,26,27#17,18,19,20,21,22#para -1#28#23#exact -1#16#24#exact -1#17#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#36#31#exact -1#35#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact \n"}
nil_first --> []nil_second --> [15, 18, 19, 20, 29]--------------------------{1=>"After the adaptation , although some of the errors such as the special use of wh-words , i.e. , \" WDT \\SPEC WP \" , were corrected , other kinds or errors related to the global change in sentence structure still remained .\n", 2=>"After the adaptation , while some of the errors such as special usage of wh-words , i.e. , \" WDT \\SPEC WP \" , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#36#11#exact -1#11#12#exact -1#12,13#13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#38,39#28#para -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#37#34,35#para -1#40#38#exact -1#41#39#stem -1#42#40#exact -1#43#41#exact -1#44#42#exact \n"}
nil_first --> [4, 36, 37]nil_second --> [4, 27, 28, 29, 30]--------------------------{1=>"To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .\n", 2=>"In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .\n", 3=>"-1#2#0#lc -1#7#2#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15,16#11,12#para -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact \n"}
nil_first --> [1, 3]nil_second --> [0, 1, 3, 4, 5, 6]--------------------------{1=>"Table \\REF gives the parsing accuracy of MST ( first order ) , MST ( second order ) , Malt , and the Enju parser for WSJ , Brown overall , Brown imperatives , and Brown questions .\n", 2=>"Table \\REF shows the parsing accuracies of MST( first order ) , MST( second order ) , Malt , and Enju parser for WSJ , Brown overall , Brown imperatives and Brown questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact \n"}
nil_first --> [2, 7, 8, 13, 14, 22, 33]nil_second --> [2, 7, 12]--------------------------{1=>"Figure \\REF plots the parsing accuracy against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .\n", 2=>"Figure \\REF shows the parsing accuracies against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}
nil_first --> [2]nil_second --> [2]--------------------------{1=>"Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \\REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \\REF .\n", 2=>"Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \\REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#29#4#exact -1#4#5#exact -1#51#6#exact -1#34#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#stem -1#28#31#exact -1#30#34#exact -1#31#35#exact -1#33#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#45#50#exact -1#46#51#exact -1#47#52#exact -1#49#54#para -1#50#55#exact -1#52#57#exact -1#53#58#exact -1#54#59#exact -1#55#60#exact -1#56#61#exact -1#57#62#exact -1#58#63#exact -1#59#64#exact -1#60#65#exact -1#61#66#exact -1#62#67#exact -1#63#68#exact \n"}
nil_first --> [32, 33, 36, 47, 48, 49, 53, 56]nil_second --> [32, 44, 48]--------------------------{1=>"After adaptation ( see \" Adapted \" column in Table \\REF ) , the parser achieved two to four percent higher accuracy for each of the Brown domains compared to the WSJ parser .\n", 2=>"When we adapted the parser model ( see fifth column in Table \\REF ) , the parser could give two to four points higher accuracies for each of the Brown domains than the WSJ parser .\n", 3=>"-1#6#2#exact -1#7#3#exact -1#2#5#lc -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#23#20#exact -1#24#21#stem -1#25,26,27,28#22,23,24#para -1#3#25#exact -1#29#26#exact -1#30#27#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact \n"}
nil_first --> [0, 1, 4, 6, 15, 19, 28, 29]nil_second --> [0, 1, 4, 5, 8, 17, 18, 22, 31]--------------------------{1=>"For QuestionBank , 25 to 35 percent improvement in accuracy was observed .\n", 2=>"For the QuestionBank , 25 to 35 points accuracy improvements were observed .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#9#7#stem -1#8#9#exact -1#10,11#10,11#para -1#12#12#exact \n"}
nil_first --> [6, 8]nil_second --> [1, 7]--------------------------{1=>"Figure \\REF shows that the improvement is proportional to the size of the training data and that this tendency does not seem to converge .\n", 2=>"Figure \\REF shows that , the improvements increased according to the size of the training data , and the tendencies would not seem to converge .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#stem -1#9,10,11,12,13#8,9,10,11#para -1#18#12#exact -1#14#13#exact -1#15#14#exact -1#17#15#exact -1#19#18#stem -1#21,22,23#19,20,21,22#para -1#24#23#exact -1#25#24#exact \n"}
nil_first --> [6, 7, 16, 17]nil_second --> [4, 7, 8, 16, 20]--------------------------{1=>"This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .\n", 2=>"This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#19#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#17#13#exact -1#11#14#exact -1#12#15#para -1#14#16#exact -1#13#17#exact -1#18#21,22,23#para -1#20#24,25#para -1#21#26#exact -1#22#27#exact \n"}
nil_first --> [7, 18, 19, 20]nil_second --> [15, 16]--------------------------{1=>"In Figure \\REF , the parser accuracy for QuestionBank , for which we could use much more training data than for Brown questions , approaches or even exceeds that of the WSJ parser for WSJ .\n", 2=>"In Figure \\REF , when we focus on the QuestionBank where we could use much more training data than Brown questions , the parser accuracies were approaching the accuracies of WSJ parser for WSJ or exceeded the accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#22#4#exact -1#23#5#exact -1#37#6#exact -1#32#7#exact -1#9#8#exact -1#21#9#exact -1#10,11#10,11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#21#exact -1#20#22#exact -1#34#24,25#para -1#35#27#stem -1#29#29#exact -1#8#30#exact -1#30#31#exact -1#31#32#exact -1#33#34#exact -1#38#35#exact \n"}
nil_first --> [20, 23, 26, 28, 33]nil_second --> [4, 5, 6, 7, 24, 25, 26, 27, 28, 36]--------------------------{1=>"However , as there is no more training data for Brown imperatives and questions , we need to either prepare more training data or explore approaches that enable the parsers to be adapted with small amounts of training data .\n", 2=>"However , we have no more training data for Brown imperatives and questions . We should prepare more training data or explore approaches to enable us to sufficiently adapt parsers with small training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#2#15#exact -1#15#16,17#para -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23,24#26,27#para -1#29#29#exact -1#26#30#exact -1#28#31,32#para -1#30#33#exact -1#31#34#exact -1#32#36,37#para -1#33#38#exact -1#34#39#exact \n"}
nil_first --> [2, 3, 4, 14, 18, 28, 35]nil_second --> [3, 13, 14, 25, 27]--------------------------{1=>"To capture an overview of the adaptation effects , we observed the error reduction in the Malt parser .\n", 2=>"In order to capture the outline of the adaptation effects , we observed error reduction for the Malt parser .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#4#11#exact -1#13#12#exact -1#14#13#exact -1#0#14#lc -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}
nil_first --> [2, 3]nil_second --> [1, 5, 15]--------------------------{1=>"Tables \\REF and \\REF give the recall errors on labeled dependencies , which were observed more than ten times for 100 analysis sentences in each domain .\n", 2=>"Table \\REF and \\REF show the recall errors on labeled dependencies which were observed more than ten times for 100 analysis sentences of each domain .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22,23#23,24#para -1#24#25#exact -1#25#26#exact \n"}
nil_first --> [4, 11]nil_second --> [4]--------------------------{1=>"For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags , respectively .\n", 2=>"For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#36#exact \n"}
nil_first --> [34, 35]nil_second --> []--------------------------{1=>"Since ROOT dependencies , that is , heads of sentences , are critical to the construction of sentences , we focus mainly on this type of error .\n", 2=>"Since ROOT dependencies , that is , heads of sentences would be critical to construction of sentences , we mainly focus on that type of errors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#17#10#exact -1#10,11#11#para -1#12#12#exact -1#13#13#exact -1#14,15#14,15,16#para -1#16#17#exact -1#18#19#exact -1#20#20#exact -1#19#21#exact -1#21#22#exact -1#22,23#23,24#para -1#24#25#exact -1#25#26#stem -1#26#27#exact \n"}
nil_first --> [18]nil_second --> []--------------------------{1=>"For Brown imperatives and questions , the reduction in ROOT dependencies was prominent .\n", 2=>"For Brown imperatives and questions , we could observe that the reduction of ROOT dependency was prominent .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10#6#exact -1#11#7#exact -1#13#9#exact -1#14#10#stem -1#15#11#exact -1#16#12#exact -1#17#13#exact \n"}
nil_first --> [8]nil_second --> [6, 7, 8, 9, 12]--------------------------{1=>"On investigation , we found that the WSJ parser often made mistakes in parsing sentences which began or ended with the name of the person being addressed .\n", 2=>"When we focus on this type of errors , we could find that the WSJ parser could often make mistakes in parsing sentences which began or ended with the names of persons who were talk to .\n", 3=>"-1#3#0#lc -1#8#2#exact -1#9#3#exact -1#11#4#syn -1#12#5#exact -1#13#6#exact -1#14#7#exact -1#15#8#exact -1#17#9#exact -1#18,19#10,11#para -1#20#12#exact -1#21#13#exact -1#22#14#exact -1#23#15#exact -1#24#16#exact -1#25#17#exact -1#26#18#exact -1#27#19#exact -1#28#20#exact -1#29,30#21,22#para -1#31#23,24#para -1#36#27#exact \n"}
nil_first --> [1, 25, 26]nil_second --> [0, 1, 2, 4, 5, 6, 7, 10, 16, 32, 33, 34, 35]--------------------------{1=>"For example , in Brown imperatives , for the sentence \" See for yourself , Miss Zion . \" , the WSJ parser mistook the name \" Zion \" to be ROOT , and the main verb \" See \" to be a modifier of the name .\n", 2=>"For example in Brown imperatives , for the sentence \" See for yourself , Miss Zion . \" , the WSJ parser regarded the person name \" Zion \" as ROOT , and the main verb \" See \" as modifiers of the name .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#13#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#18#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#31#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#23#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#30#31#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#40#43#stem -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#44#47#exact \n"}
nil_first --> [23, 29, 30, 32, 40, 41, 42]nil_second --> [22, 24, 29, 39]--------------------------{1=>"The adapted parser correctly assigned ROOT to the main verb .\n", 2=>"The adapted parser could then correctly give ROOT to the main verb .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#3#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact \n"}
nil_first --> [4]nil_second --> [3, 4, 6]--------------------------{1=>"We also found that the WSJ parser often made mistakes in parsing sentences containing quotation , exclamation , or question marks , such as \" \" Hang on \" !! \" \" or \" Why did you kill it \" ? ? \" or \" \" \" \" .\n", 2=>"We could also often find that the WSJ parser could often make mistakes in parsing sentences containing quotation , exclamation , and question marks , such as \" \" Hang on \" !! \" \" or \" Why did you kill it \" ? ? \" or \" \" \" \" .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#4#2#syn -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#10#7#exact -1#11,12#8,9#para -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#35#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#46#32#exact -1#47#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#36#44#exact -1#48#45#exact -1#49#46#exact -1#50#47#exact -1#51#48#exact \n"}
nil_first --> [43]nil_second --> [1, 3, 9, 21]--------------------------{1=>"For such sentences , the WSJ parser regarded the first \" ! \" or \" ? \" as ROOT , and \" Hang \" or \" did \" as the modifier of the punctuation .\n", 2=>"For such sentences , the WSJ parser regarded the first \" ! \" or \" ? \" as ROOT , and \" Hang \" or \" did \" as the modifier of the marks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#34#34#exact \n"}
nil_first --> [33]nil_second --> [33]--------------------------{1=>"A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .\n", 2=>"We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .\n", 3=>"-1#3#4#exact -1#4#5#para -1#5#6#exact -1#6#7#stem -1#7#8#para -1#21#9#exact -1#2#10#exact -1#23#11#exact -1#11#12#exact -1#12#13#exact -1#31#14#syn -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#34#19#exact -1#25#20#exact -1#22#23#exact -1#27#24#exact -1#24#25#exact -1#14#26#exact \n"}
nil_first --> [0, 1, 2, 3, 21, 22]nil_second --> [0, 1, 8, 9, 10, 13, 15, 20, 26, 28, 29, 30, 32, 33]--------------------------{1=>"The adapted parser could handle this dubious construction and assigned ROOT to the main verbs as the corpus required .\n", 2=>"However , the adapted parser could take in such doubtful construction and gave ROOT to the main verbs as the corpus required .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#9#6#syn -1#10#7#exact -1#11#8#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact \n"}
nil_first --> [4, 5, 9]nil_second --> [0, 1, 6, 7, 8, 12]--------------------------{1=>"On the other hand , we also observed some unsolved errors , of which we discuss two .\n", 2=>"On the other hand , we also observed some still unsolved errors . We would show the two kinds of major errors among them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#19#12#exact -1#22,23#13#para -1#13#14#lc -1#17#16#exact -1#12#17#exact \n"}
nil_first --> [11, 15]nil_second --> [9, 14, 15, 16, 18, 20, 21]--------------------------{1=>"First , Brown imperatives and questions , include many colloquial sentences , which have rather flexible constructions , especially imperatives , such as \" Lift , don't shove lift! \" , \" Come out , come out in the meadow! \" , etc.\n", 2=>"First , Brown imperatives and questions , include many conversation sentences , and therefore rather flexible constructions could be observed especially for imperatives , such as \" Lift , don't shove lift! \" , \" Come out , come out in the meadow! \" , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#14#13,14#para -1#15#15#exact -1#16#16#exact -1#23#17#exact -1#20#18#exact -1#22#19#exact -1#28#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#33#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#37#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#44#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#45#42#exact \n"}
nil_first --> [9, 12, 41]nil_second --> [9, 12, 13, 17, 18, 19, 21]--------------------------{1=>"The parsing models based on the plausibility of constructions were not able to capture such sentences .\n", 2=>"The parsing models based on the plausibility of constructions could hardly capture such sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}
nil_first --> [9, 10, 11, 12]nil_second --> [9, 10]--------------------------{1=>"Second , having different sentence constructions within a single sentence , such as , where a to-infinitive phrase or subordinate clause precedes an imperative or question , often confused the parser .\n", 2=>"Second , when the different constructions of sentences were in one sentence , such as , the case where to-infinitive phrases or subordinate clauses precede imperatives and questions , the parser would often be confused .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#3#exact -1#11#4#exact -1#5#5#exact -1#10#8#syn -1#7#9#stem -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#18#14#exact -1#19#16#exact -1#20#17#stem -1#21#18#exact -1#22#19#exact -1#23#20#stem -1#24#21#stem -1#25#23#stem -1#27#25#stem -1#28#26#exact -1#32#27#exact -1#34#28#exact -1#3#29#exact -1#30#30#exact -1#35#31#exact \n"}
nil_first --> [2, 6, 7, 15, 22, 24]nil_second --> [2, 6, 8, 9, 16, 17, 26, 29, 31, 33]--------------------------{1=>"For example , for the imperative sentence , \" To find the estimated net farm income , subtract the estimated annual farming expenditure . . . \" , both the WSJ and adapted parsers regarded \" find \" as ROOT , because the parsers regarded the words following \" find \" as a that-clause complementing \" find \" , as in \" To find [ ( that ) the estimated net farm income , subtract the estimated annual farming . . .] \" .\n", 2=>"For example , for the imperative sentence \" To find estimated net farm income , subtract estimated annual farming expenditures . . . \" , both of the WSJ and adapted parsers regarded \" find \" as ROOT , because the parsers regarded the words following \" find \" as a that-clause complement for the \" find \" , like \" To find [ ( that ) estimated net farm income , subtract estimated annual farming . . .] \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#27#11#exact -1#67#12#exact -1#68#13#exact -1#69#14#exact -1#70#15#exact -1#71#16#exact -1#72#17#exact -1#40#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#stem -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#43#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#54#42#exact -1#41#43#exact -1#42#44#exact -1#44#45,46#para -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#51#53#exact -1#52#54#stem -1#55#55#exact -1#56#56#exact -1#57#57#exact -1#58#58#exact -1#60#61#exact -1#61#62#exact -1#62#63#exact -1#63#64#exact -1#64#65#exact -1#65#66#exact -1#66#67#exact -1#10#68,69#para -1#11#70#exact -1#12#71#exact -1#13#72#exact -1#15#74#exact -1#73#75,76#para -1#74#77#exact -1#75#78#exact -1#76#79#exact -1#77#80#exact -1#78#81#exact -1#79#82#exact -1#80#83#exact \n"}
nil_first --> [59, 60, 73]nil_second --> [26, 53, 59]--------------------------{1=>"This type of error cannot be solved merely by increasing the training data .\n", 2=>"This type of errors would hardly be solved only by increasing the training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}
nil_first --> [4]nil_second --> [4, 5]--------------------------{1=>"Imperative or question sentences typically consist not only of a pure imperative or question clause , but also of other constructions of phrases or clauses .\n", 2=>"Imperatives or questions sentences consist not only of pure imperative or question clause , but also of other constructions of phrases or clauses .\n", 3=>"-1#9#0#lc -1#10#1#exact -1#11#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9,10#para -1#0#11#stem -1#1#12#exact -1#2#13#stem -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}
nil_first --> [4]nil_second --> []--------------------------{1=>"These complex sentences were parsed without being partitioned into separate constructions , and as a result the parser sometimes became confused .\n", 2=>"The parser would parse such complex sentences without partition into each construction , and therefore it would sometimes be confused .\n", 3=>"-1#4,5#0,1#para -1#6#2#exact -1#7#5#exact -1#18#6#stem -1#8#7#stem -1#9#8#exact -1#11#10#stem -1#12#11#exact -1#13#12#exact -1#0#16#lc -1#1#17#exact -1#17#18#exact -1#19#20#exact -1#20#21#exact \n"}
nil_first --> [3, 4, 9, 13, 14, 15, 19]nil_second --> [2, 3, 10, 14, 15, 16]--------------------------{1=>"Both the Brown questions and QuestionBank are in the question domain .\n", 2=>"Both of Brown questions and QuestionBank are in the domain of question .\n", 3=>"-1#0#0#exact -1#8#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8,9#para -1#9#10#exact -1#12#11#exact \n"}
nil_first --> []nil_second --> [1, 10]--------------------------{1=>"In this section , we examine whether a parser adapted to one domain could be ported to another domain .\n", 2=>"In this section , we examined whether the parser adapted to one domain would be portable to the other domain .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#para -1#14#14#exact -1#16#16#exact -1#19#18#exact -1#20#19#exact \n"}
nil_first --> [7, 15, 17]nil_second --> [7, 15, 17, 18]--------------------------{1=>"QuestionBank does not provide function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .\n", 2=>"QuestionBankdoes not give function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .\n", 3=>"-1#1#2#exact -1#2#3#para -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .\n", 2=>"Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .\n", 3=>"-1#1#3#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}
nil_first --> [0, 1, 2, 4]nil_second --> [0, 2, 15]--------------------------{1=>"While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .\n", 2=>"While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#8#6,7#para -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#9#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24,25,26#22,23,24,25#para -1#27#27#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact \n"}
nil_first --> [5, 26, 28]nil_second --> [4, 6, 7, 10, 28, 29]--------------------------{1=>"In this paper , we investigate the effects of word segmentation methods on SMT , by comparing the evaluation results of the translation outputs , while varying word segmentation methods .\n", 2=>"In this paper , we investigate the effects of word segmentation methods on SMT , by comparing evaluation results of translation outputs while varying word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17,18#para -1#18#19#exact -1#19#20#exact -1#20#21,22#para -1#21#23#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact \n"}
nil_first --> [24]nil_second --> []--------------------------{1=>"The experimental results confirmed that supervised morphological analyzers were competitive with , and performed considerably better than an unsupervised analyzer and a heuristic segmentation method .\n", 2=>"The experiments revealed that supervised morphological analyzers were competitive , and considerably better than an unsupervised analyzer and a heuristic segmentation method .\n", 3=>"-1#0#0#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact \n"}
nil_first --> [1, 2, 3, 10, 13]nil_second --> [1, 2]--------------------------{1=>"However , a character-based segmentation achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on the corpus sizes and domains .\n", 2=>"However , a character-based segmentation has achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on corpus sizes and domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}
nil_first --> [22]nil_second --> [5]--------------------------{1=>"In conclusion , we discuss the problem of the comparability of evaluation metrics , and consider ways of improving word segmentation more than popular supervised morphological analyzers .\n", 2=>"For this result we discuss the problem of the comparability of evaluation metrics and the possibility of better word segmentation than popular supervised morphological analyzers .\n", 3=>"-1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#16#17#exact -1#17#18#syn -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}
nil_first --> [0, 1, 2, 13, 15, 16, 21]nil_second --> [0, 1, 2, 14, 15]--------------------------{1=>"Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .\n", 2=>"Several natural languages like Chinese and Japanese do not have to put spaces between words in their written forms .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}
nil_first --> [2, 3, 7, 10, 14]nil_second --> [1, 3, 9, 10, 11]--------------------------{1=>"Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .\n", 2=>"Since the process is fundamental and indispensable , we need to explore how word segmentation affects Natural Language Processing applications .\n", 3=>"-1#0#0#exact -1#13#1#exact -1#14#2#exact -1#3#3#exact -1#4#5#exact -1#2#6#exact -1#7#7#exact -1#5#8#exact -1#15#9#para -1#6#11#exact -1#9#14,15#para -1#8#17#exact -1#11#18#exact -1#12#19#exact -1#16#23#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact -1#20#27#exact \n"}
nil_first --> [4, 10, 12, 13, 16, 20, 21, 22]nil_second --> [1, 10]--------------------------{1=>"Thus , we investigate how Japanese word segmentation affects SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .\n", 2=>"Therefore , we investigate how Japanese word segmentation affects on SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .\n", 3=>"-1#0#0#syn -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}
nil_first --> []nil_second --> [9]--------------------------{1=>"In addition , we examine an unsupervised morphological analyzer , and its results .\n", 2=>"We also examine an unsupervised morphological analyzer and its results .\n", 3=>"-1#1#0,1#para -1#0#3#lc -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact \n"}
nil_first --> [2, 9]nil_second --> []--------------------------{1=>"We focus on the meta-evaluation of the current evaluation metrics and determine whether the metrics are consistent or not , when we vary word segmentation methods .\n", 2=>"In addition , we focus on the meta-evaluation of the current evaluation metrics and find whether the metrics are consistent or not , when we vary word segmentation methods .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#15#11,12#para -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact \n"}
nil_first --> []nil_second --> [0, 1, 2, 14]--------------------------{1=>"Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on the outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .\n", 2=>"Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#24#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}
nil_first --> [25]nil_second --> []--------------------------{1=>"They acquired a 2 .3 score improvement in comparing the worst to best combinations .\n", 2=>"They acquired the 2 .3 score improvement from the worst to the best combinations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#2#9#exact -1#9#10#exact -1#10#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}
nil_first --> [2, 7, 8]nil_second --> [7, 8, 11]--------------------------{1=>"( 2010 ) suggested a new short unit word segmentation standard in Chinese , which defines a more frequent string subset as a word .\n", 2=>"( 2010 ) suggested a new short unit word segmentation standard in Chinese which defines a more frequent string subset as a word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}
nil_first --> [13]nil_second --> []--------------------------{1=>"For instance , one word  globalization was separated into two words  global and  -lization .\n", 2=>"For instance , They separated one word \"  globalization \" into two words \"  global \" and \"  -lization \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#3#exact -1#6#4#exact -1#4#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#18#14#exact -1#23#17#exact \n"}
nil_first --> [5, 6, 7, 12, 13, 15, 16]nil_second --> [3, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 21, 22]--------------------------{1=>"However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .\n", 2=>"Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .\n", 3=>"-1#1#1#exact -1#8#2,3#para -1#4#4#exact -1#3#5,6#para -1#5#7#exact -1#7#9#exact -1#9,10#10,11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}
nil_first --> [0, 8]nil_second --> [0, 2, 6]--------------------------{1=>"In addition , comparing morphological analyzers is necessary , because different analyzers produce different outputs to SMT .\n", 2=>"In addition , comparison of morphological analyzers are necessary because different analyzers produce different outputs to SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7,8#6,7#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}
nil_first --> [8]nil_second --> []--------------------------{1=>"We therefore conduct several translation tasks between English and Japanese .\n", 2=>"Therefore , we conduct several translation tasks between English and Japanese .\n", 3=>"-1#2#0#lc -1#0#1#lc -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact \n"}
nil_first --> []nil_second --> [1]--------------------------{1=>"- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affects the SMT evaluation metrics , depending on the corpus sizes and domains .\n", 2=>"- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affect SMT evaluation metrics , depending on corpus sizes and domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#stem -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact \n"}
nil_first --> [21, 28]nil_second --> []--------------------------{1=>"- Whether or not SMT evaluation metrics provide a consistent measure , while varying word segmentation methods .\n", 2=>"- Whether or not SMT evaluation metrics provide a consistent measure while varying word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}
nil_first --> [11]nil_second --> []--------------------------{1=>"We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .\n", 2=>"We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#19#14#exact -1#12#15#exact -1#13#16#exact -1#15#18#exact -1#16#19#exact -1#20,21#22,23#para -1#18#24#para -1#14#27#exact -1#22#28#exact -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact \n"}
nil_first --> [1, 2, 12, 17, 20, 21, 25, 26]nil_second --> [1, 17]--------------------------{1=>"As shown in Table 1 , the following word segmentation methods output delimiters ( | represents a delimiter ) for a given input character sequence .\n", 2=>"As shown in Table 1 , the following word segmentation methods output delimiters ( \" | \" represents a delimiter ) for a given input character sequence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact \n"}
nil_first --> [14]nil_second --> [14, 15, 16]--------------------------{1=>"It is ; however , unclear as to which analyzer works better for the SMT task .\n", 2=>"It is , however , not clear which analyzer works better for the SMT task than the other analyzers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5,6#5#para -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#19#16#exact \n"}
nil_first --> [2, 6]nil_second --> [2, 15, 16, 17, 18]--------------------------{1=>"- JUMAN also regards word segmentation as a sequence labeling problem , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .\n", 2=>"- JUMAN also regards word segmentation as a sequence labeling , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact \n"}
nil_first --> [10]nil_second --> []--------------------------{1=>"The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98% for news texts .\n", 2=>"The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98\\% for news text .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#stem -1#21#21#exact \n"}
nil_first --> [17]nil_second --> [17]--------------------------{1=>"On the other hand , the unsupervised method , latticelm , achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news texts , while the method does not have any answers for word definitions .\n", 2=>"On the other hand , the unsupervised method latticelm achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news text , while the method does not have any answers of word definitions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#17#8#exact -1#8#9#exact -1#25#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#stem -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29,30,31#31,32,33,34#para -1#33#35#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact \n"}
nil_first --> [19, 27, 36]nil_second --> [32, 34]--------------------------{1=>"Therefore , it is not possible to compare its result with the supervised results , even though it is fair to compare it from the SMT contribution point-of-view .\n", 2=>"Therefore , it is not possible to compare such a result with the supervised results . Even though , it is fair to compare it with SMT contribution point of view .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5,6#2,3,4,5#para -1#22#6#exact -1#23#7#exact -1#24#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#18#14#exact -1#16#15#lc -1#17#16#exact -1#20,21#17,18,19,20#para -1#7#21#exact -1#19#22#exact -1#26#25#exact -1#27#26#exact -1#15#28#exact \n"}
nil_first --> [23, 24, 27]nil_second --> [8, 9, 25]--------------------------{1=>"Furthermore , their policies concerning word segmentation definitions vary significantly .\n", 2=>"Furthermore , their policies about word segmentation definitions are very much different .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8#para -1#10#9#para -1#12#10#exact \n"}
nil_first --> []nil_second --> [8, 9]--------------------------{1=>"While MeCab can change its definitions by external lexicons , and JUMAN has its own internal standard , KyTea is based on the short unit standard of the Balanced Corpus of Contemporary Written Japanese , which is considered to have one of the shortest definitions of Japanese words .\n", 2=>"While MeCab can change its definitions by external lexicons and JUMAN has its own internal standard , KyTea is based on the short unit standard of Balanced Corpus of Contemporary Written Japanese , which is considered one of the shortest definitions of Japanese words .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#16#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#32#17#exact -1#17#18#exact -1#18,19,20#19,20,21,22#para -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#21#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#33,34,35#35,36,37,38#para -1#36#40#exact -1#37#41#exact -1#38#42#exact -1#39#43#exact -1#40#44#exact -1#41#45#exact -1#42#46#exact -1#43#47#exact -1#44#48#exact \n"}
nil_first --> [34, 39]nil_second --> []--------------------------{1=>"For example , if we are given a string , ( if someone sees ) , MeCab separates it into two words ,  |  and JUMAN retains the same string , but KyTea outputs it as three words ,  |  |  where every character is a word .\n", 2=>"For example , if we are given a string \" ( if someone see ) \" , MeCab separates it into two words \"  |  \" and JUMAN keep the same string , but KyTea outputs it as three words \"  |  |  \" where every character is a word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#16#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#34#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#25#24#exact -1#28#26#exact -1#29#27#exact -1#30#28#syn -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#44#42#exact -1#45#43#exact -1#46#44#exact -1#49#46#exact -1#50#47#exact -1#51#48#exact -1#52#49#exact -1#53#50#exact -1#54#51#exact -1#55#52#exact \n"}
nil_first --> [10, 14, 22, 23, 25, 32, 40, 41, 45]nil_second --> [9, 10, 14, 15, 23, 24, 26, 27, 42, 43, 47, 48]--------------------------{1=>"For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .\n", 2=>"In the case of latticelm , as it has no supervised definition of words , it uses the expectation maximized length of words for every word depending on training data .\n", 3=>"-1#23#0#lc -1#4#1#exact -1#5#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#24#21#exact -1#25#22#exact -1#26#24#exact -1#27#25#exact -1#1#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact \n"}
nil_first --> [3, 20, 23]nil_second --> [0, 2, 3, 6]--------------------------{1=>"In our experiments , we further investigate such morphological analysis accuracies and word definition problems .\n", 2=>"We also investigate such morphological analysis accuracy and word definition problems in our experiments .\n", 3=>"-1#11#0#lc -1#12#1#exact -1#13#2#exact -1#0#4#lc -1#1#5#para -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#stem -1#7#11#exact -1#8#12#exact -1#9#13#exact -1#10#14#exact -1#14#15#exact \n"}
nil_first --> [3]nil_second --> []--------------------------{1=>"One method is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .\n", 2=>"One is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}
nil_first --> [1]nil_second --> []--------------------------{1=>"The CHAR method considers every Unicode character as a word , as proposed by Xu et al.\n", 2=>"The CHAR method considers every Unicode character as a word as proposed by Xu et al.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}
nil_first --> [10]nil_second --> []--------------------------{1=>"Another corpus we use in the experiments is a Wikipedia corpus : the Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .\n", 2=>"Another corpus we use in the experiments is a Wikipedia corpus , Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}
nil_first --> [11, 12]nil_second --> [11]--------------------------{1=>"From these corpora , we prepared three data sets , as explained below .\n", 2=>"From these corpora , we prepared three data sets as explained below .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}
nil_first --> [9]nil_second --> []--------------------------{1=>"For REUTERS , we used all 56 ,282 sentences .\n", 2=>"In the case of REUTERS , we have used all 56 ,282 sentences .\n", 3=>"-1#4#1#exact -1#5#2#exact -1#6#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact \n"}
nil_first --> [0]nil_second --> [0, 1, 2, 3, 7]--------------------------{1=>"For this data , we combined the JENAAD and REUTERS news corpora to acquire one news corpus .\n", 2=>"In this data , we have combined JENAAD and REUTERS news corpora to get one news corpus .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#syn -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}
nil_first --> [0, 6]nil_second --> [0, 5]--------------------------{1=>"We used all 56 ,282 and 150 ,000 sentences , respectively .\n", 2=>"We have used all 56 ,282 and 150 ,000 sentences respectively .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#10#exact -1#11#11#exact \n"}
nil_first --> [9]nil_second --> [1]--------------------------{1=>"For each corpus , we divided the sentences into the first 1 ,000 for testing , the next 500 for development , and the remaining for training .\n", 2=>"For each corpus , we divide it into the first 1 ,000 , the next 500 , and the rest for test , development , and training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#8#6#exact -1#7#8#exact -1#13#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#20#13#exact -1#21#14#stem -1#22#15#exact -1#18#16#exact -1#14#17#exact -1#15#18#exact -1#23#19,20#para -1#24#21#exact -1#25#22#exact -1#19#24#syn -1#26#26#exact -1#27#27#exact \n"}
nil_first --> [7, 23, 25]nil_second --> [6, 12, 16, 17]--------------------------{1=>"In total , we gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively .\n", 2=>"We have gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively , in total .\n", 3=>"-1#21#0#lc -1#22#1#exact -1#13#2#exact -1#0#3#lc -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#20#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#23#22#exact \n"}
nil_first --> []nil_second --> [1]--------------------------{1=>"Since the WIKIPEDIA corpus is a multi-category XML dataset , we sorted them by the DOCID in ascending order , and by the document categories : LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .\n", 2=>"Firstly , since the WIKIPEDIA corpus is a multi-category XML dataset , we have sorted them by the DOCID in the ascending order and by the document categories LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#21#17#exact -1#22#18#exact -1#1#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#42#40#exact -1#43#41#exact -1#44#42#exact -1#45#43#exact -1#46#44#exact -1#47#45#exact -1#48#46#exact -1#49#47#exact -1#50#48#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact \n"}
nil_first --> [25]nil_second --> [0, 13, 20]--------------------------{1=>"Next , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .\n", 2=>"Secondly , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}
nil_first --> [0]nil_second --> [0]--------------------------{1=>"Then , sentence pairs that include a character | in English or Japanese were removed , because it caused a problem with Moses .\n", 2=>"Thirdly , sentence pairs that include a character \" | \" in English or Japanese are removed because it caused a problem with Moses .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#syn -1#16#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}
nil_first --> [0, 8, 15]nil_second --> [0, 8, 9, 10]--------------------------{1=>"In order to adjust the balance of the domains , we sampled the data twice : First , we extracted the first line for every 477 lines .\n", 2=>"In order to adjust the balance of the domains , we have sampled the data twice : First we extract the first line for every 477 lines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5,6#para -1#13#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#20#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#18#exact -1#19#19#stem -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}
nil_first --> [17, 20]nil_second --> [11]--------------------------{1=>"Then , we merged the remaining 476 ,012 lines , and from this extract , we extracted the first line for every 952 lines .\n", 2=>"After this , we have merged the remaining 476 ,012 lines and from this extract the first line for every 952 lines .\n", 3=>"-1#2#1#exact -1#3#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}
nil_first --> [0, 9, 14, 15, 16]nil_second --> [0, 1, 4]--------------------------{1=>"Finally , we obtained 1 ,000 test , 500 development , and 475 ,512 training data .\n", 2=>"Finally , we have obtained 1 ,000 test , 500 development , and 475 ,512 training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}
nil_first --> []nil_second --> [3]--------------------------{1=>"We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .\n", 2=>"We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33,34,35#33,34,35,36#para -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#54#54#exact -1#55#55#exact \n"}
nil_first --> []nil_second --> [36]--------------------------{1=>"Currently , the most popular way to evaluate SMT is to use word-based evaluation metrics , such as BLEU and RIBES .\n", 2=>"Currently , the most popular way to evaluate Statistical Machine Translation is to use word-based evaluation metrics such as BLEU and RIBES .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}
nil_first --> [8, 15]nil_second --> [8, 9, 10]--------------------------{1=>"If we do not have segmented reference and test data , we cannot evaluate the outputs by word-based evaluation metrics .\n", 2=>"If we do not have segmented reference and test data , we cannot evaluate outputs by word-based evaluation metrics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}
nil_first --> [14]nil_second --> []--------------------------{1=>"For example , for English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .\n", 2=>"For example , in the case of English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact \n"}
nil_first --> [3]nil_second --> [3, 4, 5, 6]--------------------------{1=>"On the other hand , for Japanese-English translations , we must tokenize test data to evaluate the outputs .\n", 2=>"On the other hand , in the case of Japanese-English translations , we must tokenize test data to evaluate the outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact \n"}
nil_first --> [5]nil_second --> [5, 6, 7, 8]--------------------------{1=>"As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .\n", 2=>"As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is hard to independently evaluate the effects of word segmentation on training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20#20,21#para -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}
nil_first --> [19]nil_second --> []--------------------------{1=>"The best results were obtained when we used the same word segmentation as the training data .\n", 2=>"And the best results were obtained when we use the same word segmentation as the training data .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}
nil_first --> []nil_second --> [0]--------------------------{1=>"Hence , if we keep our word-based evaluations , this problem remains .\n", 2=>"Hence , this problem remains if we keep our word-based evaluations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#2#9#exact -1#3#10#exact -1#4#11#exact -1#11#12#exact \n"}
nil_first --> [8]nil_second --> []--------------------------{1=>"In order to manage this issue , we used one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .\n", 2=>"In order to manage such a problem , we use one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#5#para -1#7#6#exact -1#8#7#exact -1#9#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}
nil_first --> [4]nil_second --> [4, 5]--------------------------{1=>"As this method evaluates the character-level information , outputs are not required to be segmented , and it is free from word segmentation variations .\n", 2=>"As this method evaluates the character-level information , outputs are not required to be segmented and it is free from word segmentation variations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}
nil_first --> [15]nil_second --> []--------------------------{1=>"We have conducted English and Japanese machine translation in both directions , following the steps below :\n", 2=>"We have conducted English and Japanese machine translation in both directions by the following steps :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#exact -1#12#13#exact -1#14#14#exact -1#15#16#exact \n"}
nil_first --> [11, 15]nil_second --> [11]--------------------------{1=>"1 .Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .\n", 2=>"1Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"2 .Run Japanese word segmentation methods and a normalization script , which was introduced by the NTCIR-9 PATMT task .\n", 2=>"2Run Japanese word segmentation methods and a normalization script which was introduced by the NTCIR-9 PATMT task .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}
nil_first --> [0, 1, 10]nil_second --> [0]--------------------------{1=>"3 .Tokenize and lowercase English texts by Moses' tokenizer and lowercase scripts .\n", 2=>"3Tokenize and lowercase English text by Moses' tokenizer and lowercase scripts .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#stem -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"4 .Create language models from target languages' training data , with SRILM 1 .5 .12 .\n", 2=>"4Create language models from target languages' training data , with SRILM 1 .5 .12 .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"5 .Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .\n", 2=>"5Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"6 .Decode source test data with Moses ( 2010-08-13 ) .\n", 2=>"6Decode source test data with Moses ( 2010-08-13 ) .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"7 .Compute evaluation scores of the outputs .\n", 2=>"7Compute evaluation scores of the outputs .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact \n"}
nil_first --> [0, 1]nil_second --> [0]--------------------------{1=>"This method enabled more accurate translations within English-Japanese translations than with the conventional settings .\n", 2=>"This method enabled more accurate translations within English-Japanese translations than the conventional settings .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}
nil_first --> [10]nil_second --> []--------------------------{1=>"- Remove articles a , an , and the\n", 2=>"- Remove articles \" a \" , \" an \" , and \" the \"\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#4#exact -1#10#6#exact -1#11#7#exact \n"}
nil_first --> [3, 5]nil_second --> [3, 4, 5, 7, 8, 9]--------------------------{1=>"All evaluation metrics have been used in both directions between English and Japanese , to measure the consistency and sufficiency of the metrics in the language pair .\n", 2=>"All evaluation metrics have been used in both directions between English and Japanese , to measure consistency and sufficiency of the metrics in the language pair .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4#para -1#22#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17#16,17,18#para -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#24#23,24,25#para -1#25#26#exact -1#26#27#exact \n"}
nil_first --> [5]nil_second --> [23]--------------------------{1=>"In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .\n", 2=>"In this case , the evaluation scores created by BLEU and RIBES are not comparative due to the differences of Japanese word definitions between the outputs of word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#24#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25,26#para -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}
nil_first --> [15, 20]nil_second --> [19]--------------------------{1=>"Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost the same , while small changes have been introduced , due to statistical errors and the differences in the methods in how to treat space characters .\n", 2=>"Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost same while small changes have been introduced due to statistical errors and the differences in the methods how to treat space characters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#28#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#29,30,31#31,32,33,34#para -1#32#35#exact -1#33,34#36,37,38#para -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#42#exact \n"}
nil_first --> [18, 25]nil_second --> []--------------------------{1=>"We found that the three supervised morphological analyzers : KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .\n", 2=>"We found that the three supervised morphological analyzers KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}
nil_first --> [8]nil_second --> []--------------------------{1=>"For instance , on REUTERS in Table 2 , BLEU scores ranged from 27 .88 to 29 .53 , while for latticelm , the score was 15 .28 and for CAT , the score was 22 .10 .\n", 2=>"For instance , on REUTERS in Table 2 , BLEU scores were ranged from 27 .88 to 29 .53 , while latticelm was 15 .28 and CAT was 22 .10 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#21#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#30#exact -1#27#34#exact -1#28#35#exact -1#29#36#exact -1#30#37#exact \n"}
nil_first --> [20, 22, 23, 24, 29, 31, 32, 33]nil_second --> [11]--------------------------{1=>"The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .\n", 2=>"The unsupervised morphological analyzer latticelm and one of heuristic methods CAT were worse than our expectations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#13#exact -1#12#16#exact -1#13#17#exact -1#15#18#exact -1#16#19#exact \n"}
nil_first --> [4, 6, 12, 14, 15]nil_second --> [11, 14]--------------------------{1=>"These two results were the worst , in all of the settings .\n", 2=>"These two were the worst or the second worst results in all settings .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#10#7#exact -1#11#8#exact -1#6#10#exact -1#12#11#exact -1#13#12#exact \n"}
nil_first --> [6, 9]nil_second --> [5, 7, 8]--------------------------{1=>"The results were better than the results for the supervised morphological analyzers in BLEU .\n", 2=>"It was relatively much better than the supervised morphological analyzers in BLEU .\n", 3=>"-1#6#0#lc -1#4#3#exact -1#5#4#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact \n"}
nil_first --> [1, 2, 5, 6, 7, 8]nil_second --> [0, 1, 2, 3]--------------------------{1=>"It was almost competitive in RIBES and BLUE in Characters .\n", 2=>"Besides , it was almost competitive in RIBES and BLUE in Characters .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact \n"}
nil_first --> []nil_second --> [0, 1]--------------------------{1=>"CHAR achieved the best score in BLEU on REUTERS ( 38 .42 ) , but the second-best was KyTea ( 29 .53 ) .\n", 2=>"For example , CHAR achieved the best 38 .42 score in BLEU on REUTERS , but the second best KyTea was 29 .53 .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#7#10#exact -1#8#11#exact -1#2#13#exact -1#15#14#exact -1#16#15#exact -1#20#17#exact -1#19#18#exact -1#21#20#exact -1#22#21#exact -1#23#23#exact \n"}
nil_first --> [9, 12, 16, 19, 22]nil_second --> [0, 1, 14, 17, 18]--------------------------{1=>"For BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .\n", 2=>"In the case of BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .\n", 3=>"-1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact \n"}
nil_first --> [0]nil_second --> [0, 1, 2, 3]--------------------------{1=>"For Japanese-English translations , the evaluation scores were generally lower than for English-Japanese translations .\n", 2=>"In this case , the evaluation scores are lower than English-Japanese translations in general .\n", 3=>"-1#11#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#12,13#8#para -1#8#9#exact -1#9#10#exact -1#10#12#exact -1#14#14#exact \n"}
nil_first --> [0, 1, 11, 13]nil_second --> [0, 1, 2]--------------------------{1=>"All supervised analyzers performed better than the unsupervised and the both heuristic methods .\n", 2=>"All supervised analyzers were better than the unsupervised and the both heuristic methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}
nil_first --> [3]nil_second --> [3]--------------------------{1=>"Conversely , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT performed competitively with the supervised analyzers in RIBES .\n", 2=>"On the other hand , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT were competitive to the supervised analyzers in RIBES .\n", 3=>"-1#0,1,2,3#0#para -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact \n"}
nil_first --> [13, 14, 15]nil_second --> [16, 17, 18]--------------------------{1=>"latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .\n", 2=>"For example , latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact \n"}
nil_first --> []nil_second --> [0, 1, 2]--------------------------{1=>"The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .\n", 2=>"The results were the worst scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS . The only one exception was in the case of the best 56 .55 BLEU in Characters on REUTERS .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#4#exact -1#3#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#21#18#exact -1#18#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact \n"}
nil_first --> [2, 3, 6, 16, 17]nil_second --> [4, 14, 15, 16, 17, 19, 20, 22]--------------------------{1=>"We found that the results of the supervised morphological analyzers were better in both English-Japanese and Japanese-English experiments .\n", 2=>"We found the results of the supervised morphological analyzers are better in both English-Japanese and Japanese-English experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}
nil_first --> [2]nil_second --> []--------------------------{1=>"Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .\n", 2=>"And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .\n", 3=>"-1#17#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#para -1#22#18#exact -1#18,19#19,20#para -1#20#21#exact -1#21#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact \n"}
nil_first --> [0, 23, 34]nil_second --> [0]--------------------------{1=>"This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions , in most cases .\n", 2=>"This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions in most cases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}
nil_first --> [20]nil_second --> []--------------------------{1=>"On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT performed much poorer than the supervised morphological analyzers .\n", 2=>"On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT were very much worse than the supervised morphological analyzers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19#18#exact -1#20,21#19,20#para -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}
nil_first --> [17]nil_second --> [17, 18]--------------------------{1=>"It excelled with English-Japanese translations , but not with Japanese-English translations .\n", 2=>"It was good at English-Japanese but not at Japanese-English translations .\n", 3=>"-1#0#0#exact -1#4#3#exact -1#9#4#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#10#11#exact \n"}
nil_first --> [1, 2, 5, 8, 10]nil_second --> [1, 2, 3, 7]--------------------------{1=>"We consider the possible reasons for this result in the following list :\n", 2=>"We consider the possible reasons for this result :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#12#exact \n"}
nil_first --> [8, 9, 10, 11]nil_second --> []--------------------------{1=>"- Long sentences were not translated as well as other word segmentation outputs .\n", 2=>"- Long sentences were translated worse than the other word segmentation outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#8#6,7,8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}
nil_first --> [4]nil_second --> [5, 6, 7]--------------------------{1=>"However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .\n", 2=>"However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}
nil_first --> []nil_second --> [8]--------------------------{1=>"The current evaluation metrics that we pursued in this paper were insufficient to discuss the relative advantages and disadvantages of word segmentation in detail , since the scores that were produced were inconsistent , as explained below :\n", 2=>"The current evaluation metrics we pursued in this paper were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation , since they did not produce consistent scores as explained below :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10,11,12#10,11,12#para -1#13#13#exact -1#17#14#exact -1#18#15#exact -1#19,20,21,22#16,17,18#para -1#23#20#exact -1#24#21#exact -1#25#24#exact -1#26#25#exact -1#32#27#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact \n"}
nil_first --> [4, 19, 22, 23, 26, 28, 29, 30, 31, 32, 33]nil_second --> [14, 15, 16, 27, 28, 29, 30, 31]--------------------------{1=>"Moreover , there is also a case in which RIBES and BLEU in Characters were incompatible with each other .\n", 2=>"Moreover , there is also a case that RIBES and BLEU in Characters were incompatible with each other .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#11#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}
nil_first --> [8, 12]nil_second --> [7]--------------------------{1=>"For example , on WIKIPEDIA in Table 2 , while CHAR was the highest , and performed better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .\n", 2=>"For example , on WIKIPEDIA in Table 2 , while CHAR was relatively the highest and greatly better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#25#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact \n"}
nil_first --> [16, 25]nil_second --> [12, 16]--------------------------{1=>"This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .\n", 2=>"This work focused on how the difference of word segmentation affects SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16,17#14,15,16#para -1#25#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#28#25#exact -1#26#26#exact -1#27#27#exact -1#29#28,29#para -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}
nil_first --> [7]nil_second --> [7]--------------------------{1=>"In summary , we found that the representative morphological analyzers were competitive and much better than both the unsupervised analyzer and one of our heuristic methods .\n", 2=>"In summary , we found that the representative morphological analyzers were competitive and much better than both unsupervised analyzer and one of our heuristic methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}
nil_first --> [17]nil_second --> []