--------------------------
{1=>"The target parsers are adapted to sentences of these constructions extracted from fiction and query texts .\n", 2=>"The target parsers are adapted to the sentences for these constructions extracted from fiction and query texts .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> [7]
nil_second --> [6, 8]
--------------------------
{1=>"Analysis of the experimental results illustrates the need to handle different sentence constructions through fundamental improvement of the parsers such as re-construction of feature designs .\n", 2=>"The analysis of the experimental results will illustrate the necessity for handling various sentence constructions by fundamental improvement of parsers such as re-construction of feature designs .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#7#5#stem -1#8#6#exact -1#9,10#7,8#para -1#11#9#stem -1#12#10#para -1#13#11#exact -1#14#12#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#0#17#lc -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> [13]
nil_second --> [6, 15]
--------------------------
{1=>"Recent research on parsing technologies has achieved high parsing accuracy in the same domain as the training data , but once we move to unfamiliar domains , the performance decreases to unignorable levels .\n", 2=>"Recent research on parsing technologies has achieved high parsing accuracies on the same domains as the training data , but once we move to unfamiliar domains , the performances decrease at unignorable levels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10,11,12#10,11,12#para -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#stem -1#29#29#stem -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [30]
nil_second --> [30]
--------------------------
{1=>"Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .\n", 2=>"Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .\n", 3=>"-1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#28#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}nil_first --> [0, 1, 28]
nil_second --> [0, 1, 8]
--------------------------
{1=>"However , there are some cases where we cannot achieve such high parsing accuracy as parsing the Penn Treebank ( PTB ) merely by re-training or adaptation .\n", 2=>"However , there are some cases where we cannot achieve as high parsing accuracies as parsing the Penn Treebank just by re-training or adaptation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#22#syn -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact \n"}nil_first --> [10, 19, 20, 21]
nil_second --> [10]
--------------------------
{1=>"For example , the parsing accuracy for the Brown corpus is significantly lower than that for the Wall Street Journal ( WSJ ) portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .\n", 2=>"For example , the parsing accuracy for the Brown corpus is significantly lower than for the WSJ portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#21#exact -1#17#23#exact -1#18#24#exact -1#19#25#exact -1#20#26#exact -1#21#27#exact -1#22#28#exact -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact -1#28#34#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#32#38#exact -1#33#39#exact -1#34#40#exact -1#35#41#exact -1#36#42#exact -1#37#43#exact -1#38#44#exact \n"}nil_first --> [14, 17, 18, 19, 20, 22]
nil_second --> []
--------------------------
{1=>"This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions that have not been extensively studied in recent parsing research : imperatives and questions .\n", 2=>"This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions which were not extensively studied in the recent parsing research : imperatives and questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#21#20,21#para -1#20#22#syn -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [19]
nil_second --> [19, 25]
--------------------------
{1=>"In these constructions , words in certain syntactic positions disappear or the order of the words changes .\n", 2=>"In these constructions , words in some syntactic positions disappear or the orders of words change .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14,15#para -1#15#16#stem -1#16#17#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"To do so , we first prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .\n", 2=>"In order to do so , we prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11,12#10,11,12,13#para -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> [5]
nil_second --> [0, 1, 13, 14]
--------------------------
{1=>"In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracy of their part-of-speech ( POS ) tagger .\n", 2=>"In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracies of a part-of-speech tagger for them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22,23,24#22,23,24,25#para -1#26#26#stem -1#27#27#exact -1#31,32#28#para -1#29#29#exact -1#30#33#exact -1#33#34#exact \n"}nil_first --> [30, 31, 32]
nil_second --> [25, 28]
--------------------------
{1=>"Since domain adaptation is an extensive research area in parsing research \\CITE , many ideas have been proposed , including un- or semi-supervised approaches \\CITE and supervised approaches \\CITE .\n", 2=>"Since domain adaptation has been an extensive research area in parsing research \\CITE , a lot of ideas have been proposed , including un- / semi-supervised approaches \\CITE and supervised approaches \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact \n"}nil_first --> [13, 21]
nil_second --> [14, 15, 16, 24]
--------------------------
{1=>"The main focus of these works is on adapting parsing models trained with a specific genre of text ( in most cases the Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .\n", 2=>"Their main focus was on adapting parsing models trained with a specific genre of text ( in most cases Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#13#3#exact -1#3#5,6#para -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#26#16#exact -1#27#17#exact -1#15#18#exact -1#16,17,18#19,20,21,22#para -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#14#31#exact -1#28#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact \n"}nil_first --> [4, 30]
nil_second --> []
--------------------------
{1=>"The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression .\n", 2=>"A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions .\n", 3=>"-1#10#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#stem -1#9#7#exact -1#11#8,9#para -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#syn -1#18#16#exact -1#19#17#stem -1#20#18#exact \n"}nil_first --> []
nil_second --> [0, 6, 8]
--------------------------
{1=>"Compared to domain adaptation , structural types of sentences have received little attention to date .\n", 2=>"Compared to domain adaptation , structural types of sentences have gained little attention to date .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"This work highlighted the low accuracy of state-of-the-art parsers on questions , and proposed a supervised parser adaptation by manually creating a treebank of questions .\n", 2=>"The work pointed out low accuracy of state-of-the-art parsers on questions , and proposed supervised parser adaptation by manually creating a treebank of questions .\n", 3=>"-1#1#1#exact -1#0#3#lc -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#20#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [0, 2, 21]
nil_second --> [2, 3]
--------------------------
{1=>"QuestionBank was used for the supervised training of an LFG parser , resulting in a significant improvement in parsing accuracy .\n", 2=>"QuestionBank was used for the supervised training of an LFG parser , and achieved a significant improvement in parsing accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#17#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [12, 17]
nil_second --> [12, 13]
--------------------------
{1=>"In this work , question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories .\n", 2=>"They collected question sentences from TREC 9-12 competitions , and annotated these sentences with POSs and CCG lexical categories .\n", 3=>"-1#8#3#exact -1#2#4#exact -1#3#5#exact -1#1#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#9#12#exact -1#10#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [0, 1, 2, 6, 15]
nil_second --> [0, 11, 12, 14]
--------------------------
{1=>"The authors reported a significant improvement in CCG parsing without phrase structure annotations .\n", 2=>"They reported a significant improvement in CCG parsing without phrase structure annotations .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"Although QuestionBank and the resource of \\CITE claim to be corpora of questions , they are biased because the sentences come from QA queries .\n", 2=>"Although QuestionBank and the resource of \\CITE are claimed to be a corpus of questions , they are biased because the sentences come from QA queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10#7,8,9#para -1#12#10#syn -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> []
nil_second --> [7, 11]
--------------------------
{1=>"For example , such queries rarely include yes / no questions or tag questions .\n", 2=>"For example , such queries rarely include yes / no questions and tag questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [11]
nil_second --> [11]
--------------------------
{1=>"For our study , sentences were collected from the Brown corpus , which includes a wider range of types of questions and imperatives .\n", 2=>"In our work , sentences are collected from the Brown corpus , which includes a wider range of types of questions and imperatives .\n", 3=>"-1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [0]
nil_second --> [0]
--------------------------
{1=>"In the experiments , we also used QuestionBank for comparison .\n", 2=>"In the experiments , we will additionally use QuestionBank for comparison .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#para -1#7#6#stem -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"We used the tagger in \\CITE .\n", 2=>"We use a tagger in \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"The MST and Malt parsers are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \\CITE and transition-based algorithm \\CITE , respectively .\n", 2=>"The MST parser and Malt parser are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \\CITE and transition-based algorithm \\CITE respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#8#4#exact -1#6#5#exact -1#7#6#exact -1#2#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [24]
nil_second --> [5]
--------------------------
{1=>"Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree , we used the non-projective versions because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .\n", 2=>"Although the publicly available implementation of each parser also has an option to restrict the output to be a projective dependency tree , we used the non-projective version because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#stem -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact \n"}nil_first --> []
nil_second --> [17]
--------------------------
{1=>"We used the pennconverter \\CITE to convert a PTB-style treebank into dependency trees .\n", 2=>"We used pennconverter \\CITE to convert a PTB-style treebank to dependency trees .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [2, 10]
nil_second --> [9]
--------------------------
{1=>"To evaluate the output from each of the parsers , we used the labeled attachment accuracy excluding punctuation .\n", 2=>"For the evaluation of the output from each of the MST and Malt parser , we used the labeled attachment accuracy excluding the punctuations .\n", 3=>"-1#2,3,4#1,2#para -1#5#3#exact -1#6,7,8,9#4,5,6#para -1#1#7#exact -1#13#8#stem -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#17#12#exact -1#18#13#exact -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#23#17#stem -1#24#18#exact \n"}nil_first --> [0]
nil_second --> [0, 10, 11, 12, 22]
--------------------------
{1=>"The Enju parser \\CITE is a deep parser based on the HPSG ( Head Driven Phrase Structure Grammar ) formalism .\n", 2=>"The Enju parser \\CITE is a deep parser based on the HPSG formalism .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#19#exact -1#13#20#exact \n"}nil_first --> [12, 13, 14, 15, 16, 17, 18]
nil_second --> []
--------------------------
{1=>"It produces an analysis of a sentence including the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .\n", 2=>"It produces an analysis of a sentence that includes the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23,24,25,26#22,23,24#para -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [25]
nil_second --> []
--------------------------
{1=>"We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank .\n", 2=>"We used a toolkit distributed with the Enju parser for training the parser with a PTB-style treebank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#11#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [11]
nil_second --> [2]
--------------------------
{1=>"The toolkit initially converts a PTB-style treebank into an HPSG treebank and then trains the parser on this .\n", 2=>"The toolkit initially converts the PTB-style treebank into an HPSG treebank and then trains the parser on it .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact \n"}nil_first --> [4, 17]
nil_second --> [4, 17]
--------------------------
{1=>"As evaluation metrics for the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .\n", 2=>"As the evaluation metrics of the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> [3]
nil_second --> [1, 4]
--------------------------
{1=>"This section explains how we collected the treebanks of imperatives and questions used in the experiments in Section \\REF .\n", 2=>"This section explains how we collected the treebanks of imperatives and questions , which were used in the experiments in Section \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact \n"}nil_first --> []
nil_second --> [12, 13, 14]
--------------------------
{1=>"The Penn Treebank 3 contains treebanks of several genres of texts .\n", 2=>"Penn Treebank 3 contains treebanks of several genres of texts .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [0]
nil_second --> []
--------------------------
{1=>"Although the WSJ treebank has been used extensively for parsing experiments , we used the treebank of the Brown Corpus in our experiments .\n", 2=>"While the Wall Street Journal ( WSJ ) treebank has extensively been used for parsing experiments , we use the treebank of the Brown Corpus in our experiments .\n", 3=>"-1#1#1#exact -1#6#2#exact -1#8#3#exact -1#9#4#exact -1#11#5#exact -1#12#6#exact -1#10#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#17#12#exact -1#18#13#stem -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#22#17#exact -1#23#18#exact -1#24#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#28#23#exact \n"}nil_first --> [0]
nil_second --> [0, 2, 3, 4, 5, 7]
--------------------------
{1=>"As the Brown Corpus portion includes texts of literary works , it is expected to contain inherently a larger number of imperatives and questions than the WSJ portion .\n", 2=>"Because the Brown Corpus portion includes texts of literary works , it is expected that it inherently contains a larger number of imperatives and questions than the WSJ portion .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13#11,12,13,14#para -1#17#15#stem -1#16#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [0]
nil_second --> [0, 14, 15]
--------------------------
{1=>"The Brown Corpus portion of the Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .\n", 2=>"The Brown Corpus portion of Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#16#5#exact -1#17#6#exact -1#18#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#5#18#exact -1#6#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Interrogative sentences are annotated with the phrase label \" SBARQ \" or \" SQ \" , where \" SBARQ \" denotes wh-questions , while \" SQ \" denotes yes / no questions .\n", 2=>"Interrogative sentences are annotated with the phrase label \" SBARQ \" or \" SQ \" , where \" SBARQ \" represents wh-questions , while \" SQ \" denotes yes / no questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#27#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [27]
nil_second --> [20]
--------------------------
{1=>"All sentences annotated with these phrase labels were extracted .\n", 2=>"We extracted those sentences annotated with these phrase labels .\n", 3=>"-1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#1#8#exact -1#9#9#exact \n"}nil_first --> [0, 7]
nil_second --> [0, 2]
--------------------------
{1=>"Imperatives and questions appear not only at the top level but also as embedded clauses .\n", 2=>"Imperatives and questions appear not only at the top level but also appear as embedded clauses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"However , is these were embedded in another imperative or question , we only extracted the outermost one .\n", 2=>"When they are embedded in another imperative or question , we only extracted the outermost one .\n", 3=>"-1#9#1#exact -1#2#3,4#para -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [0, 2, 11]
nil_second --> [0, 1]
--------------------------
{1=>"Extracted sentences were post-processed to fit the natural sentence form ; that is , with first characters capitalized and question marks or periods added as appropriate .\n", 2=>"Extracted sentences are post-processed so that they have natural sentence forms : first characters are capitalized , and question marks or periods are added when appropriate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#25#5#para -1#8#6,7#para -1#9#8#exact -1#10#9#stem -1#5#11#exact -1#21,22#12#para -1#16#13#exact -1#12#15#exact -1#13#16#exact -1#15#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#23#23#exact -1#26#26#exact \n"}nil_first --> [4, 10, 14, 22, 24, 25]
nil_second --> [4, 6, 7, 11, 14, 24]
--------------------------
{1=>"The numbers of sentences for each section are given in Table \\REF .\n", 2=>"The number of sentences for each section is shown in Table \\REF .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"Although we also applied a similar method to the WSJ portion , we only obtained 115 imperatives and 432 questions .\n", 2=>"Although we also applied a similar method to the WSJ portion , we could obtain only 115 imperatives and 432 questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#15#13#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> [14]
nil_second --> [13, 14]
--------------------------
{1=>"This data was not used in the experiments .\n", 2=>"We will not use this data in the experiments .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#2#3#exact -1#3#4#stem -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact \n"}nil_first --> [2]
nil_second --> [0, 1]
--------------------------
{1=>"As described below , we also used QuestionBank in the experiments .\n", 2=>"As we will describe below , we additionally use QuestionBank in experiments .\n", 3=>"-1#0#0#exact -1#3#1#stem -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#para -1#8#6#stem -1#9#7#exact -1#10#8#exact -1#11#9,10#para -1#12#11#exact \n"}nil_first --> []
nil_second --> [1, 2]
--------------------------
{1=>"The advantage , however , of using the Brown treebank is that it includes annotations of function tags and empty categories , and therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \\CITE , which relies on function tags and empty categories .\n", 2=>"However , an advantage of using the Brown treebank is that it includes annotations of function tags and empty categories . Therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \\CITE , which relies on function tags and empty categories .\n", 3=>"-1#26#0#lc -1#3#1#exact -1#22#2#exact -1#0#3#lc -1#1#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#34#21#exact -1#40#22#exact -1#21#23#lc -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#41#42,43#para -1#42#44#exact -1#43#45#exact \n"}nil_first --> [24, 28, 36]
nil_second --> [2, 20]
--------------------------
{1=>"Hence , we show experimental results for Enju only with the Brown data .\n", 2=>"Hence , we will show experimental results on Enju only with the Brown data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> [6]
nil_second --> [3, 7]
--------------------------
{1=>"QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .\n", 2=>"QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"We extracted 3 ,859 sentences annotated with \" SBARQ \" or \" SQ \" .\n", 2=>"We extracted 3 ,859 sentences that are annotated with \" SBARQ \" or \" SQ \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact \n"}nil_first --> []
nil_second --> [5, 6]
--------------------------
{1=>"During the experiments , we found several annotation errors that caused fatal errors in the treebank conversion .\n", 2=>"During experiments , we found several annotation errors that caused fatal errors of treebank conversion .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [1, 13, 14]
nil_second --> [12]
--------------------------
{1=>"We manually corrected the annotations of twelve sentences .\n", 2=>"We therefore corrected annotations of twelve sentences manually .\n", 3=>"-1#0#0#exact -1#7#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#8#exact \n"}nil_first --> [3]
nil_second --> [1]
--------------------------
{1=>"We intend making these corrections publicly available .\n", 2=>"We plan to make these corrections publicly available .\n", 3=>"-1#0,1#0,1#para -1#3#2#stem -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"We also found and corrected obvious inconsistencies in the corpus : character \" ' \" replaced by \" $<$ \" ( 737 sentences ) , token \" ? \" tagged with \" ? \" instead of \" . \" ( 2 ,051 sentences ) , and phrase labels annotated as the POS ( one sentence ) .\n", 2=>"We also found and corrected obvious inconsistencies in the corpus : character \" ' \" replaced by \" $<$ \" ( 737 sentences ) , token \" ? \" tagged not with \" . \" but with \" ? \" ( 2 ,051 sentences ) , and phrase labels annotated as POS ( one sentence ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#30#exact -1#32#31#exact -1#38#32#exact -1#39#33#exact -1#30#34#para -1#34#36#exact -1#33#37#exact -1#37#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#54#54#exact -1#55#55#exact -1#56#56#exact \n"}nil_first --> [35, 50]
nil_second --> [35, 36]
--------------------------
{1=>"We examined the performance of the three parsers and the POS tagger with Brown imperatives and questions , and QuestionBank questions .\n", 2=>"We examined performances of the three parsers and the POS tagger for Brown imperatives and questions , and QuestionBank questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#8#2#exact -1#2#3#stem -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [9, 12]
nil_second --> [11]
--------------------------
{1=>"By observing the effect of the parser or tagger adaptation in each domain , we can identify the difficulties in parsing imperative and question sentences .\n", 2=>"By observing the effects of parser or tagger adaptation to each domain , we would like to see the difficulties in parsing imperative and question sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#18#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#17#15,16#para -1#19,20#17,18,19#para -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> []
nil_second --> [14, 15, 16]
--------------------------
{1=>"We also examined the portability of sentence construction properties between two similar domains : questions in Brown and in QuestionBank .\n", 2=>"We also examined the portability of sentence construction properties between two similar domains : questions in Brown and QuestionBank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [18]
nil_second --> []
--------------------------
{1=>"We created experimental datasets for five domains : WSJ , Brown overall , Brown imperatives , Brown questions , and QuestionBank questions .\n", 2=>"We made experimental datasets for five domains : Wall Street Journal ( WSJ ) , Brown overall sentences , Brown imperatives , Brown questions , and QuestionBank questions .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#12#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#18#12#exact -1#19#13#exact -1#20#14#exact -1#21#15#exact -1#22#16#exact -1#23#17#exact -1#24#18#exact -1#25#19#exact -1#26#20#exact -1#27#21#exact -1#28#22#exact \n"}nil_first --> []
nil_second --> [8, 9, 10, 11, 13, 17]
--------------------------
{1=>"- Divided into three parts , for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .\n", 2=>"- Divided into three parts for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#17#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#23#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#28#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#35#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact \n"}nil_first --> [36]
nil_second --> []
--------------------------
{1=>"- Randomly divided into three parts for training ( 19 ,395 sentences ) , development set ( 2 ,424 sentences ) , and final test ( 2 ,424 sentences ) .\n", 2=>"- Randomly divided into three parts for training ( 19 ,395 sentences ) , development set ( 2 ,424 sentences ) , and final test ( 2 ,424 sentences )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"- divided into two parts : one for ten-fold cross validation test ( 65 $\\times$ 10 sentences ) and the other for error analysis ( 100 sentences ) .\n", 2=>"- divided into two parts : one for ten-folds cross validation test ( 65 $\\times$ 10 sentences ) and the other for error analysis ( 100 sentences )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [28]
nil_second --> []
--------------------------
{1=>"- divided into two parts : one for ten-fold cross validation test ( 112 $\\times$ 10 sentences ) and the other for error analysis ( 141 sentences ) .\n", 2=>"- divided into two parts : one for ten-folds cross validation test ( 112 $\\times$ 10 sentences ) and the other for error analysis ( 141 sentences )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [28]
nil_second --> []
--------------------------
{1=>"- from the top of the corpus divided into three parts for final test ( 1 ,000 sentences ) , training ( 2 ,560 sentences ) , and analysis ( 299 sentences ) .\n", 2=>"- from the top of the corpus divided into three parts for final test ( 1 ,000 sentences ) , training ( 2 ,560 sentences ) , and analysis ( 299 sentences )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [33]
nil_second --> []
--------------------------
{1=>"To adapt each parser and the POS tagger to a target domain , we trained the parser using combined training data for the target domain and the original parser .\n", 2=>"In order to adapt each parser or POS tagger to a target domain , we trained the parser on combined training data for the target domain and for the original parser .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#26#4#exact -1#16#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#23#15#exact -1#17#16#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#28#22#exact -1#24#23#exact -1#25#24#exact -1#29#26,27#para -1#30#28#exact -1#31#29#exact \n"}nil_first --> [17, 25]
nil_second --> [0, 1, 6, 18, 27]
--------------------------
{1=>"For a domain containing only a small amount of training data , we replicated the training data a certain number of times and utilized the concatenated replicas for training .\n", 2=>"For a domain which contains only small training data , we replicated the training data for certain times and just utilized the concatenated replicas for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5,6#para -1#7#8,9#para -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17,18#para -1#17#20,21#para -1#18#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact \n"}nil_first --> [7, 19]
nil_second --> [15, 19]
--------------------------
{1=>"- For Brown overall , we trained the model with the combined training data for the target domain and the original model .\n", 2=>"- For Brown overall , we trained the model with the combined training data for the target domain and for the original model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"For Brown imperatives / questions and QuestionBank , we replicated the training data a certain number of times and utilized the concatenated replicas and WSJ training data for training .\n", 2=>"For Brown imperatives / questions and QuestionBank , we replicated the training data for certain times and utilized the concatenated replicas and WSJ training data for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13,14#para -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact \n"}nil_first --> [15]
nil_second --> [13]
--------------------------
{1=>"For the POS tagger , the number of replicas of training data was determined as either 1 , 2 , 4 , 8 , 16 , 32 , 64 , or 128 , by testing these numbers on the development test sets in three of the ten datasets for cross validation .\n", 2=>"For POS tagger , the number of replicas of training data was determined among 1 , 2 , 4 , 8 , 16 , 32 , 64 , and 128 , by testing these numbers on development test sets in three of ten datasets of cross validation .\n", 3=>"-1#0#0#exact -1#4#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#5,6#5,6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38,39#para -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45,46#para -1#43#47#exact -1#45#49#exact -1#46#50#exact -1#47#51#exact \n"}nil_first --> [14, 15, 30, 48]
nil_second --> [13, 28, 44]
--------------------------
{1=>"- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and the original model .\n", 2=>"- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and for the original model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> []
nil_second --> [20]
--------------------------
{1=>"For Brown imperatives and questions , we replicated the training data ten times and utilized the concatenated replicas and WSJ training data for training .\n", 2=>"For Brown imperatives and questions , we replicated the training data for ten times and utilized the concatenated replicas and WSJ training data for training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"- We used the toolkit in the Enju parser \\CITE .\n", 2=>"- We used a toolkit in the Enju parser \\CITE\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact \n"}nil_first --> [6, 10]
nil_second --> [3]
--------------------------
{1=>"Table \\REF gives the POS tagging accuracy for the target domains .\n", 2=>"Table \\REF shows the POS tagging accuracies for the target domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"When we applied the WSJ tagger to other domains , the tagging accuracy basically decreased .\n", 2=>"When we applied WSJ tagger to other domains , the tagging accuracy more or less decreased .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#9#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#15#14#exact -1#16#15#exact \n"}nil_first --> [10, 13]
nil_second --> [12, 13, 14]
--------------------------
{1=>"For Brown overall , compared with the WSJ , the accuracy did not decrease much .\n", 2=>"Among them , for Brown overall sentences , the accuracy did not decrease much from WSJ .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#2#3#exact -1#8#6#exact -1#15#7#exact -1#7#8#exact -1#9#9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#16#15#exact \n"}nil_first --> [4, 5]
nil_second --> [0, 1, 6, 14]
--------------------------
{1=>"The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .\n", 2=>"The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .\n", 3=>"-1#15#0#lc -1#16#1#exact -1#18#2#exact -1#19#3#exact -1#20#4#exact -1#4#5#exact -1#21#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#3#16#exact -1#6#17#stem -1#22#18#exact -1#23#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#29,30#25#para -1#33#27#exact -1#24#28#exact -1#34#29#exact -1#35#30#exact -1#36#31#exact -1#37#32#exact -1#38#33#exact -1#39#34#exact \n"}nil_first --> [14, 15, 23, 24, 26]
nil_second --> [0, 1, 2, 5, 14, 17, 28, 31, 32]
--------------------------
{1=>"Figure \\REF shows the POS tagging accuracy for the target domains for varying sizes of the target training data .\n", 2=>"Figure \\REF shows the POS tagging accuracy for the target domains given by changing the size of the target training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#syn -1#14,15#13#para -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact \n"}nil_first --> [11]
nil_second --> [11, 12]
--------------------------
{1=>"This graph shows that for both types of sentences , the first 300 training sentences greatly improved the accuracy , but thereafter , the effect of adding training data declined .\n", 2=>"This graph shows that for both types of sentences , first 300 training sentences improved the accuracy rapidly , and after that , the effect of adding training corpus declined .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#15#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#23#17#exact -1#16#18#exact -1#18#19#exact -1#22#22#exact -1#24,25#23,24,25#para -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [15, 20, 21, 28]
nil_second --> [17, 19, 20, 21, 28]
--------------------------
{1=>"To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\\% in Table \\REF ) , preparing much more training data does not appear to be enough .\n", 2=>"In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\\% in Table \\REF ) , it would not seem to be enough only to prepare much more training data .\n", 3=>"-1#2#0#lc -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#30#20#stem -1#31#21#exact -1#32#22#exact -1#33#23#exact -1#34#24#exact -1#23,24,25#25,26,27,28#para -1#26#29#exact -1#27#30#exact -1#35#31#exact \n"}nil_first --> [1, 10]
nil_second --> [0, 1, 3, 21, 22, 28, 29]
--------------------------
{1=>"Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .\n", 2=>"We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .\n", 3=>"-1#0#2#lc -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#11#10#exact -1#12#11#exact -1#13,14#12,13#para -1#15#14#exact -1#16#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#24#22#exact -1#26#23,24#para -1#28#25#para -1#27#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#29#32#para -1#30#33#exact -1#35#34#exact \n"}nil_first --> [0, 1, 16, 21, 31]
nil_second --> [1, 9, 10, 20, 22, 23, 25]
--------------------------
{1=>"Tables \\REF , \\REF , and \\REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank , respectively .\n", 2=>"Table \\REF , \\REF , and \\REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank respectively .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , \" VB \\SPEC \" .\n", 2=>"In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , \" VB \\SPEC \" .\n", 3=>"-1#1#1#exact -1#3#3#exact -1#4#4#exact -1#6#5#syn -1#7#6#exact -1#8#7#exact -1#9#8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact \n"}nil_first --> [0, 2, 19, 20]
nil_second --> [0, 2, 5, 20, 21, 22]
--------------------------
{1=>"We then analyzed why each of these errors had occurred .\n", 2=>"We then analyzed why each of such errors had occurred .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"These two types of errors arise from the following differences in sentence constructions between the WSJ declarative and Brown imperative sentences .\n", 2=>"These two types of errors would respectively come from the following differences in sentence constructions between WSJ declarative and the Brown imperative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#19#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact \n"}nil_first --> [5]
nil_second --> [5, 6, 7]
--------------------------
{1=>"First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .\n", 2=>"Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#3#exact -1#3#4#stem -1#4#5#exact -1#5,6#6,7#para -1#7#9#exact -1#8#10#stem -1#10#14#exact -1#11#15#stem -1#12#16#exact -1#13,14#17,18#para -1#15#20#exact -1#16#21#stem -1#17#22#exact \n"}nil_first --> [2, 8, 11, 12, 13, 19]
nil_second --> [9]
--------------------------
{1=>"Since The WSJ tagger was trained on a domain consisting mainly of declarative sentences , with the training based on N-gram sequences of words or POSs , preference was given to noun phrase-derived tags at the beginning of a sentence .\n", 2=>"The WSJ tagger was trained on the domain mainly consisting of declarative sentences , and the training was based on N-gram sequences of words or POSs . The tagger therefore preferred to give noun phrase-derived tags to the beginning of a sentence .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#40#7#exact -1#7#8#exact -1#9#9#exact -1#8#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#6#16#exact -1#16#17#exact -1#17,18,19#18,19#para -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#30#27#stem -1#32#29#syn -1#31#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#37,38#34,35,36,37#para -1#41#38,39#para -1#42#40#exact \n"}nil_first --> [0, 15, 26, 28]
nil_second --> [14, 15, 26, 27, 28, 29, 36, 39]
--------------------------
{1=>"Second , the main verb in an imperative sentence takes a base form , whereas the main verb in a declarative sentence takes a form based on tense .\n", 2=>"Secondly , main verbs in imperative sentences take base forms while main verbs in declarative sentences take the forms according to tense .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#17#2#exact -1#2#3#exact -1#3#4#stem -1#4#5#exact -1#5#7#exact -1#6#8#stem -1#7#9#stem -1#8#10,11#para -1#9#12#stem -1#11#15,16#para -1#12#17#stem -1#13#18#exact -1#14#20#exact -1#15#21#stem -1#16#22#stem -1#18#24#stem -1#21#27#exact -1#22#28#exact \n"}nil_first --> [6, 13, 14, 19, 23, 25, 26]
nil_second --> [10, 19, 20]
--------------------------
{1=>"A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .\n", 2=>"The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .\n", 3=>"-1#1#1#exact -1#2#2#para -1#15#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#16#7#exact -1#6#8#exact -1#7#9#exact -1#13#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#28#16#exact -1#21#17#exact -1#14#18#stem -1#17#21#exact -1#18#22#stem -1#19#23#exact -1#20#24#syn -1#30#25#exact -1#22#26#exact -1#23#27#stem -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#stem -1#0#34#lc -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#44#exact \n"}nil_first --> [0, 19, 20, 32, 33, 42, 43]
nil_second --> [29]
--------------------------
{1=>"Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .\n", 2=>"The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#5#para -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#9#11,12#para -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact \n"}nil_first --> [0, 4, 10]
nil_second --> [8]
--------------------------
{1=>"After adapting the tagger to Brown imperatives , the N-gram model of the tagger would have learned that the first word in a sentence tends to be a verb , and that the main verb tends to take the base form ( VB ) .\n", 2=>"After adapting the tagger to Brown imperatives , the N-gram model of tagger would have learned that the first word in a sentence tends to be a verb , and the main verb tends to take base form ( VB ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#17#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#30#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#31#32,33#para -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact \n"}nil_first --> [31, 38]
nil_second --> []
--------------------------
{1=>"Table \\REF shows that after adaptation the above two types of errors decreased to some extent , although a few mistags of verbs still remained .\n", 2=>"Table \\REF shows that the above two types of errors did decrease to some extent . However , we can also observe that not a few mis-tags to verbs were still left after the adaptation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#32#4#exact -1#34#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#11#12#stem -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#17#16#exact -1#24#18#exact -1#25#19#exact -1#28#22#exact -1#30#23#exact -1#15#25#exact \n"}nil_first --> [17, 20, 21, 24]
nil_second --> [10, 16, 18, 19, 20, 21, 22, 23, 26, 27, 29, 31, 33, 35]
--------------------------
{1=>"By investigating the remaining errors associated with VB , we found that several errors still occurred even in simple imperative sentences such as \" VB \\SPEC NN \" for \" Charge \" in \" Charge something for it \" , and that some errors tended to occur after a to-infinitive phrase or conjunction , such as \" VB \\SPEC NN \" for \" subtract \" in \" To find the estimated net farm income , subtract . . . \" .\n", 2=>"When we observe each of the left errors around VB , we found that several errors still occurred even in simple imperative sentences such as \" VB \\SPEC NN \" for \" Charge \" in \" Charge something for it . \" , and that some errors tended to occur after to-infinitive phrase or conjunction , such as \" VB \\SPEC NN \" for \" subtract \" in \" To find estimated net farm income , subtract . . . \"\n", 3=>"-1#5#2#exact -1#6#3#syn -1#7#4#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact -1#47#44#exact -1#48#45#exact -1#49#46#exact -1#50#47#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact -1#57#55#exact -1#58#56#exact -1#59#57#exact -1#60#58#exact -1#61#59#exact -1#62#60#exact -1#63#61#exact -1#64#62#exact -1#65#63#exact -1#66#64#exact -1#67#65#exact -1#68#66#exact -1#69#67#exact -1#70#68#exact -1#71#69,70#para -1#72#71#exact -1#73#72#exact -1#74#73#exact -1#75#74#exact -1#76#75#exact -1#77#76#exact -1#78#77#exact -1#79#78#exact -1#80#79#exact -1#40#80#exact \n"}nil_first --> [0, 1, 5, 6, 48]
nil_second --> [0, 1, 2, 3, 4, 8]
--------------------------
{1=>"The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .\n", 2=>"The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5,6#3,4#para -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#15#13#exact -1#16#14#exact -1#3,4#15#para -1#17#16#exact -1#33#17#exact -1#23#18#exact -1#22#19#exact -1#24#20#exact -1#25#21#exact -1#26,27#22,23#para -1#28#24#exact -1#29#25#exact -1#30#27#exact -1#31#28#exact -1#21#30#para -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact \n"}nil_first --> [12, 26, 29]
nil_second --> [14, 18, 19, 20, 32]
--------------------------
{1=>"We also analyzed the errors in Brown questions and QuestionBank , and again found that many errors were due to the fact that the WSJ tagger was trained on a corpus consisting mainly of declarative sentences .\n", 2=>"We also analyzed the errors in Brown questions and QuestionBank , and again found that the WSJ tagger seems to make many errors due to the fact that the tagger was trained on a corpus mainly consisting of declarative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#21#15#exact -1#22#16#exact -1#23,24,25,26,27#17,18,19,20,21,22#para -1#28#23#exact -1#16#24#exact -1#17#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#36#31#exact -1#35#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact \n"}nil_first --> []
nil_second --> [15, 18, 19, 20, 29]
--------------------------
{1=>"After the adaptation , although some of the errors such as the special use of wh-words , i.e. , \" WDT \\SPEC WP \" , were corrected , other kinds or errors related to the global change in sentence structure still remained .\n", 2=>"After the adaptation , while some of the errors such as special usage of wh-words , i.e. , \" WDT \\SPEC WP \" , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#36#11#exact -1#11#12#exact -1#12,13#13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#38,39#28#para -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#37#34,35#para -1#40#38#exact -1#41#39#stem -1#42#40#exact -1#43#41#exact -1#44#42#exact \n"}nil_first --> [4, 36, 37]
nil_second --> [4, 27, 28, 29, 30]
--------------------------
{1=>"To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .\n", 2=>"In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .\n", 3=>"-1#2#0#lc -1#7#2#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15,16#11,12#para -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact \n"}nil_first --> [1, 3]
nil_second --> [0, 1, 3, 4, 5, 6]
--------------------------
{1=>"Table \\REF gives the parsing accuracy of MST ( first order ) , MST ( second order ) , Malt , and the Enju parser for WSJ , Brown overall , Brown imperatives , and Brown questions .\n", 2=>"Table \\REF shows the parsing accuracies of MST( first order ) , MST( second order ) , Malt , and Enju parser for WSJ , Brown overall , Brown imperatives and Brown questions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact \n"}nil_first --> [2, 7, 8, 13, 14, 22, 33]
nil_second --> [2, 7, 12]
--------------------------
{1=>"Figure \\REF plots the parsing accuracy against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .\n", 2=>"Figure \\REF shows the parsing accuracies against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \\REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \\REF .\n", 2=>"Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \\REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#29#4#exact -1#4#5#exact -1#51#6#exact -1#34#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#stem -1#28#31#exact -1#30#34#exact -1#31#35#exact -1#33#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#45#50#exact -1#46#51#exact -1#47#52#exact -1#49#54#para -1#50#55#exact -1#52#57#exact -1#53#58#exact -1#54#59#exact -1#55#60#exact -1#56#61#exact -1#57#62#exact -1#58#63#exact -1#59#64#exact -1#60#65#exact -1#61#66#exact -1#62#67#exact -1#63#68#exact \n"}nil_first --> [32, 33, 36, 47, 48, 49, 53, 56]
nil_second --> [32, 44, 48]
--------------------------
{1=>"After adaptation ( see \" Adapted \" column in Table \\REF ) , the parser achieved two to four percent higher accuracy for each of the Brown domains compared to the WSJ parser .\n", 2=>"When we adapted the parser model ( see fifth column in Table \\REF ) , the parser could give two to four points higher accuracies for each of the Brown domains than the WSJ parser .\n", 3=>"-1#6#2#exact -1#7#3#exact -1#2#5#lc -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#23#20#exact -1#24#21#stem -1#25,26,27,28#22,23,24#para -1#3#25#exact -1#29#26#exact -1#30#27#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact \n"}nil_first --> [0, 1, 4, 6, 15, 19, 28, 29]
nil_second --> [0, 1, 4, 5, 8, 17, 18, 22, 31]
--------------------------
{1=>"For QuestionBank , 25 to 35 percent improvement in accuracy was observed .\n", 2=>"For the QuestionBank , 25 to 35 points accuracy improvements were observed .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#9#7#stem -1#8#9#exact -1#10,11#10,11#para -1#12#12#exact \n"}nil_first --> [6, 8]
nil_second --> [1, 7]
--------------------------
{1=>"Figure \\REF shows that the improvement is proportional to the size of the training data and that this tendency does not seem to converge .\n", 2=>"Figure \\REF shows that , the improvements increased according to the size of the training data , and the tendencies would not seem to converge .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#stem -1#9,10,11,12,13#8,9,10,11#para -1#18#12#exact -1#14#13#exact -1#15#14#exact -1#17#15#exact -1#19#18#stem -1#21,22,23#19,20,21,22#para -1#24#23#exact -1#25#24#exact \n"}nil_first --> [6, 7, 16, 17]
nil_second --> [4, 7, 8, 16, 20]
--------------------------
{1=>"This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .\n", 2=>"This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#19#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#17#13#exact -1#11#14#exact -1#12#15#para -1#14#16#exact -1#13#17#exact -1#18#21,22,23#para -1#20#24,25#para -1#21#26#exact -1#22#27#exact \n"}nil_first --> [7, 18, 19, 20]
nil_second --> [15, 16]
--------------------------
{1=>"In Figure \\REF , the parser accuracy for QuestionBank , for which we could use much more training data than for Brown questions , approaches or even exceeds that of the WSJ parser for WSJ .\n", 2=>"In Figure \\REF , when we focus on the QuestionBank where we could use much more training data than Brown questions , the parser accuracies were approaching the accuracies of WSJ parser for WSJ or exceeded the accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#22#4#exact -1#23#5#exact -1#37#6#exact -1#32#7#exact -1#9#8#exact -1#21#9#exact -1#10,11#10,11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#21#exact -1#20#22#exact -1#34#24,25#para -1#35#27#stem -1#29#29#exact -1#8#30#exact -1#30#31#exact -1#31#32#exact -1#33#34#exact -1#38#35#exact \n"}nil_first --> [20, 23, 26, 28, 33]
nil_second --> [4, 5, 6, 7, 24, 25, 26, 27, 28, 36]
--------------------------
{1=>"However , as there is no more training data for Brown imperatives and questions , we need to either prepare more training data or explore approaches that enable the parsers to be adapted with small amounts of training data .\n", 2=>"However , we have no more training data for Brown imperatives and questions . We should prepare more training data or explore approaches to enable us to sufficiently adapt parsers with small training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#2#15#exact -1#15#16,17#para -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23,24#26,27#para -1#29#29#exact -1#26#30#exact -1#28#31,32#para -1#30#33#exact -1#31#34#exact -1#32#36,37#para -1#33#38#exact -1#34#39#exact \n"}nil_first --> [2, 3, 4, 14, 18, 28, 35]
nil_second --> [3, 13, 14, 25, 27]
--------------------------
{1=>"To capture an overview of the adaptation effects , we observed the error reduction in the Malt parser .\n", 2=>"In order to capture the outline of the adaptation effects , we observed error reduction for the Malt parser .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#4#11#exact -1#13#12#exact -1#14#13#exact -1#0#14#lc -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> [2, 3]
nil_second --> [1, 5, 15]
--------------------------
{1=>"Tables \\REF and \\REF give the recall errors on labeled dependencies , which were observed more than ten times for 100 analysis sentences in each domain .\n", 2=>"Table \\REF and \\REF show the recall errors on labeled dependencies which were observed more than ten times for 100 analysis sentences of each domain .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22,23#23,24#para -1#24#25#exact -1#25#26#exact \n"}nil_first --> [4, 11]
nil_second --> [4]
--------------------------
{1=>"For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags , respectively .\n", 2=>"For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#36#exact \n"}nil_first --> [34, 35]
nil_second --> []
--------------------------
{1=>"Since ROOT dependencies , that is , heads of sentences , are critical to the construction of sentences , we focus mainly on this type of error .\n", 2=>"Since ROOT dependencies , that is , heads of sentences would be critical to construction of sentences , we mainly focus on that type of errors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#17#10#exact -1#10,11#11#para -1#12#12#exact -1#13#13#exact -1#14,15#14,15,16#para -1#16#17#exact -1#18#19#exact -1#20#20#exact -1#19#21#exact -1#21#22#exact -1#22,23#23,24#para -1#24#25#exact -1#25#26#stem -1#26#27#exact \n"}nil_first --> [18]
nil_second --> []
--------------------------
{1=>"For Brown imperatives and questions , the reduction in ROOT dependencies was prominent .\n", 2=>"For Brown imperatives and questions , we could observe that the reduction of ROOT dependency was prominent .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10#6#exact -1#11#7#exact -1#13#9#exact -1#14#10#stem -1#15#11#exact -1#16#12#exact -1#17#13#exact \n"}nil_first --> [8]
nil_second --> [6, 7, 8, 9, 12]
--------------------------
{1=>"On investigation , we found that the WSJ parser often made mistakes in parsing sentences which began or ended with the name of the person being addressed .\n", 2=>"When we focus on this type of errors , we could find that the WSJ parser could often make mistakes in parsing sentences which began or ended with the names of persons who were talk to .\n", 3=>"-1#3#0#lc -1#8#2#exact -1#9#3#exact -1#11#4#syn -1#12#5#exact -1#13#6#exact -1#14#7#exact -1#15#8#exact -1#17#9#exact -1#18,19#10,11#para -1#20#12#exact -1#21#13#exact -1#22#14#exact -1#23#15#exact -1#24#16#exact -1#25#17#exact -1#26#18#exact -1#27#19#exact -1#28#20#exact -1#29,30#21,22#para -1#31#23,24#para -1#36#27#exact \n"}nil_first --> [1, 25, 26]
nil_second --> [0, 1, 2, 4, 5, 6, 7, 10, 16, 32, 33, 34, 35]
--------------------------
{1=>"For example , in Brown imperatives , for the sentence \" See for yourself , Miss Zion . \" , the WSJ parser mistook the name \" Zion \" to be ROOT , and the main verb \" See \" to be a modifier of the name .\n", 2=>"For example in Brown imperatives , for the sentence \" See for yourself , Miss Zion . \" , the WSJ parser regarded the person name \" Zion \" as ROOT , and the main verb \" See \" as modifiers of the name .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#13#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#18#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#31#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#23#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#30#31#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#40#43#stem -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#44#47#exact \n"}nil_first --> [23, 29, 30, 32, 40, 41, 42]
nil_second --> [22, 24, 29, 39]
--------------------------
{1=>"The adapted parser correctly assigned ROOT to the main verb .\n", 2=>"The adapted parser could then correctly give ROOT to the main verb .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#3#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact \n"}nil_first --> [4]
nil_second --> [3, 4, 6]
--------------------------
{1=>"We also found that the WSJ parser often made mistakes in parsing sentences containing quotation , exclamation , or question marks , such as \" \" Hang on \" !! \" \" or \" Why did you kill it \" ? ? \" or \" \" \" \" .\n", 2=>"We could also often find that the WSJ parser could often make mistakes in parsing sentences containing quotation , exclamation , and question marks , such as \" \" Hang on \" !! \" \" or \" Why did you kill it \" ? ? \" or \" \" \" \" .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#4#2#syn -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#10#7#exact -1#11,12#8,9#para -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#35#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#46#32#exact -1#47#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#36#44#exact -1#48#45#exact -1#49#46#exact -1#50#47#exact -1#51#48#exact \n"}nil_first --> [43]
nil_second --> [1, 3, 9, 21]
--------------------------
{1=>"For such sentences , the WSJ parser regarded the first \" ! \" or \" ? \" as ROOT , and \" Hang \" or \" did \" as the modifier of the punctuation .\n", 2=>"For such sentences , the WSJ parser regarded the first \" ! \" or \" ? \" as ROOT , and \" Hang \" or \" did \" as the modifier of the marks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#34#34#exact \n"}nil_first --> [33]
nil_second --> [33]
--------------------------
{1=>"A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .\n", 2=>"We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .\n", 3=>"-1#3#4#exact -1#4#5#para -1#5#6#exact -1#6#7#stem -1#7#8#para -1#21#9#exact -1#2#10#exact -1#23#11#exact -1#11#12#exact -1#12#13#exact -1#31#14#syn -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#34#19#exact -1#25#20#exact -1#22#23#exact -1#27#24#exact -1#24#25#exact -1#14#26#exact \n"}nil_first --> [0, 1, 2, 3, 21, 22]
nil_second --> [0, 1, 8, 9, 10, 13, 15, 20, 26, 28, 29, 30, 32, 33, 35]
--------------------------
{1=>"The adapted parser could handle this dubious construction and assigned ROOT to the main verbs as the corpus required .\n", 2=>"However , the adapted parser could take in such doubtful construction and gave ROOT to the main verbs as the corpus required .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#9#6#syn -1#10#7#exact -1#11#8#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact \n"}nil_first --> [4, 5, 9]
nil_second --> [0, 1, 6, 7, 8, 12]
--------------------------
{1=>"On the other hand , we also observed some unsolved errors , of which we discuss two .\n", 2=>"On the other hand , we also observed some still unsolved errors . We would show the two kinds of major errors among them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#19#12#exact -1#22,23#13#para -1#13#14#lc -1#17#16#exact -1#12#17#exact \n"}nil_first --> [11, 15]
nil_second --> [9, 14, 15, 16, 18, 20, 21, 24]
--------------------------
{1=>"First , Brown imperatives and questions , include many colloquial sentences , which have rather flexible constructions , especially imperatives , such as \" Lift , don't shove lift! \" , \" Come out , come out in the meadow! \" , etc.\n", 2=>"First , Brown imperatives and questions , include many conversation sentences , and therefore rather flexible constructions could be observed especially for imperatives , such as \" Lift , don't shove lift! \" , \" Come out , come out in the meadow! \" , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#14#13,14#para -1#15#15#exact -1#16#16#exact -1#23#17#exact -1#20#18#exact -1#22#19#exact -1#28#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#33#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#37#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#44#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#45#42#exact \n"}nil_first --> [9, 12, 41]
nil_second --> [9, 12, 13, 17, 18, 19, 21]
--------------------------
{1=>"The parsing models based on the plausibility of constructions were not able to capture such sentences .\n", 2=>"The parsing models based on the plausibility of constructions could hardly capture such sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [9, 10, 11, 12]
nil_second --> [9, 10]
--------------------------
{1=>"Second , having different sentence constructions within a single sentence , such as , where a to-infinitive phrase or subordinate clause precedes an imperative or question , often confused the parser .\n", 2=>"Second , when the different constructions of sentences were in one sentence , such as , the case where to-infinitive phrases or subordinate clauses precede imperatives and questions , the parser would often be confused .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#3#exact -1#11#4#exact -1#5#5#exact -1#10#8#syn -1#7#9#stem -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#18#14#exact -1#19#16#exact -1#20#17#stem -1#21#18#exact -1#22#19#exact -1#23#20#stem -1#24#21#stem -1#25#23#stem -1#27#25#stem -1#28#26#exact -1#32#27#exact -1#34#28#exact -1#3#29#exact -1#30#30#exact -1#35#31#exact \n"}nil_first --> [2, 6, 7, 15, 22, 24]
nil_second --> [2, 6, 8, 9, 16, 17, 26, 29, 31, 33]
--------------------------
{1=>"For example , for the imperative sentence , \" To find the estimated net farm income , subtract the estimated annual farming expenditure . . . \" , both the WSJ and adapted parsers regarded \" find \" as ROOT , because the parsers regarded the words following \" find \" as a that-clause complementing \" find \" , as in \" To find [ ( that ) the estimated net farm income , subtract the estimated annual farming . . .] \" .\n", 2=>"For example , for the imperative sentence \" To find estimated net farm income , subtract estimated annual farming expenditures . . . \" , both of the WSJ and adapted parsers regarded \" find \" as ROOT , because the parsers regarded the words following \" find \" as a that-clause complement for the \" find \" , like \" To find [ ( that ) estimated net farm income , subtract estimated annual farming . . .] \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#27#11#exact -1#67#12#exact -1#68#13#exact -1#69#14#exact -1#70#15#exact -1#71#16#exact -1#72#17#exact -1#40#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#stem -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#43#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#54#42#exact -1#41#43#exact -1#42#44#exact -1#44#45,46#para -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#51#53#exact -1#52#54#stem -1#55#55#exact -1#56#56#exact -1#57#57#exact -1#58#58#exact -1#60#61#exact -1#61#62#exact -1#62#63#exact -1#63#64#exact -1#64#65#exact -1#65#66#exact -1#66#67#exact -1#10#68,69#para -1#11#70#exact -1#12#71#exact -1#13#72#exact -1#15#74#exact -1#73#75,76#para -1#74#77#exact -1#75#78#exact -1#76#79#exact -1#77#80#exact -1#78#81#exact -1#79#82#exact -1#80#83#exact \n"}nil_first --> [59, 60, 73]
nil_second --> [26, 53, 59]
--------------------------
{1=>"This type of error cannot be solved merely by increasing the training data .\n", 2=>"This type of errors would hardly be solved only by increasing the training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> [4]
nil_second --> [4, 5]
--------------------------
{1=>"Imperative or question sentences typically consist not only of a pure imperative or question clause , but also of other constructions of phrases or clauses .\n", 2=>"Imperatives or questions sentences consist not only of pure imperative or question clause , but also of other constructions of phrases or clauses .\n", 3=>"-1#9#0#lc -1#10#1#exact -1#11#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9,10#para -1#0#11#stem -1#1#12#exact -1#2#13#stem -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"These complex sentences were parsed without being partitioned into separate constructions , and as a result the parser sometimes became confused .\n", 2=>"The parser would parse such complex sentences without partition into each construction , and therefore it would sometimes be confused .\n", 3=>"-1#4,5#0,1#para -1#6#2#exact -1#7#5#exact -1#18#6#stem -1#8#7#stem -1#9#8#exact -1#11#10#stem -1#12#11#exact -1#13#12#exact -1#0#16#lc -1#1#17#exact -1#17#18#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [3, 4, 9, 13, 14, 15, 19]
nil_second --> [2, 3, 10, 14, 15, 16]
--------------------------
{1=>"Both the Brown questions and QuestionBank are in the question domain .\n", 2=>"Both of Brown questions and QuestionBank are in the domain of question .\n", 3=>"-1#0#0#exact -1#8#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8,9#para -1#9#10#exact -1#12#11#exact \n"}nil_first --> []
nil_second --> [1, 10]
--------------------------
{1=>"In this section , we examine whether a parser adapted to one domain could be ported to another domain .\n", 2=>"In this section , we examined whether the parser adapted to one domain would be portable to the other domain .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#para -1#14#14#exact -1#16#16#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [7, 15, 17]
nil_second --> [7, 15, 17, 18]
--------------------------
{1=>"QuestionBank does not provide function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .\n", 2=>"QuestionBankdoes not give function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .\n", 3=>"-1#1#2#exact -1#2#3#para -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .\n", 2=>"Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .\n", 3=>"-1#1#3#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [0, 1, 2, 4]
nil_second --> [0, 2, 15]
--------------------------
{1=>"However , we would expect that sentence constructions are basically common and portable between two domains , which would provide a correct boundary for phrases and therefore , the correct dependencies in phrases would be introduced by the adaptation .\n", 2=>"However , we would be able to expect that sentence constructions would be basically common and portable between two domains , which would contribute to give correct boundary for phrases and therefore the correct dependencies in phrases would be introduced by the adaptation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4#para -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11,12#8#para -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#25#19#para -1#26#21#exact -1#27#22#exact -1#28#23#exact -1#29#24#exact -1#30#25#exact -1#31#26#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#35#31#exact -1#36#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact -1#41#37#exact -1#42#38#exact -1#43#39#exact \n"}nil_first --> [20, 27]
nil_second --> [23, 24]
--------------------------
{1=>"Table \\REF gives the parsing or tagging accuracy of each parser and the POS tagger for Brown questions and QuestionBank .\n", 2=>"Table \\REF shows the parsing or tagging accuracies of each parser and the POS tagger for Brown questions and QuestionBank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"These results differ from those in Table \\REF in that the parsers and the tagger have been adapted to another question domain .\n", 2=>"the difference from Table \\REF was that the parsers and the tagger were adapted to another question domain .\n", 3=>"-1#1#2#stem -1#2#3#exact -1#3#6#exact -1#4#7#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15,16#para -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact \n"}nil_first --> [0, 1, 4, 5, 8]
nil_second --> [0, 5]
--------------------------
{1=>"The table shows that the parsers adapted to the Brown questions improved their parsing accuracy with QuestionBank , whereas the parsers adapted to QuestionBank decreased in accuracy .\n", 2=>"The table shows that the parsers adapted to Brown questions improved the parsing accuracies for QuestionBank , while the parsers adapted to QuestionBank decreased .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#stem -1#15#16#exact -1#16#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#27#exact \n"}nil_first --> [12, 15, 18, 25, 26]
nil_second --> [14, 17]
--------------------------
{1=>"Using Brown questions , many wh-questions were learnt , which is what QuestionBank mainly contains . On the other hand , despite yes-no questions constituting more than half the Brown corpus , these were not learnt using QuestionBank for training .\n", 2=>"With Brown questions , we could learn wh-questions which QuestionBank mainly contain , while with QuestionBank , we could not we could not learn yes-no questions which more than half of Brown corpus contain .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#7#5#exact -1#12#8#exact -1#26#9#exact -1#9#12#exact -1#10#13#exact -1#11#14#stem -1#34#15#exact -1#16#20#exact -1#24#22#exact -1#25#23#exact -1#27,28,29,30#25,26,27,28#para -1#31#29#exact -1#32#30#exact -1#19#33,34#para -1#6#35#syn -1#15#37#exact \n"}nil_first --> [0, 4, 6, 7, 10, 11, 16, 17, 18, 19, 21, 24, 31, 32, 36, 38, 39, 40]
nil_second --> [0, 4, 5, 8, 13, 14, 17, 18, 20, 21, 22, 23, 33]
--------------------------
{1=>"A question domain contains various types of questions with various sentence constructions .\n", 2=>"A question domain contains various types of questions and gives various sentence constructions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#8,9#para -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [8, 9]
--------------------------
{1=>"In order to parse questions correctly , we need to capture each of these correctly .\n", 2=>"In order to parse questions correctly , we should capture each of them correctly .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [13]
nil_second --> [12]
--------------------------
{1=>"This type of problem was not so obvious when we were working mainly with declarative sentences .\n", 2=>"This type of problem would not be noticed so much when we were working mainly on declarative sentences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#4#syn -1#5#5#exact -1#8#6#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact \n"}nil_first --> [7, 13]
nil_second --> [4, 7, 9, 15]
--------------------------
{1=>"Through experiments with various parsers we observed that simple supervised adaptation methods are insufficient to achieve parsing accuracy comparable with that of declarative sentences .\n", 2=>"Through the experiments on various parsers we observed that simple supervised adaptation methods are insufficient to arrive at theparsing accuracy comparable to that of declarative sentences .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16,17#15#para -1#19#17#exact -1#20#18#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> [2, 16, 19]
nil_second --> [1, 3, 18, 21]
--------------------------
{1=>"This observation holds both for POS tagging and syntactic parsing , and indicates that the parsers need to be fundamentally improved , such as re-constructing feature designs or changing parsing models .\n", 2=>"This observation holds both for POS tagging and syntactic parsing , and itindicates that we need fundamental improvement of parsers , such as re-constructing feature designs or changing parsing models .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#19#15#exact -1#15#16#exact -1#16#19#stem -1#17,18#20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact \n"}nil_first --> [12, 14, 17, 18]
nil_second --> [12, 14]
--------------------------
{1=>"Following on from this study , future work includes investigating parsing frameworks that are robust for sentences with different sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives and questions , among others .\n", 2=>"Following the present work , future work should include investigating parsing frameworks that are robust for sentences with various sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives , questions , and more .\n", 3=>"-1#0#0#exact -1#3#4#syn -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#33#18#exact -1#34#19#exact -1#35#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#18#33#para -1#19#34#exact -1#20#35#exact -1#36#36#exact -1#37#37#exact -1#41#38#exact -1#39#39#exact -1#40#40#exact -1#43#43#exact \n"}nil_first --> [1, 2, 3, 41, 42]
nil_second --> [1, 2, 7, 38, 42]
--------------------------
{1=>"While word segmentation is necessary for processing the Chinese and Japanese languages , its effects on Statistical Machine Translation ( SMT ) have not yet been thoroughly discussed for such languages .\n", 2=>"While word segmentation is a necessary step to process languages like Chinese and Japanese , its effects on Statistical Machine Translation ( SMT ) have not been discussed intensively in such languages .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#8#6,7#para -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#9#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24,25,26#22,23,24,25#para -1#27#27#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact \n"}nil_first --> [5, 26, 28]
nil_second --> [4, 6, 7, 10, 28, 29]
--------------------------
{1=>"In this paper , we investigate the effects of word segmentation methods on SMT , by comparing the evaluation results of the translation outputs , while varying word segmentation methods .\n", 2=>"In this paper , we investigate the effects of word segmentation methods on SMT , by comparing evaluation results of translation outputs while varying word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17,18#para -1#18#19#exact -1#19#20#exact -1#20#21,22#para -1#21#23#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact \n"}nil_first --> [24]
nil_second --> []
--------------------------
{1=>"The experimental results confirmed that supervised morphological analyzers were competitive with , and performed considerably better than an unsupervised analyzer and a heuristic segmentation method .\n", 2=>"The experiments revealed that supervised morphological analyzers were competitive , and considerably better than an unsupervised analyzer and a heuristic segmentation method .\n", 3=>"-1#0#0#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact \n"}nil_first --> [1, 2, 3, 10, 13]
nil_second --> [1, 2]
--------------------------
{1=>"However , a character-based segmentation achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on the corpus sizes and domains .\n", 2=>"However , a character-based segmentation has achieved 10 .27 positive and 1 .95 negative differences in word-based and character-based BLEU , depending on corpus sizes and domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [22]
nil_second --> [5]
--------------------------
{1=>"In conclusion , we discuss the problem of the comparability of evaluation metrics , and consider ways of improving word segmentation more than popular supervised morphological analyzers .\n", 2=>"For this result we discuss the problem of the comparability of evaluation metrics and the possibility of better word segmentation than popular supervised morphological analyzers .\n", 3=>"-1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#16#17#exact -1#17#18#syn -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [0, 1, 2, 13, 15, 16, 21]
nil_second --> [0, 1, 2, 14, 15]
--------------------------
{1=>"Several languages , including Chinese and Japanese , do not require spaces between words , in their written forms .\n", 2=>"Several natural languages like Chinese and Japanese do not have to put spaces between words in their written forms .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [2, 3, 7, 10, 14]
nil_second --> [1, 3, 9, 10, 11]
--------------------------
{1=>"Since word segmentation is a fundamental process , and is therefore indispensable , it is important that we explore how word segmentation affects Natural Language Processing applications .\n", 2=>"Since the process is fundamental and indispensable , we need to explore how word segmentation affects Natural Language Processing applications .\n", 3=>"-1#0#0#exact -1#13#1#exact -1#14#2#exact -1#3#3#exact -1#4#5#exact -1#2#6#exact -1#7#7#exact -1#5#8#exact -1#15#9#para -1#6#11#exact -1#9#14,15#para -1#8#17#exact -1#11#18#exact -1#12#19#exact -1#16#23#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact -1#20#27#exact \n"}nil_first --> [4, 10, 12, 13, 16, 20, 21, 22]
nil_second --> [1, 10]
--------------------------
{1=>"Thus , we investigate how Japanese word segmentation affects SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .\n", 2=>"Therefore , we investigate how Japanese word segmentation affects on SMT between English and Japanese , by comparing various word segmentation methods and evaluation metrics .\n", 3=>"-1#0#0#syn -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"In addition , we examine an unsupervised morphological analyzer , and its results .\n", 2=>"We also examine an unsupervised morphological analyzer and its results .\n", 3=>"-1#1#0,1#para -1#0#3#lc -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact \n"}nil_first --> [2, 9]
nil_second --> []
--------------------------
{1=>"We focus on the meta-evaluation of the current evaluation metrics and determine whether the metrics are consistent or not , when we vary word segmentation methods .\n", 2=>"In addition , we focus on the meta-evaluation of the current evaluation metrics and find whether the metrics are consistent or not , when we vary word segmentation methods .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#15#11,12#para -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 14]
--------------------------
{1=>"Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on the outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .\n", 2=>"Al-Haj and Lavie ( 2012 ) compared 12 heuristic word segmentation methods based on outputs of a standard Arabic POS tagger , and found the optimum combination in terms of BLEU on English-Arabic SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#24#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [25]
nil_second --> []
--------------------------
{1=>"They acquired a 2 .3 score improvement in comparing the worst to best combinations .\n", 2=>"They acquired the 2 .3 score improvement from the worst to the best combinations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#2#9#exact -1#9#10#exact -1#10#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [2, 7, 8]
nil_second --> [7, 8, 11]
--------------------------
{1=>"( 2010 ) suggested a new short unit word segmentation standard in Chinese , which defines a more frequent string subset as a word .\n", 2=>"( 2010 ) suggested a new short unit word segmentation standard in Chinese which defines a more frequent string subset as a word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"For instance , one word “全球化 globalization” was separated into two words “全球 global” and “化 -lization” .\n", 2=>"For instance , They separated one word \" 全球化 globalization \" into two words \" 全球 global \" and \" 化 -lization \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#3#exact -1#6#4#exact -1#4#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#18#14#exact -1#23#17#exact \n"}nil_first --> [5, 6, 7, 12, 13, 15, 16]
nil_second --> [3, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 21, 22]
--------------------------
{1=>"However , it has not yet been discussed whether BLEU is a good metric for such an evaluation of word segmentation .\n", 2=>"Though , they have not discussed about BLEU is a good metric for such an evaluation of word segmentation .\n", 3=>"-1#1#1#exact -1#8#2,3#para -1#4#4#exact -1#3#5,6#para -1#5#7#exact -1#7#9#exact -1#9,10#10,11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [0, 8]
nil_second --> [0, 2, 6]
--------------------------
{1=>"In addition , comparing morphological analyzers is necessary , because different analyzers produce different outputs to SMT .\n", 2=>"In addition , comparison of morphological analyzers are necessary because different analyzers produce different outputs to SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7,8#6,7#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"We therefore conduct several translation tasks between English and Japanese .\n", 2=>"Therefore , we conduct several translation tasks between English and Japanese .\n", 3=>"-1#2#0#lc -1#0#1#lc -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affects the SMT evaluation metrics , depending on the corpus sizes and domains .\n", 2=>"- How a variety of word segmentation methods ( supervised morphological analysis , unsupervised segmentation , and heuristic methods ) affect SMT evaluation metrics , depending on corpus sizes and domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#stem -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact \n"}nil_first --> [21, 28]
nil_second --> []
--------------------------
{1=>"- Whether or not SMT evaluation metrics provide a consistent measure , while varying word segmentation methods .\n", 2=>"- Whether or not SMT evaluation metrics provide a consistent measure while varying word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"We set up word segmentation methods , corpora , and evaluation metrics , as the three parameters for our experiments , in order to observe the effects of Japanese word segmentation on SMT .\n", 2=>"We setup word segmentation methods , corpora , and evaluation metrics as three parameters of our experiments to see the effects of Japanese word segmentation on SMT .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#19#14#exact -1#12#15#exact -1#13#16#exact -1#15#18#exact -1#16#19#exact -1#20,21#22,23#para -1#18#24#para -1#14#27#exact -1#22#28#exact -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact \n"}nil_first --> [1, 2, 12, 17, 20, 21, 25, 26]
nil_second --> [1, 17]
--------------------------
{1=>"As shown in Table 1 , the following word segmentation methods output delimiters ( “|” represents a delimiter ) for a given input character sequence .\n", 2=>"As shown in Table 1 , the following word segmentation methods output delimiters ( \" | \" represents a delimiter ) for a given input character sequence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact \n"}nil_first --> [14]
nil_second --> [14, 15, 16]
--------------------------
{1=>"It is ; however , unclear as to which analyzer works better for the SMT task .\n", 2=>"It is , however , not clear which analyzer works better for the SMT task than the other analyzers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5,6#5#para -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#19#16#exact \n"}nil_first --> [2, 6]
nil_second --> [2, 15, 16, 17, 18]
--------------------------
{1=>"- JUMAN also regards word segmentation as a sequence labeling problem , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .\n", 2=>"- JUMAN also regards word segmentation as a sequence labeling , but it decides the minimum cost paths without machine learning , from segmentation and association costs in human annotated lexicons and automatically generated Web lexicons .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98% for news texts .\n", 2=>"The accuracy of supervised morphological analyzers KyTea , MeCab , and JUMAN is reported to be over 98\\% for news text .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#stem -1#21#21#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"On the other hand , the unsupervised method , latticelm , achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news texts , while the method does not have any answers for word definitions .\n", 2=>"On the other hand , the unsupervised method latticelm achieved 66 .6% accuracy ( Mochihashi et al. , 2009 ) for human annotated news text , while the method does not have any answers of word definitions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#17#8#exact -1#8#9#exact -1#25#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#stem -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29,30,31#31,32,33,34#para -1#33#35#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact \n"}nil_first --> [19, 27, 36]
nil_second --> [32, 34]
--------------------------
{1=>"Therefore , it is not possible to compare its result with the supervised results , even though it is fair to compare it from the SMT contribution point-of-view .\n", 2=>"Therefore , it is not possible to compare such a result with the supervised results . Even though , it is fair to compare it with SMT contribution point of view .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5,6#2,3,4,5#para -1#22#6#exact -1#23#7#exact -1#24#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#18#14#exact -1#16#15#lc -1#17#16#exact -1#20,21#17,18,19,20#para -1#7#21#exact -1#19#22#exact -1#26#25#exact -1#27#26#exact -1#15#28#exact \n"}nil_first --> [23, 24, 27]
nil_second --> [8, 9, 25, 28, 29, 30, 31]
--------------------------
{1=>"Furthermore , their policies concerning word segmentation definitions vary significantly .\n", 2=>"Furthermore , their policies about word segmentation definitions are very much different .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8#para -1#10#9#para -1#12#10#exact \n"}nil_first --> []
nil_second --> [8, 9]
--------------------------
{1=>"While MeCab can change its definitions by external lexicons , and JUMAN has its own internal standard , KyTea is based on the short unit standard of the Balanced Corpus of Contemporary Written Japanese , which is considered to have one of the shortest definitions of Japanese words .\n", 2=>"While MeCab can change its definitions by external lexicons and JUMAN has its own internal standard , KyTea is based on the short unit standard of Balanced Corpus of Contemporary Written Japanese , which is considered one of the shortest definitions of Japanese words .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#16#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#32#17#exact -1#17#18#exact -1#18,19,20#19,20,21,22#para -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#21#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#33,34,35#35,36,37,38#para -1#36#40#exact -1#37#41#exact -1#38#42#exact -1#39#43#exact -1#40#44#exact -1#41#45#exact -1#42#46#exact -1#43#47#exact -1#44#48#exact \n"}nil_first --> [34, 39]
nil_second --> []
--------------------------
{1=>"For example , if we are given a string , “見れば( if someone sees )” , MeCab separates it into two words , “見れ | ば” and JUMAN retains the same string , but KyTea outputs it as three words , “見 | れ | ば” where every character is a word .\n", 2=>"For example , if we are given a string \" 見れば( if someone see ) \" , MeCab separates it into two words \" 見れ | ば \" and JUMAN keep the same string , but KyTea outputs it as three words \" 見 | れ | ば \" where every character is a word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#16#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#34#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#25#24#exact -1#28#26#exact -1#29#27#exact -1#30#28#syn -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#44#42#exact -1#45#43#exact -1#46#44#exact -1#49#46#exact -1#50#47#exact -1#51#48#exact -1#52#49#exact -1#53#50#exact -1#54#51#exact -1#55#52#exact \n"}nil_first --> [10, 14, 22, 23, 25, 32, 40, 41, 45]
nil_second --> [9, 10, 14, 15, 23, 24, 26, 27, 42, 43, 47, 48]
--------------------------
{1=>"For latticelm , since it has no supervised definition of words , it uses the expectation maximized length of words for every word , depending on the training data .\n", 2=>"In the case of latticelm , as it has no supervised definition of words , it uses the expectation maximized length of words for every word depending on training data .\n", 3=>"-1#23#0#lc -1#4#1#exact -1#5#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#24#21#exact -1#25#22#exact -1#26#24#exact -1#27#25#exact -1#1#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact \n"}nil_first --> [3, 20, 23]
nil_second --> [0, 2, 3, 6]
--------------------------
{1=>"In our experiments , we further investigate such morphological analysis accuracies and word definition problems .\n", 2=>"We also investigate such morphological analysis accuracy and word definition problems in our experiments .\n", 3=>"-1#11#0#lc -1#12#1#exact -1#13#2#exact -1#0#4#lc -1#1#5#para -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#stem -1#7#11#exact -1#8#12#exact -1#9#13#exact -1#10#14#exact -1#14#15#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"One method is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .\n", 2=>"One is segmentation by character category ( CAT ) , and the other is segmentation by characters ( CHAR ) .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"The CHAR method considers every Unicode character as a word , as proposed by Xu et al.\n", 2=>"The CHAR method considers every Unicode character as a word as proposed by Xu et al.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Another corpus we use in the experiments is a Wikipedia corpus : the Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .\n", 2=>"Another corpus we use in the experiments is a Wikipedia corpus , Japanese-English Bilingual Corpus of Wikipedia 's Kyoto Articles 2 .01 ( WIKIPEDIA ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [11, 12]
nil_second --> [11]
--------------------------
{1=>"From these corpora , we prepared three data sets , as explained below .\n", 2=>"From these corpora , we prepared three data sets as explained below .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"For REUTERS , we used all 56 ,282 sentences .\n", 2=>"In the case of REUTERS , we have used all 56 ,282 sentences .\n", 3=>"-1#4#1#exact -1#5#2#exact -1#6#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 2, 3, 7]
--------------------------
{1=>"For this data , we combined the JENAAD and REUTERS news corpora to acquire one news corpus .\n", 2=>"In this data , we have combined JENAAD and REUTERS news corpora to get one news corpus .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#syn -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [0, 6]
nil_second --> [0, 5]
--------------------------
{1=>"We used all 56 ,282 and 150 ,000 sentences , respectively .\n", 2=>"We have used all 56 ,282 and 150 ,000 sentences respectively .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [9]
nil_second --> [1]
--------------------------
{1=>"For each corpus , we divided the sentences into the first 1 ,000 for testing , the next 500 for development , and the remaining for training .\n", 2=>"For each corpus , we divide it into the first 1 ,000 , the next 500 , and the rest for test , development , and training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#8#6#exact -1#7#8#exact -1#13#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#20#13#exact -1#21#14#stem -1#22#15#exact -1#18#16#exact -1#14#17#exact -1#15#18#exact -1#23#19,20#para -1#24#21#exact -1#25#22#exact -1#19#24#syn -1#26#26#exact -1#27#27#exact \n"}nil_first --> [7, 23, 25]
nil_second --> [6, 12, 16, 17]
--------------------------
{1=>"In total , we gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively .\n", 2=>"We have gathered 2000 , 1000 , and 203 ,782 sentences for test , development , and training , respectively , in total .\n", 3=>"-1#21#0#lc -1#22#1#exact -1#13#2#exact -1#0#3#lc -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#20#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"Since the WIKIPEDIA corpus is a multi-category XML dataset , we sorted them by the DOCID in ascending order , and by the document categories : LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .\n", 2=>"Firstly , since the WIKIPEDIA corpus is a multi-category XML dataset , we have sorted them by the DOCID in the ascending order and by the document categories LTT , EPR , FML , BDS , CLT , BLD , GNM , SCL , ROD , SNT , PNM , HST , RLW , and SAT .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#21#17#exact -1#22#18#exact -1#1#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#42#40#exact -1#43#41#exact -1#44#42#exact -1#45#43#exact -1#46#44#exact -1#47#45#exact -1#48#46#exact -1#49#47#exact -1#50#48#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact \n"}nil_first --> [25]
nil_second --> [0, 13, 20]
--------------------------
{1=>"Next , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .\n", 2=>"Secondly , we parsed it by xml .etree .ElementTree .parse of Python 2 .7 .2 , and obtained 477 ,036 sentence pairs without parsing errors .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [0]
nil_second --> [0]
--------------------------
{1=>"Then , sentence pairs that include a character “|” in English or Japanese were removed , because it caused a problem with Moses .\n", 2=>"Thirdly , sentence pairs that include a character \" | \" in English or Japanese are removed because it caused a problem with Moses .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#syn -1#16#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> [0, 8, 15]
nil_second --> [0, 8, 9, 10]
--------------------------
{1=>"In order to adjust the balance of the domains , we sampled the data twice : First , we extracted the first line for every 477 lines .\n", 2=>"In order to adjust the balance of the domains , we have sampled the data twice : First we extract the first line for every 477 lines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5,6#para -1#13#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#20#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#18#exact -1#19#19#stem -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [17, 20]
nil_second --> [11]
--------------------------
{1=>"Then , we merged the remaining 476 ,012 lines , and from this extract , we extracted the first line for every 952 lines .\n", 2=>"After this , we have merged the remaining 476 ,012 lines and from this extract the first line for every 952 lines .\n", 3=>"-1#2#1#exact -1#3#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [0, 9, 14, 15, 16]
nil_second --> [0, 1, 4]
--------------------------
{1=>"Finally , we obtained 1 ,000 test , 500 development , and 475 ,512 training data .\n", 2=>"Finally , we have obtained 1 ,000 test , 500 development , and 475 ,512 training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .\n", 2=>"We have launched two word-based evaluation methods : BLEU ( Papineni et al. , 2002 ) with 4-gram setting and RIBES ( Isozaki et al. , 2010a ) , which has been reported to have a much higher correlation to human evaluation than BLEU within English-Japanese translation tasks ( Sudoh et al. , 2011 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33,34,35#33,34,35,36#para -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#54#54#exact -1#55#55#exact \n"}nil_first --> []
nil_second --> [36]
--------------------------
{1=>"Currently , the most popular way to evaluate SMT is to use word-based evaluation metrics , such as BLEU and RIBES .\n", 2=>"Currently , the most popular way to evaluate Statistical Machine Translation is to use word-based evaluation metrics such as BLEU and RIBES .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> [8, 15]
nil_second --> [8, 9, 10]
--------------------------
{1=>"If we do not have segmented reference and test data , we cannot evaluate the outputs by word-based evaluation metrics .\n", 2=>"If we do not have segmented reference and test data , we cannot evaluate outputs by word-based evaluation metrics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"For example , for English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .\n", 2=>"For example , in the case of English-Japanese translations , we must tokenize reference data to evaluate SMT outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact \n"}nil_first --> [3]
nil_second --> [3, 4, 5, 6]
--------------------------
{1=>"On the other hand , for Japanese-English translations , we must tokenize test data to evaluate the outputs .\n", 2=>"On the other hand , in the case of Japanese-English translations , we must tokenize test data to evaluate the outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact \n"}nil_first --> [5]
nil_second --> [5, 6, 7, 8]
--------------------------
{1=>"As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is therefore difficult to independently evaluate the effects of word segmentation on training data .\n", 2=>"As a result , we need to tokenize every sentence by word segmentation before evaluation , and it is hard to independently evaluate the effects of word segmentation on training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20#20,21#para -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"The best results were obtained when we used the same word segmentation as the training data .\n", 2=>"And the best results were obtained when we use the same word segmentation as the training data .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"Hence , if we keep our word-based evaluations , this problem remains .\n", 2=>"Hence , this problem remains if we keep our word-based evaluations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#2#9#exact -1#3#10#exact -1#4#11#exact -1#11#12#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"In order to manage this issue , we used one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .\n", 2=>"In order to manage such a problem , we use one character-based metric BLEU in Characters ( De-noual and Lepage , 2005 ) with 4-gram .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#5#para -1#7#6#exact -1#8#7#exact -1#9#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> [4]
nil_second --> [4, 5]
--------------------------
{1=>"As this method evaluates the character-level information , outputs are not required to be segmented , and it is free from word segmentation variations .\n", 2=>"As this method evaluates the character-level information , outputs are not required to be segmented and it is free from word segmentation variations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"We have conducted English and Japanese machine translation in both directions , following the steps below :\n", 2=>"We have conducted English and Japanese machine translation in both directions by the following steps :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#exact -1#12#13#exact -1#14#14#exact -1#15#16#exact \n"}nil_first --> [11, 15]
nil_second --> [11]
--------------------------
{1=>"1 .Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .\n", 2=>"1Apply the Head-Finalization ( Isozaki et al. , 2010b ) to English text in the case of English-Japanese translation .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"2 .Run Japanese word segmentation methods and a normalization script , which was introduced by the NTCIR-9 PATMT task .\n", 2=>"2Run Japanese word segmentation methods and a normalization script which was introduced by the NTCIR-9 PATMT task .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [0, 1, 10]
nil_second --> [0]
--------------------------
{1=>"3 .Tokenize and lowercase English texts by Moses' tokenizer and lowercase scripts .\n", 2=>"3Tokenize and lowercase English text by Moses' tokenizer and lowercase scripts .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#stem -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"4 .Create language models from target languages' training data , with SRILM 1 .5 .12 .\n", 2=>"4Create language models from target languages' training data , with SRILM 1 .5 .12 .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"5 .Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .\n", 2=>"5Create translation models with Giza++ 1 .0 .5 ( 2011-09-24 ) .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"6 .Decode source test data with Moses ( 2010-08-13 ) .\n", 2=>"6Decode source test data with Moses ( 2010-08-13 ) .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"7 .Compute evaluation scores of the outputs .\n", 2=>"7Compute evaluation scores of the outputs .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact \n"}nil_first --> [0, 1]
nil_second --> [0]
--------------------------
{1=>"This method enabled more accurate translations within English-Japanese translations than with the conventional settings .\n", 2=>"This method enabled more accurate translations within English-Japanese translations than the conventional settings .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"- Remove articles “a” , “an” , and “the”\n", 2=>"- Remove articles \" a \" , \" an \" , and \" the \"\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#4#exact -1#10#6#exact -1#11#7#exact \n"}nil_first --> [3, 5, 8]
nil_second --> [3, 4, 5, 7, 8, 9, 12, 13, 14]
--------------------------
{1=>"All evaluation metrics have been used in both directions between English and Japanese , to measure the consistency and sufficiency of the metrics in the language pair .\n", 2=>"All evaluation metrics have been used in both directions between English and Japanese , to measure consistency and sufficiency of the metrics in the language pair .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4#para -1#22#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17#16,17,18#para -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#24#23,24,25#para -1#25#26#exact -1#26#27#exact \n"}nil_first --> [5]
nil_second --> [23]
--------------------------
{1=>"In this case , the evaluation scores created by BLEU and RIBES are not comparative , due to the differences in the Japanese word definitions among the outputs of word segmentation methods .\n", 2=>"In this case , the evaluation scores created by BLEU and RIBES are not comparative due to the differences of Japanese word definitions between the outputs of word segmentation methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#24#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25,26#para -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}nil_first --> [15, 20]
nil_second --> [19]
--------------------------
{1=>"Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost the same , while small changes have been introduced , due to statistical errors and the differences in the methods in how to treat space characters .\n", 2=>"Furthermore , the CHAR scores in BLEU and BLEU in Characters should be regarded as almost same while small changes have been introduced due to statistical errors and the differences in the methods how to treat space characters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#28#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#29,30,31#31,32,33,34#para -1#32#35#exact -1#33,34#36,37,38#para -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#42#exact \n"}nil_first --> [18, 25]
nil_second --> []
--------------------------
{1=>"We found that the three supervised morphological analyzers : KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .\n", 2=>"We found that the three supervised morphological analyzers KyTea , MeCab , and JUMAN were much higher than latticelm and CAT , and were competitive .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"For instance , on REUTERS in Table 2 , BLEU scores ranged from 27 .88 to 29 .53 , while for latticelm , the score was 15 .28 and for CAT , the score was 22 .10 .\n", 2=>"For instance , on REUTERS in Table 2 , BLEU scores were ranged from 27 .88 to 29 .53 , while latticelm was 15 .28 and CAT was 22 .10 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#21#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#30#exact -1#27#34#exact -1#28#35#exact -1#29#36#exact -1#30#37#exact \n"}nil_first --> [20, 22, 23, 24, 29, 31, 32, 33]
nil_second --> [11]
--------------------------
{1=>"The unsupervised morphological analyzer , latticelm , and one of heuristic methods , CAT , performed worse than expectations .\n", 2=>"The unsupervised morphological analyzer latticelm and one of heuristic methods CAT were worse than our expectations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#13#exact -1#12#16#exact -1#13#17#exact -1#15#18#exact -1#16#19#exact \n"}nil_first --> [4, 6, 12, 14, 15]
nil_second --> [11, 14]
--------------------------
{1=>"These two results were the worst , in all of the settings .\n", 2=>"These two were the worst or the second worst results in all settings .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#10#7#exact -1#11#8#exact -1#6#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> [6, 9]
nil_second --> [5, 7, 8]
--------------------------
{1=>"The results were better than the results for the supervised morphological analyzers in BLEU .\n", 2=>"It was relatively much better than the supervised morphological analyzers in BLEU .\n", 3=>"-1#6#0#lc -1#4#3#exact -1#5#4#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact \n"}nil_first --> [1, 2, 5, 6, 7, 8]
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"It was almost competitive in RIBES and BLUE in Characters .\n", 2=>"Besides , it was almost competitive in RIBES and BLUE in Characters .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact \n"}nil_first --> []
nil_second --> [0, 1]
--------------------------
{1=>"CHAR achieved the best score in BLEU on REUTERS ( 38 .42 ) , but the second-best was KyTea ( 29 .53 ) .\n", 2=>"For example , CHAR achieved the best 38 .42 score in BLEU on REUTERS , but the second best KyTea was 29 .53 .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#7#10#exact -1#8#11#exact -1#2#13#exact -1#15#14#exact -1#16#15#exact -1#20#17#exact -1#19#18#exact -1#21#20#exact -1#22#21#exact -1#23#23#exact \n"}nil_first --> [9, 12, 16, 19, 22]
nil_second --> [0, 1, 14, 17, 18]
--------------------------
{1=>"For BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .\n", 2=>"In the case of BLEU in Characters on REUTERS , CHAR achieved 38 .61 , while the worst supervised result was KyTea 's 39 .82 .\n", 3=>"-1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"For Japanese-English translations , the evaluation scores were generally lower than for English-Japanese translations .\n", 2=>"In this case , the evaluation scores are lower than English-Japanese translations in general .\n", 3=>"-1#11#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#12,13#8#para -1#8#9#exact -1#9#10#exact -1#10#12#exact -1#14#14#exact \n"}nil_first --> [0, 1, 11, 13]
nil_second --> [0, 1, 2]
--------------------------
{1=>"All supervised analyzers performed better than the unsupervised and the both heuristic methods .\n", 2=>"All supervised analyzers were better than the unsupervised and the both heuristic methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [3]
nil_second --> [3]
--------------------------
{1=>"Conversely , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT performed competitively with the supervised analyzers in RIBES .\n", 2=>"On the other hand , the unsupervised morphological analyzer latticelm and one of heuristic methods CAT were competitive to the supervised analyzers in RIBES .\n", 3=>"-1#0,1,2,3#0#para -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact \n"}nil_first --> [13, 14, 15]
nil_second --> [16, 17, 18]
--------------------------
{1=>"latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .\n", 2=>"For example , latticelm was 62 .51 and KyTea was 62 .90 on REUTERS .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact \n"}nil_first --> []
nil_second --> [0, 1, 2]
--------------------------
{1=>"The results for CHAR were the lowest scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS , with the exception of the best 56 .55 BLEU in Characters on REUTERS .\n", 2=>"The results were the worst scores in BLEU and RIBES on REUTERS and JENAAD+REUTERS . The only one exception was in the case of the best 56 .55 BLEU in Characters on REUTERS .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#4#exact -1#3#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#21#18#exact -1#18#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact \n"}nil_first --> [2, 3, 6, 16, 17]
nil_second --> [4, 14, 15, 16, 17, 19, 20, 22]
--------------------------
{1=>"We found that the results of the supervised morphological analyzers were better in both English-Japanese and Japanese-English experiments .\n", 2=>"We found the results of the supervised morphological analyzers are better in both English-Japanese and Japanese-English experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"Furthermore , the differences in the word definition of KyTea , MeCab , and JUMAN were not substantial , especially for English-Japanese translations , although the word definition of KyTea is much shorter than for MeCab and JUMAN .\n", 2=>"And the differences in the word definition of KyTea , MeCab , and JUMAN were not remarkable , especially in English-Japanese translations , although the word definition of KyTea is much shorter than MeCab and JUMAN .\n", 3=>"-1#17#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#para -1#22#18#exact -1#18,19#19,20#para -1#20#21#exact -1#21#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact \n"}nil_first --> [0, 23, 34]
nil_second --> [0]
--------------------------
{1=>"This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions , in most cases .\n", 2=>"This result implies that phrase-based SMT can output sufficiently reasonable word / phrase alignments that can treat different word definitions in most cases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT performed much poorer than the supervised morphological analyzers .\n", 2=>"On the other hand , the unsupervised morphological analyzer latticelm and one of our heuristic methods CAT were very much worse than the supervised morphological analyzers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19#18#exact -1#20,21#19,20#para -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> [17]
nil_second --> [17, 18]
--------------------------
{1=>"It excelled with English-Japanese translations , but not with Japanese-English translations .\n", 2=>"It was good at English-Japanese but not at Japanese-English translations .\n", 3=>"-1#0#0#exact -1#4#3#exact -1#9#4#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#10#11#exact \n"}nil_first --> [1, 2, 5, 8, 10]
nil_second --> [1, 2, 3, 7]
--------------------------
{1=>"We consider the possible reasons for this result in the following list :\n", 2=>"We consider the possible reasons for this result :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#12#exact \n"}nil_first --> [8, 9, 10, 11]
nil_second --> []
--------------------------
{1=>"- Long sentences were not translated as well as other word segmentation outputs .\n", 2=>"- Long sentences were translated worse than the other word segmentation outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#8#6,7,8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [4]
nil_second --> [5, 6, 7]
--------------------------
{1=>"However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .\n", 2=>"However , this result indicates that there is a possibility of better word segmentation than popular supervised morphological analyzers and CHAR word segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"The current evaluation metrics that we pursued in this paper were insufficient to discuss the relative advantages and disadvantages of word segmentation in detail , since the scores that were produced were inconsistent , as explained below :\n", 2=>"The current evaluation metrics we pursued in this paper were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation , since they did not produce consistent scores as explained below :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10,11,12#10,11,12#para -1#13#13#exact -1#17#14#exact -1#18#15#exact -1#19,20,21,22#16,17,18#para -1#23#20#exact -1#24#21#exact -1#25#24#exact -1#26#25#exact -1#32#27#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact \n"}nil_first --> [4, 19, 22, 23, 26, 28, 29, 30, 31, 32, 33]
nil_second --> [14, 15, 16, 27, 28, 29, 30, 31]
--------------------------
{1=>"Moreover , there is also a case in which RIBES and BLEU in Characters were incompatible with each other .\n", 2=>"Moreover , there is also a case that RIBES and BLEU in Characters were incompatible with each other .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#11#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [8, 12]
nil_second --> [7]
--------------------------
{1=>"For example , on WIKIPEDIA in Table 2 , while CHAR was the highest , and performed better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .\n", 2=>"For example , on WIKIPEDIA in Table 2 , while CHAR was relatively the highest and greatly better than the supervised morphological analyzers in RIBES , MeCab achieved the best score and notably better than CHAR in BLEU in Characters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#25#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact \n"}nil_first --> [16, 25]
nil_second --> [12, 16]
--------------------------
{1=>"This work focused on how the differences in word segmentation affected SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .\n", 2=>"This work focused on how the difference of word segmentation affects SMT outputs , the quality of the unsupervised word segmentation on SMT , and the meta-evaluation of the current evaluation metrics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16,17#14,15,16#para -1#25#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#28#25#exact -1#26#26#exact -1#27#27#exact -1#29#28,29#para -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"In summary , we found that the representative morphological analyzers were competitive and much better than both the unsupervised analyzer and one of our heuristic methods .\n", 2=>"In summary , we found that the representative morphological analyzers were competitive and much better than both unsupervised analyzer and one of our heuristic methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Nevertheless , a heuristic word segmentation method CHAR achieved relatively good word-based BLEU scores and competitive character-based BLEU results , compared to the supervised analyzers .\n", 2=>"After all , a heuristic word segmentation method CHAR achieved relatively good word-based BLEU scores and competitive character-based BLEU results , compared to the supervised analyzers .\n", 3=>"-1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> [0]
nil_second --> [0, 1]
--------------------------
{1=>"Additionally , as we could not always obtain consistent scores from the current evaluation metrics , the data was insufficient for discussing the relative advantages and disadvantages of word segmentation , with accuracy .\n", 2=>"Additionally , as we could not always obtain consistent scores from the current evaluation metrics , they were not sufficient to discuss more accurately about the relative advantages and disadvantages of word segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#25#16#exact -1#16,17#18#para -1#18,19,20#19,20#para -1#21#21#stem -1#26#22,23#para -1#27,28,29,30#24,25,26#para -1#31#28#exact -1#32#29#exact -1#23#32#para -1#33#33#exact \n"}nil_first --> [17, 27, 30, 31]
nil_second --> [22, 24]
--------------------------
{1=>"We have also suggested that it is possible to implement more optimized word segmentation on SMT .\n", 2=>"We also suggested it is possible to implement more optimized word segmentation on SMT .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3,4#4,5,6#para -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"However , the problem with these approaches , is that they disregard the interdependencies of word senses , and that it is limited in its applicability to the word senses for which training instances are served .**[<-This sentence is a bit confusing]\n", 2=>"However , problems with their approaches are the disregard of the interdependencies of word senses , and the limited applicability to those word senses for which training instances are served .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#2#exact -1#2,3#3,4#para -1#5#5,6#para -1#15#7#exact -1#6#8#syn -1#8#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#18#exact -1#25#19,20,21#para -1#18#22#exact -1#4#24#para -1#19#25#exact -1#20#26#exact -1#17#27#exact -1#22#28#exact -1#23#29#exact -1#24#30#exact -1#26#32#exact -1#27#33#exact -1#28#34#exact -1#29#35#exact \n"}nil_first --> [9, 10, 17, 23, 31, 36, 37, 38, 39, 40, 41]
nil_second --> [9, 21, 30]
--------------------------
{1=>"In particular , we assume that strong dependencies between the sense of a syntactic head and those of its dependents exist .\n", 2=>"Particularly , we assume that there exist strong dependencies between the sense of a syntactic head and those of its dependents .\n", 3=>"-1#0#0,1#para -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10,11,12#9,10,11,12#para -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#6#20#exact -1#21#21#exact \n"}nil_first --> []
nil_second --> [5, 13]
--------------------------
{1=>"Furthermore , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can also work for words that do not appear in the training data ; these combined features help relieve the data sparseness problem .\n", 2=>"Also , we define these sense dependencies in combination with various coarse-grained sense tag sets , so that our model can even work for words that do not appear in the training data , and these combined features help relieve the data sparseness problem .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#0#21#lc -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26,27,28,29#25,26,27,28,29#para -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact \n"}nil_first --> [0, 33]
nil_second --> [21, 25, 33, 34]
--------------------------
{1=>"In experiments , we display the appropriateness of considering the sense dependencies , as well as the advantage of [having ? Using ?] the combination of fine- and coarse-grained tag sets .\n", 2=>"In experiments , we show the appropriateness of considering the sense dependencies , as well as the advantage of the combination of fine- and coarse-grained tag sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#syn -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15,16#13,14,15#para -1#19#16#exact -1#17#17#exact -1#18#18#exact -1#20#23,24#para -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact \n"}nil_first --> [19, 20, 21, 22]
nil_second --> []
--------------------------
{1=>"We also present an in-depth analysis of the effectiveness of the sense dependency features by using intuitive examples .\n", 2=>"We also present an in-depth analysis on the effectiveness of the sense dependency features with intuitive examples .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [14, 15]
nil_second --> [14]
--------------------------
{1=>"It is considered to be an intermediate , but necessary step for many NLP applications , including machine translation and information extraction , which[what does \" which \" refer to ? Machine translation ? Information extraction , or both ? Clarify] require the knowledge of word senses to perform better .\n", 2=>"It is considered to be an intermediate , but necessary step toward many NLP applications including machine translation and information extraction , which require the knowledge of word senses to achieve better performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#21#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#26#exact -1#29#29#exact -1#25#34#para -1#23#41#exact -1#24#42#exact -1#26#44#exact -1#27#45#exact -1#28#46#exact -1#32#48#stem -1#31#49#exact -1#33#50#exact \n"}nil_first --> [11, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 43, 47]
nil_second --> [11, 30]
--------------------------
{1=>"One major obstacle for large-scale and precise WSD is solving the data sparseness problem caused by the fine-grained nature of sense distinction .\n", 2=>"One major obstacle to large-scale and precise WSD is the data sparseness problem caused by the fine-grainedness of the sense distinction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [9, 17, 18]
nil_second --> [16, 18]
--------------------------
{1=>"Some researchers have addressed the scarcity of the training data directly , and have explored the methods to obtain more tagged instances , by co-training and self-training .\n", 2=>"Some researchers have addressed directly the scarcity of the training data , and explored the methods to obtain more tagged instances , by the co-training and self-training .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#4#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [13]
nil_second --> [23]
--------------------------
{1=>"Although the use of global information has succeeded in dramatically increasing the performance of WSD , there is much room left to examine the effectiveness of local or syntactic information .\n", 2=>"Although the use of the global information has succeeded in dramatically increase the performance of WSD , there are much room left to examine the effectiveness of local or syntactic information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11,12#10,11#para -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18,19#17,18#para -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"One such information yet to be explored , is the interdependency of word senses .\n", 2=>"One of such information yet to be explored is the interdependencies of word senses .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [7]
nil_second --> [1]
--------------------------
{1=>"Although the use of local and syntactic information has been common in WSD , traditional approaches to WSD are based on the individual classification framework for each word ; each word 's sense is treated independently , regardless of any interdependencies or cooccurrences of word senses .\n", 2=>"Although the use of local and syntactic information has been common in WSD , traditional approaches to WSD are based on the individual classification framework for each word , in which each word 's sense is treated independently , regardless of any interdependencies nor cooccurrences of word senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#42#40#exact -1#43#41#para -1#44#42#exact -1#45#43#exact -1#46#44#exact -1#47#45#exact -1#48#46#exact \n"}nil_first --> [28]
nil_second --> [28, 29, 30]
--------------------------
{1=>"In turn , the resulting sense assignment may not be semantically consistent over the sentence .\n", 2=>"As a result , the resulting sense assignment may not semantically consistent over the sentence .\n", 3=>"-1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [0, 1, 9]
nil_second --> [0, 1, 2]
--------------------------
{1=>"To solve this problem is of great interest from both a practical and theoretical viewpoint .\n", 2=>"To solve this problem is of great interest from both practical and theoretical perspectives .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11,12#para -1#12#13#exact -1#14#15#exact \n"}nil_first --> [14]
nil_second --> [13]
--------------------------
{1=>"We focus on using the interdependency of word senses , so that we can directly address the issue of semantic ambiguity in a whole sentence that arose from the interaction of each word 's sense ambiguity . **[ <- this part is confusing .]\n", 2=>"We focus on the use of the interdependency of word senses , so that we can directly address the issue of semantic ambiguity of a whole sentence arose from the interaction of each word 's sense ambiguity .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5#2,3#para -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17,18,19,20#15,16,17#para -1#23#18#exact -1#21#19#exact -1#22#20#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27,28#25,26,27#para -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact \n"}nil_first --> [21, 37, 38, 39, 40, 41, 42, 43]
nil_second --> []
--------------------------
{1=>"Specifically , we assume that are strong sense dependencies between a syntactic head , and its dependents in the dependency tree , rather than between neighboring words of a sentence .\n", 2=>"Specifically , we assume that there exist strong sense dependencies between a syntactic head and its dependents in the dependency tree , rather than between neighboring words in the sentence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5,6#para -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#21#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26,27#26,27#para -1#28,29#28,29#para -1#30#30#exact \n"}nil_first --> [21]
nil_second --> [5, 6]
--------------------------
{1=>"We confirm the validity of this assumption , by showing the superiority of the tree-structured models over the linear-chain models .\n", 2=>"We confirm the appropriateness of this assumption by showing the superiority of the tree-structured models over the linear-chain models .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9,10,11,12#10,11,12#para -1#16#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [3, 7, 17]
nil_second --> [3]
--------------------------
{1=>"The combined features also enable our model to work , even for words that do not appear in the training data , which traditional individual classifiers cannot handle .\n", 2=>"The combined features also enable our model to work even for those words that do not appear in the training data , which the traditional individual classifiers cannot handle .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#21#9#exact -1#9#10#exact -1#10#11#exact -1#12#12#exact -1#14,15,16,17#13,14,15,16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [21]
nil_second --> [11, 13, 23]
--------------------------
{1=>"We solve WSD as a labeling problem to a sentence described as a dependency tree , where the vertices correspond to the words , and the edges correspond to the sense dependencies .\n", 2=>"We solve WSD as a labeling problem to a sentence described as a dependency tree , where the vertices correspond to words and the edges correspond to the sense dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#23#21#exact -1#21#22#exact -1#22#24#exact -1#27#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#28#29,30#para -1#29#31#exact -1#30#32#exact \n"}nil_first --> [23]
nil_second --> []
--------------------------
{1=>"T-CRFs also enable us to incorporate various sense tag sets all together into a simple framework .\n", 2=>"T-CRFs also enable us to incorporate various sense tag sets all together in a simple framework .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [12]
nil_second --> [12]
--------------------------
{1=>"These results are confirmed on three data sets ( the SemCor corpus and the Senseval-2 and -3 English all-words task test sets ) , and on two sense inventories ( WordNet synsets and supersenses ) .\n", 2=>"These results are confirmed on three data sets ( the SemCor corpus and the Senseval-2 and -3 English all-words task test sets ) and on two sense inventories ( WordNet synsets and supersenses ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [23]
nil_second --> []
--------------------------
{1=>"Our final model is shown to perform comparably to state-of-the-art WSD systems .\n", 2=>"Our final model is shown to perform comparably with state-of-the-art WSD systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"In Section 4 , we describe our model with intuitive examples , and we describe the machine learning method that we use .\n", 2=>"In Section 4 , we describe our model with intuitive examples , and the machine learning method we use .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#17#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#19,20,21#para -1#19#22#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"In Section 5 , 6 , and 7 , we present our experimental setup , the results , and an in-depth analysis on the contribution of the sense dependency features .\n", 2=>"In Section 5 , 6 , and 7 , we present our experimental setup and results , and an in-depth analysis on the contribution of the sense dependency features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14#exact -1#22#15#exact -1#15#16#exact -1#14#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#23,24,25#23,24,25,26#para -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"Finally , in Section 8 , we present our concluding remarks .\n", 2=>"Finally , in Section 8 , we present concluding remarks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"WordNet also serves as an ontology of various kinds of meta data , relations among words and senses , and a well-organized hierarchical classification of word senses that are defined .\n", 2=>"It also serves as an ontology , in which various kinds of meta data , relations among words and senses , and well-organized hierarchical classification of word senses are defined .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#11#6#exact -1#9#7#exact -1#10#8#exact -1#25#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#26#25#exact -1#27#26#exact -1#28#27,28#para -1#29#29#exact -1#30#30#exact \n"}nil_first --> [0, 20, 24]
nil_second --> [0, 6, 7, 8]
--------------------------
{1=>"As shown in Figure 1 , in WordNet , nouns and verbs are organized into hierarchical structures with IS-A ( hypernym-hyponym ) relationships among words , .\n", 2=>"In the WordNet , nouns and verbs are organized in hierarchical structures with IS-A ( hypernym-hyponym ) relationships among words , as shown in Figure 1 .\n", 3=>"-1#21#0#lc -1#22#1#exact -1#23#2#exact -1#24#3#exact -1#25#4#exact -1#20#5#exact -1#9#6#exact -1#2#7#exact -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6#11#exact -1#7#12#exact -1#8#13#exact -1#10#15#exact -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact -1#19#24#exact -1#26#26#exact \n"}nil_first --> [14, 25]
nil_second --> [0, 1]
--------------------------
{1=>"Nouns have a far deeper structure than verbs do , while that the structure of verbs is transversely developed .\n", 2=>"Nouns have a far deeper structure than verbs , while that of verbs is transversely developed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact \n"}nil_first --> [8, 12, 13]
nil_second --> []
--------------------------
{1=>"All nouns and verbs , with the exception of some top-level concepts , are classified into primitive groups called supersenses , which we will describe later .\n", 2=>"All nouns and verbs except some top-level concepts are classified into primitive groups called supersenses , which we describe later .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#15#4#exact -1#4#5,6,7,8#para -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#8#13#exact -1#9#14#exact -1#10#15#exact -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#16#21#exact -1#17#22#exact -1#18#24#exact -1#19#25#exact -1#20#26#exact \n"}nil_first --> [12, 20, 23]
nil_second --> []
--------------------------
{1=>"Figure 1 shows the WordNet hierarchical structure for the first sense ( financial bank ) of a noun bank , where each line indicates a synset with the list of words headed by its supersense label ; an arrow denotes that the two synsets are in an IS-A relation .\n", 2=>"Figure 1 shows the WordNet hierarchical structure for the first sense ( financial bank ) of a noun bank , where each line shows a synset with the list of words headed by its supersense label , and an arrow denotes that two synsets are in an IS-A relation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#syn -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41,42#para -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact \n"}nil_first --> [36]
nil_second --> [36, 37]
--------------------------
{1=>"The synset {group#1 , grouping#1} is a broad semantic category that governs the supersense group noun .group .\n", 2=>"The synset {group#1 , grouping#1} is a broad semantic category that governs the supersense noun group .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#14#exact -1#14#15#exact -1#16#17#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"Hence , we can expect them to act as a good smoothing feature for WSD , which would make up for the problem of the sparseness of features , commonly associated with finer-grained senses .\n", 2=>"Hence , we can expect them to act as a good smoothing feature for WSD , which would make up for the sparseness of features associated with finer-grained senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9#6,7,8#para -1#10#9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#22#25#exact -1#24#27#exact -1#25#30#exact -1#26#31#exact -1#27#32#exact -1#28#33#exact -1#29#34#exact \n"}nil_first --> [22, 24, 26, 28, 29]
nil_second --> []
--------------------------
{1=>"Since data sparsity has been a significant issue in WSD , the sense frequency information is necessary in achieving a good performance .\n", 2=>"Since the data sparsity has been a significant problem in WSD , the sense frequency information is necessary to achieve good performance .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18,19#17,18#para -1#20#19,20#para -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"In this section , we introduce two kinds of sense frequency information .\n", 2=>"In this section , we introduce two kinds of the sense frequency information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"Since senses of a word are ordered according to frequency , the sense ranking acts as a useful feature that offers a preference for frequent senses .\n", 2=>"Since senses of a word are ordered according to frequency , the sense ranking acts as a useful feature offering a preference for frequent senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#stem -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"Since the sense ranking in the WordNet is based on the word frequency in the SemCor , this baseline performs far better on the SemCor : 75 .9% for the brown1 section and 74 .3% for the brown2 section .\n", 2=>"Since the sense ranking in the WordNet is based on the word frequency in the SemCor , this baseline performs far better on the SemCor : 75 .9% for the brown1 section and 74 .3% for the brown2 section .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9,10#7,8,9#para -1#14#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#23#14#exact -1#24#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#29#23#exact -1#15#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#36#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}nil_first --> [36]
nil_second --> []
--------------------------
{1=>"When sufficient training data is available for every sense this method is considered to be a good feature that reflects the sense frequency information .\n", 2=>"It is considered to be a good feature that reflects the sense frequency information when sufficient training data is available for every sense .\n", 3=>"-1#14#0#lc -1#15#1#exact -1#16#2#exact -1#17#3#exact -1#18#4#exact -1#19#5#exact -1#20#6#exact -1#21#7#exact -1#22#8#exact -1#1#10,11#para -1#2#12#exact -1#4,5,6#13,14,15,16#para -1#7#17#exact -1#8#18#exact -1#9#19#exact -1#10#20#exact -1#11#21#exact -1#12#22#exact -1#13#23#exact -1#23#24#exact \n"}nil_first --> [9]
nil_second --> [0, 3]
--------------------------
{1=>"For such a reason , we use this first sense feature instead of the ranking feature , for the supersense-based evaluation .\n", 2=>"For this reason , we use this first sense feature instead of the ranking feature , for the supersense-based evaluation .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [1, 2]
nil_second --> [1]
--------------------------
{1=>"introduces an unsupervised graph-based algorithm , and shows a significant improvement over the sequence labeling model over the individual label assignment .\n", 2=>"introduces an unsupervised graph-based algorithm , and showed a significant superiority of the sequence labeling model over the individual label assignment .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#8#8#exact -1#9#9#exact -1#16#11#exact -1#17#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#12#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [10, 16]
nil_second --> [10, 11]
--------------------------
{1=>"built a model based on various word semantic similarity measures , and graph centrality algorithms , which also used the graph structure that incorporates the word-sense dependencies .\n", 2=>"built a model based on various word semantic similarity measures and graph centrality algorithms , which also used the graph structure incorporating the word-sense dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#14#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21,22#22,23,24#para -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"Thus , the effectiveness of sense dependencies for unsupervised WSD has been shown by several researches .\n", 2=>"Thus , the effectiveness of sense dependencies for the unsupervised WSD has been shown by several researches .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"On the contrary , the traditional approach to supervised WSD is to solve an independent classification problem for each word .\n", 2=>"On the other hand , the traditional approach to the supervised WSD is to solve an independent classification problem for each word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2#para -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"This approach has been developed along with research based on the lexical sample task in the Sensevals .\n", 2=>"This approach has been developed along with the researches based on the lexical sample task in the Sensevals .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"However , as described in Section 1 , this approach cannot handle the interdependencies among word senses , and may output a semantically inconsistent assignment of senses .\n", 2=>"However , as we described in Section 1 , this approach cannot deal with the interdependencies among word senses , and may output a semantically inconsistent assignment of senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12,13,14#11,12#para -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"Recently , with the growing interest in the all-words task , a few supervised WSD systems have incorporated the sense dependencies .\n", 2=>"Recently , with the growing interest on the all-words task , a few supervised WSD systems have incorporated the sense dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"SenseLearner and SuperSenseLearner incorporate sequential sense dependencies into the supervised WSD frameworks .\n", 2=>"SenseLearner and SuperSenseLearner incorporate sequencial sense dependencies into the supervised WSD frameworks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"also took a sequential tagging approach for the disambiguation of WordNet supersenses . [<-This sentence is a bit confusing]\n", 2=>"also took a sequencial tagging approach for the disambiguation of WordNet supersenses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [3, 13, 14, 15, 16, 17, 18]
nil_second --> [3]
--------------------------
{1=>"The dependencies that they considered , however , are rather simple ones between the adjacent words , and between either WordNet synsets or supersenses .\n", 2=>"However , the dependencies they considered are rather simple ones between the adjacent words , and between either WordNet synsets or supersenses .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#3#exact -1#5#4#exact -1#14#5#exact -1#0#6#lc -1#1#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [2, 16]
nil_second --> []
--------------------------
{1=>"Note additionally , that they do not mention the means or the quality of contribution in improving supervised WSD .\n", 2=>"Note additionally that they do not mention how and how much they contribute to the improvement of supervised WSD .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#14#8#exact -1#16#13#exact -1#12#14#stem -1#15#16#stem -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [2, 9, 10, 11, 12, 15]
nil_second --> [7, 8, 9, 10, 11, 13]
--------------------------
{1=>"The exponential family model proposed by , captures the occurrences and co-occurrences of words and senses in a joint probability distribution .\n", 2=>"One interesting model related is the exponential family model proposed by , which captures the occurrences and co-occurrences of words and senses in a joint probability distribution .\n", 3=>"-1#5#0#lc -1#6#1#exact -1#7#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#13#7#exact -1#14#8#exact -1#15#9#exact -1#16#10#exact -1#17#11#exact -1#18#12#exact -1#19#13#exact -1#20#14#exact -1#21#15#exact -1#22#16#exact -1#23#17#exact -1#24#18#exact -1#25#19#exact -1#26#20#exact -1#27#21#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3, 4, 12]
--------------------------
{1=>"In this context , it is of interest to note whether the sense dependencies on a syntactic structure , rather than on a linear chain , works effectively or not .**[why ?]\n", 2=>"In this context , it is of an interest if the sense dependencies on a syntactic structure , rather than on a linear chain , works effectively or not .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [8, 9, 10, 30, 31]
nil_second --> [7, 9, 29]
--------------------------
{1=>"Furthermore , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not been explicitly examined thus far .\n", 2=>"Also , despite the approaches described above , the contribution of sense dependencies for the supervised WSD has not explicitly examined thus far .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [0, 19]
nil_second --> [0]
--------------------------
{1=>"In Section 1 , we presented one of the most significant issues in WSD - the data sparsity problem .\n", 2=>"In Section 1 , we presented one of the most significant problems in WSD - the data sparsity .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12#11#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact \n"}nil_first --> [12, 18]
nil_second --> []
--------------------------
{1=>"This problem may even be magnified , when taking into consideration the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .\n", 2=>"This problem may even be magnified when we consider the interdependencies of word senses , since the number of features is roughly squared by the combination of two word senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#14#6#exact -1#6#7#exact -1#8#10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}nil_first --> [8, 9, 16]
nil_second --> [7]
--------------------------
{1=>"In order to relieve this problem , we use the hierarchical information in the WordNet , including the superordinate words and supersenses , as described in Section 2 .1 and 2 .2 .\n", 2=>"In order to relieve this problem , we use the hierarchical information in the WordNet , including the superordinate words and supersenses , which we describe in Section 2 .1 and 2 .2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#25,26#24,25#para -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact \n"}nil_first --> [23]
nil_second --> [23, 24]
--------------------------
{1=>"The use of hierarchical information has been motivated by several different researches .\n", 2=>"The use of the hierarchical information has been motivated by several researches .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [10]
nil_second --> [3]
--------------------------
{1=>"For example , a WSD system by , ranked second in the Senseval-3 , consists of two models : the first model applied to words seen in the training data , and the second model performs a generalized disambiguation process for words unseen in the data , by using the hierarchical information in the WordNet .\n", 2=>"For example , a WSD system by , which was ranked second in the Senseval-3 , consists of two models : the first model applied to words seen in the training data , and the second model that performs a generalized disambiguation process for words unseen in the data by using the hierarchical information in the WordNet .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact -1#47#44#exact -1#48#45#exact -1#49#47#exact -1#50#48#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact -1#57#55#exact \n"}nil_first --> [46]
nil_second --> [8, 9, 37]
--------------------------
{1=>"The fine granularity of the WordNet synsets is not just a major obstacle in achieving a high-performance WSD , but is sometimes too fine-grained even for a human to disambiguate .\n", 2=>"The fine granularity of the WordNet synsets is not just a major obstacle to high-performance WSD , but is sometimes too fine-grained even for a human to disambiguate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#24#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#26,27#para -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [13, 14]
nil_second --> [13]
--------------------------
{1=>"This is reflected in the low inter-annotator agreement of sense tagging ( typically around 70% ) , which implies that WSD models would be unlikely to perform better than the accuracy achieved .\n", 2=>"This is reflected in the low inter-annotator agreement of sense tagging ( typically around 70% ) , which implies that WSD models are unlikely to perform better than this accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22,23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#29#29,30#para -1#30#32#exact \n"}nil_first --> [31]
nil_second --> [28]
--------------------------
{1=>"Also , this fine-grained nature is reported to be inappropriate for many NLP applications .\n", 2=>"Also , this fine-grainedness is reported to be not appropriate for many NLP applications .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8,9#9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [3, 4]
nil_second --> [3]
--------------------------
{1=>"In particular , the use of the supersenses has recently been investigated by , and has received much attention in the WSD field .\n", 2=>"Especially , the use of the supersenses has recently been investigated by , and receiving much attention in the WSD field .\n", 3=>"-1#17#0#lc -1#0#1#para -1#1#2#exact -1#2,3,4,5#3,4,5#para -1#18#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#syn -1#15#17#exact -1#16#18#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [16, 19, 20]
nil_second --> []
--------------------------
{1=>"In this case , the inter-annotator agreements have reached nearly90% .\n", 2=>"In this case , the inter-annotator agreements are turned out to reach around 90% .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10#7#para -1#11#8#stem -1#14#10#exact \n"}nil_first --> [9]
nil_second --> [7, 12, 13]
--------------------------
{1=>"For this reason , we use the WordNet supersenses , as well as the synsets as our sense inventory .\n", 2=>"For this reason , we use as our sense inventory the WordNet supersenses as well as the synsets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13,14,15#10,11,12,13#para -1#17#14#exact -1#6#15#exact -1#7#16#exact -1#8#17#exact -1#9#18#exact -1#18#19#exact \n"}nil_first --> [9]
nil_second --> [16]
--------------------------
{1=>"One problem is the independent classification of each word 's sense , regardless of the sense dependencies among words .\n", 2=>"One is the independent classification of each word 's sense regardless of the sense dependencies among words .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [1, 11]
nil_second --> []
--------------------------
{1=>"The other problem is the scarcity of the training data that arose from the fine granularity of the sense distinction .\n", 2=>"The other is the scarcity of the training data arose from the fine granularity of the sense distinction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9,10#10,11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"The first [method ?] is the use of the syntactic dependencies of word senses on a dependency tree .\n", 2=>"The first is the use of the syntactic dependencies of word senses on a dependency tree .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#4#exact -1#3,4,5,6#5,6,7#para -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [2, 3, 8]
nil_second --> []
--------------------------
{1=>"In particular , we assume that there are strong dependencies of word senses between a head and its dependents in the dependency tree , rather than between neighboring words in the sentence .\n", 2=>"Particularly , we assume that there exist strong dependencies of word senses between a head and its dependents in the dependency tree , rather than between neighboring words in the sentence .\n", 3=>"-1#18#0#lc -1#0#1#stem -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#28#19#exact -1#29#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#19#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [29]
nil_second --> [6]
--------------------------
{1=>"The second [method ?] combines various coarse-grained sense tag sets with the WordNet synsets .\n", 2=>"The second is the combination of various coarse-grained sense tag sets with the WordNet synsets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> [2, 3, 4]
nil_second --> [2, 3, 4, 5]
--------------------------
{1=>"One way directly uses them as the sense inventory , instead of as a finer sense inventory .\n", 2=>"One way is to use them directly as the sense inventory instead of a finer sense inventory .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#6#2#exact -1#4#3#stem -1#5#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#10#exact -1#12#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [9, 12]
nil_second --> [2, 3]
--------------------------
{1=>"This method serves us many more training instances for each coarser sense , while we can no longer distinguish finer senses .\n", 2=>"This method serves us much more training instances for each coarser sense , while we can no longer distinguish finer senses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16,17#14,15,16#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [4, 17]
nil_second --> [4]
--------------------------
{1=>"The other way uses them**[<-define \" them \" ] in combination with finer sense tag sets .\n", 2=>"The other is to use them in combination with finer sense tag sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#6#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact \n"}nil_first --> [2, 3, 4, 5, 7, 8]
nil_second --> [2, 3, 4]
--------------------------
{1=>"Although sense disambiguation is still based on the finer senses , the coarser sense tags will help the discrimination of the finer senses , thereby serving generalized information for each fine-grained sense .\n", 2=>"Although the sense disambiguation is still based on the finer senses , the coarser sense tags will help the discrimination of the finer senses , serving generalized information for each fine-grained sense .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [24]
nil_second --> [1]
--------------------------
{1=>"This approach has been taken in several hierarchical WSD methods , but has never been combined with the sense dependencies in a way that have used them .\n", 2=>"This approach has been taken in several hierarchical WSD methods , but never combined with the sense dependencies as we use .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12,13,14#para -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#20#24,25#para -1#21#27#exact \n"}nil_first --> [20, 21, 22, 23, 26]
nil_second --> [18, 19]
--------------------------
{1=>"The process of WSD is summarized below .\n", 2=>"The process of WSD is summarized as below .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"By using T-CRFs , we can model this as the maximization of the probability of word sense trees , given the scores for vertices and edges .\n", 2=>"By using T-CRFs , we can model this as the maximization of the probability of word sense trees , given scores for vertices and edges .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"In the training phase , all vertex features and edge features are extracted using the gold-standard senses , and the weight vectors are optimized over the training data .\n", 2=>"In the training phase , all vertex features and edge features are extracted using the gold-standard senses , and the weight vectors for them are optimized over the training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact \n"}nil_first --> []
nil_second --> [22, 23]
--------------------------
{1=>"CRFs are state-of-the-art methods for sequence labeling problems in many NLP tasks .\n", 2=>"CRFs are the state-of-the-art methods for sequence labeling problems in many NLP tasks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"The conditional probability of a label sequence / MATH conditioned on a data sequence / MATH is given by / MATH\t , where / MATH and / MATH are the feature vectors for an edge and a vertex , / MATH and / MATH are the weight vectors , / MATH and / MATH are the set of components of / MATH associated with an edge / MATH and a vertex / MATH , and / MATH is the partition function that constrains the sum of all the probabilities to be 1 .\n", 2=>"The conditional probability of a label sequence / MATH conditioned on a data sequence / MATH is given by / MATH\t , where / MATH and / MATH are the feature vectors for an edge and a vertex , / MATH and / MATH are the weight vectors for them , / MATH and / MATH are the set of components of / MATH associated with an edge / MATH and a vertex / MATH , and / MATH is the partition function which constrains the sum of all the probabilities to be 1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#50#48#exact -1#51#49#exact -1#52#50#exact -1#53#51#exact -1#54#52#exact -1#55#53#exact -1#56#54#exact -1#57#55#exact -1#58#56#exact -1#59#57#exact -1#60#58#exact -1#61#59#exact -1#62#60#exact -1#63#61#exact -1#64#62#exact -1#65#63#exact -1#66#64#exact -1#67#65#exact -1#68#66#exact -1#69#67#exact -1#70#68#exact -1#71#69#exact -1#72#70#exact -1#73#71#exact -1#74#72#exact -1#75#73#exact -1#76#74#exact -1#77#75#exact -1#78#76#exact -1#79#77#exact -1#80#78#exact -1#81#79#exact -1#82#80#exact -1#84#82#exact -1#85#83#exact -1#86#84#exact -1#87#85#exact -1#88#86#exact -1#89#87#exact -1#90#88#exact -1#91#89#exact -1#92#90#exact -1#93#91#exact -1#94#92#exact \n"}nil_first --> [81]
nil_second --> [48, 49, 83]
--------------------------
{1=>"Tree-structured CRFs ( T-CRFs ) are different from widely used linear-chain CRFs , in that the random variables are organized in a tree structure ( acyclic graph ) .\n", 2=>"Tree-structured CRFs ( T-CRFs ) are different from widely used linear-chain CRFs in that the random variables are organized in a tree structure ( acyclic graph ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"Hence , we can consider them relevant in modeling the syntactic dependencies of word senses , which cannot be represented by linear structures .\n", 2=>"Hence , we can consider them appropriate for modeling the syntactic dependencies of word senses , which cannot be represented by linear structures .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"In this model , the optimal label assignment / MATH for an observation sequence / MATH is then calculated by / MATH , where / MATH denotes a vertex corresponding to a word , while / MATH denotes the vertex corresponding to its parent in the dependency tree .\n", 2=>"In this model , the optimal label assignment / MATH for an observation sequence / MATH is then calculated by / MATH , where / MATH denotes a vertex corresponding to a word while / MATH denotes the vertex corresponding to its parent in the dependency tree .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact \n"}nil_first --> [33]
nil_second --> []
--------------------------
{1=>"If we interpret / MATH as the vertex associated with the preceding word in a sentence , it delineates into a linear-chain CRF .\n", 2=>"If we interpret / MATH as the vertex associated with the preceding word in a sentence , it reduces to a linear-chain CRF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [18, 19]
nil_second --> [18, 19]
--------------------------
{1=>"In the beginning , we parse this sentence with Sagae and Tsujii 's dependency parser , which outputs parsed trees in the CoNLL-X dependency format .\n", 2=>"At the beginning , we parse this sentence with the Sagae and Tsujii 's dependency parser , which outputs parsed trees in the CoNLL-X dependency format .\n", 3=>"-1#21#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#9#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact \n"}nil_first --> [20]
nil_second --> [0, 22]
--------------------------
{1=>"Thus , for the synset-based model , we convert the outputted dependency tree into a tree of content words , as exemplified on the right-hand side of Figure 2 .\n", 2=>"For this reason , for the synset-based model , we convert the outputted dependency tree into a tree of content words , as exemplified on the right-hand side of Figure 2 .\n", 3=>"-1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 2]
--------------------------
{1=>"Then , on the right-hand side of Figure 2 , the dependency between confidence and bank is now described as a direct edge .\n", 2=>"Then , on the right-hand side of Figure 2 , we can see that the dependency between confidence and bank is now described as a direct edge .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact \n"}nil_first --> []
nil_second --> [10, 11, 12, 13]
--------------------------
{1=>"For the supersense-based model , we further convert the tree into a tree of nouns and verbs , because supersenses are defined for only these two parts of speech .\n", 2=>"Note that for the supersense-based model , we further convert the tree into a tree of nouns and verbs , because supersenses are defined for only these two parts of speech .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact \n"}nil_first --> []
nil_second --> [0, 1]
--------------------------
{1=>"The inclusion of removed words and dependency relation labels are performed in the same manner as in the synset-based model ; the tree on the right hand side of Figure 2 in this case remains unchanged , because the sentence does not contain any adjectives nor adverbs .\n", 2=>"The inclusion of removed words and dependency relation labels are performed in the same manner as in the synset-based model , and the tree on the right hand side of Figure 2 in this case remains unchanged because the sentence does not contain any adjectives nor adverbs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25,26,27,28#24,25,26#para -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#20#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact \n"}nil_first --> [20, 27]
nil_second --> [21]
--------------------------
{1=>"For the linear-chain models , parsing a sentence is unnecessary .\n", 2=>"For the linear-chain models , we do not need to parse a sentence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#10#5#stem -1#11#6#exact -1#12#7#exact -1#7#9#para -1#13#10#exact \n"}nil_first --> [8]
nil_second --> [5, 6, 8, 9]
--------------------------
{1=>"Next , as the same reason as for the tree-structured case , we remove those words that we do not need to disambiguate from the graph , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .\n", 2=>"Next , as the same reason for the tree-structured case , we remove from the graph those words that we do not need to disambiguate , in order to capture the direct dependencies between content words ( or nouns and verbs in the supersense-based model ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#16#14#exact -1#17#15#exact -1#18,19,20,21,22,23#16,17,18,19,20#para -1#28#21#exact -1#24#22#exact -1#13#23#exact -1#14#24#exact -1#15#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact \n"}nil_first --> [6, 29]
nil_second --> []
--------------------------
{1=>"Thus , the process of the tree compaction[ ? ?] is performed in the same manner , as described in Figure 3 .\n", 2=>"Thus , the process of the tree compaction is performed in the same manner , as described in Figure 3 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5#2,3,4#para -1#11#5#exact -1#6#6#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact \n"}nil_first --> [7, 8, 9, 13]
nil_second --> [7]
--------------------------
{1=>"Here , we focus on three words : destroy , confidence , and bank in Sentence ( I ) . For simplicity , we consider only two major senses for each word as described in Table 3 , so that the number of possible sense assignments is / MATH .\n", 2=>"Here , we focus on three words destroy , confidence , and bank in Sentence ( i ) , and for simplicity consider only two major senses for each word as described in Table 3 , so that the number of possible sense assignments is in this case / MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#lc -1#17#18#exact -1#50#19#exact -1#20#20#lc -1#21#21#exact -1#18#22#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#38,39,40#39,40,41,42#para -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#48#47#exact -1#49#48#exact \n"}nil_first --> [7, 23, 49]
nil_second --> [19, 37, 45, 46, 47]
--------------------------
{1=>"The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) , but is unrelated to a natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .\n", 2=>"The first intuition would be that confidence( n )#2 is strongly related to a group or an institution ( financial bank ) but not related to natural landscape ( river bank ) , while confidence( n )#1 depends mostly on persons and not on other entities .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#32#22#exact -1#22#23#exact -1#37#24#para -1#23,24,25#25,26#para -1#26#27,28#para -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact -1#46#48#exact \n"}nil_first --> [34, 39]
nil_second --> []
--------------------------
{1=>"Because bank does not have a \" person \" meaning , the weight of confidence( n )#2-bank( n )#1 is expected to be higher than for other possible sense bigrams .\n", 2=>"Because bank does not have a \" person \" meaning , the weight of confidence( n )#2-bank( n )#1 is expected to be higher than other possible sense bigrams .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20#19,20,21,22#para -1#23#23#exact -1#24#24#exact -1#25#25,26#para -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> []
nil_second --> [21, 22]
--------------------------
{1=>"A similar argument can be made for the dependency between destroy and confidence .\n", 2=>"A similar argument can be made for the dependency between destroy and confidence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> []
nil_second --> [6, 7]
--------------------------
{1=>"Given confidence does not have an \" object \" meaning , the weights of destroy( v )#2-confidence( n )#1 and destroy( v )#2-confidence( n )#2 would be the largest [largest what ?] among others .\n", 2=>"Given confidence does not have an \" object \" meaning , the weights of destroy( v )#2-confidence( n )#1 and destroy( v )#2-confidence( n )#2 would be the largest among others .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact \n"}nil_first --> [29, 30, 31]
nil_second --> []
--------------------------
{1=>"The detailed description of sense bigrams are provided in Section 4 .7 .\n", 2=>"The detailed description of sense bigrams are given in Section 4 .7 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"These labels represent word senses at various levels , and are to be combined with the vertex and edge features .\n", 2=>"These labels represent word senses at various levels , and to be combined with the vertex and edge features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10,11#para -1#12,13#12,13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"We implement as vertex features a set of typical contextual features widely used in many supervised WSD models .\n", 2=>"We implement as vertex features a set of typical contextual features widely used in a lot of supervised WSD models .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact \n"}nil_first --> [14]
nil_second --> [14, 15, 16]
--------------------------
{1=>"Most of these features are those used by with the exception of the syntactic features .\n", 2=>"Most of these features are those used by with the exception of the syntactic features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"In order to see the efficiency of sense dependency features , we include as vertex features the word forms , lemmas , and parts of speech of both the parent and the child words in the dependency tree .\n", 2=>"In order to see whether the sense dependency features are certainly effective or not , we include as vertex features the word forms , lemmas , and parts of speech of both the parent and the child words in the dependency tree .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#11#5,6#para -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#35#31#exact -1#36#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact -1#41#37#exact -1#42#38#exact \n"}nil_first --> []
nil_second --> [4, 9, 10, 12, 13]
--------------------------
{1=>"Using this contextual information , and the set of vertex labels / MATH , we construct a set of features on a vertex / MATH by / MATH .\n", 2=>"Using this contextual information and the set of vertex labels / MATH , we construct a set of features on a vertex / MATH by / MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#12#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"Note that this feature is not combined with any sense label nor with any contextual information .\n", 2=>"Note that this feature is not combined with any sense labels nor contextual information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#11#11#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [12, 13]
nil_second --> []
--------------------------
{1=>"For the supersense-based model , we use vertex features based on , which include some features from the named entity recognition literature , including the word shape features , along with the standard feature set for WSD .\n", 2=>"For the supersense-based model , we use vertex features based on , which includes some features from the named entity recognition literature such as the word shape features along with the standard feature set for WSD .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact \n"}nil_first --> [22, 23, 28]
nil_second --> [22, 23]
--------------------------
{1=>"Unlike in the synset-based model , we do not incorporate the syntactic information of the parent and child words , since it has not been reported to improve the performance .\n", 2=>"Unlike in the synset-based model , we do not incorporate the syntactic information of the parent and child words , since it has been reported not to improve the performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22,23#21,22,23,24#para -1#24#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> []
nil_second --> [21, 25]
--------------------------
{1=>"In this section , we introduce corpora that we have used for the evaluation .\n", 2=>"In this section , we introduce corpora we use for the evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9,10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"SemCor is a corpus in which all content words are annotated with the WordNet synsets , and consists of balanced 352 files from the Brown Corpus .\n", 2=>"SemCor is a corpus , in which all content words are annotated with the WordNet synsets , and consists of balanced 352 files from the Brown Corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"Also , we use two data sets from the Senseval ( International Workshop on Evaluating Word Sense Disambiguation Systems ) exercises : the Senseval-2 English all-words task test set , consisting of three articles from the Wall Street Journal , and the Senseval-3 English all-words task test set , consisting of two articles from the Wall Street Journal and a fiction excerpt from the unannotated portion of the Brown corpus .\n", 2=>"Also , we use two data sets from the Senseval ( International Workshop on Evaluating Word Sense Disambiguation Systems ) exercises : the Senseval-2 English all-words task test set , consisting of three articles from the Wall Street Journal , and the Senseval-3 English all-words task test set , consisting of two articles from the Wall Street Journal and a fiction excerpt from the unannotated portion of the Brown corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#36,37,38#35,36,37,38#para -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#55,56,57#54,55,56,57#para -1#58#58#exact -1#59#59#exact -1#60#60#exact -1#61#61#exact -1#62#62#exact -1#63#63#exact -1#64#64#exact -1#65#65#exact -1#66#66#exact -1#67#67#exact -1#68#68#exact -1#69#69#exact -1#70#70#exact \n"}nil_first --> []
nil_second --> [35, 54]
--------------------------
{1=>"These data sets are different from the originals because multi-word expressions are already segmented .\n", 2=>"Note that these data sets are different from the originals in that multi-word expressions are already segmented .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact \n"}nil_first --> [8]
nil_second --> [0, 1, 10, 11]
--------------------------
{1=>"However , our model cannot output any answers to multi-word expressions that have no directly corresponding WordNet synsets , because we treat expression as one unit in the process of WSD .\n", 2=>"However , on the other hand , our model cannot output any answers to multi-word expressions that have no directly corresponding WordNet synsets , because we treat expression as one unit in the process of WSD .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact -1#17#12#exact -1#18#13#exact -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#22#17#exact -1#23#18#exact -1#24#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#28#23#exact -1#29#24#exact -1#30#25#exact -1#32,33,34#26,27,28,29#para -1#35#30#exact -1#36#31#exact \n"}nil_first --> []
nil_second --> [2, 3, 4, 5, 6, 31]
--------------------------
{1=>"For example , the multi-word expression tear-filled is treated as one instance , but are untagged with any WordNet synsets in the converted corpus , while in the original corpus it[define \" it \" ] is tagged with two WordNet synsets for tear and filled .\n", 2=>"For example , the multi-word expression tear-filled is treated as one instance but not tagged with any WordNet synsets in the converted corpus , while in the original corpus it is tagged with two WordNet synsets for tear and filled .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#23#12#exact -1#12#13#exact -1#30#14#syn -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#32#exact -1#14#36#exact -1#32#37#exact -1#33#38#exact -1#34#39#exact -1#35#40#exact -1#36#41#exact -1#37#42#exact -1#38#43#exact -1#39#44#exact -1#40#45#exact \n"}nil_first --> [15, 24, 30, 31, 33, 34, 35]
nil_second --> [13, 31]
--------------------------
{1=>"Next , for the evaluation on SemCor , one half of the rest ( e.g. SEM-E1 ) is used for development and the other half ( e.g. SEM-E2 ) is used for evaluation .\n", 2=>"Next , for the evaluation on SemCor , one half of the rest ( e.g. SEM-E1 ) is used for development and the other half ( e.g. SEM-E2 ) is used for evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10,11#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"For the evaluation on the Senseval data sets , all instances of the rest ( e.g. SEM-E ) are used for development , and one of the Senseval data sets ( SE2 or SE3 ) is used for evaluation .\n", 2=>"For the evaluation on the Senseval data sets , all instances of the rest ( e.g. SEM-E ) is used for development and one of the Senseval data sets ( SE2 or SE3 ) is used for evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18,19#18,19#para -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"As the evaluation measure , we use the standard recall measure , which is equivalent to the precision as we output answers to all instances . **[This section is a bit monotonous]\n", 2=>"As the evaluation measure , we use the standard recall measure , which is equivalent to the precision as we output answers to all instances .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [26, 27, 28, 29, 30, 31]
nil_second --> []
--------------------------
{1=>"As noted , in the WordNet , the labeling of supersensesis semantically inconsistent , and top level synsets are tagged as the supersense noun .Tops[ ? ?] rather than the specific supersense they govern .\n", 2=>"As they noted , in the WordNet , there is semantically inconsistent labeling of supersenses such that top level synsets are tagged as the supersense noun .Tops rather than the specific supersense they govern .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#23#7#exact -1#12#8#exact -1#13#9#exact -1#10#11#exact -1#11#12#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#29#21#exact -1#24#22#exact -1#25#23#exact -1#27#27#exact -1#28#28#exact -1#30#29,30#para -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> [10, 13, 14, 24, 25, 26]
nil_second --> [1, 8, 9, 14, 15, 16, 26]
--------------------------
{1=>"For this reason , we adopted the modification of noun supersenses in the same way as , substituting noun .Tops labels with more specific supersense labels when possible , and left some general nouns with noun .TopsoteNouns , which are left with noun .Tops are : entity , thing , anything , something , nothing , object , living thing , organism , benthos , heterotroph , life , and biont . .\n", 2=>"For this reason , we adopted the modification of noun supersenses in the same way as , substituting noun .Tops labels with more specific supersense labels when possible , and left some general nouns with noun .TopsoteNouns which are left with noun .Tops are : entity , thing , anything , something , nothing , object , living thing , organism , benthos , heterotroph , life , and biont . .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12,13,14,15#11,12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#46#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#48#47#exact -1#47#48#exact -1#50#49#exact -1#49#50#exact -1#52#51#exact -1#51#52#exact -1#54#53#exact -1#53#54#exact -1#56#55#exact -1#55#56#exact -1#59#57#exact -1#57#58#exact -1#58#59#exact -1#61#60#exact -1#60#61#exact -1#63#62#exact -1#62#63#exact -1#65#64#exact -1#64#65#exact -1#67#66#exact -1#66#67#exact -1#68#69#exact -1#69#70#exact -1#70#71#exact -1#71#72#exact \n"}nil_first --> [68]
nil_second --> [11]
--------------------------
{1=>"We ignore the adjective and adverb instances in the evaluation .**[This section is a bit confusing . Maybe break up the longer sentences to clarify]\n", 2=>"We ignore the adjective and adverb instances in the evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#16#exact \n"}nil_first --> [10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24]
nil_second --> []
--------------------------
{1=>"In this section , we focus on the contribution of the sense dependencies .\n", 2=>"In this section , we focus on the contribution of the sense dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"Each figure displays the mean recall ( equivalent to the precisions ) averaged over the five trials of the cross validation , the \" Diff . \" rows show the differences between the dependency models and the non-dependency models , and / MATH and / MATH denote the statistical significance of / MATH and / MATH respectively .\n", 2=>"In this section , each figure shows the mean recall ( equivalent to the precisions ) averaged over the five trials of the cross validation , the \" Diff . \" rows show the differences between the dependency models and the non-dependency models , and / MATH and / MATH denote the statistical significance of / MATH and / MATH respectively .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#syn -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33,34,35,36#29,30,31#para -1#51#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact -1#41#37#exact -1#42#38#exact -1#43#39#exact -1#44#40#exact -1#45#41#exact -1#46#42#exact -1#47#43#exact -1#48#44#exact -1#49#45#exact -1#50#46#exact -1#52#47,48#para -1#53#49#exact -1#54#50#exact -1#55#51#exact -1#56#52#exact -1#57#53#exact -1#58#54#exact -1#59#55#exact -1#60#56#exact -1#61#57#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"From Table 7 , it can be seen that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .\n", 2=>"We can see from Table 7 that with the sense frequency information , the tree-structured models ( statistically ) significantly outperformed the non-dependency models on all the data sets .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#12#3#exact -1#1#5#exact -1#2#6,7#para -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact \n"}nil_first --> [4, 14]
nil_second --> [0]
--------------------------
{1=>"These improvements seem insignificant in figures ; however , considering that for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model by only 0 .37% on SEM , the further improvement of 0 .21% is substantial , because it indicates that our dependency model could handle 57% more instances over the first sense baseline .\n", 2=>"These improvements seem small in terms of figures ; However , considering for instance the No-Dep-SS-FS model outperforms the Baseline-SS-FS model only by 0 .37% on SEM , the further improvement of 0 .21% is considerable because this means our dependency model could handle 57% more instances over the first sense baseline .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#para -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#lc -1#10#8#exact -1#11#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#22#20#exact -1#21#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#para -1#36,37#36,37#para -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact \n"}nil_first --> [10, 35, 38, 39]
nil_second --> [5, 6, 38]
--------------------------
{1=>"Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed worse than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness [of ...] regardless of the existence of the sense frequency information .\n", 2=>"Note that , without the sense frequency information , the synset-based tree-structured model ( Tree-WS ) performed poorer than the non-dependency model ( NoDep-WS ) on all the data sets , whereas the supersense-based model ( Tree-SS ) exhibited the robustness regardless of the existence of the sense frequency information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17,18#17,18#para -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#43#exact -1#43,44,45,46#44,45,46,47,48#para -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact \n"}nil_first --> [41, 42]
nil_second --> [42]
--------------------------
{1=>"These results suggest that for the synset-based model , in which most synsets do not have enough instances in the training data , the combination with sense-frequency information is necessary in order to avoid the data sparseness problem .\n", 2=>"These results suggest that for the synset-based model , in which most synsets do not have enough instances in the training data , the combination with sense-frequency information is necessary in order to avoid the data sparseness problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30,31,32,33#30,31,32,33,34#para -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact \n"}nil_first --> []
nil_second --> [34]
--------------------------
{1=>"Nonetheless , by the incorporation of the sense dependencies , the improvements with the sense ranking features was even less , and the deteriorations without them[define \" them \" ] were even more than in the tree-structured case .\n", 2=>"However , by the incorporation of the sense dependencies , the improvements with the sense ranking features are even smaller , and the deteriorations without them are even larger than in the tree-structured case .\n", 3=>"-1#0#0#syn -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#13#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#22#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#syn -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#31#22#exact -1#23#23#exact -1#24#24#exact -1#25#27#exact -1#26,27#30,31#para -1#28,29#32,33#para -1#30#34#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact \n"}nil_first --> [19, 25, 26, 28, 29, 35]
nil_second --> [19]
--------------------------
{1=>"In this section , let us focus on the difference between the tree-structured models and the linear-chain models .\n", 2=>"In this section , let us focus on the difference between the tree-structured models and the linear-chain models .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10#para -1#15#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"In the results shown in Table 9 , although some of the differences are marginal , we can see that the tree-structured models outperformed the linear-chain models , focusing on the statistically significant differences .**[<-This is a confusing sentence]\n", 2=>"In the results shown in Table 9 , although some of the differences are marginal , we can see that the tree-structured models outperformed the linear-chain models , focusing on the statistically significant differences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17,18,19,20#16,17,18,19#para -1#24#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#30#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [30, 34, 35, 36, 37, 38]
nil_second --> [34]
--------------------------
{1=>"Thus , although both the dependency trees and the linear chains capture useful dependencies of word senses , the dependencies on the tree structures capture more important information .\n", 2=>"These results suggest that although both the dependency trees and the linear chains capture useful dependencies of word senses , the dependencies on the tree structures capture more important information .\n", 3=>"-1#19#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact \n"}nil_first --> [0, 17]
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"Whereas Tree-WS-SR and Tree-WS use all four sense labels for the edge features ( / MATH ) , Tree-WS-SR' and Tree-WS' only use the synset labels ( / MATH ) . Thus , we can see the contribution of the coarse-grained sense labels .\n", 2=>"Whereas Tree-WS-SR and Tree-WS use all four sense labels for the edge features ( / MATH ) , Tree-WS-SR' and Tree-WS' only use the synset labels ( / MATH ) , so that we can see the contribution of the coarse-grained sense labels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#43#30#exact -1#31#31#syn -1#30#32#exact -1#34,35,36#33,34,35,36#para -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact \n"}nil_first --> [43]
nil_second --> [32, 33]
--------------------------
{1=>"Since synset-based models can directly be used as supersense taggers by a simple conversion of senses , we compared the performance of the synset-based model with that of the supersense-based model .\n", 2=>"Since synset-based models can directly be used as supersense taggers by a simple conversion of senses , we compared the performance of the synset-based model with that of the supersense-based model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20,21,22#19,20,21#para -1#28#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [28]
nil_second --> []
--------------------------
{1=>"Interestingly , when evaluated at the supersense level , the synset-based models considerably outperformed the supersense-based models , with an overall improvements of 0 .69% with the sense frequency information and 1 .41% without it , as shown in Table fcomp-ws-ss-tree .\n", 2=>"Interestingly , when evaluated at the supersense level , the synset-based models considerably outperformed the supersense-based models , with the overall improvements of 0 .69% with the sense frequency information and 1 .41% without it , as shown in Table fcomp-ws-ss-tree .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19,20#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"Thus , even though the granularity of the supersenses is sufficient for many NLP tasks , they are too coarse-grained to capture enough information for WSD models ; therefore , for the supersense-based disambiguation , we can improve the performance by considering finer-grained senses .\n", 2=>"These results suggest that even though the granularity of the supersenses is sufficient for many NLP tasks , they are too coarse-grained to capture enough information for WSD models ; Therefore , even for the supersense-based disambiguation , we can improve the performance by considering finer-grained senses .\n", 3=>"-1#30#0#syn -1#31#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#37#29#exact -1#33#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact -1#47#44#exact \n"}nil_first --> [28, 34]
nil_second --> [0, 1, 2, 3, 32]
--------------------------
{1=>"However , taking into consideration that all systems in Table 12 except for Simil-Prime utilize other sense-annotated corpora in addition to SemCor , such as the Senseval data sets or example sentences in the WordNet , and that our model cannot handle multi-word expressions that do not exist in the WordNet as noted in Section 5 .1 , we can conclude that the performance of our T-CRF model is comparable to that of state-of-the-art WSD systems . **[This is a long sentence- shorten .]\n", 2=>"However , considering that all systems in Table 12 except for Simil-Prime utilize other sense-annotated corpora in addition to SemCor , such as the Senseval data sets or example sentences in the WordNet , and our model cannot handle multi-word expressions that do not exist in the WordNet as noted in Section 5 .1 , we can conclude that the performance of our T-CRF model is comparable to that of state-of-the-art WSD systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2,3,4#para -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#41#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#58#44#exact -1#42#45#exact -1#43,44,45,46#46,47,48#para -1#59#49#exact -1#47#50#exact -1#48#51#exact -1#49#52#exact -1#50#53#exact -1#51#54#exact -1#52#55#exact -1#53#56#exact -1#54#57#exact -1#55#58#exact -1#56#59#exact -1#57#60#exact -1#68#61#exact -1#60#62,63#para -1#61#64#exact -1#62#65#exact -1#63#66#exact -1#64#67#exact -1#65#68#exact -1#66#69#exact -1#67#70#exact -1#69#72#exact -1#70#73#exact -1#71#74#exact -1#72#75#exact -1#73#76#exact \n"}nil_first --> [71, 77, 78, 79, 80, 81, 82, 83]
nil_second --> []
--------------------------
{1=>"Table 13 shows the list of the 15 largest-weighted sense dependency features in the tree-structured , synset-based model ( Tree-WS ) .\n", 2=>"Table 13 shows the list of the 15 largest-weighted sense dependency features in the tree-structured , synset-based model ( Tree-WS ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#13#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"We call a feature either with a positive lambda or with an alpha larger than 1 as an excitatory feature , and those features with either with a negative lambda or an alpha smaller than 1 as an inhibitory feature .\n", 2=>"We call a feature either with a positive lambda or with an alpha larger than 1 as an excitatory feature , while that either with a negative lambda or an alpha smaller than 1 as an inhibitory feature .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#37#23#stem -1#24#24#exact -1#23#25#exact -1#26#26,27,28#para -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#38#40#exact \n"}nil_first --> [21, 22, 39]
nil_second --> [21, 22, 25]
--------------------------
{1=>"When compared to the outputs of the tree-structured model , we can see that the linear-chain model captures more successive noun-noun dependencies , while the tree-structured model captures more adjective-noun and verb-object dependencies .\n", 2=>"When compared to the outputs of the tree-structured model , we can see that the linear-chain model captures more successive noun-noun dependencies , while the tree-structured model captures more adjective-noun and verb-object dependencies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11,12,13,14#10,11,12,13#para -1#24#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [24]
nil_second --> []
--------------------------
{1=>"Thus , although the difference of the recalls is small , we can assume that the sense dependency features in the tree-structured model , and those in the linear-chain model have different contributions to the results .\n", 2=>"Thus , although the difference of the recalls is small , we can assume that the sense dependency features in the tree-structured model and those in the linear-chain model have different contributions to the results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13,14#11,12,13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact \n"}nil_first --> [23]
nil_second --> [11, 12]
--------------------------
{1=>"In this section , we present an instance-based analyses based on the first 100 instances for which the answer of the dependency model Tree-WS-SR differs from that of the non-dependency model NoDep-WS-SR in the first trial on SemCor .\n", 2=>"In this section , we present instance-based analyses based on the first 100 instances for which the answer of the dependency model Tree-WS-SR differs from that of the non-dependency model NoDep-WS-SR in the first trial on SemCor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"One noteworthy point is that more number of noun-noun dependencies are found in the positive instances than in the negative instances , further suggesting that noun-noun dependencies are particularly likely to capture useful dependencies and contribute to positive instances .\n", 2=>"One noteworthy point is that more number of noun-noun dependencies are found in the positive instances than in the negative instances , which might suggest that noun-noun dependencies are particularly likely to capture useful dependencies and contribute to positive instances .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#24,25#23,24#para -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact \n"}nil_first --> [22]
nil_second --> [22, 23]
--------------------------
{1=>"The first sentence is :\n", 2=>"The first sentence is\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"Surprisingly , the verb take has as many as 42 senses in the WordNet .\n", 2=>"The verb take has surprisingly as many as 42 senses in the WordNet .\n", 3=>"-1#4#0#lc -1#0#2#lc -1#1#3#exact -1#2#4#exact -1#3#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"But fortunately , the first six senses belong to different supersenses , and our dependency model succeeded in outputting the correct sense take#4 ( SS :verb .contact , take physically ) by making use of the strong dependency SS :verb .contact-SS :noun .substance ( / MATH ) , given dust#1 belongs to noun .substance .\n", 2=>"But , fortunatelly , the first six senses belong to different supersenses , and our dependency model succeeded in outputting the correct sense take#4 ( SS :verb .contact , take physically ) by making use of the strong dependency SS :verb .contact-SS :noun .substance ( / MATH ) , given dust#1 belongs to noun .substance .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#34,35#32,33,34,35#para -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#50#exact -1#52#51#exact -1#53#52#exact -1#54#53#exact -1#55#54#exact \n"}nil_first --> [1]
nil_second --> [1, 2, 33, 36]
--------------------------
{1=>"While this verb-object dependency had a large excitatory weight , the corresponding verb-subject dependency had an inhibitory weight ( G1 :have( v )#2-( SBJ )-SS :noun .attribute ( / MATH ) ) , which indicates that the dependency relationlabel also contributed to the result .\n", 2=>"While this verb-object dependency had a large excitatory weight , the corresponding verb-subject dependency had an inhibitory weight ( G1 :have( v )#2-( SBJ )-SS :noun .attribute ( / MATH ) ) , which means the dependency relationlabel also contributed to the result .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38,39,40,41#39,40,41#para -1#42#42,43#para -1#43#44#exact \n"}nil_first --> [34, 35]
nil_second --> [34]
--------------------------
{1=>"From the phrase career as a player , we can assume that the correct sense of career can be either of two senses , with the possibility that there is a preference for career#2 , as captured by the largest-weighted dependency WS :career%1%2-( NMOD )-SS :noun .person ( / MATH ) between career and player .\n", 2=>"From the phrase career as a player , we can assume that the correct sense of career can be either of two senses , and possibly there is a preference for career#2 , as captured by the largest-weighted dependency WS :career%1%2-( NMOD )-SS :noun .person ( / MATH ) between career and player .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10,11#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#25#24,25,26#para -1#26,27,28#27,28,29,30#para -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#51#53#exact -1#52#54#exact -1#53#55#exact \n"}nil_first --> []
nil_second --> [8, 9, 24]
--------------------------
{1=>"Since dependencies of this type were not observed in the negative instances , they seem to particularly contribute to the positive instances .\n", 2=>"Since dependencies of this type were not observed in the negative instances at all , they seem to particularly contribute to the positive instances .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact \n"}nil_first --> []
nil_second --> [12, 13]
--------------------------
{1=>"Through our result , we observed that the noun-noun dependencies in coordination relations work remarkably well .\n", 2=>"Another interesting result observed is that the noun-noun dependencies in coordination relations work remarkably strongly .\n", 3=>"-1#2#2#exact -1#3#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact \n"}nil_first --> [0, 1, 3, 4, 15]
nil_second --> [0, 1, 4, 14]
--------------------------
{1=>"He also bought a huge square of pegboard for hanging up his tools , and lumber for his workbench , sandpaper and glue and assorted nails , levels and T squares and plumb lines and several gadgets that he had no idea how touse or what they were for .\n", 2=>"He also bought a huge square of pegboard for hanging up his tools , and lumber for his workbench , sandpaper and glue and assorted nails , levels and T squares and plumb lines and several gadgets that he had no idea how touse or what they were for .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#40,41#39,40,41,42#para -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact \n"}nil_first --> []
nil_second --> [39, 42]
--------------------------
{1=>"Here , the correct sense for nail is nail#2 ( noun .artifact , a thin pointed piece of metal ) , and that for level is level#5 ( noun .artifact , indicator of the horizontal ) .\n", 2=>"Here , the correct sense for nail is nail#2 ( noun .artifact , a thin pointed piece of metal ) and that for level is level#5 ( noun .artifact , indicator of the horizontal ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#29#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"In this paper , we proposed a novel approach for the all-words WSD , focusing on the use of syntactic dependencies of word senses , and investigated the contribution of these dependencies to WSD .\n", 2=>"In this paper , we proposed a novel approach to the all-words WSD , focusing on the use of syntactic dependencies of word senses , and investigated the contribution of these dependencies to WSD .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16,17,18#15,16,17,18#para -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> [9]
nil_second --> [9, 15]
--------------------------
{1=>"In our experiments , the sense dependency features were shown to work effectively for WSD , with a 0 .29% , 0 .64% , and 0 .30% improvement of recalls for SemCor , Senseval-2 , and Senseval-3 data sets , respectively .\n", 2=>"In our experiments , the sense dependency features were shown to work effectively for WSD , with 0 .29% , 0 .64% , and 0 .30% improvements of recalls for SemCor , Senseval-2 , and Senseval-3 data sets respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#stem -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#40#exact -1#39#41#exact \n"}nil_first --> [17, 39]
nil_second --> []
--------------------------
{1=>"Despite the small improvements in overall figures , these improvements indeed correspond to 25%-57% improvements over the first sense baseline .\n", 2=>"Despite the small improvements in terms of overall figures , these improvements indeed correspond to 25%-57% improvements over the first sense baseline .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact \n"}nil_first --> []
nil_second --> [5, 6]
--------------------------
{1=>"The dependency tree structures were shown to be appropriate in modeling the dependencies of word senses , because the results of the tree-structured models outperformed the [results of ?] linear-chain models .\n", 2=>"The dependency tree structures was shown to be appropriate for modeling the dependencies of word senses , by the results that the tree-structured models outperformed the linear-chain models .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#syn -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact \n"}nil_first --> [9, 17, 20, 26, 27, 28]
nil_second --> [9, 17, 20]
--------------------------
{1=>"In the analysis section , we presented an in-depth analysis of the observed instances , and observed that the noun-noun dependencies particularly contribute to the positive instances .\n", 2=>"In the analysis section , we presented an in-depth analysis of the observed instances , and saw that the noun-noun dependencies particularly contribute to the positive instances .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [16]
nil_second --> [16]
--------------------------
{1=>"However , our experiments showed that even when combined with the coarse-grained tag sets , the sense dependency features do not improve the performance , unless combined with proper sense frequency information . This is due to the data sparseness problem .\n", 2=>"However , our experiments on the other hand showed that even when combined with the coarse-grained tag sets , the sense dependency features do not improve the performance unless combined with proper sense frequency information , due to the data sparseness problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23,24,25,26#19,20,21#para -1#5#22#exact -1#27#23#exact -1#35#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#42#32#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact \n"}nil_first --> [33, 34, 41]
nil_second --> [4, 6, 7]
--------------------------
{1=>"The supersense-based WSD models , on the contrary , exhibited the robustness [of ...] regardless of the existence of the sense frequency information , while they are defeated by the synset-based models in recalls .\n", 2=>"The supersense-based WSD models , on the contrary , exhibited the robustness regardless of the existence of the sense frequency information , while they are defeated by the synset-based models in recalls .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#14#exact -1#14,15,16,17#15,16,17,18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact \n"}nil_first --> [12, 13]
nil_second --> [13]
--------------------------
{1=>"These results show the importance of fine-grained and coarse-grained sense information , and show that the combination of both enables us to build a more precise and robust WSD system .\n", 2=>"These results show the importance of fine-grained and coarse-grained sense information , and that the combination of both enables us to build a precise and robust WSD system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24,25#para -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"Although our model was based on a simple framework , and was trained only on the SemCor corpus , the results that we gained were promising . They suggested that our model still has a great potential for improvement .\n", 2=>"Although our model was based on a simple framework and trained only on the SemCor corpus , the results we gained were promising , suggesting that our model still has a great potential for improvement .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#30#6#exact -1#7#7#exact -1#8#8#exact -1#16#9#exact -1#9#10#exact -1#21#11#syn -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#23#18#exact -1#17#19#exact -1#18#20#exact -1#25#21#exact -1#19#22#exact -1#20#23#exact -1#22#25#exact -1#35#26#exact -1#24#28#stem -1#26#30#exact -1#27#31#exact -1#28#32#exact -1#29#33#exact -1#31,32,33#34,35,36,37#para -1#34#38#exact \n"}nil_first --> [24, 27, 29, 39]
nil_second --> []
--------------------------
{1=>"Generating short summary videos for rushes is a challenging task due to the difficulty in eliminating redundancy and determining the important objects and events to be placed in the summary .\n", 2=>"Generating short summary videos for rushes is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12,13#12,13,14#para -1#14#16#exact -1#16#17#exact -1#17,18#18#para -1#26#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23,24,25#25,26,27#para -1#27#28,29#para -1#28#30#exact \n"}nil_first --> [15, 24]
nil_second --> [15]
--------------------------
{1=>"This makes approaches using one keyframe for a shot representation fail when trying to form a cluster .\n", 2=>"This makes approaches using one keyframe for shot representation failed in doing clustering .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#stem -1#12#16#stem -1#13#17#exact \n"}nil_first --> [7, 11, 12, 13, 14, 15]
nil_second --> [10, 11]
--------------------------
{1=>"In addition , even repetitive segments can be precisely determined , but the summary generated by concatenating together the selected segments still takes longer than the upper limit .\n", 2=>"In addition , even repetitive segments can be determined precisely , the summary generated by concatenating together selected segments still has longer duration than the upper limit .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#8#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#24#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#syn -1#21#23#exact -1#23#24#exact -1#25#25,26#para -1#26#27#exact -1#27#28#exact \n"}nil_first --> [11]
nil_second --> [22]
--------------------------
{1=>",We introduce two approaches to solve these problems .\n", 2=>"In this paper , we introduce two approaches to these problems .\n", 3=>"-1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact \n"}nil_first --> [0, 5]
nil_second --> [0, 1, 2, 3, 4]
--------------------------
{1=>"In the first approach , one keyframe is used for representing a shot when forming a cluster; and sub-segments are selected using the motion information for generating the summary .\n", 2=>"In the first approach , one keyframe is used for shot representation in doing clustering; and sub-segments are selected using motion information for generating the summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#17#10#syn -1#10#12#exact -1#15#17#exact -1#16#18#exact -1#18#19,20#para -1#19#21#exact -1#24#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#25#27,28#para -1#26#29#exact \n"}nil_first --> [11, 13, 14, 15, 16]
nil_second --> [11, 12, 13, 14]
--------------------------
{1=>"Meanwhile , in the second approach , all the frames of a given shot are used for clustering; and a simple skimming method is used to select the sub-segments .\n", 2=>"Meanwhile , in the second approach , all frames of a shot are used for clustering; and a simple skimming method is used to select sub-segments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#28#exact -1#26#29#exact \n"}nil_first --> [8, 12, 27]
nil_second --> []
--------------------------
{1=>"The experimental results on the TRECVID 2008 dataset and a comparison between the two approaches are also reported .\n", 2=>"Experimental results on the TRECVID 2008 dataset and comparison between the two approaches are reported .\n", 3=>"-1#10#0#lc -1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9,10#para -1#9#11#exact -1#11#12,13#para -1#12#14#exact -1#13#15#exact -1#14#17#exact -1#15#18#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"Video summarization significantly helps to meet this need by developing a condensed version of a full length digital video using only the most important contents \\CITE .\n", 2=>"Video summarization is a significant research that helps to meet these needs by developing a condensed version of a full length digital video with the most important contents \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3,4#2#para -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#11#7#stem -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact \n"}nil_first --> [6, 19, 20]
nil_second --> [2, 5, 6, 10, 23]
--------------------------
{1=>"Summary videos can help users more efficiently and effectively browse and navigate through large video archives .\n", 2=>"Summary videos can help users to browse and navigate large video archives efficiently and effectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12,13#5,6,7#para -1#14#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#13#exact -1#10#14#exact -1#11#15#exact -1#15#16#exact \n"}nil_first --> [12]
nil_second --> [5]
--------------------------
{1=>"Generating summary videos for BBC rushes \\CITE is a challenging task due to the difficulty with redundancy elimination and determining the most important objects and events to be placed in the summary .\n", 2=>"Generating summary videos for BBC rushes \\CITE is a challenging task due to difficulty in redundancy elimination and determination of important objects and events being placed in the summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#27#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18,19#19#para -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24,25,26#27,28,29#para -1#28#30,31#para -1#29#32#exact \n"}nil_first --> [15, 20, 21, 26]
nil_second --> [14]
--------------------------
{1=>"Since the length of the summary is limited to 2\\% duration of the original video , there is a trade-off between the recall and usability ( e.g. user friendly through smooth presentation , / being easy to understand ) .\n", 2=>"Since the length of the summary is limited to 2\\% duration of the original video , there is a trade-off between recall and usability ( e.g user friendly through smooth presentation , being easy to understand ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact \n"}nil_first --> [21, 26, 33]
nil_second --> [25]
--------------------------
{1=>"High recall , i.e. many objects and events ( called scenes ) included in the summary , usually reduces the number of frames for each scene .\n", 2=>"High recall , i.e many objects and events ( called scenes ) are included in the summary , usually reduce the number of frames for each scene .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19,20#18,19#para -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> [3]
nil_second --> [3, 12]
--------------------------
{1=>"For example , the maximum duration for a summary of a 30 minute video is 36 seconds ( \\MATH ) .\n", 2=>"For example , the maximum duration for the summary of a 30 minute length video is 36 seconds ( \\MATH ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"For an event such as \" `Woman attacks man on bench on left and runs off with large bag .\n", 2=>"For the event such as \" `Woman attacks man on bench on left and runs off with large bag .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [1]
nil_second --> [1]
--------------------------
{1=>"On the other hand , a smooth presentation of these events would consume a large number of frames , which would decrease the recall .\n", 2=>"On the contrary , smooth presentation of events consumes a lot number of frames , that decrease the recall .\n", 3=>"-1#0#0#exact -1#17#1#exact -1#1,2#2,3#para -1#3#4#exact -1#9#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#10#exact -1#8#12#stem -1#11,12#14,15,16#para -1#13#17#exact -1#14#18#exact -1#16#21#exact -1#18#23#exact -1#19#24#exact \n"}nil_first --> [9, 11, 13, 19, 20, 22]
nil_second --> [10, 15]
--------------------------
{1=>"Video segmentation : This step breaks down the original video into segments , such as shots or sub-shots .\n", 2=>"Video segmentation : This step decomposes the original video into segments , such shots or sub-shots .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [5, 6, 14]
nil_second --> [5]
--------------------------
{1=>"Each segment should be aligned so that it is a part of a scene .\n", 2=>"Each segment should be aligned such that it is a part of a scene .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10,11,12#9,10,11,12#para -1#13#13#exact -1#14#14#exact \n"}nil_first --> [5]
nil_second --> [5, 9]
--------------------------
{1=>"Redundancy elimination : This step groups the segments that belong to the same take into clusters .\n", 2=>"Redundancy elimination : This step groups segments that belong to the same take into clusters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"Junk elimination : This step removes the color bars , clapboards , and the all black or all white frames that are unnecessary in the final summary video .\n", 2=>"Junk elimination : This step removes color bars , clapboards , all black or all white frames that are unnecessary for the final summary video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#21#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#22#23,24,25#para -1#23#26#exact -1#24#27#exact -1#25#28#exact \n"}nil_first --> [12, 13]
nil_second --> [20]
--------------------------
{1=>"Summary generation : This step selects the frames from the representative segments of clusters and concatenates them to form the final summary video .\n", 2=>"Summary generation : This step selects frames from representative segments of clusters and concatenate to form the final summary video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#16#6#exact -1#6#7#exact -1#7#8#exact -1#8#9,10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#stem -1#14#17#exact -1#15#18#exact -1#17#19,20#para -1#18#21#exact -1#19#22#exact -1#20#23#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"While the steps for video segmentation and junk elimination are easy to handle , the steps for redundancy elimination and summary generation are difficult .\n", 2=>"While the steps of video segmentation and junk elimination are easy to handle , the steps of redundancy elimination and summary generation are difficult .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [3, 16]
nil_second --> [3, 16]
--------------------------
{1=>"For example , as for redundancy elimination , the question is how to represent a segment in a feature vector and how to compute the similarity between two segments having different lengths and motion patterns .\n", 2=>"For example , as for redundancy elimination , the question is how to represent a segment into a feature vector and how to compute the similarity between two segments having different length and motion pattern .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10,11,12#9,10,11#para -1#22#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#stem -1#32#32#exact -1#33#33#exact -1#34#34#stem -1#35#35#exact \n"}nil_first --> [16, 22]
nil_second --> [16]
--------------------------
{1=>"The question is how to determine the most important parts of the selected segments so that they convey as much of the information of the scene as possible .\n", 2=>"The question is how to determine the important part of the selected segment such that it conveys information of the scene as much as possible .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8,9#9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#stem -1#14#15#exact -1#16#17#stem -1#21#18#exact -1#22#19#exact -1#18#20#exact -1#19#21#exact -1#17#22#exact -1#20#24,25#para -1#23#26#exact -1#24#27#exact -1#25#28#exact \n"}nil_first --> [7, 14, 16, 23]
nil_second --> [13, 15]
--------------------------
{1=>"The first approach represents each segment by using one key-frame and groups similar segments by clustering them on these key-frames .\n", 2=>"The first approach represents each segment by one key-frame and groups similar segments by doing clustering on these key-frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [7, 16]
nil_second --> [14]
--------------------------
{1=>"Then the portion of each segment that has the highest motion is included in the final summary .\n", 2=>"Then the portion of each segment that has high motion is used to include into the final summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#15#8#exact -1#8#9#syn -1#9#10#exact -1#10#11#exact -1#11,12,13#12#para -1#16#13,14,15#para -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [14]
--------------------------
{1=>"Specifically , for each segment , a set of frames are extracted by sampling at a certain time interval ( e.g. 5 frames ) .\n", 2=>"Specifically , for each segment , a set of frames are extracted by sampling at a certain time interval ( e.g 5 frames ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [20]
nil_second --> [20]
--------------------------
{1=>"The clustering process is performed on the frames of all the segments .\n", 2=>"The clustering process is performed on the frames of all segments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Then , segments that share a large enough number of frames with respect to their size are merged into one cluster .\n", 2=>"Then , the segments that share a large enough number of frames with respect to their size are merged into one cluster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"In order to generate the final summary , with each representative segment , the middle part is selected with a skim rate of 2 frames .\n", 2=>"In order to generate the final summary , with each representative segment , the middle part is selected with the skim rate of 2 frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [19]
nil_second --> [19]
--------------------------
{1=>"Section \\REF describes our experimental results on the TRECVID 2008 dataset .\n", 2=>"Section \\REF describes experimental results on the TRECVID 2008 dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"By definition , all rushes are unedited; therefore they must consist of hard cuts only .\n", 2=>"From the definition , all rushes are unedited; therefore it must consist of hard cut only .\n", 3=>"-1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#10#8,9#para -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#stem -1#15#14#exact -1#16#15#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 9]
--------------------------
{1=>"The shot boundary detection algorithm in \\CITE is used to determine the shot boundary and to partition the input video into shots .\n", 2=>"The shot boundary detection algorithm in \\CITE is used to determine shot boundary and partition the input video into shots .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#15#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact \n"}nil_first --> [15, 17]
nil_second --> []
--------------------------
{1=>"Next , these values are sorted into ascending order .\n", 2=>"Next , these values are sorted into an ascending order .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"The sum of the middle eight of these 16 values is used to define the cut between frames \\MATH and \\MATH if these values exceed the threshold \\MATH .\n", 2=>"The sum of the middle eight of these 16 values are used to define a cut between frames \\MATH and \\MATH if these values exceed a threshold \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#13,14#13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#25,26#para -1#27#27#exact -1#28#28#exact \n"}nil_first --> []
nil_second --> [25]
--------------------------
{1=>"Therefore , if the algorithm detected a cut between frames \\MATH and \\MATH , whose magnitude is larger than the threshold \\MATH , these cuts are rejected since they are the motions of large objects .\n", 2=>"Therefore , if the algorithm detected a cut between frames \\MATH and \\MATH , whose magnitude is larger than a threshold \\MATH , these cuts are rejected since they are motions from large objects .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19,20#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30,31#para -1#32#32,33#para -1#33#34#exact -1#34#35#exact \n"}nil_first --> []
nil_second --> [19, 31]
--------------------------
{1=>"Finally , short shots of less than 25 frames ( 1 second ) are removed .\n", 2=>"Finally , the short shots with less than 25 frames ( 1 second ) are removed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#6,7#4,5,6#para -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact \n"}nil_first --> []
nil_second --> [2, 5]
--------------------------
{1=>"The first frame of the shot is chosen as the base frame \\MATH and the next frame \\MATH for a comparison .\n", 2=>"A first frame of the shot is chosen as the base frame \\MATH and next frame \\MATH for comparison .\n", 3=>"-1#9#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#0#19#lc -1#18#20#exact -1#19#21#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"The \\MATH distance used to compute the distance of the frame sequence until the sum of the sorted value of the lower eight is larger than the threshold \\MATH . //[distance / length?]\n", 2=>"The \\MATH distance used to compute the distance of frame sequence until the sum of the sorted value of lower eight is larger than a threshold \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#12#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#15#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20,21#para -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#26,27#para -1#26#28#exact -1#27#29#exact \n"}nil_first --> [16, 30, 31, 32]
nil_second --> [24]
--------------------------
{1=>"Finally , the short sub-shots of less than 25 frames are removed .\n", 2=>"Finally , the short sub-shots with less than 25 frames are removed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"We use the keyframe extraction algorithm proposed in \\CITE to extract the representative keyframes from each sub-shot .\n", 2=>"We employ a keyframe extraction algorithm proposed in \\CITE to extract the representative keyframes from each sub-shot .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#11#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"In this approach , the cosine distance is used to measure the difference between neighboring frames in each sub-shot .\n", 2=>"In this approach , cosine distance is used to measure the difference between neighboring frames in each sub-shot .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#10#4#exact -1#4#5#exact -1#5#6#exact -1#7,8,9#7,8,9,10#para -1#11#11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"We used the algorithm proposed in \\CITE by using the \\MATH distance to compute the histogram differences between any two neighboring blocks in each column .\n", 2=>"We employ the algorithm proposed in \\CITE by using \\MATH distance to compute histogram differences between any two neighboring blocks in each column .\n", 3=>"-1#0#0#exact -1#7,8#1#para -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#1#8#syn -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [7, 9, 14]
nil_second --> []
--------------------------
{1=>"Next , we sort these values into ascending order .\n", 2=>"Next , we sort these values into an ascending order .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"In rushes videos , there are many types of clapper boards , but the same type of clapper board is often used in the same movie .\n", 2=>"In rushes videos , there are many types of clapper boards , appearance but the same type of clapper boards is often used in the same movie .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#18,19#para -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> []
nil_second --> [12, 19]
--------------------------
{1=>"There are many types of clapper boards , such as scale , rotation , and illumination changes .\n", 2=>"The clapper boards have many types , such as scale , rotation , and illumination changes .\n", 3=>"-1#3#1#para -1#4#2#exact -1#5#3#exact -1#1#5#exact -1#2#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [0, 4]
nil_second --> [0]
--------------------------
{1=>"A set of 80 example keyframes of clapper boards were extracted from the development set and is used as a set of queries .\n", 2=>"A set of 80 example keyframes of clapper boards are extracted from the development set and used as a set of queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#syn -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16,17#para -1#17,18,19,20#18,19,20#para -1#21#22#exact -1#22#23#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"If the result of the NDK algorithm returns a match from a keyframe with a query then the sub-shot is defined as a clapper board sub-shot .\n", 2=>"If a result of the NDK algorithm returns a match between a keyframe with a query then we define the sub-shot is a clapper board sub-shot .\n", 3=>"-1#0#0#exact -1#19#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#1#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#20#18#exact -1#21#19#exact -1#17,18#20#para -1#11#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [10, 17, 21]
nil_second --> [10, 22]
--------------------------
{1=>"The unused keyframes containing story units for the generated video summary are removed .\n", 2=>"The unused keyframes containing of story units for generate video summary are removed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [7]
nil_second --> [4]
--------------------------
{1=>"However , rushes videos containing a repetitive story , such as a retake of scenes , are unedited .\n", 2=>"However , rushes videos containing of repetitive story , such as retake scenes , are unedited .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#5#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [5, 11]
nil_second --> []
--------------------------
{1=>"To efficiently create rushes videos , the repetitive contents must be eliminated .\n", 2=>"To create the efficiently of rushes videos , the repetitive contents must be eliminated .\n", 3=>"-1#0#0#exact -1#3#1#exact -1#1#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact \n"}nil_first --> []
nil_second --> [2, 4]
--------------------------
{1=>"With this characteristic in mind , a clustering technique can be used to separate the data into groups of similar contents .\n", 2=>"From this characteristic , clustering technique can be used to separate the data into groups of similar contents .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#5#exact -1#4#7#exact -1#5#8#exact -1#6,7,8#9,10,11,12#para -1#9,10,11#13,14#para -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact \n"}nil_first --> [0, 3, 4, 6]
nil_second --> [0]
--------------------------
{1=>"Each group , called a cluster , consists of contents that are similar between themselves and dissimilar to the contents of other groups .\n", 2=>"Each group , called cluster , consists of contents that are similar between themselves and dissimilar to contents of other groups .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17,18#18,19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"Figure \\REF shows an example of a clustering result .\n", 2=>"Figure \\REF shows an example of clustering result .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"So far , we have completely removed the unused contents from rushes videos and reduced any repetition of the story contents .\n", 2=>"So far , we completely remove the unused contents from rushes video and reduce repetition of the story contents .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#stem -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11,12#12,13#para -1#13#14#stem -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [4, 15]
nil_second --> []
--------------------------
{1=>"The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of a summary is 2\\% of the original video ) , less repetitive content , and must have as many objects and events as possible .\n", 2=>"The objective of rushes summarization at TRECVID 2008 is to generate short summaries ( the upper limit of the duration of summary is 2\\% of the original video ) , less repetitive of content , and must have many objects and events as possible .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15,16,17,18#15,16#para -1#24#17#exact -1#25#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#32#25#exact -1#26#26,27#para -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#42#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#43#43,44#para -1#44#45#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"To reach this objective , only the most important keyframes should be selected to generate a summary video .\n", 2=>"To reach this objective , the important keyframes should be selected to generate summary video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact \n"}nil_first --> [5, 7, 15]
nil_second --> []
--------------------------
{1=>"To generate a summary , we first compute its maximum duration in seconds \\MATH ,\n", 2=>"To generate summary , we first compute its maximum duration in seconds \\MATH ,\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"Third , merge the consecutive sub-shots in each cluster into shots and compute the priority of each shot based on the priority of the shot weighted duration and shot weighted average motion magnitude using the following equation : \\MATH</p>\n", 2=>"Third , merge consecutive sub-shots in each cluster into shots and compute the priority of each shot based on priority of shot weighted duration and shot weighted average motion magnitude using the following equation : \\MATH</p>\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#12#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#31#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19,20#20,21,22#para -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#32#34,35#para -1#33#36#exact -1#34#37#exact -1#35#38#exact \n"}nil_first --> [23]
nil_second --> []
--------------------------
{1=>"Fourth , sub-shots in the selected shot in descending order are sorted based on the average motion magnitude .\n", 2=>"Forth , sort sub-shots in the selected shot in descending order based on the average motion magnitude .\n", 3=>"-1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#2#11#stem -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [0, 10]
nil_second --> [0]
--------------------------
{1=>"Fifth , for each selected sub-shot , 25 frames ( 1 second ) around the middle are extracted to generate the final summary .\n", 2=>"Fifth , for each selected sub-shot , extract 25 frames ( 1 second ) around the middle to generate the final summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#7#17#stem -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"This system has some modifications from the system developed for the same task last year \\CITE .\n", 2=>"This system is adopted with some modifications from the system developed for the same task last year \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#para -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact \n"}nil_first --> []
nil_second --> [3, 4]
--------------------------
{1=>"Specifically , the original video is broken down into segments , which are shots with a hard cut transition .\n", 2=>"Specifically , the original video is decomposed into segments , which are shots with hard cut transition .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15,16#para -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [6, 7]
nil_second --> [6]
--------------------------
{1=>"These segments are further broken down into fragments so that each fragment represents a portion of a scene .\n", 2=>"These segments are further decomposed into fragments so that each fragment represents a portion of a scene .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [4, 5]
nil_second --> [4]
--------------------------
{1=>"In order to reduce the computation time , we only extract a subset of the frames from the original video by sampling it at a five frame interval ( i.e. extract frames 0 , 5th , 10th , and so on ) .\n", 2=>"In order to reduce the computation time , we only extract a subset of frames from the original video by sampling at a five frame interval ( i.e extract frames 0th , 5th , 10th , and so on ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14#exact -1#14#15#exact -1#15#16#exact -1#17#17,18#para -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#28#30#exact -1#29#31#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact \n"}nil_first --> [22, 29, 32]
nil_second --> [27, 30]
--------------------------
{1=>"The segment boundary , which is located at the hard cut transition , is determined by using a loose threshold on the Euclidean distance between two consecutive frames .\n", 2=>"The segment boundary , which is located at hard cut transition , is determined by using a loose threshold on the Euclidean distance between two consecutive frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#20#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"Meanwhile , the fragment boundary is determined by using a strict threshold to detect any dramatic motion .\n", 2=>"Meanwhile , the fragment boundary is determined by using a strict threshold to detect dramatic motion .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"Instead of selecting one keyframe to represent one fragment as many other systems do , we use all the frames of each fragment for the redundancy elimination .\n", 2=>"Instead of selecting one keyframe to represent one fragment as many other systems do , we use all frames of each fragment for redundancy elimination .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [18, 24]
nil_second --> []
--------------------------
{1=>"We use GreedyRSC \\CITE to do the clustering on the set of all the sampled frames extracted from the original video .\n", 2=>"We use GreedyRSC \\CITE to do clustering on the set of all sampled frames extracted from the original video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#6#exact -1#6#7#exact -1#7#8#exact -1#16#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#17#18,19#para -1#18#20#exact -1#19#21#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"The number of clusters is determined automatically using this method .\n", 2=>"The number of clusters is determined automatically by this method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"We compute the similarity between two fragments by counting the number of shared characters between two strings and being normalized to the size of each string .\n", 2=>"We compute the similarity value between two fragments by counting the number of shared characters between two strings and being normalized to the size of each string .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#22,23,24#20,21,22,23#para -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> []
nil_second --> [4, 21]
--------------------------
{1=>"If this value is larger than the threshold , these two segments are merged into one cluster .\n", 2=>"If this value is larger than a threshold , these two segments are merged into one cluster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"We found that this approach is more effective than the approach using one keyframe for one fragment since the more keyframes that are used , the more information is available to make the right decision .\n", 2=>"We found that this approach is more effective than the approach using one keyframe for one fragment since the more number of keyframes is used , the more information is available to make right decision .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#22#20#exact -1#23,24#22,23#para -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32,33#para -1#34#34#exact -1#35#35#exact \n"}nil_first --> [21]
nil_second --> [20, 21]
--------------------------
{1=>"We select junk frames such as color bar frames , and single color ( black or white ) frames to form the reference junk frame set .\n", 2=>"We select junk frames such as color bar frames , single color ( black or white ) frames to form the reference junk frame set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"To check whether a fragment is junk , we compare the frames of this fragment to the frames of the reference junk frame set .\n", 2=>"To check whether a fragment is a junk , we compare the frames of this fragment to the frames of the reference junk frame set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"We empirically select the thresholds for each type of junk .\n", 2=>"We empirically select thresholds for each type of junk .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4,5,6,7#5,6,7#para -1#8#9#exact -1#9#10#exact \n"}nil_first --> [3, 8]
nil_second --> []
--------------------------
{1=>"If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered junk and all the fragments of the cluster containing the junk fragment are eliminated .\n", 2=>"If the similarity between one frame in the input fragment and one frame in the reference junk frame set is lower than the predefined thresholds, the input fragment is considered as junk and all fragments of the cluster containing junk fragment are eliminated .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20,21,22#19,20,21#para -1#25#22#exact -1#23#23#exact -1#24#24#exact -1#36#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33,34#para -1#35#35#exact -1#37#36,37#para -1#38#38#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact \n"}nil_first --> [39]
nil_second --> [30]
--------------------------
{1=>"In our system, we only check the fragments that are located at the two ends of the original video for reducing the computation time .\n", 2=>"In our system, we only check fragments that are located at two ends of the original video for reducing computation time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#14#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12,13#para -1#12#14#exact -1#13#15#exact -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"For each cluster, we merge adjacent fragments into longer fragments and select the longest fragment as the representative fragment to be included in the final summary .\n", 2=>"For each cluster, we merge adjacent fragments into longer fragments and select the longest fragment as the representative fragment to be included in the final summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20,21,22#19,20,21,22#para -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"Since the length of these fragments is still larger than the maximum length of the final summary, we use the following simple strategy to shrink these fragments .\n", 2=>"Since the length of these fragments is still larger than the maximum length of the final summary, we employ a simple strategy to shrink these fragments as follows .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#syn -1#26,27#19,20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#28#27#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"Second, for each fragment, we extract the portion that is expanded from the central part of the fragment .\n", 2=>"Second, for each fragment, we extract the portion which is expanded from the central of the fragment .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14#para -1#14#15#exact -1#15#16#exact -1#17#18#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"This portion covers a duration twice the size of the fragment quota by selecting the frames with a sampling rate of two frames .\n", 2=>"This portion covers a duration twice as much as the fragment quota by selecting frames with sampling rate of 2 frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8#5,6#para -1#18#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#19#20,21#para -1#20#22#exact -1#21#23#exact \n"}nil_first --> [7, 14, 17]
nil_second --> []
--------------------------
{1=>"Specifically, we select frames \\MATH, \\MATH, ..., \\MATH, \\MATH, ..., \\MATH, \\MATH, where \\MATH is the middle frame of the fragment, and \\MATH is half the number of frames computed from the quota\\MATH and the frame rate ( 25fps ) \\MATH :\n", 2=>"Specifically, we select frames \\MATH, \\MATH, ..., \\MATH, \\MATH, ..., \\MATH, \\MATH, where \\MATH is the middle frame of the fragment, and \\MATH is half of number of frames computed from the quota\\MATH and frame rate ( 25fps ) \\MATH :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#31#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#32#32#exact -1#33#33#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact \n"}nil_first --> [31, 34]
nil_second --> [25]
--------------------------
{1=>"We have tested our approaches on 40 videos from the TRECVID 2008 test set .\n", 2=>"We have tested our approaches with 40 videos of TRECVID 2008 test set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7,8#7,8#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [5, 9]
nil_second --> [5]
--------------------------
{1=>"The NII-2system achieves a higher recall ( IN ) than the NII-1 system because NII-1 only uses one keyframe for each sub-shot and has a shorter duration ( DU ) for summary videos .\n", 2=>"The system NII-2 achieves higher recall ( IN ) than the system NII-1 since NII-1 only uses one keyframe for each sub-shot and has shorter duration ( DU ) for summary videos .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#1#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24,25#para -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [1, 3, 13]
nil_second --> [2, 11, 13]
--------------------------
{1=>"However, NII-1 has better quality .\n", 2=>"However, NII-1 has a better score in quality .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#7#4#exact -1#8#5#exact \n"}nil_first --> []
nil_second --> [3, 5, 6]
--------------------------
{1=>"The summary videos generated by NII-1 have less duplication ( RE ), are presented in a smoother way ( TE ), and are easy to judge for inclusions ( TT ) .\n", 2=>"The summary videos generated by NII-1 have fewer duplications ( RE ), are presented in a smoother way ( TE ) and are easy to judge for inclusions ( TT ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#para -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [20]
nil_second --> [20]
--------------------------
{1=>"The clapper board detection process using NDK consumes around half of the processing time of NII-1, but its performance is low due to the large variations in clapper boards in the videos ( see Figure \\REF ) .\n", 2=>"The clapper board detection process using NDK consumes around half of processing time of NII-1 but performance is low due to large variations of clapper boards in videos ( see Figure \\REF ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10#8,9,10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17,18#para -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23,24#para -1#22#25#exact -1#26#26#exact -1#24#27#exact -1#25#28#exact -1#27#30,31#para -1#28#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact \n"}nil_first --> [15, 29]
nil_second --> [14, 23]
--------------------------
{1=>"The comparable performance in the junk elimination of both systems suggests that simpler methods are more favorable .\n", 2=>"The comparable performance in junk elimination of both systems suggests that simple methods are more favorable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#syn -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"In addition, by using simple features and sampling frames in the original video, NII-2 significantly increases the processing time ( computed from the time the input video is taken to the time the summary video is picked ) to quasi real-time .\n", 2=>"In addition, by using simple features and sampling frames in the original video, NII-2 significantly speeds up the processing time ( computed from the time taking the input video to the time picking the summary video ) to quasi real-time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#25#28#syn -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#32#36#stem -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact \n"}nil_first --> [15, 27, 35]
nil_second --> [15, 16]
--------------------------
{1=>"Practical summarization systems usually have a good balance between the fraction of inclusions and user-friendliness .\n", 2=>"Practical summarization systems usually have good balance between fraction of inclusions and user-friendliness .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5,6#para -1#6#7#exact -1#7#8#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"In Table \\REF, we present the performance of such systems .\n", 2=>"In Table \\REF, we show performance of such systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#syn -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"The 14 systems listed in this table have an IN score that is above the median ( 0.45 ); and other scores, such as RE and TE, are larger than half of the maximum score ( 2.5 ) .\n", 2=>"The 14 systems listed in this table have IN score larger than the median ( 0.45 ); and other scores such as RE and TE larger than half of maximum score ( 2.5 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#10#28#exact -1#27,28#29,30,31,32#para -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact \n"}nil_first --> [8, 11, 12, 13, 21, 26, 27]
nil_second --> [11, 19, 24, 25, 26]
--------------------------
{1=>"Compared to the other systems listed in this table, our NII-2system is one of the fastest .\n", 2=>"Compared to other systems listed in this list, our system NII-2 is one of the fastest systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#14#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#11,12,13#11,12,13,14#para -1#15#15#exact -1#17#16#exact \n"}nil_first --> [8, 10]
nil_second --> [7, 9, 10, 16]
--------------------------
{1=>"Compared to the other systems participating in this task of TRECVID 2008, NII-1 performed better in such measures as DU and TT ( see Figure \\REF and Figure \\REF; while NII-2 performs well in the IN measure ( see Figure \\REF ) .\n", 2=>"Compared to the other systems participating in this task of TRECVID 2008, NII-1 has good performance in measures such as DU and TT ( see Figure \\REF and Figure \\REF; while NII-2 achieves good performance in measure IN ( see Figure \\REF ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#para -1#14#14#syn -1#16#15#exact -1#18#16#exact -1#17#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#15#31#stem -1#33#32#syn -1#35#33#exact -1#37#35#exact -1#36#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact \n"}nil_first --> [34]
nil_second --> [32, 34]
--------------------------
{1=>"One of the most difficult steps is redundancy elimination .\n", 2=>"One of most difficult steps is redundancy elimination .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"The lack of discriminative representation of the segments and robust clustering methods is the main reason \\CITE .\n", 2=>"Lack of discriminative representation of segments and robust clustering methods is the main reason \\CITE .\n", 3=>"-1#11#0#lc -1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#12#13,14#para -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"Fragmentation is where samples of one cluster are put into several different clusters .\n", 2=>"Fragmentation is the case that samples of one cluster are put into several different clusters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2#para -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"Therefore, it is necessary to develop robust methods for detecting repetitive segments .\n", 2=>"Therefore, it is necessary to develop robust methods for detection of repetitive segments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"Using all the frames of one segment instead of using one keyframe as proposed in NII-2 is one of the current efforts being made towards this end .\n", 2=>"Using all frames of one segment instead of using one keyframe as proposed in NII-2 is one of the efforts toward this direction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#18#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15,16,17#16,17,18,19#para -1#19#21#exact -1#20#24#stem -1#21#25#exact -1#23#27#exact \n"}nil_first --> [20, 22, 23, 26]
nil_second --> [22]
--------------------------
{1=>"Although the results are not as high as expected, we still believe that this approach is promising .\n", 2=>"Although the result is not very high as expected, we still believe that this approach is promising .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#7#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [7]
nil_second --> [5]
--------------------------
{1=>"In the first approach, NII-1, clustering the set of keyframes extracted from the sub-shots helps to eliminate redundancy .\n", 2=>"In the first approach, NII-1, redundancy elimination is done by doing clustering on the set of keyframes extracted from sub-shots .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#11#5#exact -1#13#6#exact -1#14#7#exact -1#15#8#exact -1#16#9#exact -1#17#10#exact -1#18#11#exact -1#19#13#exact -1#6#16#stem -1#5#17#exact -1#20#18#exact \n"}nil_first --> [12, 14, 15]
nil_second --> [7, 8, 9, 10, 12]
--------------------------
{1=>"With each representative segment of each cluster, the portion with the highest degree of motion is selected to form the summary .\n", 2=>"With each representative segment of each cluster, the portion that has high degree of motion is selected to form the summary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#19#10#exact -1#11#11#syn -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19,20#para -1#21#21#exact \n"}nil_first --> [9]
nil_second --> [9, 10]
--------------------------
{1=>"This approach has a good usability score but is not very good at recall .\n", 2=>"This approach achieves good performance in usability score but low performance in recall .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [2, 3, 8, 9, 10, 11, 12]
nil_second --> [2, 4, 5, 9, 10, 11]
--------------------------
{1=>"In the second approach, NII-2, all the frames of each sub-shot are used to compute the similarity among the sub-shots in the clustering process .\n", 2=>"In the second approach, NII-2, all frames of each sub-shot are used to compute the similarity among sub-shots in clustering process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#14#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact \n"}nil_first --> [15, 18, 21]
nil_second --> []
--------------------------
{1=>"With each representative segment of each cluster, the middle part is selected to form the summary with a skipping rate of two frames .\n", 2=>"With each representative segment of each cluster, the middle part is selected to form the summary with skipping rate of 2 frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#syn -1#21#22#exact -1#22#23#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"This approach is good for recall and has a reasonably good usability score .\n", 2=>"This approach achieves good performance in recall and reasonable performance in usability score .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#6#5#exact -1#7#6#exact -1#8#8,9#para -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [2, 4, 7, 10]
nil_second --> [2, 4, 5, 9, 10]
--------------------------
{1=>"Compared to other systems participating in the TRECVID 2008 summarization task, NII-2 is among the best systems with a good balance between recall and usability .\n", 2=>"Compared to other systems participating in TRECVID 2008 summarization task, NII-2 is among best systems that have good balance between recall and usability .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14,15#para -1#14#16#exact -1#17#17,18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [6]
nil_second --> [15, 16]
--------------------------
{1=>"Face Retrieval Improvement by the Learning of Visual Consistency\n", 2=>"Face Retrieval Improvement by Learning Visual Consistency\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact \n"}nil_first --> [4, 6]
nil_second --> []
--------------------------
{1=>"Searching for images of people is one of the essential tasks required by users for image and video search engines .\n", 2=>"Searching persons is one of the essential tasks required by users for image and video search engines .\n", 3=>"-1#0#0#exact -1#11#1#exact -1#12#2#stem -1#4#3#exact -1#1,2#4,5#para -1#3#6#exact -1#5,6#7,8,9#para -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact \n"}nil_first --> [14, 15]
nil_second --> []
--------------------------
{1=>"However , the current search engines have limited capabilities for this task since they usually rely on texts associated with image and video , which are likely to return many irrelevant results .\n", 2=>"However , the current search engines have limited capabilities for this task since they usually rely on texts associated with image and video which are likely to return many irrelevant results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#25,26#24,25,26,27#para -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [23]
nil_second --> [23, 24]
--------------------------
{1=>"We propose a method to effectively retrieve relevant faces for one person by learning visual consistency from results retrieved from text correlation based search engines .\n", 2=>"In this paper , we propose a method to effectively retrieve relevant faces for one person by learning visual consistency from results retrieved from text correlation based search engines .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"This problem is challenging because ( i ) there is no label provided making it difficult to use supervised-based ranking methods .\n", 2=>"This problem is challenging because ( i ) no any label is provided leading to be difficult to use supervised-based ranking methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8,9#para -1#8#10#exact -1#10#11#exact -1#12#12#exact -1#16,17#14,15,16#para -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> [13]
nil_second --> [9, 13, 14, 15]
--------------------------
{1=>"( ii ) current face recognition techniques are still immature with wild-face databases even with supervised learning methods .\n", 2=>"( ii ) current face recognition techniques are still unmatured with wild-face databases even with supervised learning methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"In the proposed method , we treat this problem as a classification problem in which input faces are classified as 'person-X' ( the queried person ) or 'non-person-X' , and the faces are ranked based on their relevant score inferred from the classifier 's probability output .\n", 2=>"In the proposed method , we treat the problem as a classification problem which input faces are classified as 'personX' ( the queried person ) or 'non-personX' and the faces are ranked based on their relevant score that is inferred from the classifier 's probability output .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact \n"}nil_first --> [20, 27, 28]
nil_second --> [19, 26, 37, 38]
--------------------------
{1=>"To train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers , which are trained using different subsets .\n", 2=>"In order to train this classifier , we use a bagging-based framework to combine results from multiple weak classifiers which are trained using different subsets .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> [17]
nil_second --> [0, 1]
--------------------------
{1=>"Experimental results on various face sets retrieved from the captions of news photos show that the retrieval performance improved after each iteration with the final performance outperforming the baseline algorithms .\n", 2=>"Experimental results on various face sets retrieved from the caption of news photos show that the retrieval performance is improved after each iteration leading the final performance outperforms the baseline algorithms .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#24,25#22,23,24#para -1#26#25#exact -1#27#26#stem -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact \n"}nil_first --> []
nil_second --> [18, 23]
--------------------------
{1=>"With the rapid growth of digital technology , large image and video databases are more available than ever to users .\n", 2=>"With the rapid growing of digital technology , large image and video databases are available easier than ever to users .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> []
nil_second --> [14]
--------------------------
{1=>"Therefore , effective and efficient tools are needed for indexing and retrieving based on visual contents .\n", 2=>"Therefore , effective and efficient tools are strongly needed for indexing and retrieving based on visual contents .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"A typical example for this application is searching for a specific person by providing his or her name .\n", 2=>"One of the typical examples for this application is to search a specific person by providing his or her name .\n", 3=>"-1#11#0#lc -1#3#1#exact -1#4#2#stem -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#10#7#stem -1#12#9,10#para -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact \n"}nil_first --> [8]
nil_second --> [0, 1, 2, 9]
--------------------------
{1=>"Usually , most current search engines use the texts associated with images or videos as significant clues for returning results .\n", 2=>"Usually , most of current search engines use text associated with images or videos as a significant clue to return the results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#20#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15#exact -1#17#16#stem -1#19#18#stem -1#21#19#exact -1#22#20#exact \n"}nil_first --> [17]
nil_second --> [3, 15, 18]
--------------------------
{1=>"However , other un-queried faces and names appear simultaneously and are aligned ( as shown in Figure \\REF ) , which significantly lowers retrieval performance .\n", 2=>"However , since it is not necessary faces and names appear simultaneously and are aligned ( as shown in Figure \\REF ) , the main drawback of this approach is existence of many irrelevant results that makes the retrieval performance very low .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#38#23#exact -1#39#24#exact -1#42#25#exact \n"}nil_first --> [2, 3, 20, 21, 22]
nil_second --> [2, 3, 4, 5, 6, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41]
--------------------------
{1=>"Therefore , it is necessary to improve the retrieval performance by taking into account the visual information from the retrieved faces .\n", 2=>"Therefore it is necessary to improve the retrieval performance by taking into account visual information from the retrieved faces .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10,11,12#11,12,13,14#para -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"-Large variations in face appearance due to pose changes , illumination conditions , occlusions , and facial expressions make face recognition difficult even with state of the art techniques \\CITE .\n", 2=>"-Large variations in face appearance due to pose changes , illumination conditions , occlusions and facial expressions make face recognition difficult even with state of the art techniques \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23,24,25#24,25,26,27#para -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [14]
nil_second --> [26]
--------------------------
{1=>"-The fact the retrieved face set consists of faces of several people with no label makes supervised learning methods as well as unsupervised learning methods such as , \\MATH -means , inapplicable .\n", 2=>"-The fact the retrieved face set consists of faces of several persons while no any label is given makes supervised learning methods as well as unsupervised learning methods such as \\MATH -means inapplicable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11,12#para -1#13#13#exact -1#15#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#28#exact -1#31#29#exact -1#32#31#exact -1#33#32#exact \n"}nil_first --> [27, 30]
nil_second --> [12, 14, 16, 17]
--------------------------
{1=>"We propose a method to solve the above-mentioned problem .\n", 2=>"In this paper , we propose a method to solve the mentioned problem .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#12#8#exact -1#13#9#exact \n"}nil_first --> [7]
nil_second --> [0, 1, 2, 3, 11]
--------------------------
{1=>"The main idea is to assume that there is visual consistency among the results returned from current text-based search engines .\n", 2=>"The main idea is to learn visual consistency assumed to exist among the results returned from current text-based search engines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#5#stem -1#10#7,8#para -1#6#9#exact -1#7#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [6]
nil_second --> [5, 9]
--------------------------
{1=>"This stage is based on the observation that facial images of the queried person tend to form dense clusters while irrelevant facial images are sparse since they look different from each other .\n", 2=>"This stage is stemmed from the observation that faces relevant to the queried person tend to form dense clusters while irrelevant faces are very sparse since they look different from each other .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#syn -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [4, 8, 9, 10, 21, 22]
nil_second --> [4, 8, 9, 10, 21, 23]
--------------------------
{1=>"The output is a rank list in which faces with larger number of neighbors within a certain distance are considered as relevant and are therefore put at the top of the list . //[What do you mean by �gneighbors�h ? Do you mean the un-queried faces ? ]\n", 2=>"The output is a rank list in which faces having larger number of neighbors within a distance are predicted as relevant ones and therefore are put on the top .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#19#20#exact -1#20#21#exact -1#22#22#exact -1#24#23#exact -1#23#24#exact -1#25#25#exact -1#27#27#exact -1#28#28#exact -1#29#32#exact \n"}nil_first --> [9, 16, 19, 26, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
nil_second --> [9, 18, 21, 26]
--------------------------
{1=>"Since the above ranking method is based on the number of neighbors , it is sensitive to the specified distance .\n", 2=>"Since the above ranking method is based on the number of neighbors , it is sensitive to the chosen distance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8#5,6,7#para -1#17#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [17, 18]
nil_second --> [18]
--------------------------
{1=>"A second stage is necessary to improve this rank list .\n", 2=>"It is necessary to use the second stage to improve the rank list .\n", 3=>"-1#5,6#0,1#para -1#7#2#exact -1#0,1,2,3#3,4#para -1#8#5#exact -1#9#6#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact \n"}nil_first --> [7]
nil_second --> [4, 10]
--------------------------
{1=>"We model this problem as a classification problem in which input faces are classified as person-X ( the queried person ) or non-person-X ( the irrelevant person ) .\n", 2=>"We model this problem as a classification problem which input faces are classified as personX ( the queried person ) or non-personX ( the irrelevant person ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [15, 22]
nil_second --> [14, 21]
--------------------------
{1=>"This subset then is used to train a classifier using supervised methods such as support vector machines ( SVM ) .\n", 2=>"This subset then is used to train a classifier using a supervised method such as support vector machines ( SVM ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#stem -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"The trained classifier is used to re-rank faces in the original input set .\n", 2=>"The trained classifier is used to re-rank faces in the original input set again .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"This stage is effective for improving the rank list for the following reasons :\n", 2=>"This stage is effective for improving the rank list due to the following reasons :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11,12#9,10,11#para -1#13#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [9, 10]
--------------------------
{1=>"-Supervised learning methods such , as SVMs , provide a strong theoretical background in finding optimal decision boundary even with existence of noisy data .\n", 2=>"-Supervised learning methods such as SVM have strong theoretical background in finding optimal decision boundary even with existence of noisy data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact \n"}nil_first --> [4, 6, 7, 8, 9]
nil_second --> [5, 6]
--------------------------
{1=>"Furthermore , recent studies suggest that \\CITE SVM classifiers provide probability outputs that are suitable for ranking .\n", 2=>"Furthermore , with recent studies \\CITE SVM classifiers can provide probability outputs that are suitable for ranking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#12#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [4]
nil_second --> [2, 8]
--------------------------
{1=>"-We propose a general framework to boost the face retrieval performance from results retrieved from text correlation-based search engines by the learning of visual consistency .\n", 2=>"-We propose a general framework to boost the face retrieval performance from the results retrieved from text correlation based search engines by learning visual consistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#12#20#exact -1#22#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [16, 22]
nil_second --> [17, 18]
--------------------------
{1=>"It seamlessly integrates current data mining methods such as outlier detection , supervised learning , and unsupervised learning based on bagging for a practical problem . //[What or who is �glearning�h visual consistency ? Are the search engines learning ? ]\n", 2=>"It integrates seamlessly current existing data mining methods such as outliers detection , supervised learning and unsupervised learning based on bagging for a practical problem .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#1#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#stem -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#4#29#syn \n"}nil_first --> [14, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
nil_second --> []
--------------------------
{1=>"There are several more proposed approaches for general object classification than for those for face retrieval .\n", 2=>"There are several approaches proposed for general object classification rather than for face retrieval .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#3#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#10#exact -1#11#11#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [3, 12, 13]
nil_second --> [9]
--------------------------
{1=>"For example , as described in \\CITE , objects are retrieved by an image search engine and then are re-ranked by the learning of visual consistencies from the retrieved objects .\n", 2=>"For example , as described in \\CITE , objects are retrieved by an image search engine and then are re-ranked by learning visual consistencies from the retrieved objects .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#25#21#exact -1#21#22#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [23, 27]
nil_second --> []
--------------------------
{1=>"Compared to the problem of face-based recognition , the problem of object classification is easier since classification of different object types such as airplane and non-airplane only needs to handle inter-variations between different categories , while discriminating between person-A and person-B requires handling of both intra-variations and inter-variations of the same category .\n", 2=>"Compared to the problem of face retrieval based recognition , the problem of object classification is easier since classification of different object types such as airplane and non-airplane only needs to handle inter-variations between different categories while discriminating personA and personB requires to handle both intra-variations and inter-variations of the same category .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#35#exact -1#37#36#exact -1#39#39#exact -1#41#41#exact -1#48#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#49,50#48,49,50#para -1#51#51#exact -1#52#52#exact \n"}nil_first --> [5, 34, 37, 38, 40, 42]
nil_second --> [5, 6, 7, 38, 40, 42, 43]
--------------------------
{1=>"Furthermore , in order to work in unsupervised mode , these approaches need a method to collect negative samples ( e.g. non-airplane ) , which are inapplicable to our problem .\n", 2=>"Furthermore , in order to work in unsupervised mode , these approaches need a method to collect negative samples ( e.g. non-airplane ) which are inapplicable in our problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [23, 27]
nil_second --> [26]
--------------------------
{1=>"A graph-based approach was proposed by \\CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .\n", 2=>"Working closely to our problem , in \\cite{Ozkan06CVPR} , a graph based approach was proposed \\CITEin which a graph is formed by faces as nodes and weights of edges linked between nodes are the similarity of faces .\n", 3=>"-1#9#0#lc -1#12#2#exact -1#13#3#exact -1#14#4#exact -1#21#5#exact -1#5#7#exact -1#6#8#exact -1#16#9#exact -1#17#10#exact -1#18#11#exact -1#19#12#exact -1#20#13#exact -1#22#15#exact -1#23#16#exact -1#24#17#exact -1#8#18#exact -1#25#19#exact -1#33#20#exact -1#26#21#exact -1#27#22#exact -1#28#23#exact -1#29#24#exact -1#30#25#exact -1#31#26#exact -1#32#27#exact -1#34#29#exact -1#35#30#exact -1#36#31#exact -1#1#33,34#para -1#2#36#exact -1#3#37#exact -1#4#38#exact -1#37#39#exact \n"}nil_first --> [1, 6, 14, 28, 32, 35]
nil_second --> [0, 7, 10, 11, 15]
--------------------------
{1=>"Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph with an available solution . //[Do graphs have solutions ? They just provide information .]\n", 2=>"By assuming that the number of faces of the queried person are larger than that of other persons , and these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph whose solution is available .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#38#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15,16#para -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#55#36#exact -1#39#37#exact -1#40#38#exact -1#42,43,44#39,40,41,42#para -1#45#43#exact -1#46#44#exact -1#47#45#exact -1#48#46#exact -1#49#47#exact -1#50#48#exact -1#51#49#exact -1#52#50#exact -1#56#53#exact -1#54#54#exact -1#57#55#exact -1#11#58#para \n"}nil_first --> [17, 51, 52, 56, 57, 59, 60, 61, 62, 63, 64, 65]
nil_second --> [0, 17, 18, 19, 41, 53]
--------------------------
{1=>"Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to dimensionality .\n", 2=>"Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to the curse of dimensionality .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#26#23#exact -1#27#24#exact \n"}nil_first --> []
nil_second --> [23, 24, 25]
--------------------------
{1=>"Although the result was impressive , it is not easy to apply it to our problem since a large number of irrelevant faces ( more than 12% ) are eliminated manually before performing clustering .\n", 2=>"Although the result was impressive , it is not easy to apply for our problem since a large number of irrelevant faces ( more than 12% ) are eliminated manually before doing clustering .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9,10#6,7,8,9#para -1#11#11#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17,18,19#17,18,19,20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [10, 12, 13, 32]
nil_second --> [12, 16, 31]
--------------------------
{1=>"Section \\REF briefly introduces typical outlier detection methods .\n", 2=>"Section \\REF introduce briefly typical outliers detection methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#8#8#exact \n"}nil_first --> [3]
nil_second --> [2]
--------------------------
{1=>"Given a set of faces returned by any text-based correlation search engine , our method is used to perform a ranking process summarized as follows :\n", 2=>"Given a set of faces returned by any text-based correlation search engine , our method performs a ranking process summarized as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#21#15#syn -1#15#18#stem -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#22#25#exact \n"}nil_first --> [16, 17, 24]
nil_second --> []
--------------------------
{1=>"-Step 4 : Train an ensemble classifier \\MATH using this rank list by Bag-Rank-SVM .\n", 2=>"-Step 4 : Train a ensemble classifier \\MATH using this rank list by Bag-Rank-SVM .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"-Step 6 : Repeat steps 4 and 5 $T$ times and return ranked faces produced by the last classifier \\MATH to users .\n", 2=>"-Step 6 : Repeat steps from 4 and 5 $T$ times and return ranked faces produced by the last classifier \\MATH to users .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"Steps 1 and 2 are typical for any face processing system and described in detail in \\REF .\n", 2=>"Steps from 1 and 2 are typical for any face processing system and described in details in \\REF .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#stem -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"Step 3 used to find initial ranks for faces described in \\REF .\n", 2=>"Step 3 used to find initial ranks for faces is described in \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"-Step 4 : Repeat steps Step 1 to Step 3 \\MATH times .\n", 2=>"-Step 4 : Repeat steps from Step 1 to Step 3 \\MATH times .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"Since it is not guaranteed that the top \\MATH and bottom \\MATH of faces in the rank list correctly correspond to the faces of the queried person-\\MATH and faces of non person-\\MATH as shown in Figure \\REF , randomly selecting subsets to train weak classifiers , and then combining these classifiers might help reduce the risk of using noisy training sets .\n", 2=>"Since it is not guaranteed top \\MATH and bottom \\MATH of faces in the rank list are correctly correspondent to faces of the queried person \\MATH and faces of non person \\MATH as shown in Figure \\REF , selecting randomly subsets to train weak classifiers and then combining these classifiers might help to reduce risk of using noisy training sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#13#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#22#15#exact -1#14#16#exact -1#15#17#exact -1#17#18#exact -1#18#19#stem -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#23#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#39#38#exact -1#38#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#53#53#exact -1#54,55#54,55,56#para -1#56#57#exact -1#57#58#exact -1#58#59#exact -1#59#60#exact -1#60#61#exact \n"}nil_first --> [5, 21, 24, 26, 31, 45]
nil_second --> [16, 24, 25, 30, 31, 52]
--------------------------
{1=>"We introduce two common outlier detection methods , distance-based outlier detection ( DBO ) \\CITE and local outlier factor-based method ( LOF ) \\CITE .\n", 2=>"We introduce here two common outliers detection methods including distance-based outliers detection( DBO ) \\CITE and local outliers factor based method ( LOF ) \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#stem -1#6#5#exact -1#7#6#exact -1#9#8#exact -1#10#9#stem -1#21#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#stem -1#20#19#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> [7, 10, 18, 20]
nil_second --> [2, 8, 11, 18, 19]
--------------------------
{1=>"Adapting the definition from Knorr \\CITE , given a set of objects \\MATH , an object \\MATH is considered as an outlier if there are fewer than \\MATH neighboring objects in \\MATH lying within a distance \\MATH .\n", 2=>"Adapting the definition \\CITE , given a set of objects \\MATH , an object \\MATH is considered as an outliers if there are fewer than \\MATH neighboring objects in \\MATH lying within a distance \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#stem -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [3, 4]
nil_second --> []
--------------------------
{1=>"This outlier detection process is summarized as follows :\n", 2=>"The outliers detection process is summarized as follows :\n", 3=>"-1#1#1#stem -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact \n"}nil_first --> [0]
nil_second --> [0]
--------------------------
{1=>"-Step 2 : For each object , compute \\MATH , which is the number of neighboring objects lying within a distance \\MATH .\n", 2=>"-Step 2 : For each object , compute \\MATH which is the number of neighboring objects lying within a distance \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11,12#10,11,12,13#para -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [9]
nil_second --> [9, 10]
--------------------------
{1=>"In our experiments , the distance between two objects is the Euclidean distance between two faces and is computed in the eigen-subspace ( described in section \\REF ) .\n", 2=>"In our experiments , the distance between two objects is Euclidean distance between two faces and is computed in the eigen-subspace ( described in section \\REF ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#19#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"-Step 1 : For each data object \\MATH compute the \\MATH ( the distance to the \\MATH nearest neighbor ) and \\MATH ( all points in a \\MATH sphere ) .\n", 2=>"-Step 1 : For each data object \\MATH compute \\MATH ( the distance to the \\MATH nearest neighbor ) and \\MATH ( all points in a \\MATH sphere ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9#exact -1#9#10#exact -1#10#11#exact -1#14#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"- Step 2 : Compute the reachability distance for each data object \\MATH with respect to data object \\MATH as : \\MATH , where \\MATH is the distance from data object \\MATH to data object \\MATH .\n", 2=>"- Step 2 : Compute reachability distance for each data object \\MATH with respect to data object \\MATH as : \\MATH , where \\MATH is distance from data object \\MATH to data object \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26,27#para -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"-Step 3 : Compute local reachability density of data object \\MATH as inverse of the average reachability distance based on the \\MATH ( minimum number of data objects ) of the nearest neighbors to data object \\MATH .\n", 2=>"-Step 3 : Compute local reachability density of data object \\MATH as inverse of the average reachability distance based on the \\MATH ( minimum number of data objects ) nearest neighbors of data object \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#31#29#exact -1#29#31#exact -1#30#32#exact -1#32#33,34#para -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"-Step 4 : Compute LOF of data object \\MATH as the average of the ratios of the local reachability density of data object \\MATH and local reachability density of \\MATH of nearest neighbors .\n", 2=>"-Step 4 : Compute LOF of data object \\MATH as average of the ratios of the local reachability density of data object \\MATH and local reachability density of \\MATH nearest neighbors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#10#exact -1#10#11#exact -1#11#12#exact -1#15#13#exact -1#13#14#exact -1#14#15#exact -1#16#16,17#para -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"Using a robust face detector , 44 , 773 faces were detected and normalized to the size of 86\\MATH86 pixels .\n", 2=>"Using a robust face detector , 44 , 773 faces were detected and normalized to the size of 86\\MATH86 pixels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15,16,17#14,15,16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> []
nil_second --> [14]
--------------------------
{1=>"We selected sixteen government leaders including George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals such as John Paul II ( the Former Pope ) and Kofi Annan and Hans Blix ( UN ) since their images appeared frequently in the dataset \\CITE .\n", 2=>"We selected sixteen celebrities who are government leaders such as George W . Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key persons such as John Paul II ( the Former Pope ) , Kofi Annan and Hans Blix ( UN ) . These persons are selected since their appearances are highly frequent in the dataset \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#7#4#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#35#31#exact -1#36#32#exact -1#37#33#exact -1#38#34#exact -1#39#35#exact -1#40#36#exact -1#41#37#exact -1#42#38#exact -1#43#39#exact -1#44#40#exact -1#45#41#exact -1#46#42#exact -1#47#43#exact -1#48#44#exact -1#49#45#exact -1#50#46#exact -1#51#47#exact -1#52#48#exact -1#53#49#exact -1#54#50#exact -1#55#51#exact -1#56#52#exact -1#57#53#syn -1#58#54#exact -1#60,61,62#55,56,57,58#para -1#63#59#exact -1#64#60#exact -1#65#61#exact -1#66#62#exact -1#67#63#exact -1#71#64#exact -1#69#65#exact -1#70#66#exact -1#72#68#exact -1#73#69#exact -1#74#70#exact -1#75#71#exact -1#76#72#exact -1#82#73#exact -1#83#74#exact -1#84#76#stem -1#87,88#77,78#para -1#89#79#exact -1#90#80#exact -1#91#81#exact -1#92#82#exact \n"}nil_first --> [5, 67, 75]
nil_second --> [3, 4, 5, 8, 9, 59, 68, 77, 78, 79, 80, 81, 85, 86]
--------------------------
{1=>"In total , 3 , 907 faces were retrieved in which 2 , 094 faces were relevant .\n", 2=>"In total , 3 , 907 faces are retrieved in which 2 , 094 faces are relevant .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14,15#7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#7#15#syn -1#16#16#exact -1#17#17#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"On average , the precision was 52.49% . //[precision / accuracy ? ]\n", 2=>"On average , the precision is 52.49% .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#syn -1#6#6#exact -1#7#7#exact \n"}nil_first --> [8, 9, 10, 11, 12]
nil_second --> []
--------------------------
{1=>"To compensate for illumination effects , the subtraction of the best-fit brightness plane followed by histogram equalization was applied .\n", 2=>"To compensate for illumination effects , the subtraction of the bestfit brightness plane followed by histogram equalization was applied .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"We then used principle component analysis \\CITE to reduce the number of dimensions of the feature vector for face representation .\n", 2=>"We then used PCA \\CITE to reduce the number of dimensions of the feature vector for face representation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#6#exact -1#6,7,8,9#7,8,9,10,11#para -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [3, 4, 5]
nil_second --> [3, 5]
--------------------------
{1=>"Eigenfaces were computed from the original face set returned by the text-based query method .\n", 2=>"Eigenfaces were computed from the original face set returned by the text based query method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> [11]
nil_second --> [11, 12]
--------------------------
{1=>"A number of eigenfaces was selected so that 97% of the total energy was retained \\CITE . //[What is that number ? ]\n", 2=>"The number of eigenfaces was selected so that 97% of the total energy are retained \\CITE .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#syn -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [0, 17, 18, 19, 20, 21, 22]
nil_second --> [0]
--------------------------
{1=>"We evaluated the retrieval performance with measures that are commonly used in information retrieval such as precision , recall , and average precision .\n", 2=>"We evaluated the retrieval performance with measures that are popularly used in information retrieval such as precision , recall and average precision .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"Given a queried person , assuming that \\MATH is the total number of faces returned , \\MATH is the number of relevant faces , \\MATH is the number of relevant faces , we calculated recall and precision as follows : //[Nrel and Nhit are exactly the same here . They should be different .]\n", 2=>"Given a queried person , assuming that \\MATH is the total number of faces returned , \\MATH is the number of relevant faces , \\MATH is the number of relevant faces , we calculate recall and precision as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10,11,12#9,10,11#para -1#20#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#28#20#exact -1#29#21#exact -1#30#22#exact -1#31#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#21#29#exact -1#22#30#exact -1#23#31#exact -1#32#32#exact -1#33#33#stem -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}nil_first --> [28, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
nil_second --> []
--------------------------
{1=>"To evaluate ranked lists , average precision is used .\n", 2=>"To evaluate ranked lists , the average precision is used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0.0 , 0.1 , 0.2 , ... , 1.0 .\n", 2=>"The average precision is computed by taking average of the interpolated precision measured at the 11 recall levels of 0.0 , 0.1 , 0.2 , ... , 1.0 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#7#exact -1#7#8#exact -1#8#9#exact -1#14#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .\n", 2=>"In addition , to evaluate performance of multiple queries , we used mean average precision that is the mean of average precisions computed from queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#17#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15,16#17,18#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [16, 19]
nil_second --> []
--------------------------
{1=>"We show in Figure \\REF the retrieval performance of the outlier detection methods and the baseline method using text correlation .\n", 2=>"We show in Figure \\REF the retrieval performance of outliers detection methods and the baseline method using text correlation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#13#9#exact -1#9#10#stem -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"In the baseline method , faces were sorted by the time the associated news article was published .\n", 2=>"In the baseline method , faces are sorted by the time that the associated news article is published .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#syn -1#7#7#exact -1#8#8#exact -1#9,10,11,12#9,10,11#para -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#17#15,16#para -1#18#17#exact \n"}nil_first --> []
nil_second --> [16]
--------------------------
{1=>"We studied the effect of choosing the number of times \\MATH appeared in the Bag-Rank-SVM algorithm .\n", 2=>"We studied effect of choosing number of times \\MATH in the Bag-Rank-SVM algorithm .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#10#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5,6#6,7,8#para -1#7#9#exact -1#8#10#exact -1#9#12#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact \n"}nil_first --> [11, 13]
nil_second --> []
--------------------------
{1=>"The subsets \\MATH and \\MATH were generated by randomly selecting with replacement 70% samples of \\MATH and \\MATH .[�gWith replacement�h does not make sense here . I am not sure what you want to say .]\n", 2=>"The subsets \\MATH and \\MATH are generated by randomly selecting with replacement 70% samples of \\MATH amd \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#25#exact \n"}nil_first --> [16, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
nil_second --> [16]
--------------------------
{1=>"Figure \\REF shows the performance of single and ensemble classifiers .\n", 2=>"Figure \\REF shows performance of single classifiers and ensemble classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [3]
nil_second --> [6]
--------------------------
{1=>"We set the number of iterations for the Bag-Rank-SVM algorithm at five and set the number of iterations of the outer loop $T=30$ to see how much the final performance changes .\n", 2=>"We set the number of iterations for the Bag-Rank-SVM algorithm being 5 and set the number of iterations of the outer loop $T=30$ to see how much the final performance changes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#syn -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"As shown in Figure \\REF , the performance did not change much after five iterations .\n", 2=>"As shown in Figure \\REF , the performance does not change so much after 5 iterations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10#8,9,10#para -1#12#11#exact -1#13#12#exact -1#14#13#syn -1#15#14#exact -1#16#15#exact \n"}nil_first --> []
nil_second --> [8, 11]
--------------------------
{1=>"The performance of different methods shown in Figure \\REF indicates that our proposed method outperformed the distance-based outlier detection method and performed comparable to the supervised method using 5% annotation data .\n", 2=>"The performance of different methods shown in Figure \\REF indicates that our proposed method outperforms the distance-based outliers detection method and has comparable performance with the supervised method using 5% annotation data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#stem -1#15#15#exact -1#16#16#exact -1#17#17#stem -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#para -1#22#22#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact \n"}nil_first --> [23]
nil_second --> [23, 24]
--------------------------
{1=>"As shown in Figures \\REF , \\REF , \\REF , our proposed method produced better results in terms of average precision in which relevant faces were put at the top of the returned list .\n", 2=>"As shown in Figure \\REF , \\REF , \\REF , our proposed method produces better results in terms of average precision in which relevant faces are put on the top of the returned list .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#syn -1#26#26#exact -1#28,29,30,31#27,28,29,30,31#para -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> []
nil_second --> [27]
--------------------------
{1=>"We presented a method for effectively ranking faces retrieved using text-based correlation methods when searching for a specific person .\n", 2=>"We present a method to effectively rank faces retrieved by text-based correlation methods when searching a specific person .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [9, 15]
nil_second --> [9]
--------------------------
{1=>"Using the rank list estimated from the previous steps , we automatically selected a subset of positive and negative samples to train a classifier using SVM with probability outputs . //[Since this is the conclusion , you might want to be more specific on what �gthe previous steps�h are . ]\n", 2=>"Using the rank list estimated from the previous steps , we automatically select a subset of positive and negative samples to train a classifier using SVM with probability outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
nil_second --> []
--------------------------
{1=>"Since the labels of training sets were still noisy , the classifiers trained from these datasets were weak .\n", 2=>"Since labels of training sets are still noisy , the classified trained by these datasets are weak .\n", 3=>"-1#0#0#exact -1#9#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5,6#6,7#para -1#7#8#exact -1#8#9#exact -1#10#11#stem -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#syn -1#16#17#exact -1#17#18#exact \n"}nil_first --> [10, 13]
nil_second --> [12]
--------------------------
{1=>"By combining multiple weak classifiers in a bagging framework , we constructed the final strong classifier , which produced good results .\n", 2=>"By combining multiple weak classifiers in a bagging framework , the final strong classifier is constructed and produce good results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#15#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#17#18#stem -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [10, 16, 17]
nil_second --> [14, 16]
--------------------------
{1=>"To obtain the initial rank for the first step , we proposed using a common outlier detection method .\n", 2=>"To get initial rank for the first step , we propose to use common outliers detection method .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#5#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10,11#11#para -1#12#12#stem -1#13#13,14#para -1#14#15#stem -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"Human face processing techniques for broadcast video , including face detection , tracking , and recognition , have long been a topic that has attracted a lot of research interest due to its crucial value in various applications , such as in video structuring , indexing , retrieval , and summarization .\n", 2=>"Human face processing techniques for broadcast video including face detection , tracking and recognition have long been a topic that attracts much research interest due to its crucial value in various applications including video structuring , indexing , retrieval , summarization , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#10#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#35#11#exact -1#11#12#exact -1#37#13#exact -1#12#14#exact -1#13#15#exact -1#39#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17,18#20,21,22,23#para -1#20#24#stem -1#21#25,26#para -1#22#27,28#para -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact -1#28#34#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#41#38#exact -1#33#42#exact -1#34#43#exact -1#36#45#exact -1#38#47#exact -1#40#50#exact \n"}nil_first --> [39, 40, 41, 44, 46, 48, 49, 51]
nil_second --> [19, 32, 42]
--------------------------
{1=>"The main reason for this is that the human face provides rich information for people 's appearances , such as for a government leader in a news video , a pitcher in a sports video or a hero in a movie , and is the basis for interpreting facts .\n", 2=>"The main reason is human face provides rich information for people 's appearance such as a government leader in a news video , a pitcher in a sport video or a hero in a movie , and is the basis for interpreting facts .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#9#3#exact -1#3#5#exact -1#38#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#8#12#exact -1#40#13#exact -1#10#14#exact -1#11#15#exact -1#12#16#stem -1#22#17#exact -1#13#18#exact -1#14#19#exact -1#15#21#exact -1#16#22#exact -1#17#23#exact -1#18#24#exact -1#19#25#exact -1#20#26#exact -1#21#27#exact -1#35#28#exact -1#23#29#exact -1#24#30#exact -1#25#31#exact -1#26#32#exact -1#27#33#stem -1#28#34#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#32#38#exact -1#33#39#exact -1#34#40#exact -1#36#42#exact -1#37#43#exact -1#39#44,45#para -1#41#47#exact -1#42#48#exact -1#43#49#exact \n"}nil_first --> [4, 6, 20, 41, 46]
nil_second --> []
--------------------------
{1=>"This article describes some state-of-the art techniques for face detection , tracking , and recognition with applications to broadcast video .\n", 2=>"This article describes state-of-the art techniques for face detection , tracking and recognition with application to broadcast video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#stem -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [3, 12]
nil_second --> []
--------------------------
{1=>"Face detection , which is the task of localizing faces in an input image , is a fundamental part of any face processing system .\n", 2=>"Face detection which is the task of localizing faces in an input image is fundamental for any face processing system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#14#16,17#para -1#16#18,19,20#para -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact \n"}nil_first --> [2, 14]
nil_second --> [15]
--------------------------
{1=>"The extracted faces can then be used for initializing face tracking or automatic face recognition .\n", 2=>"The extracted faces can then be used for initializing of face tracking or automatic face recognition .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"- Robustness : it should be capable of handling appearance variations , such as pose changes , size , illuminations , occlusions , complex backgrounds , facial expressions , and low resolutions .\n", 2=>"- Robustness : it should be capable of handling appearance variations of pose changes , size , illuminations , occlusions , complex background , facial expressions , low resolutions , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#14#11#exact -1#12#14#exact -1#13#15#exact -1#18#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#23#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#stem -1#29#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#30#exact -1#28#31#exact \n"}nil_first --> [12, 13, 29, 32]
nil_second --> [11, 30]
--------------------------
{1=>"- Quickness : it should be fast in order to perform real-time processing , which is an important factor in processing large video archives .\n", 2=>"- Fastness : it should be fast for real-time processing which is an important factor in processing large video archives .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#15#7#exact -1#8#11#exact -1#9#12#exact -1#10#14#exact -1#12,13#15,16,17,18#para -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact \n"}nil_first --> [1, 8, 9, 10, 13, 19]
nil_second --> [1, 7, 11, 14]
--------------------------
{1=>"For example , the training time is short , the number of parameters is small , and training samples are collected cheaply .\n", 2=>"For example , the training time is short , the number of parameters is small and training samples are collected without costly .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#22#22#exact \n"}nil_first --> [15, 21]
nil_second --> [20, 21]
--------------------------
{1=>"Many approaches have been proposed for building faster and more robust face detectors \\CITE .\n", 2=>"Many approaches have been proposed for building fast and robust face detectors \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"Among them , those using advanced learning methods , such as neural network , support vector machines and boosting , are the best .\n", 2=>"Among them , those using advanced learning methods such as neural network , support vector machines and boosting are the best .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#12#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [13, 19]
nil_second --> []
--------------------------
{1=>"Typically , detecting the faces in an image takes the following steps :\n", 2=>"Typically , detecting faces in an image includes the following steps :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#8#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#9#9,10#para -1#10#11#exact -1#11#12#exact \n"}nil_first --> [8]
nil_second --> [7]
--------------------------
{1=>"- Window scanning : in order to detect faces at multiple locations and sizes , a fixed window size ( e.g. 24 x 24 pixels ) is used to extract image patterns at every location and scale .\n", 2=>"- Window scanning : in order to detect faces at multiple locations and sizes , a fixed window size ( e.g. 24x24 pixels ) is used to extract image patterns at every location and scale .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [21, 22, 23]
nil_second --> [21]
--------------------------
{1=>"The number of patterns extracted from a 320 x 240 frame image is large , approximately 160 ,000 , in which only a small number of patterns contain a face .\n", 2=>"The number of patterns extracted from one 320x240 frame image is large , approximately 160 ,000 in which only a small number of patterns containing face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#19#6#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#20#22,23#para -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#stem -1#25#29#exact -1#26#30#exact \n"}nil_first --> [7, 8, 9, 18, 28]
nil_second --> [6, 7]
--------------------------
{1=>"The most popular feature type is the Haar wavelet because it is very fast to compute using the integral image \\CITE .\n", 2=>"The most popular feature type is Haar wavelet since it is very fast to compute using the integral image \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#16#6#exact -1#6#7#exact -1#7#8#exact -1#8,9,10#9,10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Other feature types can be listed including the pixel intensity \\CITE , local binary patterns \\CITE , and edge orientation histogram \\CITE .\n", 2=>"Other feature types can be listed including pixel intensity \\CITE , local binary patterns \\CITE and edge orientation histogram \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact \n"}nil_first --> [7, 16]
nil_second --> []
--------------------------
{1=>"- Classification : the extracted features are passed through a classifier that has been previously trained to classify the input pattern associated with these features as a face or a non-face . //[trained / programmed ?]\n", 2=>"- Classification : the extracted features is passed through a classifier which is trained beforehand to classify the input pattern associated with these features as a face or a non-face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#syn -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12,13#para -1#13#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact \n"}nil_first --> [11, 14, 32, 33, 34, 35]
nil_second --> [11, 14]
--------------------------
{1=>"In order to return a single final detection per face , it is necessary to combine the overlapping detections into a single detection .\n", 2=>"In order to return one final detection per face , it is necessary to combine overlapping detections into a single detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#18#4#exact -1#19#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10,11,12,13#11,12,13#para -1#14#14,15#para -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#4#21#syn -1#20#22#exact -1#21#23#exact \n"}nil_first --> [16, 20]
nil_second --> []
--------------------------
{1=>"Since the vast majority of processed patterns are non-face , the single classifier based systems , such as the neural network \\CITE and the support vector machines \\CITE , are usually slow .\n", 2=>"Since the number of processed patterns is large while the vast majority of them are non-face , a single classifier based systems such as neural network \\CITE and support vector machines \\CITE are usually slow .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#10#2#exact -1#11#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#14#7#exact -1#15#8#exact -1#16#9#exact -1#17,18#10,11#para -1#19#12#exact -1#20#13#exact -1#21#14#exact -1#22#16#exact -1#23#17#exact -1#9#18#exact -1#24#19#exact -1#25#20#exact -1#26#21#exact -1#27#22#exact -1#28#23,24#para -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#35#32#exact \n"}nil_first --> [15, 28]
nil_second --> [2, 6, 7, 8, 12, 13]
--------------------------
{1=>"In this structure , fast and simple classifiers are used as filters in the earliest stages to quickly reject a large number of the non-face patterns and then slower but more accurate classifiers are used for classifying the face-like patterns .\n", 2=>"In this structure , fast and simple classifiers are used as filters at the earliest stages to quickly reject a large number of non-face patterns and slower yet more accurate classifiers are then used for classifying face-like patterns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20,21,22#19,20,21,22,23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#32#27#exact -1#26#28#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact \n"}nil_first --> [12, 29, 37]
nil_second --> [12, 27]
--------------------------
{1=>"In this way , the complexity of classifiers can be adapted to correspond to the increasing difficulty with the input patterns .\n", 2=>"In this way , the complexity of classifiers can be adapted corresponding to the increasing difficulty in the input patterns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#11#12#stem -1#13,14#13,14,15#para -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [17]
nil_second --> [16]
--------------------------
{1=>"Training classifiers usually consist of the following steps :\n", 2=>"Training classifiers usually consists of several steps :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#4#4#exact -1#6#7#exact -1#7#8#exact \n"}nil_first --> [5, 6]
nil_second --> [5]
--------------------------
{1=>"- Training set preparation : Supervised learning methods require a large number of training samples to obtain accurate classifiers .\n", 2=>"- Training set preparation : Supervised learning methods require a large number of training samples to obtain accurate classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10,11,12#9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"Face patterns are manually collected from images containing faces and then are scaled to the same size and normalized to a canonical pose in which the eyes , mouth , and nose are aligned .\n", 2=>"Face patterns are manually collected in images containing faces and then are scaled to the same size and normalized to a canonical pose which eyes , mouth and nose are aligned .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15,16,17#14,15,16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23,24#para -1#14#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact \n"}nil_first --> [29]
nil_second --> []
--------------------------
{1=>"Then these face patterns can be used to generate other artificial faces by randomly rotating the images ( about their center points ) by up to 10 degrees , scaling them between 90 and 110% , translating them up to half a pixel , and mirroring them to enlarge the number of positive samples \\CITE .\n", 2=>"Then these face patterns can be used to generate other artificial faces by randomly rotating the images ( about their center points ) up to 10 degree , scaling between 90% and 110% , translating up to half a pixel , and mirroring to enlarge the number of positive samples \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5,6#para -1#24#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#24#exact -1#36#25#exact -1#25#26#exact -1#26#27#stem -1#27#28#exact -1#28#29#exact -1#29#31#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#38#exact -1#43#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#44#48#exact -1#45#49#exact -1#46#50#exact -1#47#51#exact -1#48#52#exact -1#49#53#exact -1#50#54#exact -1#51#55#exact \n"}nil_first --> [23, 30, 32, 37, 46, 47]
nil_second --> [30]
--------------------------
{1=>"In \\CITE a smaller number of training samples can be used to build a robust face detector by using an edge orientation histogram .\n", 2=>"In \\CITE a smaller number of training samples can be used to build a robust face detector by using edge orientation histogram feature .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10#para -1#12,13#11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#23#23#exact \n"}nil_first --> [19]
nil_second --> [22]
--------------------------
{1=>"- Learning method selection : Basically , in an ideal situation with the proper settings , the advanced learning methods , such as the neural network , support vector machines , and AdaBoost , can perform similarly .\n", 2=>"- Learning method selection : Basically , in the ideal case with proper settings , advanced learning methods such as neural network , support vector machines and AdaBoost produce similar performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9#para -1#11#11#exact -1#12#12,13#para -1#13#14#exact -1#14#15#exact -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#22#20#exact -1#18#21#exact -1#19#22#exact -1#20#24#exact -1#21#25#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#31#exact -1#27#32#exact -1#30#35#stem -1#29#36#stem -1#31#37#exact \n"}nil_first --> [10, 23, 26, 30, 33, 34]
nil_second --> [10, 28]
--------------------------
{1=>"However , in practice , it is difficult to find these proper settings .\n", 2=>"However , in practice , it is difficult to find these proper settings .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8#5,6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"Using a neural network requires the design of layers , nodes , etc. , which is complicated .\n", 2=>"Using neural network requires the design of layers , nodes , etc.hich is complicated .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#14,15#para -1#13#16#exact -1#14#17#exact \n"}nil_first --> [1, 12, 13]
nil_second --> [11]
--------------------------
{1=>"Therefore , it is preferable to use support vector machines because only two parameters are necessary if a RBF kernel is used and many tools are available .\n", 2=>"Therefore , it is preferable to use support vector machines since the number of parameters is only two if using RBF kernel and many tools are available .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#16#10,11#para -1#17#12#exact -1#14#13#exact -1#25#14#exact -1#18#16#exact -1#20#18#exact -1#21#19#exact -1#15#20#exact -1#19#21#stem -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#25,26#para -1#27#27#exact \n"}nil_first --> [15, 17]
nil_second --> [10, 11, 12, 13]
--------------------------
{1=>"The advantage of AdaBoost is it can be used for both selecting features and learning the classifier .\n", 2=>"The advantage of AdaBoost is it can be used for both selecting features and learning the classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [8, 9]
--------------------------
{1=>"Face tracking is the process of locating a moving face or several of them over a period of time using a camera , as illustrated in Fig. 1 .\n", 2=>"Face tracking is the process of locating a moving face or several ones in time using a camera , as illustrated in Figure 1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#16#15#exact -1#14#16,17,18#para -1#15#19#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#23#27#exact -1#24#28#exact \n"}nil_first --> [12, 13, 14, 20, 26]
nil_second --> [12, 13, 22]
--------------------------
{1=>"A given face is first initialized manually or by a face detector .\n", 2=>"Face is first initialized manually or by a face detector .\n", 3=>"-1#7#0#lc -1#0#2#lc -1#1#3#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact \n"}nil_first --> [1, 9]
nil_second --> []
--------------------------
{1=>"The face tracker then analyzes the subsequent video frames and outputs the location of the initialized face within these frames by estimating the motion parameters of the moving face .\n", 2=>"Face tracker then analyses subsequent video frames and outputs the location of the initialized face within these frames by estimating the motion parameters of the moving face .\n", 3=>"-1#9#0#lc -1#0#1#lc -1#1#2#exact -1#2#3#exact -1#12#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#20#11#exact -1#10#12#exact -1#11#13#exact -1#24#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact \n"}nil_first --> [4, 22, 26]
nil_second --> [3]
--------------------------
{1=>"This is different from face detection , the outcome of which is the position and scale of one single face in one single frame ; face tracking enables the information acquisition of multiple consecutive faces within consecutive video frames .\n", 2=>"Different from face detection , the outcome of which is the position and scale of one single face in one single frame , face tracking enables the information acquisition of multiple consecutive faces within consecutive video frames .\n", 3=>"-1#9#1#exact -1#0#2#lc -1#1#3#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact \n"}nil_first --> [0, 11, 24]
nil_second --> [22]
--------------------------
{1=>"Although frame-based face detection techniques have been successfully demonstrated on real images , the current ability for detecting faces from video is still primitive .\n", 2=>"Although frame-based face detection techniques have demonstrated success on real images , the current ability on detecting faces from video is still primitive .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#20#6#syn -1#6#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#21,22#para -1#22#23#exact -1#23#24#exact \n"}nil_first --> [7, 16]
nil_second --> [7, 15]
--------------------------
{1=>"The quality of the detector responses can decrease due to different reasons including occlusions , lighting conditions , and face poses .\n", 2=>"The detector responses can decrease due to different reasons including occlusions , lighting conditions and face pose .\n", 3=>"-1#0#3#lc -1#1#4#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#18#exact -1#15#19#exact -1#16#20#stem -1#17#21#exact \n"}nil_first --> [0, 1, 2, 17]
nil_second --> []
--------------------------
{1=>"Without any additional information , these responses can easily be rejected , even if they indicate the presence of a face .\n", 2=>"Without any additional information , these responses can easily be rejected even if they indicate the presence of a face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15,16#16,17,18,19#para -1#19#20#exact -1#20#21#exact \n"}nil_first --> [11]
nil_second --> [17, 18]
--------------------------
{1=>"It is therefore important to incorporate the temporal information in a video sequence to provide more complete video segments displaying the person of interest , which is always named as / already called ? face tracking .\n", 2=>"It is therefore important to incorporate the temporal information in a video sequence to provide more complete video segments displaying the person of interest , which is always named as face tracking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact \n"}nil_first --> [30, 31, 32, 33]
nil_second --> []
--------------------------
{1=>"One of the main applications for face tracking is in the person retrieval from broadcast video , for example : \" Intelligent fast-forwards�E, where the video jumps to the next scene containing a certain person / actor ; or retrieval of different TV interventions , e.g. interviews , shows , etc. , of a given person in a video or a large collection of TV broadcast videos .\n", 2=>"One of the main applications of face tracking is person retrieval from broadcast video , for example : intelligent fast-forwards \" , where the video jumps to the next scene containing a certain person / actor ; or retrieval of different TV interventions , e.g. interviews , shows , etc. , of a given person in a video or a large collection of TV broadcast videos .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#15#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#55#9#exact -1#23#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17,18#para -1#17#19#exact -1#20#20#exact -1#18#21#lc -1#22#23#exact -1#27#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#28#28,29#para -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#56#57#exact -1#57#58#exact -1#58#59#exact -1#59#60#exact -1#60#61#exact -1#61#62#exact -1#62#63#exact -1#63#64#exact -1#64#65#exact -1#65#66#exact -1#66#67#exact \n"}nil_first --> [22, 56]
nil_second --> [5, 19, 21]
--------------------------
{1=>"In [5] , the person retrieval system for a feature-length movie video is proposed using straightforward face tracking .\n", 2=>"In [5] , a person retrieval system for feature-length movie video is proposed using straightforward face tracking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"At run time a user outlines a face in a video frame , and the face tracks within the movie are then ranked according to their similarity to the outlined query face in the same way as Google .\n", 2=>"At run time a user outlines a face in a frame of the video , and the face tracks within the movie are then ranked according to the similarity to the outlined query face in the manner of Google .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#13#10#exact -1#10#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#35,36#para -1#38#37#exact -1#39#38#exact \n"}nil_first --> [25, 34]
nil_second --> [11, 12, 27, 37]
--------------------------
{1=>"Since one face track corresponds to one identity , the workload of intra-shot face matching is greatly reduced , which is not available in frame-based face detection .\n", 2=>"Since one face track corresponds to one identity , the workload of intra-shot face matching is greatly reduced , which is not available in frame-based face detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#21,22,23#19,20,21,22,23#para -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> []
nil_second --> [19, 20]
--------------------------
{1=>"Face tracking is also used in the area of face-name association , the objective of which is to label television or movie footage with the identity of the person present in each frame of the video .\n", 2=>"Face tracking also finds applications in the area of face-name association , the objective of which is to label television or movie footage with the identity of the person present in each frame of the video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#16#2#exact -1#2#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#3#16#para -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"Everingham et al. [8] proposed an automatic face-name association system .\n", 2=>"Everingham et al [8] proposed an automatic face-name association system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"This system uses a face tracker similar to the one in [5] that can extract a few hundred tracks of each particular character in a single shot .\n", 2=>"This system uses a face tracker similar with [5] to extract a few hundred tracks of a particular character each in a single shot .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#7#exact -1#22#9#syn -1#20#10#exact -1#8#11#exact -1#10#14#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#19#20#exact -1#17#21#exact -1#18#22#exact -1#16#24#exact -1#23#26#exact -1#24#27#exact \n"}nil_first --> [8, 12, 13, 23, 25]
nil_second --> [7, 21]
--------------------------
{1=>"Based on the temporal information obtained from the face tracker , the textual information for TV and the movie footage including the subtitles and transcripts is employed to assign the character 's name to each face track .\n", 2=>"Based on the temporal information obtained from the face tracker , textual information for TV and movie footage including subtitles and transcripts is employed to assign the character 's name to each face track .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#26#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#27#29,30#para -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact \n"}nil_first --> [17, 21]
nil_second --> []
--------------------------
{1=>"For instance , shots containing a particular person can be retrieved by a keyword like \" Bush \" or \" Julia Roberts \" instead of the use of an outlined query face as used in [5] .\n", 2=>"For instance , shots containing a particular person can be retrieved by a keyword like \" Bush \" or \" Julia Roberts \" instead of an outlined query face as used in [5] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#30,31#26#para -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#32#35#exact -1#33#36#exact \n"}nil_first --> [25, 27, 33, 34]
nil_second --> []
--------------------------
{1=>"Besides broadcast video , face tracker also has important applications in the videos used in humanoid robotics , visual surveillance , human-computer interaction ( HCI ) , video conferencing , and face-based biometric person authentication among others .\n", 2=>"Besides broadcast video , face tracker also has important applications in the video used in humanoid robotics , visual surveillance , human-computer interaction ( HCI ) , video conferencing , face-based biometric person authentication , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [30, 35, 36, 37]
nil_second --> [34, 35]
--------------------------
{1=>"Choosing a face tracker can be a difficult task because of the variety of face trackers currently available .\n", 2=>"Choosing a face tracker can be a difficult task due to the variety of face trackers available .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"The application provider will have to decide which face tracker is best suited to his / her individual needs and , of course , the type of video that he / she wants to use as the target .\n", 2=>"The application provider will have to decide which face tracker is best suited to his / her individual needs and , of course , the type of video that he / she wants to use as the target .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4,5,6#3,4,5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"Generally speaking , the important issues that should be addressed include speed , robustness , and accuracy .\n", 2=>"Generally speaking , the important issues that should be addressed include speed , robustness and accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"Can the system run in real time ? Similar to many other processing tools for broadcast video , speed is not the most critical issue because offline processing is permitted in most video structuring and indexing cases .\n", 2=>"Can the system run in real time ? Similar with many other processing tools for broadcast video , speed is not the most critical issue because offline processing is permitted in most cases of video structuring and indexing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10,11#9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20#19,20,21,22#para -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#32#36#exact -1#38#37#exact \n"}nil_first --> []
nil_second --> [21, 22, 33]
--------------------------
{1=>"However , a real-time face tracker will become necessary if a target archive is established from too large a quantity of videos , e.g. 24-hour continuous video recording that needs daily structuring .\n", 2=>"However , a real-time face tracker will become necessary if the target archive is established from too large quantities of videos , e.g. 24-hour continuous video recording that needs daily structuring .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18,19#19,20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [18]
nil_second --> []
--------------------------
{1=>"On the other hand , the speed of the tracker is critical in most of the application cases for non-broadcast video , e.g. HCI .\n", 2=>"On the other hand , the speed of the tracker is critical in most cases of applications for non-broadcast video , e.g. HCI .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12,13#12,13,14,15#para -1#14#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [16]
nil_second --> [8, 15, 16]
--------------------------
{1=>"It should be noted that there is always a tradeoff between speed and performance-related issues including the robustness and accuracy .\n", 2=>"It should be noted that there is always a tradeoff between speed and performance-related issues including robustness and accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"Can the system cope with varying illuminations , facial expressions , scales , poses , camerawork , occlusion , and large head motions ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who is moving from an indoor to an outdoor environment .\n", 2=>"Can the system cope with varying illumination , facial expression , scale , pose , camerawork , occlusion and large head motion ? A number of illumination factors , e.g. light sources , background colors , luminance levels , and media , impact greatly on the change in appearance of a moving face , for instance , when tracking a person who are moving from indoor to outdoor environment .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10#10#exact -1#11#11#stem -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#28#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#stem -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#32#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#35#33#exact -1#33#34#exact -1#34#35#exact -1#38#36#exact -1#36#37#exact -1#37#38#exact -1#41#39#exact -1#39#40#exact -1#40#41#exact -1#53#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#56#54#exact -1#54#55#exact -1#55#56#exact -1#57#58#exact -1#58#59#exact -1#59,60#60,61,62,63#para -1#63#64#exact -1#64#65#exact -1#65#67#exact -1#66#68#exact -1#67#70#exact -1#68#71#exact -1#69#72#exact \n"}nil_first --> [57, 66, 69]
nil_second --> [61, 62]
--------------------------
{1=>"Face tracking also tends to fail under large facial deformations of the eyes , nose , mouth , etc. due to the facial expression variation .\n", 2=>"Face tracking also tends to fail under large facial deformations of eyes , nose , mouth , etc. due to facial expression variation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"A smaller face scale always leads to a lower resolution and will reject most face trackers designed by computer vision researchers .\n", 2=>"Small face scale always leads to low resolution and will reject most face trackers designed by computer vision researchers .\n", 3=>"-1#0#0,1#para -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#8#syn -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"Pose variations , i.e. head rotations including the pitch , roll , and yaw , is another influencing factor , which can cause disappearances of parts of faces .\n", 2=>"Pose variation , i.e. head rotations including pitch , roll and yaw , is another influencing factor , which can cause disappearance of part of the face .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#25#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#12#11#exact -1#10#12#exact -1#11#13#exact -1#17#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21,22#23,24#para -1#23,24#25,26#para -1#26#27#stem -1#27#28#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"In some cases , the scale and pose variations might be caused by camerawork changes .\n", 2=>"In some cases , the variation of scale and pose might be caused by camerawork change .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#5#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#stem -1#16#15#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"The partial disappearance of a face is also apt to happen due to occlusion by other objects , and motion information may be distracted by an alternate motion .\n", 2=>"Disappearance of part of the face is also apt to happen due to occlusion by other objects , and motion information may be distracted by alternate motion of them .\n", 3=>"-1#4#0#lc -1#0#2#lc -1#1#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#26#exact -1#26#27#exact -1#29#28#exact \n"}nil_first --> [1, 4, 25]
nil_second --> [2, 3, 27, 28]
--------------------------
{1=>"Moreover , the task of face tracking becomes even more difficult when the head is moving fast relative to the frame rate , so that the tracker fails to �arrive in time�E.\n", 2=>"Moreover , the task of face tracking becomes even more difficult when the head are moving fast relative to the frame rate so that the tracker fails to arrive in time \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15#14#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#29#30#exact \n"}nil_first --> [15, 22, 29, 31]
nil_second --> [28, 30, 31, 32]
--------------------------
{1=>"This problem is difficult to solve because it has a fixed threshold .\n", 2=>"This problem is difficult to solve due to a fixed threshold .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [7, 8]
nil_second --> []
--------------------------
{1=>"Lowering the threshold of the face detector reduces the number of false rejections , but increases the number of false detections , and vice versa .\n", 2=>"Lowering the threshold of the face detector reduces false rejections but increases the number of false detections , and vice versa .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#9#12#exact -1#17#13#exact -1#10#14#exact -1#11#15#exact -1#8#18,19#para -1#16#20#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact \n"}nil_first --> [16, 17, 21]
nil_second --> []
--------------------------
{1=>"A tracker might accumulate motion errors and eventually lose track of a face , for instance , when tracking faces that change from a frontal view to a profile position .\n", 2=>"A tracker might accumulate motion errors and eventually lose track of the face , for instance , when tracking faces that change from a frontal view to a profile position .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#23#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#27#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [27]
nil_second --> [11]
--------------------------
{1=>"Face tracking can be considered an algorithm that analyzes the video frames and outputs the location of moving faces within the video frame .\n", 2=>"Face tracking can be considered as an algorithm that analyses the video frames and outputs the location of moving faces within the video frame .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> [8]
nil_second --> [5, 9]
--------------------------
{1=>"For each tracked face , three steps are involved , which are the initialization , tracking , and stopping procedures , as illustrated in Fig. 2 .\n", 2=>"For each tracked face , three steps are involved that are initialization , tracking and a stopping procedure , as illustrated in Figure 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#12#9#exact -1#9,10#10,11#para -1#11#13#exact -1#18#14#exact -1#13#15#exact -1#14#17#exact -1#16#18#exact -1#17#19#stem -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#23#25#exact -1#24#26#exact \n"}nil_first --> [12, 16, 20, 24]
nil_second --> [15, 22]
--------------------------
{1=>"Most of the developed methods use a face detector for the initialization of their tracking processes .\n", 2=>"Most of the developed methods use a face detector as the initialization of their tracking process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#stem -1#16#16#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"An always ignored but existing difficulty with this step lies in the control of the false face detections described above .\n", 2=>"An always ignored but existing difficulty of this step lies in the control of false face detections described above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"Although there have been literatures on profile or intermediate pose face detectors , this kind of work suffers from the false-detection problem far more than a frontal face detector .\n", 2=>"Although there have been literatures in profile or intermediate pose face detector , this kind of work suffers from the false-detection problem far more than frontal face detector .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#stem -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [5, 25]
nil_second --> [5]
--------------------------
{1=>"To alleviate these two problems , Chaudhury et al. [1] used two face probability maps instead of a fixed threshold to initialize the face tracker , one for frontal views and one for profiles .\n", 2=>"To alleviate these two problems , Chaudhury et al [1] used two face probability maps instead of a fixed threshold to initialize face tracker , one for frontal views and one for profiles .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22,23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"The information from the two face probability maps is combined to represent an intermediate head pose .\n", 2=>"The information from two face probability maps is combined to represent intermediate head pose .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [3, 12]
nil_second --> []
--------------------------
{1=>"Their experiments showed that the proposed probabilistic detector improved the accuracy more than a traditional face detector and is able to handle the head movement covering a range of �90 degrees out-of-plane rotation ( yaw ) .\n", 2=>"Their experiments showed that the proposed probabilistic detector improved the accuracy over traditional face detector and is able to handle the head movement covering a range of �90 degrees out-of-plane rotation ( yaw ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11,12#para -1#24#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact \n"}nil_first --> [26]
nil_second --> []
--------------------------
{1=>"After initialization , one should choose what features to track before tracking a face .\n", 2=>"After initialization , one should choose what features to track before tracking the face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [12]
nil_second --> [12]
--------------------------
{1=>"The exploitation of color is one of the more common choices in order to be invariant to facial expressions , scale , and pose changes [4 , 9] .\n", 2=>"The exploitation of color is one of the common choices in order to be invariant to facial expression , scale and pose change [4 , 9] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6#4,5,6,7#para -1#8#8,9#para -1#9#10#exact -1#12,13#11,12,13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#stem -1#18#19#exact -1#19#20#exact -1#24#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#stem -1#23#25#exact -1#25#27#exact -1#26#28#exact \n"}nil_first --> [26]
nil_second --> [7, 10, 11]
--------------------------
{1=>"However , color-based face trackers often depend on a learning set dedicated to the type of processed videos and are not guaranteed to be easily expendable to unknown videos with varying illumination conditions or different races .\n", 2=>"However , color-based face trackers often depend on a learning set dedicated to the type of processed videos and are not guaranteed to be easily expendable to unknown videos with varying illumination conditions or different races .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13,14,15#12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"Two other choices are the key-point [5 , 8] and facial features [3 , 6 , 10] , e.g. eyes , nose , mouth , etc. , both of which are more robust to varying illuminations and occlusions .\n", 2=>"Another two choices are key-point [5 , 8] and facial features [3 , 6 , 10] , e.g. eyes , nose , mouth , etc. , both of which are more robust to varying illumination and occlusion .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#stem -1#35#36#exact -1#36#37#stem -1#37#38#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"Facial features enable the tracking of higher-level information from a human face , but are weak in lower video quality .\n", 2=>"Facial features enable to track higher-level information from a human face but are weak in low video quality .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#syn -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [3, 4, 5, 12]
nil_second --> [3, 4]
--------------------------
{1=>"Most facial-feature-based face trackers [6 , 10] have been tested using only non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .\n", 2=>"Most facial-feature-based face trackers [6 , 10] are only tested by using non-broadcast video , e.g. webcam video , and their application potentiality to broadcast video is questionable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7,8#para -1#9#9#exact -1#11#10#exact -1#8#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"One example of an appearance-based face tracker is [1] , which was introduced above .\n", 2=>"One example of appearance-based face tracker is [1] that has been introduced above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8,9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [3, 9]
nil_second --> []
--------------------------
{1=>"Another example was proposed by Li et al. [9] , which uses a multi-view face detector to detect and track faces from different poses .\n", 2=>"Another example is proposed by Li et al [9] , which uses a multi-view face detector to detect and track faces of different poses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21,22#21,22#para -1#23#23#exact -1#24#24#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"It is based on the idea that a head can be considered an object of interest instead of a face , because the face is not always present in the tracking process .\n", 2=>"It is based on the idea that head can be considered as the object of interest instead of face because face is not always present in the tracking process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12,13#12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#21#exact -1#26#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact \n"}nil_first --> [18, 20, 29]
nil_second --> [11]
--------------------------
{1=>"An extended particle filter is proposed to fuse these two interrelated information together so as to handle the occlusion due to out-of-plane head rotation ( yaw ) that is more than �90 degrees .\n", 2=>"An extended particle filter is proposed to fuse these two interrelated information so as to handle the occlusion due to out-of-plane head rotation ( yaw ) that is more than �90 degrees .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#27,28,29#27,28,29,30#para -1#30#31#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [12]
nil_second --> [26]
--------------------------
{1=>"Some examples of simple motion models are as follows .\n", 2=>"Examples of simple motion models are as follows .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [0]
nil_second --> []
--------------------------
{1=>"Based on the assumption that a face can be considered a planar object , the corresponding motion model can be a 2D transformation , e.g. affine transformation or homography , of an image of the face , e.g. the initial frame [3 , 6] .\n", 2=>"Based on the assumption that face can be considered as a planar object , the corresponding motion model can be a 2D transformation , e.g. affine transformation or homography , of an image of the face , e.g. the initial frame [3 , 6] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#20#5#exact -1#5#6#exact -1#6#7#exact -1#7,8,9,10#8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31,32#31,32,33,34#para -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact \n"}nil_first --> [20]
nil_second --> [33, 34]
--------------------------
{1=>"Some researchers view a face as a rigid 3D object , thus the motion model defines its aspect depending on its 3D position and orientation [10] .\n", 2=>"Some researchers assume the face as a rigid 3D object , thus the motion model defines its aspect depending on its 3D position and orientation [10] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#6#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [2, 6]
nil_second --> [2, 3]
--------------------------
{1=>"However , a face is actually both 3D and deformable .\n", 2=>"However , face is actually both 3D and deformable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"Some systems try to model faces in this sense , and the image of deformed face can be covered with a mesh , i.e. a sophisticated geometry and texture face model [2 , 7] .\n", 2=>"Some system try to model face in this sense , and the image of deformable faces can be covered with a mesh , i.e. a sophisticated geometry and texture face model [2 , 7] .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#15#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11,12#10,11,12,13#para -1#14#14#stem -1#5#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"The motion of the face is defined by the position of the nodes of the mesh .\n", 2=>"The motion of the face is defined by the position of the nodes of the mesh .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10#para -1#14#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"Generally if the quality of the video is high , a more sophisticated motion model is used , and then the face tracker generates a more accurate result .\n", 2=>"Generally if the quality of the video is high , more sophisticated motion model is used , more accurate result the face tracker generates .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5#2,3,4#para -1#20#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#21#20,21#para -1#22#22#exact -1#23#23#exact -1#17,18#24,25,26#para -1#19#27#exact -1#24#28#exact \n"}nil_first --> [10, 18, 19]
nil_second --> []
--------------------------
{1=>"For instance , a sophisticated geometry and texture model might suffer from false face detections and a level of drifting [less than / that is worse than ?] a simple 2D transformation model .\n", 2=>"For instance , a sophisticated geometry and texture model might suffer from false face detections and drifting less than a simple 2D transformation model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#19#16#exact -1#16#19#exact -1#18#21#exact -1#20#28,29#para -1#21#30#exact -1#22#31#exact -1#23#32#exact -1#24#33#exact \n"}nil_first --> [17, 18, 20, 22, 23, 24, 25, 26, 27]
nil_second --> [17]
--------------------------
{1=>"However , it must be noted that most 3D-based and mesh-based face trackers require a relatively clear appearance , high resolution , and a limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .\n", 2=>"But note that most 3D-based and mesh-based face trackers require relatively clear appearance , high resolution , and limited pose variation of the face , e.g. out-of-plane head rotations ( roll and yaw ) that are far less than �90 degrees .\n", 3=>"-1#13#1#exact -1#1#4,5#para -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#8#12#exact -1#9#13#exact -1#10#14,15#para -1#11#16#exact -1#12#17#exact -1#24#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact -1#18#23,24#para -1#19#25#exact -1#20#26#exact -1#21#27#exact -1#22#28#exact -1#23#29#exact -1#25#31#exact -1#26#32#exact -1#27#33#exact -1#28#34#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#32#38#exact -1#33#39#exact -1#34#40#exact -1#35#41#exact -1#36#42#exact -1#37#43#exact -1#38#44#exact -1#39#45#exact -1#40#46#exact -1#41#47#exact \n"}nil_first --> [0, 2, 3, 30]
nil_second --> [0]
--------------------------
{1=>"This constitutes a major deficiency for the face tracking algorithms that are generally not able to stop a face track in case of tracking errors , i.e. drifting .\n", 2=>"This constitutes a major deficiency of face tracking algorithms that are generally not able to stop a face track in case of tracking error , i.e. drifting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#stem -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [5]
nil_second --> [5]
--------------------------
{1=>"Arnaud et al. [3] proposed an approach that uses a general object tracker for face tracking and a stopping criterion based on the addition of an eye tracker to alleviate drifting .\n", 2=>"Arnaud et al [3] proposed an approach that uses a general object tracker for face tracking and a stopping criterion based on the addition of an eye tracker to alleviate drifting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21,22,23,24#21,22,23#para -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [2, 24]
nil_second --> [2]
--------------------------
{1=>"The two positions of the tracked eyes are compared with the tracked face position .\n", 2=>"Two positions of tracked eyes are compared with tracked face position .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact \n"}nil_first --> [0, 4, 10]
nil_second --> []
--------------------------
{1=>"If neither of the eyes is in the face region , it will be determined as drifting and the tracking process will be stopped .\n", 2=>"If none of the two eyes are in the face region , it will be determined as drifting and the tracking process will be stopped .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#exact -1#5#4#exact -1#12,13,14#5#para -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#22,23#11,12#para -1#15#13,14#para -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#6#21,22#para -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"In addition , most mesh-based and top-down trackers are assumed to be able to avoid drifting .\n", 2=>"Besides , most mesh-based trackers and top-down trackers are considered to be able to avoid drifting .\n", 3=>"-1#0#0,1#para -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11,12,13#10,11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [9]
nil_second --> [4, 9, 10]
--------------------------
{1=>"However , while most of the attempts have been on the face tracking for high-quality videos by computer vision researchers , only a limited number of face trackers are designed for broadcast video .\n", 2=>"However , while most attempts have been made on face tracking for videos with high quality by computer vision researchers , only a limited number of face trackers are designed for broadcast video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#25#4#exact -1#4#5,6#para -1#5#7#exact -1#6#8#exact -1#8#9#exact -1#9#10,11#para -1#10#12#exact -1#11#13#exact -1#12#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21,22,23,24#21,22,23,24,25#para -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [14]
nil_second --> [7, 13, 14, 15]
--------------------------
{1=>"A general evaluation criterion , in terms of speed , robustness , and accuracy , is needed for a performance comparison between the face trackers with different purposes .\n", 2=>"A general evaluation criterion , in terms of speed , robustness and accuracy , is needed for performance comparison between face trackers of different purposes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#11#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18,19#para -1#18#20#exact -1#19#21#exact -1#20#22,23#para -1#21#24#exact -1#22,23#25,26#para -1#24#27#exact -1#25#28#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"We propose a method for retrieving relevant faces of one person by learning the visual consistency among results retrieved from text-correlation-based search engines .\n", 2=>"We propose a method to retrieve relevant faces for one person by learning the visual consistency among results retrieved from text-correlation-based search engines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [8]
nil_second --> [4]
--------------------------
{1=>"In the first step , each candidate face obtained from a text-based search engine is ranked with a score that measures the distribution of visual similarities among the faces .\n", 2=>"In the first step , each candidate face obtained from a text-based search engine is ranked by a score that measures the distribution of visual similarities among the faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> [16]
nil_second --> [16]
--------------------------
{1=>"Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list , respectively .\n", 2=>"Faces that are possibly very relevant or irrelevant are ranked at the top or bottom of the list .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#20#exact \n"}nil_first --> [18, 19]
nil_second --> []
--------------------------
{1=>"In this way , the accuracy of the ranked list increases after a number of iterations .\n", 2=>"In this way , the accuracy of the ranked list increases after a number of iterations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5,6#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"This trend has shown the need for effective and efficient tools for indexing and retrieving visual content .\n", 2=>"This trend has shown the need for effective and efficient tools for indexing and retrieving based on visual content .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact \n"}nil_first --> []
nil_second --> [15, 16]
--------------------------
{1=>"However , other un-queried faces and names may appear with the queried ones ( Figure xx ) , and this significantly lowers the retrieval performance .\n", 2=>"However , other un-queried faces and names may appear with the queried ones ( as shown in Figure xx ) , and this significantly lowers retrieval performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact \n"}nil_first --> [22]
nil_second --> [14, 15, 16]
--------------------------
{1=>"One way to improve the retrieval performance is to take into account visual information present in the retrieved faces .\n", 2=>"One way to improve the retrieval performance is to take into account visual information present in the retrieved faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10,11#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"-Large variations in facial appearance due to pose changes , illumination conditions , occlusions , and facial expressions make face recognition difficult even with state-of-the-art techniques\\CITE ( see example in Figure xx ) .\n", 2=>"-Large variations in facial appearance due to pose changes , illumination conditions , occlusions and facial expressions make face recognition difficult even with state-of-the-art techniques\\CITE ( see an example in Figure xx ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [14]
nil_second --> [27]
--------------------------
{1=>"The main idea is the assumption that there is visual consistency among the results returned from text-based search engines and this visual consistency is then learned through an interactive process .\n", 2=>"The main idea is to assume that there is visual consistency among the results returned from text-based search engines ; and then learn this visual consistency through an interactive process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#12#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#21#23,24#para -1#22#25#stem -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [5]
nil_second --> [4, 5, 19]
--------------------------
{1=>"This score is used to form a ranked list , in which faces with high-density scores are considered relevant and are put at the top .\n", 2=>"This score is used to form a ranked list , in which faces having high density scores are considered relevant and are put at the top of the list .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#29#25#exact \n"}nil_first --> [13, 14]
nil_second --> [13, 14, 15, 26, 27, 28]
--------------------------
{1=>"The above ranking method is weak since dense clusters have no guarantee of containing relevant faces .\n", 2=>"The above ranking method is weak since dense clusters have no guarantee of containing relevant faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10,11,12#9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"To obtain the final strong classifier , we use the [idea / concept?] of ensemble learning \\CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .\n", 2=>"To get the final strong classifier , we use the idea of ensemble learning \\CITE in which weak classifiers trained on different subsets are combined to improve the stability and classification accuracy of single classifiers .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#26,27,28#27,28,29,30#para -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [10, 11, 12]
nil_second --> [10, 25]
--------------------------
{1=>"-Supervised learning methods , such as an SVM , provide a strong theoretical background for finding the optimal decision boundary even with noisy data .\n", 2=>"-Supervised learning methods , such as SVM , provide a strong theoretical background for finding the optimal decision boundary even with noisy data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"The framework seamlessly integrates data mining techniques such as supervised learning and unsupervised learning based on bagging .\n", 2=>"The framework seamlessly integrates data mining techniques such as supervised learning , and unsupervised learning based on bagging .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"We demonstrate its feasibility with a practical web mining application .\n", 2=>"We demonstrate its feasibility of a practical web mining application .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"For examples , as described by Fergus et al. \\CITE , [Reference numbers generally should not be grammatically part of the sentence .\n", 2=>"For examples , as described in \\CITE , [Reference numbers generally should not be grammatically part of the sentence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5#para -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact \n"}nil_first --> [6, 7, 8]
nil_second --> []
--------------------------
{1=>"It is better to use the authors�f names .] objects retrieved using an image search engine are re-ranked by extending the constellation model .\n", 2=>"It is better to use the authors�f names .]objects retrieved by an image search engine are re-ranked by extending the constellation model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [8, 9, 11]
nil_second --> [8, 10]
--------------------------
{1=>"The main contribution of these approaches is probabilistic models that can be learned with a small number of training images .\n", 2=>"The main contribution of these approaches are probabilistic models that can be learned with a small number of training images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10,11#6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [10]
nil_second --> [6]
--------------------------
{1=>"However , these models are complicated since they require several hundred parameters for learning and are susceptible to over-fitting .\n", 2=>"However , these models are complicated , since they require several hundred parameters for learning , and they are susceptible to over-fitting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#16#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact \n"}nil_first --> []
nil_second --> [6, 15, 17]
--------------------------
{1=>"Although the result was impressive , it is not easy to apply it to our problem since it is based on a strong assumption that requires a perfect alignment when a news photo only has one face and its caption only has one name .\n", 2=>"Although the result was impressive , it is not easy to apply it to our problem since it is based on a strong assumption that requires a perfect alignment in the case that the news photo only has one face and its caption only has one name .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9,10#6,7,8,9#para -1#13#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19,20#17,18,19,20#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact -1#47#44#exact \n"}nil_first --> [13, 29, 30]
nil_second --> [17, 18, 29, 30, 31, 32, 33]
--------------------------
{1=>"A graph-based approach was proposed by Ozkan and Duygu \\CITE , in which a graph is formed from faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .\n", 2=>"A graph-based approach was proposed by \\CITE , in which a graph is formed by faces as nodes , and the weights of edges linked between nodes are the similarity of faces , is closely related to our problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#19#7#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact \n"}nil_first --> [6, 8, 17, 22]
nil_second --> [14]
--------------------------
{1=>"Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and can therefore , be solved by taking an available solution . //It might be unclear as to what \" available solution \" you are talking about . You might want to give more detail here .\n", 2=>"Assuming that the number of faces of the queried person is larger than that of others and that these faces tend to form the most similar subset among the set of retrieved faces , this problem is considered equal to the problem of finding the densest subgraph of a full graph ; and therefore can be solved by taking an available solution .[It might be unclear as to what \" available solution \" you are talking about .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#40,41,42#39,40,41,42#para -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact -1#52#52#exact -1#54#53#exact -1#53#54#exact -1#55#56#exact -1#56#57#exact -1#57#58#exact -1#58#59#exact -1#59#60#exact -1#60#61#exact -1#61#62#exact -1#77#63#exact -1#63#65#exact -1#64#66#exact -1#65#67#exact -1#66#68#exact -1#67#69#exact -1#68#70#exact -1#69#71#exact -1#70#72#exact -1#71#73#exact -1#72#74#exact -1#74,75,76#75,76,77,78#para -1#73#80#lc -1#39#83#exact \n"}nil_first --> [55, 64, 79, 81, 82, 84, 85, 86, 87, 88]
nil_second --> [62]
--------------------------
{1=>"Although experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of the relevant faces of the queried person and it is easy to extend for the ranking problem .\n", 2=>"You might want to give more detail here .] Although , experimental results showed the effectiveness of this method , it is still questionable whether the densest subgraph intuitively describes most of relevant faces of the queried person and it is easy to extend for the ranking problem .\n", 3=>"-1#9#0#exact -1#11#1#exact -1#12#2#exact -1#13#3#exact -1#14#4#exact -1#15#5#exact -1#16#6#exact -1#17#7#exact -1#18#8#exact -1#19#9#exact -1#20#10#exact -1#21#11#exact -1#22#12#exact -1#23#13#exact -1#24#14#exact -1#25#15#exact -1#26#16#exact -1#27#17#exact -1#28#18#exact -1#29#19#exact -1#30#20#exact -1#31#21#exact -1#45#22#exact -1#32#23#exact -1#33#24#exact -1#34#25#exact -1#35#26#exact -1#36#27#exact -1#37#28#exact -1#38#29#exact -1#39,40,41,42#30,31,32#para -1#3#33#exact -1#43#34#exact -1#44#35#exact -1#46#37#exact -1#47#38#exact -1#48#39#exact \n"}nil_first --> [36]
nil_second --> [0, 1, 2, 4, 5, 6, 7, 8, 10]
--------------------------
{1=>"Furthermore , choosing an optimal threshold to convert the initial graph into a binary one is difficult and rather ad hoc due to dimensionality .\n", 2=>"Furthermore , choosing an optimal threshold to convert the initial graph into a binary graph is difficult and rather ad hoc due to the curse of dimensionality .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#26#23#exact -1#27#24#exact \n"}nil_first --> []
nil_second --> [14, 23, 24, 25]
--------------------------
{1=>"An advantage of these methods \\CITE is they are fully unsupervised .\n", 2=>"The good point of the methods \\CITE is they are fully unsupervised .\n", 3=>"-1#3#2#exact -1#4,5#3,4#para -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 1, 2]
--------------------------
{1=>"However , a disadvantage is that no model is learned for predicting new images of the same category .\n", 2=>"However , the bad point is no model is learned to predict new images of the same category .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [2, 3, 5, 10]
nil_second --> [2, 3, 4]
--------------------------
{1=>"Furthermore , they are used for performing hard categorization on input images that are inapplicable for re-ranking . //It is not clear if \" hard categorization \" is inapplicable or if the \" input images \" are inapplicable .\n", 2=>"Furthermore , they perform hard categorization on input images that is [It is not clear if \" hard categorization \" is inapplicable or if the \" input images \" are inapplicable .]in applicable for re-ranking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#29#3#exact -1#33#5#exact -1#3#6#stem -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#syn -1#21#14#exact -1#34#16#exact -1#35#17#exact -1#12#19#exact -1#13#20#exact -1#14#21#exact -1#15#22#exact -1#16#23#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact -1#20#27#exact -1#30#28#exact -1#22#29#exact -1#23#30#exact -1#24#31#exact -1#25#32#exact -1#26#33#exact -1#27#34#exact -1#28#35#exact \n"}nil_first --> [4, 15, 18, 36, 37, 38]
nil_second --> [11, 31, 32]
--------------------------
{1=>"This leads to the reduction in the number of collected images .\n", 2=>"This leads the number of collected images is reduced .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#7,8#4#para -1#3#5,6,7#para -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#9#11#exact \n"}nil_first --> [2]
nil_second --> []
--------------------------
{1=>"However , we used an unsupervised method to select training samples automatically , which is different from the methods proposed by Fergus et al. and Li et al. \\CITE .\n", 2=>"However , different from the methods in \\cite{xx} , we used an unsupervised method to select training samples automaticallyCITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#10#3#exact -1#11#4#exact -1#12#5#exact -1#13#6#exact -1#14#7#exact -1#15#8#exact -1#16#9#exact -1#17#10#exact -1#8#12#exact -1#2,3#14,15,16#para -1#4#17#exact -1#5#18#exact -1#19#29#exact \n"}nil_first --> [11, 13, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
nil_second --> [6, 7, 18]
--------------------------
{1=>"This unsupervised method is different from the one by Ozkan and Dugyu \\CITE in the modeling of the distribution of relevant images .\n", 2=>"This unsupervised method is different from the one in \\CITE in its way of modeling the distribution of relevant images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#12#exact -1#10#13#exact -1#15#14#exact -1#14#15#exact -1#16,17#16,17,18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact \n"}nil_first --> [8, 9, 10, 11]
nil_second --> [8, 11, 12, 13]
--------------------------
{1=>"-Step 3 : Estimate the ranked list of these faces by rank-by-local-density score .\n", 2=>"-Step 3 : Estimate the ranked list of these faces by Rank-By-Local-Density-Score .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#13#exact \n"}nil_first --> [11, 12]
nil_second --> [11]
--------------------------
{1=>"-Step 4 : Improve this ranked list using rank-by-bagging-probSVM . //I found not hits for \" rank-by-bagging-probSVM \" on the Internet. You might want to double check to see if this is a standard term . The same is true for \" rank-by-local-density score \" . If this is your own term , you might want to specify this at some point .\n", 2=>"-Step 4 : Improve this ranked list by Rank-By-Bagging-ProbSVM . //I found not hits for \" rank-by-bagging-probSVM \" on the Internet . You might want to double check to see if this is a standard term . The same is true for \" rank-by-local-density score \" . If this is your own term , you might want to specify this at some point .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#16#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#8#16#lc -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#50#exact -1#52#51#exact -1#53#52#exact -1#54#53#exact -1#55#54#exact -1#56#55#exact -1#57#56#exact -1#58#57#exact -1#59#58#exact -1#60#59#exact -1#61#60#exact -1#62#61#exact -1#63#62#exact \n"}nil_first --> [7, 20]
nil_second --> [7, 20, 21]
--------------------------
{1=>"The algorithms used in Steps 3 and 4 are described in section \\REF and section \\REF , respectively .\n", 2=>"The algorithms used in Step 3 and Step 4 are described in section \\REF and section \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#stem -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#18#exact \n"}nil_first --> [16, 17]
nil_second --> [7]
--------------------------
{1=>"Among the faces retrieved by text-based search engines for a query of person-\\MATH , as shown in Figure \\REF , relevant faces usually look similar and forms the largest cluster .\n", 2=>"Among the faces retrieved by the text-based search engines for a query of person-\\MATH , as shown in Figure \\REF , relevant faces usually look similar and can form the largest cluster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#28#26#stem -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact \n"}nil_first --> []
nil_second --> [5, 27]
--------------------------
{1=>"One approach of re-ranking these faces is to cluster based on visual similarity .\n", 2=>"One approach to re-rank these faces is to do clustering based on visual similarity .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#stem -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#stem -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"Instead , a graph-based approach was proposed by Ozkan and Dugyu \\CITE in which the nodes are faces and edge weights are the similarities between two faces .\n", 2=>"Instead , in \\cite{xx} , a graph based approach was proposed CITEin which the nodes are faces and edge weights are the similarities between two faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#2#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#17#9#exact -1#2#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [3, 7, 8, 10, 11, 18]
nil_second --> [3, 4, 6, 7, 11]
--------------------------
{1=>"We use the idea of density-based clustering described by Ester et al. and Breunig et al. \\CITE to solve this problem . //idea / concept?\n", 2=>"We use the idea of density-based clustering described in \\CITE to solve this problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#16#exact -1#11,12,13#17,18,19,20#para -1#14#21#exact \n"}nil_first --> [9, 10, 11, 12, 13, 14, 15, 22, 23, 24]
nil_second --> [10]
--------------------------
{1=>"Specifically , we define the local density score ( LDS ) of a point \\MATH( i.e. a face ) as the average distance to its k-nearest neighbors .\n", 2=>"Specifically , we define local density score ( LDS ) of a point \\MATH( i.e. a face ) as the average distance to its k-nearest neighbors :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#19#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#20,21#para -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [27]
nil_second --> [26]
--------------------------
{1=>"Since faces are represented in high dimensional feature space , and face clusters might have different sizes , shapes , and densities , we do not directly use the Euclidean distance between two points in this feature space for \\MATH .\n", 2=>"Since faces are represented in high dimensional feature space , and face clusters might have different sizes , shapes and densities ; we do not use directly the Euclidean distance between two points in this feature space for \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#26#exact -1#25#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact \n"}nil_first --> [19, 22]
nil_second --> [21]
--------------------------
{1=>"Instead , we use another similarity measure defined by the number of shared neighbors between two points .\n", 2=>"Instead , we use another similarity measure defined by the number of shared neighbors between two points .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10,11#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"The efficiency of this similarity measure for density-based clustering methods was described . //There is no period here , so it is not clear if there should be a period or there should be more to this sentence that is not here . If the sentence does end here , you might want to go into more detail about who or what \" described \" this .\n", 2=>"The efficiency of this similarity measure for density-based clustering methods was described . //There is no period here , so it is not clear if there should be a period or there should be more to this sentence that is not here . If the sentence does end here , you might want to go into more detail about who or what \" described \" this .]\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20,21,22#19,20,21,22#para -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact -1#52#52#exact -1#53#53#exact -1#54#54#exact -1#55#55#exact -1#56#56#exact -1#57#57#exact -1#58#58#exact -1#59#59#exact -1#60#60#exact -1#61#61#exact -1#62#62#exact -1#63#63#exact -1#64#64#exact -1#65#65#exact \n"}nil_first --> [66]
nil_second --> [19, 66]
--------------------------
{1=>"Faces with higher scores are considered to be potential candidates that are relevant to person-\\MATH , while faces with lower scores are considered as outliers and thus are potential candidates for non-person-\\MATH .\n", 2=>"Faces with higher scores are considered to be potential candidates that are relevant to person-\\MATH , while faces with lower scores are considered as outliers and thus are potential candidates for non-person-\\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5#para -1#13#6#exact -1#21,22#7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#27#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [13, 22, 27]
nil_second --> []
--------------------------
{1=>"Step 2 : Rank these faces using LDS( p , k ) ( The higher the score the more relevant ) .\n", 2=>"Step 2 : Rank these faces using LDS( p , k ) ( The higher the more relevant ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [16, 17]
nil_second --> []
--------------------------
{1=>"One limitation of the local density score based ranking is it cannot handle faces of another person strongly associated in the \\MATH-neighbor set ( for example , many duplicates ) .\n", 2=>"One limitation of the local density score based ranking is it could not handle the case that faces of another person have strong association in \\MATH-neighbor set ( for example , many duplicates ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12#11#para -1#13#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#22#17#stem -1#23#18#stem -1#24#19#exact -1#14#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact \n"}nil_first --> []
nil_second --> [15, 16, 21]
--------------------------
{1=>"As a result , we have a model that can be used for both re-ranking current faces and predicting new incoming faces .\n", 2=>"As a result , we have a model that can be used for both re-ranking current faces and predicting new incoming faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [11, 12]
--------------------------
{1=>"Since the labels are not available for training , we use the input rank list found from the previous step to extract a subset of faces lying at the top and bottom of the ranked list to form the training set .\n", 2=>"Since the labels are not available for training , we use the input rank list found from the previous step to extract a subset of faces lying at the top and bottom of the ranked list to form the training set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#28,29,30#27,28,29,30#para -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact \n"}nil_first --> []
nil_second --> [27]
--------------------------
{1=>"After that , we use an SVM with probabilistic output \\CITE implemented in LibSVM \\CITE to learn the person-\\MATH model .\n", 2=>"After that , we use SVM with probabilistic output \\CITE implemented in LibSVM \\CITE to learn the person-\\MATH model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"This model is applied to faces of the original set , and the output probabilistic scores are used to re-rank these faces .\n", 2=>"This model is applied to faces of the original set and the output probabilistic scores are used to re-rank these faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Since it is not guaranteed that faces lying at two ends of the input rank list correctly correspond to the faces of person-\\MATH and faces of non person-\\MATH , we adopt the [idea / concept?] of a bagging framework \\CITE in which randomly selecting subsets to train weak classifiers , and then combining these classifiers help reduce the risk of using noisy training sets .\n", 2=>"Since it is not guaranteed that faces lying at two ends of the input rank list correctly correspond to the faces of person-\\MATH and faces of non person-\\MATH , we adopt the idea of bagging framework \\CITE in which randomly selecting subsets to train weak classifiers , and then combining these classifiers help reduce the risk of using noisy training sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#33#35#exact -1#34#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#44#47#exact -1#45#48#exact -1#46#49#exact -1#47#50#exact -1#48#51#exact -1#49#52#exact -1#50#53#exact -1#51#54#exact -1#52#55#exact -1#53,54,55,56#56,57,58#para -1#57#60#exact -1#58#61#exact -1#59#62#exact -1#60#63#exact -1#61#64#exact \n"}nil_first --> [32, 33, 34, 36, 59]
nil_second --> [32]
--------------------------
{1=>"The details of the Rank-By-Bagging-ProbSVM-InnerLoop method , improving an input rank list by combining weak classifiers trained from subsets annotated by that rank list , are described in Algorithm 2 .\n", 2=>"The details of Rank-By-Bagging-ProbSVM-InnerLoop method , improving an input rank list by combining weak classifiers trained from subsets annotated by that rank list are described in Algorithm 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [3, 24]
nil_second --> []
--------------------------
{1=>"Step 1 : Train a weak classifier , hi .\n", 2=>"Step 1 : Train a weak classifier hi .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"Step 1 .1 : Select a set Spos including p% of top ranked faces and then randomly select a subset S?pos from Spos .\n", 2=>"Step 1 .1 : Select a set Spos including p% top ranked faces and then randomly select a subset S?pos from Spos .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Step 1 .2 : Select a set Sneg including p% of bottom ranked faces and then randomly select a subset S? neg from Sneg .\n", 2=>"Step 1 .2 : Select a set Sneg including p% bottom ranked faces and then randomly select a subset S? neg from Sneg .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Step 1 .3 : Use S?pos and S? neg to train a weak classifier , hj , using LibSVM [8] with probability outputs .\n", 2=>"Step 1 .3 : Use S?pos and S? neg to train a weak classifier hj using LibSVM [8] with probability outputs .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [14, 16]
nil_second --> []
--------------------------
{1=>"Step 3 : Apply Hi to the original face set and form the rank list , Ranki , using the output probabilistic scores .\n", 2=>"Step 3 : Apply Hi to the original face set and form the rank list Ranki by using the output probabilistic scores .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [15, 17]
nil_second --> [16]
--------------------------
{1=>"Step 4 : Repeat steps 1 to 3 until Dist2RankList( Ranki?1 ,Ranki ) <= ? .\n", 2=>"Step 4 : Repeat steps from Step 1 to Step 3 until Dist2RankList( Ranki?1 ,Ranki ) <= ? .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact \n"}nil_first --> []
nil_second --> [5, 6, 9]
--------------------------
{1=>"Step 1 : Rankcur = Rank-By-Bagging-ProbSVM-InnerLoop ( Rankprev ) .\n", 2=>"Step 1 : Rankcur = Rank-By-Bagging-ProbSVM-InnerLoop( Rankprev ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [5, 6]
nil_second --> [5]
--------------------------
{1=>"Step 2 : dist = Dist2RankList ( Rankprev ,Rankcur ) .\n", 2=>"Step 2 : dist = Dist2RankList( Rankprev ,Rankcur ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [5, 6]
nil_second --> [5]
--------------------------
{1=>"Step 5 : Repeat steps 1 to 4 until dist <= ? .\n", 2=>"Step 5 : Repeat steps from Step 1 to Step 4 until dist <= ? .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact \n"}nil_first --> []
nil_second --> [5, 6, 9]
--------------------------
{1=>"Step 6 : Return Rankfinal .\n", 2=>"Step 5 : Return Rankfinal .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact \n"}nil_first --> [1]
nil_second --> [1]
--------------------------
{1=>"Given an input ranked list , Rank-By-Bagging-ProbSVM-InnerLoop is used to improve this list .\n", 2=>"Given an input ranked list , Rank-By-Bagging-ProbSVM-InnerLoop is used to improve this rank list .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"The \\MATH distance between two lists , \\MATH and \\MATH , is defined as follows :\n", 2=>"The \\MATH distance between two list \\MATH and \\MATH is defined as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact \n"}nil_first --> [6, 10]
nil_second --> []
--------------------------
{1=>"Since the maximum value of \\MATH is \\MATH , where \\MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :\n", 2=>"Since the maximum value of \\MATH is \\MATH where \\MATH is the number of members of the list , the normalized Kendall tau distance can be written as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#18#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11,12,13,14,15,16#12,13,14,15,16#para -1#19#17#exact -1#17#18#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [19, 20]
nil_second --> []
--------------------------
{1=>"Using this measure for checking when the loops stop means that if the ranked list does not change significantly after a number of iterations , it is reasonable to stop .\n", 2=>"Using this measure for checking when the loops stop means that if the ranked list does not change significantly after a number of iterations , it is reasonable to stop .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26,27#25,26,27,28#para -1#29#29#exact -1#30#30#exact \n"}nil_first --> []
nil_second --> [25, 28]
--------------------------
{1=>"We used the dataset described by Berg et al. \\CITE for our experiments .\n", 2=>"We used the dataset described in \\CITE for our experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5#para -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact \n"}nil_first --> [6, 7, 8]
nil_second --> []
--------------------------
{1=>"This dataset consists of approximately half a million news pictures and captions from Yahoo News collected over a period of roughly two years .\n", 2=>"This dataset consists of approximately half a million news [pictures / photos?] and captions from Yahoo News collected over a period of roughly two years .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact \n"}nil_first --> [9]
nil_second --> [9, 10, 11]
--------------------------
{1=>"This dataset is better than datasets collected from image search engines such as Google that usually limit the total number of returned images to 1 ,000 .\n", 2=>"This dataset is better than datasets collected from image search engines such as Google that usually limit the total number of returned images to 1 ,000 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17,18,19,20#17,18,19#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"Only the front of faces were considered since current frontal face detection systems \\CITE work in real time and have accuracies exceeding 95\\% .\n", 2=>"Only frontal faces were considered since current frontal face detection systems \\CITE can work in real time and have accuracies exceeding 95\\% .\n", 3=>"-1#0#0#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#13#14#exact -1#14,15,16#15,16,17,18#para -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [1, 2, 3]
nil_second --> [1, 12, 17]
--------------------------
{1=>"44 ,773 faces were detected and normalized to 86\\MATH86 pixels .\n", 2=>"44 ,773 faces were detected and normalized to the size of 86\\MATH86 pixels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact \n"}nil_first --> []
nil_second --> [8, 9, 10]
--------------------------
{1=>"We selected fifteen government leaders , including George W. Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , Abdullah Gul ( Turkey ) , and other key individuals , such as John Paul II ( the Former Pope ) and Hans Blix ( UN ) , because their images frequently appear in the dataset \\CITE .\n", 2=>"We selected fifteen government leaders , including George W. Bush ( US ) , Vladimir Putin ( Russia ) , Ziang Jemin ( China ) , Tony Blair ( UK ) , Junichiro Koizumi ( Japan ) , Roh Moo-hyun ( Korea ) , and Abdullah Gul ( Turkey ) , and other key individuals , such as John Paul II ( the Former Pope ) and Hans Blix ( UN ) , because their images frequently appear in the dataset \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#50#exact -1#52#51#exact -1#53#52#exact -1#54#53#exact -1#55#54#exact -1#56#55#exact -1#58,59,60#56,57,58,59#para -1#61#60#exact -1#62#61#exact -1#63#62#exact -1#64#63#exact -1#65#64#exact -1#66#65#exact -1#67#66#exact -1#68#67#exact -1#69#68#exact -1#70#69#exact -1#71#70#exact -1#72#71#exact -1#73#72#exact -1#74#73#exact -1#75#74#exact -1#76#75#exact -1#77#76#exact -1#78#77#exact -1#79#78#exact -1#80#79#exact -1#81#80#exact -1#82#81#exact \n"}nil_first --> []
nil_second --> [44, 57]
--------------------------
{1=>"Variations in each person 's name were collected .\n", 2=>"The variations in each person 's name were collected .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"We used an eye detector to detect the positions of the eyes of the detected faces .\n", 2=>"We used an eye detector to detect the positions of the eyes in the detected faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9,10#7,8,9#para -1#13#10#exact -1#11#11#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [12, 13]
nil_second --> [12]
--------------------------
{1=>"The eye detector , built with the same approach as that of Viola and jones \\CITE , had an accuracy of more than 95\\% .\n", 2=>"The eye detector , built with the same approach as in \\CITE , had an accuracy of more than 95\\% .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#16#11#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#17,18#20,21,22#para -1#19#23#exact -1#20#24#exact \n"}nil_first --> [10, 12, 13, 14]
nil_second --> [10]
--------------------------
{1=>"To compensate for illumination effects , the subtraction of the best-fit brightness plane followed by histogram equalization was applied .\n", 2=>"To compensate for illumination effects , the subtraction of the bestfit brightness plane followed by histogram equalization was applied .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"We then used principle component analysis \\CITE to reduce the number of dimensions of the feature vector for face representation .\n", 2=>"We then used principle component analysis \\CITE to reduce the number of dimensions of the feature vector for face representation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10,11#7,8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"Eigenfaces were computed from the original face set returned using the text-based query method .\n", 2=>"Eigenfaces were computed from the original face set returned by the text-based query method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"The number of eigenfaces used to form the eigen space was selected so that 97\\% of the total energy was retained \\CITE . //It is not clear what you mean by \" energy \" in this context . This is the first time you mention this term . You might want to specify what kind of energy you are talking about .\n", 2=>"The number of eigenfaces used to form the eigen space was selected so that 97\\% of the total energy was retained \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
nil_second --> []
--------------------------
{1=>"Given a queried person and letting \\MATH be the total number of faces returned , \\MATH the number of relevant faces , and \\MATH the total number of relevant faces , recall and precision can be calculated as follows :\n", 2=>"Given a queried person and letting \\MATH be the total number of faces returned , \\MATH the number of relevant faces , and \\MATH the total number of relevant faces , recall and precision can be calculated as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10#para -1#18#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#24,25,26,27#16,17,18#para -1#28#19#exact -1#29#20#exact -1#30#21#exact -1#22#22#exact -1#23#23#exact -1#16#24#exact -1#17#25,26#para -1#19#28#exact -1#20#29#exact -1#21#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}nil_first --> [27]
nil_second --> []
--------------------------
{1=>"Precision and recall are only used to evaluate the quality of an unordered set of retrieved faces .\n", 2=>"Precision and recall only evaluate the quality of an unordered set of retrieved faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4,5#6,7,8#para -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact \n"}nil_first --> [3, 5]
nil_second --> []
--------------------------
{1=>"To evaluate ranked lists in which both recall and precision are taken into account , average precision is usually used .\n", 2=>"To evaluate ranked lists in which both recall and precision are taken into account , the average precision is usually used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [15]
--------------------------
{1=>"The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .\n", 2=>"The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6,7,8#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"-\\MATH : the fraction of faces at the top and bottom of the ranked list that are used to form a positive set \\MATH and negative set \\MATH for training weak classifiers in Rank-By-Bagging-ProbSVM-InnerLoop .\n", 2=>"-\\MATH : the fraction of faces lying at the top and bottom of the ranked list that are used to form a positive set \\MATH and negative set \\MATH for training weak classifiers in Rank-By-Bagging-ProbSVM-InnerLoop .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8,9,10#6,7,8,9#para -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> []
nil_second --> [6, 7]
--------------------------
{1=>"We empirically selected \\MATH ( i .e 40\\% samples of the rank list were used ) since a larger \\MATH will increase the number of incorrect labels , and a smaller \\MATH will cause over-fitting .\n", 2=>"We empirically selected \\MATH ( i .e 40\\% samples of the rank list were used ) since larger \\MATH will increase the number of incorrect labels and smaller \\MATH will cause over-fitting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#28#exact -1#27#29,30#para -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact \n"}nil_first --> [17, 27]
nil_second --> []
--------------------------
{1=>"This value is used to determine when the inner loop and the outer loop stop .\n", 2=>"This value is used to determine when the inner loop and the outer loop are stopped .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#14#stem -1#16#15#exact \n"}nil_first --> []
nil_second --> [14]
--------------------------
{1=>"Note that a smaller \\MATH requires more iterations , making the system 's speed slower .\n", 2=>"Note that smaller \\MATH requires more number of iterations making the system 's speed slower .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#8#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [2, 8]
nil_second --> [6, 7]
--------------------------
{1=>"-\\MATH : the kernel type is used for the SVM .\n", 2=>"-\\MATH : the kernel type is used for SVM .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"The default is a linear kernel that is defined as : \\MATH .\n", 2=>"The default is linear kernel that is defined as : \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"We have tested other kernel types , such as RBF or polynomial , but the performance did not change much .\n", 2=>"We have tested other kernel types such as RBF or polynomial , the performance did not change so much .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#11#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [12, 13]
nil_second --> [17]
--------------------------
{1=>"We performed a comparison between our proposed method with other ones .\n", 2=>"We performed a comparison between our proposed method with other existing approaches .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#11#exact \n"}nil_first --> [10]
nil_second --> [10, 11]
--------------------------
{1=>"Text Based Baseline ( TBL ) : Once faces corresponding with images whose captions contain the query name are returned , they are ranked in time order .\n", 2=>"Text Based Baseline ( TBL ) : Once faces corresponding with images whose captions contain the query name are returned , they are ranked by the time order .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#26#24,25#para -1#27#26#exact -1#28#27#exact \n"}nil_first --> []
nil_second --> [24, 25]
--------------------------
{1=>"Once the densest subgraph was found after an edge elimination process , we counted the number of surviving edges of each node ( i .e face ) and used this number as the ranking score .\n", 2=>"Once the densest subgraph was found after an edge elimination process , we counted the number of surviving edge of each node ( i .e face ) and used this number as the score for ranking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#stem -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#35#33#exact -1#33#34#exact -1#36#35#exact \n"}nil_first --> []
nil_second --> [34]
--------------------------
{1=>"We selected a range of \\MATH values that are the same as the values used in DBO : \\MATH .\n", 2=>"We selected a range of \\MATH values that are the same as the values used in DBO : \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"Local Density Score ( LDS ) : This is the first stage of our proposed method .\n", 2=>"Local Density Score ( LDS ) : This is the first stage of our proposed method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"Since we do not know the number of returned faces from text-based search engines , we used another input value \\MATH , defined as the fraction of neighbors , and estimated \\MATH by the formula : \\MATH , where \\MATH is the number of returned faces .\n", 2=>"Since we do not know the number of returned faces from text based search engines , we used another input value \\MATH defined as the fraction of neighbors and estimated \\MATH by the formula : \\MATH , where \\MATH is the number of returned faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#36#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact \n"}nil_first --> [11, 28, 37]
nil_second --> [11, 12]
--------------------------
{1=>"For a large number of returned faces , we set \\MATH to the maximum value of 200 : \\MATH .\n", 2=>"In the case of large number of returned faces , we set \\MATH to the maximum value of 200 : \\MATH .\n", 3=>"-1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"Unsupervised Ensemble Learning Using Local Density Score ( UEL-LDS ) : This is a combination of ranking by local density scores , and the ranked list is used for training a classifier to boost the rank list .\n", 2=>"Unsupervised Ensemble Learning Using Local Density Score ( UEL-LDS ) : This is a combination of ranking by local density scores and then the ranked list is used for training classifier [Singular or plural?]to boost the rank list .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13,14,15#12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#31#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact \n"}nil_first --> [21, 30, 32]
nil_second --> [12, 22, 31, 32, 33]
--------------------------
{1=>"Our proposed methods ( LDS and UEL-LDS ) outperform other unsupervised methods such as TBL , DBO , and DSG .\n", 2=>"Our proposed methods ( LDS and UEL-LDS ) outperform other unsupervised methods such as TBL , DBO and DSG .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Furthermore , the DBO and DSG methods are sensitive to the distance threshold , while the performance of our proposed method is less sensitive .\n", 2=>"Furthermore , the performance of methods DBO and DSG are sensitive to the distance threshold ; while the performance of our proposed method is less sensitive .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#5#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> [13]
nil_second --> [3, 4, 15]
--------------------------
{1=>"It confirms that the similarity measure using shared nearest neighbors is reliable for estimation of the local density score .\n", 2=>"It confirms that the similarity measure using shared nearest neighbors is relieable for estimation of the local density score .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [11]
nil_second --> [11]
--------------------------
{1=>"However , UEL-LDS improves significantly even when the performance of LDS is poor .\n", 2=>"However , UEL-LDS improves the performance significantly even when the performance of LDS is poor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact \n"}nil_first --> []
nil_second --> [4, 5]
--------------------------
{1=>"Figure \\REF shows an examples of the top 50 faces ranked using the TBL , DBO , DSG , and LDS methods .\n", 2=>"Figure \\REF shows an examples of top 50 faces ranked by the methods TBL , DBO , DSG and LDS .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#11#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#12#21#exact -1#20#22#exact \n"}nil_first --> [11, 12, 18]
nil_second --> [10]
--------------------------
{1=>"This ranks irrelevant faces that are near duplicates ( rows 2 and 3 in Figure \\REF( b ) ) higher than relevant faces .\n", 2=>"This makes irrelevant faces that are near duplicates ( row 2 and row 3 in Figure \\REF( b ) ) ranked higher than relevant faces .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact \n"}nil_first --> [1]
nil_second --> [1, 12, 20]
--------------------------
{1=>"The ensemble classifier \\MATH is formed by combining single classifiers from \\MATH to \\MATH .\n", 2=>"The ensemble classifier \\MATH is formed by combination of single classifiers from \\MATH to \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"It clearly indicates that the ensemble classifier is more stable than single weak classifiers . //You use both plural and singular forms of \" classifier \" here , so it is a bit confusing if you are talking about a single classifier or more than one . I suggest you use the same form throughout if applicable .]\n", 2=>"It clearly indicates that the ensemble classifier is more stable that single weak classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [10, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
nil_second --> [10]
--------------------------
{1=>"We conducted another experiment to show the effectiveness of our approach in which learned models are used to annotate new faces of other databases .\n", 2=>"We conducted another experiment to show the effectiveness of our approach in which learned models can be used to annotate new faces of other databases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16,17#15,16#para -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [15]
--------------------------
{1=>"We used each name in the list as a query to obtain the top 500 images from the Google Image Search Engine ( GoogleSE ) .\n", 2=>"For each name in the list , we used it as the query to obtain top 500 images from Google Image Search Engine .\n", 3=>"-1#7#0#lc -1#8#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#10#7#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#11#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#25#exact \n"}nil_first --> [8, 17, 22, 23, 24]
nil_second --> [0, 6, 9]
--------------------------
{1=>"Next , these images were processed using the steps described in section \\REF : extracting faces , detecting eyes , and doing normalization .\n", 2=>"Next , these images were processed as the steps described in section \\REF : extracting faces , detecting eyes and doing normalization .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [6, 19]
nil_second --> [6]
--------------------------
{1=>"There were 4 ,103 faces ( including false positives - non-faces detected as faces ) detected from 7 ,500 returned images .\n", 2=>"There were 4 ,103 faces ( including false positives - non-faces were detected as faces ) detected from 7 ,500 returned images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"On average , the accuracy of the GoogleSE is 57 .08\\% .\n", 2=>"On average , the accuracy of the Google Search Engine ( GoogleSE ) is 57 .08\\% .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#11#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact -1#16#11#exact \n"}nil_first --> [6]
nil_second --> [7, 8, 9, 10, 12]
--------------------------
{1=>"The performances of SVM-SUP-05 and SVM-SUP-10 correspond to the supervised systems ( cf . section \\REF ) that used \\MATH of the data set , respectively .\n", 2=>"The performances of SVM-SUP-05 and SVM-SUP-10 correspond to the supervised systems ( cf . section \\REF ) that used \\MATH of the data set respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [24]
nil_second --> []
--------------------------
{1=>"We evaluated the performance by calculating the precision of the top 20 returned faces , which is common for image search engines and recall and precision on all detected faces of the test set .\n", 2=>"We evaluated the performance by calculating the precision at top 20 returned faces , which is popular for image search engines ; and recall and precision on all detected faces of the test set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#30#8#exact -1#31#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#32#30,31,32#para -1#33#33#exact -1#34#34#exact \n"}nil_first --> [17]
nil_second --> [8, 16, 21]
--------------------------
{1=>"The precision of the top 20 of SVM-SUP-05 is poorer than that of UEL-LDS due to the small number of training samples .\n", 2=>"The precision at top 20 of SVM-SUP-05 is poorer than that of UEL-LDS is due to small number of training samples .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#11#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#18#12#exact -1#12#13#exact -1#14#14#exact -1#15#15#exact -1#16#16,17#para -1#17#18#exact -1#19#19,20#para -1#20#21#exact -1#21#22#exact \n"}nil_first --> [3]
nil_second --> [2, 13]
--------------------------
{1=>"Figure \\REF shows top 20 faces ranked using these two methods .\n", 2=>"Figure \\REF shows top 20 faces ranked by these two methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"Consequently , as shown in Figure \\REF , the model learned by this set performed poorly in recognizing new faces returned by GoogleSE .\n", 2=>"Consequently , as shown in Figure \\REF , the model learned by this set obtained poor performance in recognizing new faces returned by GoogleSE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14#stem -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> [15]
nil_second --> [14, 15]
--------------------------
{1=>"Our approach solely relies on the above assumption ; therefore , it is not affected by the ranking of text-based search engines .\n", 2=>"Our approach solely relies on the above assumption , therefore it is not affected by the ranking of text-based search engines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#8#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"The aim of our future work is to study how to improve the quality of the training sets used in this iteration .\n", 2=>"Our future work is to study how to improve the quality of the training sets used in this iteration .\n", 3=>"-1#12#0#lc -1#13#1#syn -1#11#2#exact -1#0#3#lc -1#1#4#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#8,9,10#10,11,12,13#para -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact \n"}nil_first --> [14, 15, 16]
nil_second --> [7]
--------------------------
{1=>"This method learns the visual consistency among faces in a two-stage process .\n", 2=>"This method learns the visual consistency among the faces in a two-stage process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely to be relevant or irrelevant faces , respectively .\n", 2=>"In the first stage , a relative density score is used to form a ranked list in which faces ranked at the top or bottom of the list are likely relevant or irrelevant faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#38#exact \n"}nil_first --> [30, 31, 36, 37]
nil_second --> []
--------------------------
{1=>"This strong classifier is then applied to the original set to re-rank faces on the basis of the output probabilistic scores .\n", 2=>"This strong classifier is then applied to the original set to re-rank faces on the basis of the output probabilistic scores .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15,16#13,14,15,16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> []
nil_second --> [17]
--------------------------
{1=>"Our approach is beneficial when there are several faces in a returned image , as shown in Figure \\REF .\n", 2=>"Our approach is beneficial in the case multiple faces residing in the returned image as shown in Figure \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#7#7#para -1#8#8#exact -1#4#9#exact -1#12#11#exact -1#13#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [4, 5, 6, 10, 13]
nil_second --> [5, 6, 9, 10, 11]
--------------------------
{1=>"Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .\n", 2=>"Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6,7#4,5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle the various transition types caused by photo flashes , rapid camera movement , and object movement is still challenging .\n", 2=>"Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle various transition types caused by photo flashes , rapid camera movement and object movement is still challenging .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23,24#para -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact \n"}nil_first --> [35]
nil_second --> []
--------------------------
{1=>"We present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing . //detecting / determining?\n", 2=>"In this paper , we present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#24,25,26,27#14,15,16#para -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#18,19,20#20,21,22,23#para -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact \n"}nil_first --> [31, 32, 33]
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"This is possible by assuming that each frame is a word and then the shot boundaries are treated as text segment boundaries ( e.g. topics ) .\n", 2=>"By the formulation that each frame is considered as a word and shot boundaries are treated as boundaries of text segments ( e .g topics ) .\n", 3=>"-1#0#3#lc -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#1#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#19#19#exact -1#20#20#stem -1#17#21#exact -1#21#22#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [0, 1, 2, 4, 12, 23]
nil_second --> [2, 7, 8, 18, 22, 23]
--------------------------
{1=>"The text segmentation based approaches in natural language processing can be used .\n", 2=>"Text segmentation based approaches that have been well studied in natural language processing can be adopted .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#16#12#exact \n"}nil_first --> [0, 11]
nil_second --> [4, 5, 6, 7, 8, 15]
--------------------------
{1=>"The experimental results from various long video sequences have proven the effectiveness of our approach .\n", 2=>"Experimental results on various long video sequences show the effectiveness of our approach .\n", 3=>"-1#8#0#lc -1#0#1#lc -1#1#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#9#syn -1#9,10#10,11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact \n"}nil_first --> [3, 8]
nil_second --> [2]
--------------------------
{1=>"Recent advances in digital technology have made many video archives readily available .\n", 2=>"Recent advances in digital technology have made many video archives available .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Therefore scalable , efficient , and effective tools for indexing and retrieving video are needed .\n", 2=>"Therefore scalable , efficient and effective tools for indexing and retrieving video are needed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .\n", 2=>"With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as continuous frames from a single camera at a time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#39#36#exact -1#36#37#exact -1#37#38#stem -1#38#39#exact -1#43#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#44#46#para -1#45#47#exact \n"}nil_first --> [44, 45]
nil_second --> []
--------------------------
{1=>"By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .\n", 2=>"By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33,34#36,37#para -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#42#50#exact -1#43#51#exact -1#44#52#exact -1#45#53#exact -1#46#54#exact -1#47#55#exact -1#49#56#exact -1#48#57#exact -1#52#58#exact \n"}nil_first --> [1, 2, 6, 11, 20, 44, 45, 46, 47, 48, 49]
nil_second --> [1, 17, 41, 50, 51]
--------------------------
{1=>"A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs over a number of consecutive frames .\n", 2=>"A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs in a number of consecutive frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [22]
nil_second --> [22]
--------------------------
{1=>"A fade is usually a change in brightness with one or several solid black frames in between the key frames , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \\CITE .\n", 2=>"A fade is usually a change in brightness with one or several solid black frames in between , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#23#17#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#32#26#exact -1#33#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#35#35#exact -1#24#36#exact -1#34#37#exact -1#36#38,39#para -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact \n"}nil_first --> [18, 19]
nil_second --> []
--------------------------
{1=>"Since these approaches use threshold-based models for detection , their advantage is they are fast .\n", 2=>"Since these approaches use threshold-based models for detection , their advantage is fast speed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13,14#para -1#14#15#exact \n"}nil_first --> [12]
nil_second --> [13]
--------------------------
{1=>"Recent works \\CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \\CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .\n", 2=>"Recent works \\CITE use machine learning methods for making decision and show impressive results on test videos of TRECVID \\CITE which is a de-facto benchmark for evaluation of various techniques in shot boundary detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26,27#29#para -1#28#30,31#para -1#29#32#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact \n"}nil_first --> [11, 12, 22, 33]
nil_second --> [11]
--------------------------
{1=>"In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .\n", 2=>"In this study , we propose a new approach inspired from natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem of text segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#25#10#syn -1#9#11#exact -1#19#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#28,29,30,31#22,23,24#para -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#26#28,29#para -1#27#30#exact -1#20#32,33#para -1#32#35#exact -1#33#36#exact -1#34#37#exact \n"}nil_first --> [9, 12, 31, 34]
nil_second --> [10, 21]
--------------------------
{1=>"Specifically , each frame is considered a word and a set of consecutive frames , forming a shot , is considered a text segment .\n", 2=>"Specifically , each frame is considered as a word and a set of consecutive frames , forming a shot , is considered as a text segment .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> []
nil_second --> [6, 22]
--------------------------
{1=>"Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of the following labels :\n", 2=>"Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of labels such as\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#24#exact \n"}nil_first --> [22, 23, 25]
nil_second --> [23, 24]
--------------------------
{1=>"PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) , and POSTSEG ( word outside a segment ) .\n", 2=>"PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) and POSTSEG ( word outside a segment ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"The remainder of this paper is organized as follows .\n", 2=>"The remaining of the paper is organized as follows .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact \n"}nil_first --> [3]
nil_second --> [3]
--------------------------
{1=>"Section \\REF introduces our experiments on different long video sequences from the TRECVID dataset .\n", 2=>"Section \\REF introduces experiments on different long video sequences from TRECVID dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact \n"}nil_first --> [3, 11]
nil_second --> []
--------------------------
{1=>"The shot boundary detection process for a given video is carried out through two main stages .\n", 2=>"Given a video , the shot boundary detection process is carried out through two main stages .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#1#6#exact -1#0#7#lc -1#2#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [5]
nil_second --> [3]
--------------------------
{1=>"In the first stage , frames are extracted and labeled with pre-defined labels .\n", 2=>"In the first stage , frames are extracted and labeled by pre-defined labels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"In the second stage , the shot boundaries are identified by grouping the labeled frames into segments .\n", 2=>"In the second stage , shot boundaries are identified by grouping labeled frames into segments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [5, 12]
nil_second --> []
--------------------------
{1=>"We use the following six labels to label frames in a video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , and POST -GRAD ( post-frame of a GRADUAL transition ) .\n", 2=>"We use the following six labels to label frames in video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , POST -GRAD ( post-frame of a GRADUAL transition ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#17#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#27#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#37#28#exact -1#38#29#exact -1#39#30#exact -1#40#31#exact -1#41#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#47#38#exact -1#28#39#exact -1#29#40#exact -1#30#41#exact -1#31#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#57#48#exact -1#58#49#exact -1#59#50#exact -1#60#51#exact -1#61#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#55#56#exact -1#56#57#exact -1#67#58#exact -1#68#59#exact -1#69#60#exact -1#70#61#exact -1#51#62#exact -1#62#64#exact -1#63#65#exact -1#64#66#exact -1#65#67#exact -1#66#68#exact -1#48#69,70#para -1#49#71#exact -1#50#72#exact -1#71#73#exact \n"}nil_first --> [63]
nil_second --> []
--------------------------
{1=>"Given a sequence of labeled frames , the shot boundaries and transition types are identified by looking up and processing the frames marked with a non NORM -FRM label .\n", 2=>"Given a sequence of labeled frames , shot boundaries and transition types are identified by looking up and processing frames marked by non NORM -FRM label .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#21#exact -1#20#22#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact \n"}nil_first --> [7, 20, 23, 24]
nil_second --> [21]
--------------------------
{1=>"For example , if we encounter two consecutive frames respectively marked by IN-CUT and POST-CUT , we can declare that a shot boundary occurs at these frames and the transition type is a CUT .\n", 2=>"For example , if we encounter two consecutive frames marked by IN-CUT and POST-CUT respectively , we can declare that a shot boundary occurs at these frames and the transition type is CUT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#14#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [32]
nil_second --> []
--------------------------
{1=>"In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare that a GRADUAL shot boundary occurs at these frames .\n", 2=>"In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare a GRADUAL shot boundary occurs at these frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [18]
nil_second --> []
--------------------------
{1=>"Figure \\REF shows an example of the labeled frames of a shot transition .\n", 2=>"Figure \\REF shows an example of labeled frames of a shot transition .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .\n", 2=>"To label a frame in video , firstly we extract features for that frame and then use a classifier , that has been trained by annotated frames in advance , to classify it into one of six categories mentioned above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#17#5#exact -1#5#6#exact -1#6#7#exact -1#8#8#exact -1#7#10#exact -1#9#11#exact -1#10#12,13#para -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#18#21#exact -1#19#22#exact -1#20,21#23,24#para -1#22#25#exact -1#23#26#exact -1#27#27#exact -1#28#28#exact -1#24#29#exact -1#25#31#exact -1#26#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33,34#37,38,39,40#para -1#36#41#exact -1#37#42#exact -1#38#43#exact -1#39#44#exact -1#40#45#exact \n"}nil_first --> [9, 20, 30]
nil_second --> [35]
--------------------------
{1=>"We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .\n", 2=>"We use two typical features that are color moments , edge direction histogram for representing visual information of each frame .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#9#5#exact -1#5,6#6,7,8#para -1#7#9#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#14#16,17#para -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact \n"}nil_first --> [11, 15, 18]
nil_second --> [13]
--------------------------
{1=>"However , using this representation is not discriminative enough for frame categorization since the frames of a shot transition usually strongly relate to their neighboring frames .\n", 2=>"However , using this representation is not discriminative enough for frame categorization since frames of a shot transition usually have strong relation to their neighbor frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#20#stem -1#21#21#stem -1#22#22#exact -1#23#23#exact -1#24#24#stem -1#25#25#exact -1#26#26#exact \n"}nil_first --> [13]
nil_second --> [19]
--------------------------
{1=>"Therefore , in this study , we do not directly use the above features .\n", 2=>"Therefore , in this study , we do not directly use above features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"In this way , we can have a unified framework for the shot boundary detection and consequently avoid having to give special treatment to the different shot boundary types as described in many of the works that participated the TRECVID benchmark \\CITE .\n", 2=>"By this way , we can have a unified framework for shot boundary detection and consequently avoid to have special treatments for different shot boundary types as described in many works participated the TRECVID benchmark \\CITE .\n", 3=>"-1#28#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#32#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#syn -1#19#21#exact -1#20#22#stem -1#22#23,24,25#para -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#29#32#exact -1#30#34,35#para -1#31#37#exact -1#33#39#exact -1#34#40#exact -1#35#41#exact -1#36#42#exact \n"}nil_first --> [18, 31, 33, 36, 38]
nil_second --> [0, 21]
--------------------------
{1=>"Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing the color distributions of images \\CITE .\n", 2=>"Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing color distributions of images \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13,14,15#11,12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [18]
nil_second --> [11, 12]
--------------------------
{1=>"The first order ( mean ) , the second order ( variance ) , and the third order ( skewness ) color moments are defined as :\n", 2=>"The first order ( mean ) , the second order ( variance ) and the third order ( skewness ) color moments are defined as :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"The basic steps for computing the edge orientation histogram features are as follows :\n", 2=>"The basic steps to compute edge orientation histogram feature are as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#stem -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#stem -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"The first \\MATH bins are used to represent the edge directions quantized at a \\MATH interval and the remaining bin is used for counting the non-edge pixels .\n", 2=>"The first \\MATH bins are used to represent edge directions quantized at \\MATH interval and the remaining bin is used for counting non-edge pixels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#15#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17,18#para -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact \n"}nil_first --> [13, 24]
nil_second --> []
--------------------------
{1=>"The histogram is normalized by the total number of all the pixels to compensate for different image sizes .\n", 2=>"The histogram is normalized by the number of all pixels to compensate for different image sizes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"We use color moments and an edge orientation histogram to compute the distances between the current frame \\MATH and its neighboring frames as follows :\n", 2=>"We use color moments and edge orientation histogram to compute distances between the current frame \\MATH and it neighbor frames as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#12#11#exact -1#10#12#exact -1#11#13#exact -1#13#14,15#para -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#stem -1#18#20#stem -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"The input image is converted to a LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \\MATH grid .\n", 2=>"The input image is converted to LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \\MATH grid .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#25#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [26]
nil_second --> []
--------------------------
{1=>"For the edge orientation histogram , there are \\MATH values for each input frame image .\n", 2=>"For edge orientation histogram , there are \\MATH values for each input frame image .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"Compute \\MATH values , which are the Euclidean distances between the current frame \\MATH and its neighboring frames ranging from \\MATH .\n", 2=>"Compute \\MATH values which are the Euclidean distance between current frame \\MATH and its neighbor frames ranging from \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7,8#8,9#para -1#9#10,11#para -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#stem -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"In other words , we compute \\MATH , where \\MATH .\n", 2=>"In other words , we compute \\MATH where \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"Support Vector Machines ( SVM ) are a statistical learning method based on the structure risk minimization principle \\CITE .\n", 2=>"The Support Vector Machines ( SVM ) is a statistical learning method based on the structure risk minimization principle \\CITE .\n", 3=>"-1#1#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7,8#6,7#para -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"They have been very efficiently proved to be useful in many pattern recognition applications \\CITE .\n", 2=>"It has been very efficiently proved in many pattern recognition applications \\CITE .\n", 3=>"-1#1#0,1#para -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact \n"}nil_first --> [6, 7, 8]
nil_second --> [0]
--------------------------
{1=>"In the case of binary classification , the objective of the SVM is to find the best separating hyperplane with a maximum margin .\n", 2=>"In the binary classification case , the objective of the SVM is to find a best separating hyperplane with a maximum margin .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#2#exact -1#8#3#exact -1#2#4#exact -1#3#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#10#exact -1#10#11#exact -1#11,12,13,14#12,13,14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"The form of the SVM classifiers is : \\MATH\n", 2=>"The form of SVM classifiers is : \\MATH\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"where \\MATH is the d-dimensional vector of an observation example , \\MATH is the class label , \\MATH is the vector of the \\MATH training example , \\MATH is the number of training examples , and \\MATH is a kernel function , \\MATH is learned through the learning process .\n", 2=>"where \\MATH is the d-dimensional vector of an observation example , \\MATH is a class label , \\MATH is the vector of the \\MATH training example , \\MATH is the number of training examples , and \\MATH is a kernel function , \\MATH is learned through the learning process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#19#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#22#19#exact -1#20#20#exact -1#21#21#exact -1#29#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#46#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact \n"}nil_first --> [46]
nil_second --> [13]
--------------------------
{1=>"SVMs were originally designed for binary classification .\n", 2=>"SVM were originally designed for binary classification .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact \n"}nil_first --> [0]
nil_second --> [0]
--------------------------
{1=>"There are two common approaches for handling multi-class classification .\n", 2=>"To handle the case of multi-class classification , there are two common approaches .\n", 3=>"-1#8#0#lc -1#9#1#exact -1#10#2#exact -1#11#3#exact -1#12#4#exact -1#0,1#5,6#para -1#5#7#exact -1#6#8#exact -1#13#9#exact \n"}nil_first --> []
nil_second --> [2, 3, 4, 7]
--------------------------
{1=>"The first one is the one-against-all method that combines \\MATH binary classifiers , where \\MATH is the number of classes .\n", 2=>"The first one is the one-against-all method that combines \\MATH binary classifiers where \\MATH is the number of classes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"The \\MATH SVM classifier is trained using positive samples as examples of the \\MATH class and negative samples as the examples of the other classes .\n", 2=>"The \\MATH SVM classifier is trained by positive samples being examples of the \\MATH class and negative samples being examples of the other classes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#21#19#exact -1#19#20#exact -1#20#21#exact -1#22#22,23#para -1#23#24#exact -1#24#25#exact \n"}nil_first --> [6, 9, 18]
nil_second --> [6, 9, 18]
--------------------------
{1=>"The second one is the one-against-one method that combines \\MATH binary classifiers in which each classifier is trained on examples from the two classes .\n", 2=>"The second one is the one-against-one method that combines \\MATH binary classifiers in which each classifier is trained on examples of two classes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21,22#para -1#22#23#exact -1#23#24#exact \n"}nil_first --> [20]
nil_second --> [20]
--------------------------
{1=>"of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) , and NORM-FRM ( normal frame that does not belong to any shot transitions ) .\n", 2=>"of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) and NORM-FRM ( normal frame which does not belong to any shot transitions ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39,40,41,42#40,41,42,43#para -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact \n"}nil_first --> [34]
nil_second --> []
--------------------------
{1=>"To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?\n", 2=>"To learn this classifier , we manually annotate frames in the training data .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#1#16#exact \n"}nil_first --> [1, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25]
nil_second --> []
--------------------------
{1=>"Using the trained classifier , we can label a sequence of frames with the tags mentioned above .\n", 2=>"Using the trained classifier , we can label a sequence of frames with tags mentioned above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"A gradual transition usually has a \" ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . \" ' pattern and a cut transition usually has a \" ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . \" 'pattern .\n", 2=>"A gradual transition usually has the pattern \" ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . \" ' and a cut transition usually has the pattern \" ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . \" ' .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#33#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#6#31#exact -1#32#32#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37,38#37,38#para -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact -1#48#47#exact -1#49#48#exact -1#50#49#exact -1#51#50#exact -1#52#51#exact -1#53#52#exact -1#54#53#exact -1#55#54#exact -1#56#55#exact -1#57#56#exact -1#58#57#exact -1#59#58#exact -1#60#59#exact -1#61#60#exact -1#39#61#stem -1#63#62#exact \n"}nil_first --> [33]
nil_second --> [5, 62]
--------------------------
{1=>"The shot boundary detection process is started by checking for these transition patterns in the tagged sequence .\n", 2=>"The shot boundary detection process is started by checking these transition patterns in the tagged sequence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .\n", 2=>"Since the classifier occasionally produce false predictions due to variations caused by photo flashes , rapid camera movement and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many truth shot boundaries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#stem -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#24#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#21#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#22#24#exact -1#23#25#exact -1#28#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#40#para -1#37#41#exact -1#38#42#exact -1#39#43#exact \n"}nil_first --> [23, 30, 38, 39]
nil_second --> []
--------------------------
{1=>"We used annotated data sets from the TRECVID 2003 test sets for the training and testing .\n", 2=>"We used annotated data sets from TRECVID 2003 test sets for training and testing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11,12#12,13,14#para -1#13#15#exact -1#14#16#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"We divided eight videos , each 30-minute long , into two sets : a training set and a test set .\n", 2=>"We divided 8 videos , each 30-minute length , into two sets : training set and testing set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17,18#para -1#17#19#exact -1#18#20#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"The number of frames , the number of shot boundaries , and the types of these sets are shown in Table \\REF .\n", 2=>"The number of frames , the number of shot boundaries and types of these sets are shown in Table \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11,12#12,13,14#para -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Note that , the number of shot boundaries is equal to the number of frames with a PRE-CUT / GRAD label and the number of frames with a PRE-CUT / GRAD label is equal to the number of frames within a POST-CUT / GRAD label .\n", 2=>"Note that , the number of shot boundaries is equal to the number of frames with PRE-CUT / GRAD label and the number of frames with PRE-CUT / GRAD label is equal to the number of frames with POST-CUT / GRAD label .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10,11,12#9,10,11#para -1#22#12#exact -1#23#13#exact -1#24#14#exact -1#25#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#34#23#exact -1#35#24#exact -1#36#25#exact -1#37#26#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31,32,33#33,34,35,36#para -1#13#37#exact -1#14#38#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact \n"}nil_first --> [16, 27, 39, 40]
nil_second --> [15]
--------------------------
{1=>"We used \\MATH grid to divide the input image into sub-images .\n", 2=>"We used \\MATH grid for dividing the input image into sub-images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"As for the edge orientation histogram , we used 12-bins for the edge pixels and one bin for the non-edge pixels .\n", 2=>"As for edge orientation histogram , we used 12-bins for edge pixels and one bin for non-edge pixels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact \n"}nil_first --> [2, 18]
nil_second --> []
--------------------------
{1=>"These parameters were selected from our empirical studies when participating in TRECVID 's tasks .\n", 2=>"These parameters were selected from our empirical studies when participating TRECVID 's tasks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"where \\MATH is the \\MATH-th element of the feature vectors \\MATH , respectively , and \\MATH is the number of dimensions .\n", 2=>"where \\MATH is the \\MATH-th element of the feature vectors \\MATH respectively , \\MATH is the number of dimensions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#11#12#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [13, 14]
nil_second --> []
--------------------------
{1=>"We use LibSVM \\CITE to train the SVM classifiers with a RBF kernel .\n", 2=>"We use LibSVM \\CITE to train SVM classifiers with RBF kernel .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact \n"}nil_first --> [6, 10]
nil_second --> []
--------------------------
{1=>"The optimal \\MATH parameters are found by conducting a grid search with a 5-fold cross validation on a subset of 10 ,000 samples stratified selected from the original dataset .\n", 2=>"The optimal \\MATH parameters are found by conducting a grid search with 5-fold cross validation on a subset 10 ,000 samples stratified selected from the original dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#16#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"As for the multi-class classification , LibSVM used the one-against-one approach .\n", 2=>"As for multi-class classification , LibSVM used the one-against-one approach .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"The results that were evaluated by a tool provided by TRECVID with a standard measurements , such as the precision , recall , and F1 score , clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .\n", 2=>"The results that were evaluated by a tool provided by TRECVID with standard measurement such as precision , recall and F1 score clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12,13#para -1#13#14#stem -1#17#15#exact -1#14#16#exact -1#15#17#exact -1#30#18#exact -1#16#19#exact -1#18#21#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#27#exact -1#23#28#exact -1#24#29#exact -1#25#30#exact -1#26#31#exact -1#27#32#exact -1#28#33#exact -1#29#34#exact -1#34#35#exact -1#31#36#exact -1#32#37#exact -1#33#38#exact -1#39#39#exact -1#35#40#exact -1#36#41#exact -1#37#42#exact -1#38#43#exact -1#40,41#44,45,46#para -1#42#47#exact \n"}nil_first --> [20, 22, 26]
nil_second --> []
--------------------------
{1=>"We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process . //for / by?\n", 2=>"We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> [23, 24, 25]
nil_second --> []
--------------------------
{1=>"Specifically , we selected three sampling rates \\MATH , which were \\MATH and \\MATH .\n", 2=>"Specifically , we selected three sampling rates \\MATH which are \\MATH and \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#syn -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"As shown in Figure \\REF , the best performance was obtained at a sampling rate of \\MATH .\n", 2=>"As shown in Figure \\REF , the best performance is obtained with the sampling rate of \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#syn -1#10#10#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [11, 12]
nil_second --> [11, 12]
--------------------------
{1=>"In Table \\REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .\n", 2=>"In Table \\REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#5#6#exact -1#6#7#exact -1#8#9,10#para -1#9#11#exact -1#10#12#exact -1#12#14#stem -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19,20#para -1#18#21#exact -1#19#22,23#para -1#20#24#exact -1#21#25#exact -1#22#26#para -1#23#27#exact -1#24#28#exact \n"}nil_first --> [3, 5, 8, 13]
nil_second --> [4, 7, 11]
--------------------------
{1=>"The first one is GCM , the second one is EOH , and the last one GCM+EOH is a combination of the distances using GCM and the distances using EOH .\n", 2=>"The first one is GCM , the second one is EOH and the last one GCM+EOH is combination of distances using GCM and distances using EOH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17,18#18,19,20,21#para -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26,27#para -1#24#28#exact -1#25#29#exact -1#26#30#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"The number of dimensions of the feature vectors using GCM and EOH was 20 , while that of feature vectors using GCM+EOH was 40 .\n", 2=>"The number of dimensions of feature vectors using GCM and EOH is 20 while that of feature vectors using GCM+EOH is 40 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11,12#12,13#para -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20,21#22,23#para -1#22#24#exact \n"}nil_first --> [5, 14]
nil_second --> []
--------------------------
{1=>"We also compared the proposed method with the baseline method that computes the differences in the color histograms between two consecutive frames , and then decides the shot transition by using a predefined threshold .\n", 2=>"We also compare the proposed method with the baseline method that computes differences in color histograms between two consecutive frames and then decides a shot transitition by using a predefined threshold .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12,13#12,13,14#para -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#24#27#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact \n"}nil_first --> [15, 22, 26, 28]
nil_second --> [23, 25]
--------------------------
{1=>"Our system achieves a high precision and recall for the CUT transition and this result is comparable to the third-ranked system .\n", 2=>"Our system achieves high precision and recall for the CUT transition and the result is comparable with the third-ranked system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12,13#13,14#para -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [3, 17]
nil_second --> [16]
--------------------------
{1=>"Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .\n", 2=>"Many previous shot boundary detectors usually divided the system into sub-systems in which special treatments were proposed to handle different types of shot transitions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15,16#para -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> []
nil_second --> [15]
--------------------------
{1=>"Therefore , it is generalization is difficult for new test sets .\n", 2=>"Therefore , it is difficult to generalize for new test sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#4#stem -1#3#5#exact -1#4#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [3]
nil_second --> [5]
--------------------------
{1=>"We have proposed a unified and general framework for shot boundary detection that uses a text segmentation based approach .\n", 2=>"Different from these approaches , in this paper , we have proposed a unified and general framework for shot boundary detection using a text segmentation based approach .\n", 3=>"-1#9#0#lc -1#10#1#exact -1#11#2#exact -1#12#3#exact -1#13#4#exact -1#14#5#exact -1#15#6#exact -1#16#7#exact -1#17#8#exact -1#18#9#exact -1#19#10#exact -1#20#11#exact -1#21,22#13,14#para -1#23#15#exact -1#24#16#exact -1#25#17#exact -1#26#18#exact -1#27#19#exact \n"}nil_first --> [12]
nil_second --> [0, 1, 2, 3, 4, 5, 6, 7, 8]
--------------------------
{1=>"Firstly , we label the frames with one of the six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD , and POST -GRAD .\n", 2=>"Firstly , we label frames by one of six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD and POST -GRAD .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#6#7#exact -1#7#8#exact -1#8#9,10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact \n"}nil_first --> [4, 6, 33]
nil_second --> [5]
--------------------------
{1=>"Then we extract the shot boundaries and types from these labeled frames .\n", 2=>"Then we extract shot boundaries and types from these labeled frames .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"In order to label frames , we proposed a new feature type to model the difference and motion in color and the edges between the frames and used it in the classification with SVM classifiers .\n", 2=>"In order to label frames , we proposed a new feature type to model the difference and motion in color and edge between frames and used it in classification with SVM classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#stem -1#22#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30,31#para -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact \n"}nil_first --> [21, 24]
nil_second --> []
--------------------------
{1=>"The experiments we conducted on various videos from TRECVID 2003 have shown that our approach is effective .\n", 2=>"Experiments on various videos of TRECVID 2003 have shown that our approach is effective .\n", 3=>"-1#0#1#lc -1#1#4#exact -1#2#5#exact -1#3,4#6,7#para -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact \n"}nil_first --> [0, 2, 3]
nil_second --> []
--------------------------
{1=>"However , learning weak classifiers , which is one of the most significant tasks in using boosting , is left to users . //learning / training / identifying / finding?<--Here and throughout , I am not sure that \" learning \" is the best word choice . If you change it here , it should be changed throughout .\n", 2=>"However , learning weak classifiers which is one of the most significant tasks in using boosting is left for users .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8,9#6,7,8,9,10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#19#21#exact -1#20#22#exact -1#10#43#para \n"}nil_first --> [5, 17, 20, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
nil_second --> [18]
--------------------------
{1=>"Meanwhile , determining the appropriate number of bins for weak classifiers learned by Real AdaBoost is a challenging task because small ones might not accurately approximate the real distribution while large ones might cause over-fitting , increase computation time , and waste storage space .\n", 2=>"Meanwhile , determining the appropriate number of bins for weak classifiers learned by Real AdaBoost is a challenging task because small one might not well approximate the real distribution while large one might cause over-fitting , increase computation time and waste storage space .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#stem -1#22#22#exact -1#23#23#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#stem -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact \n"}nil_first --> [24, 39]
nil_second --> [24]
--------------------------
{1=>"We have developed Ent-Boost , a novel method for efficiently learning weak classifiers using entropy measures . //method / boosting scheme?\n", 2=>"This paper describes a novel method for efficiently learning weak classifiers using entropy measures , called Ent-Boost .\n", 3=>"-1#16#3#exact -1#14#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#17#16#exact \n"}nil_first --> [0, 1, 2, 17, 18, 19, 20]
nil_second --> [0, 1, 2, 15]
--------------------------
{1=>"Class entropy information is used to automatically estimate the optimal number of bins through discretization .\n", 2=>"The class entropy information is used to estimate the optimal number of bins automatically through discretization process .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#13#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#15#14#exact -1#17#15#exact \n"}nil_first --> []
nil_second --> [0, 16]
--------------------------
{1=>"Then Kullback-Leibler divergence , which is the relative entropy between probability distributions of positive and negative samples , is used to select the best weak classifier in the weak classifier set .\n", 2=>"Then Kullback-Leibler divergence which is the relative entropy between probability distributions of positive and negative samples is employed to select the best weak classifier in the weak classifier set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17,18#19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact \n"}nil_first --> [3, 17]
nil_second --> []
--------------------------
{1=>"Experiments showed that strong classifiers learned by Ent-Boost can achieve good performance and be stored compactly . //[be stored compactly / achieve compact storage?]\n", 2=>"Experiments have shown that strong classifiers learned by Ent-Boost can achieve good performance , and have compact storage space .\n", 3=>"-1#0#0#exact -1#1,2#1#para -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#14#12#exact -1#17#14#syn -1#19#16#exact -1#16#22#exact \n"}nil_first --> [13, 15, 17, 18, 19, 20, 21, 23]
nil_second --> [13, 15, 18]
--------------------------
{1=>"The results of building a robust face detector using Ent-Boost showed the boosting scheme to be effective .\n", 2=>"Results on building a robust face detector are also reported .\n", 3=>"-1#0,1#0,1,2#para -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#15#syn -1#10#17#exact \n"}nil_first --> [8, 9, 10, 11, 12, 13, 14, 16]
nil_second --> [8, 9]
--------------------------
{1=>"Boosting [3] and its variants [4] ? [10] have recently gained much attention from researchers because of their excellent performance .\n", 2=>"Boosting [3] and its variants [4] ,[5] ,[6] ,[7] ,[8] ,[9] ,[10] have recently gained a lot of attentions from researchers because of its excellent performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15,16#11#para -1#18#12#stem -1#19#13#exact -1#20#14#exact -1#21#15#exact -1#22#16#exact -1#23,24#17,18#para -1#25#19#exact -1#26#20#exact \n"}nil_first --> [6, 7]
nil_second --> [6, 7, 8, 9, 10, 11, 17]
--------------------------
{1=>"In regard to face detection , for example , the methods described in [4] , [5] , and [10] are state-of-the-art in terms of both accuracy and running speed .\n", 2=>"In regards to face detection , for example , the methods described in works [4] ,[5] ,[10] represent the state of the art in terms of both high accuracy and running speed .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#29#17#exact -1#17#19#syn -1#23,24,25#21,22,23,24#para -1#28#25#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact \n"}nil_first --> [14, 15, 16, 18, 20, 26]
nil_second --> [13, 15, 16, 18, 19, 20, 21, 22, 26, 27]
--------------------------
{1=>"Typically , a weak classifier is any classifier whose performance is better than random guessing ( i.e. , its error rate is less than 0 .5 ) .\n", 2=>"Typically , each weak classifier is any classifier whose performance is better than random guessing ( i.e. , error rate is less than 0 .5 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [2, 18]
nil_second --> [2]
--------------------------
{1=>"The performances of these weak classifiers are integrated into the final form of a strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .\n", 2=>"Performances of weak classifiers are integrated into the final form of the strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .\n", 3=>"-1#11#0#lc -1#0#1#lc -1#1#2#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7,8,9#9,10,11,12#para -1#15#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}nil_first --> [3, 17]
nil_second --> [10]
--------------------------
{1=>"In practical problems , designing and learning weak classifiers leave practitioners with two main challenges : computational evaluation and discriminant power .\n", 2=>"In practical problems , designing and learning weak classifiers are left for practitioners with two main challenges : computational evaluation and discriminant power .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#syn -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact \n"}nil_first --> []
nil_second --> [9, 11]
--------------------------
{1=>"Generally , for efficient computation , the dimensions of the input space of weak classifiers are reduced be to much smaller than those of the strong classifier[s?] .\n", 2=>"Generally , for efficient computation , the dimension of the input space of weak classifiers is reduced to much lower than that of the strong classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#syn -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19,20#20,21#para -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#27#exact \n"}nil_first --> [17, 22, 26]
nil_second --> [21, 25]
--------------------------
{1=>"In object-detection frameworks [4] , [5] , [11] ? [13] , weak classifiers are usually constructed from one or several features .\n", 2=>"In object-detection frameworks [4] ,[5] ,[11] ,[12] ,[13] weak classifiers are usually constructed from one or several features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact \n"}nil_first --> [4, 5, 6, 7, 8, 9, 10]
nil_second --> [4, 5, 6, 7]
--------------------------
{1=>"Given a feature type , choosing the suitable way to form a weak classifier that balances efficiency and computation is still an open problem [14] .\n", 2=>"Given a feature type , choosing the suitable way to form a weak classifier that balance efficiency and computation is still a open problem [14] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#stem -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#21,22#para -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> []
nil_second --> [21]
--------------------------
{1=>"The first trend is dealing with the problem of how to design features for best representing the target object .\n", 2=>"The first trend is dealing with the problem of how to design features for best representation of the target object .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9#6,7#para -1#16#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [9, 15]
nil_second --> [15]
--------------------------
{1=>"Besides Haar wavelet features [4] , Gabor wavelets [5] , edge orientation histograms ( EOH ) [11] , orientation dominants [12] , scale invariant feature transform ( SIFT )-based high-level features [13] , and local binary patterns ( LBP ) [15] have also been used .\n", 2=>"Besides Haar wavelet features [4] , Gabor wavelets [5] , edge orientation histogram ( EOH ) [11] , orientation dominants [12] , scale invariant feature transform ( SIFT )-based-high-level features [13] and local binary pattern ( LBP ) [15] have also been used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#30#exact -1#30#31#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#stem -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact \n"}nil_first --> [28, 29, 32]
nil_second --> [28]
--------------------------
{1=>"In Discrete AdaBoost [16] , weak classifiers are threshold-functions whose output is restricted to binary data. //[data / values??I think you need a noun here?binary what?]\n", 2=>"In Discrete AdaBoost [16] , weak classifiers are threshold-functions whose the output is restricted to binary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
nil_second --> [10, 16]
--------------------------
{1=>"This leads weak classifiers to be too weak to boost when handling complex data sets .\n", 2=>"This leads weak classifiers are too weak to boost when handling complex data sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#7#4#exact -1#4,5#5,6#para -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"Meanwhile , in Real AdaBoost [3] , a generalized version of Discrete AdaBoost , weak classifiers are piece-wise functions whose output is a real value representing the confidence-rated prediction .\n", 2=>"Meanwhile , in Real AdaBoost [3] , a generalized version of Discrete AdaBoost , weak classifiers are piece-wise functions whose the output is a real value representing the confidence-rated prediction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact \n"}nil_first --> []
nil_second --> [20]
--------------------------
{1=>"Normally , to construct such weak classifiers , one splits the input space \\MATH into non-overlapping blocks ( or subspaces ) \\MATH , \\MATH , . . . , \\MATH so that the predictions of the weak classifier are the same for all instances falling into the same block .\n", 2=>"Normally , to construct such weak classifiers , one splits the input space \\MATH into non-overlapping blocks ( or subspaces ) \\MATH , \\MATH , . . . , \\MATH so that the predictions of the weak classifier are the same for all instances falling into the same block .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#33,34#32,33,34,35#para -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact \n"}nil_first --> []
nil_second --> [32, 35]
--------------------------
{1=>"Typically , most current works [5] , [6] , [8] , [10] , [17] split the data into \\MATH bins that are equal in width . This method suffers from the following limitations : //[works / systems?]\n", 2=>"Typically , most current works [5] ,[17] ,[6] ,[8] ,[10] split the data into \\MATH bins that are equal width which suffers from following limitations :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#10#14#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#24#exact -1#21#28#exact -1#22#29#exact -1#23#30,31#para -1#24#32#exact -1#25#33#exact \n"}nil_first --> [6, 7, 8, 9, 10, 11, 12, 13, 23, 25, 26, 27, 34, 35, 36]
nil_second --> [6, 7, 8, 9, 20]
--------------------------
{1=>"-The way to choose the appropriate number of bins is undetermined .\n", 2=>"-Choosing the appropriate number of bins is undetermined .\n", 3=>"-1#1#4#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact \n"}nil_first --> [0, 1, 2, 3]
nil_second --> [0]
--------------------------
{1=>"Normally , it has been done by trial and error [6] , [17] ? a tedious task .\n", 2=>"Normally , it has been done by trials and errors [6] ,[17] - a tedious task .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#stem -1#10#10#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [11, 12, 13]
nil_second --> [11, 12]
--------------------------
{1=>"In the training cascade of classifiers [6] , [17] , when the complexity of the training data changes over time , using the same number of bins for training every layer is not optimal .\n", 2=>"In the training cascade of classifiers [6] ,[17] , when the complexity of the training data changes over time , using the same number of bins for training every layers is not optimal .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#19#7#exact -1#8#9#exact -1#9#10#exact -1#10,11,12,13#11,12,13#para -1#21#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#21#exact -1#22,23,24#22,23,24,25#para -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#stem -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [8, 20]
nil_second --> [7]
--------------------------
{1=>"Furthermore , it might lengthen computation and training time and waste storage space , which is critical in applications with limited resources , for example , face detection on mobile phones .\n", 2=>"Furthermore it might increase computation and training time , waste storage space which is critical in applications with limited resources , for example , face detection on mobile phones .\n", 3=>"-1#0#0#exact -1#8#1#exact -1#1#2#exact -1#2#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#20#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#23#22#exact -1#21#23#exact -1#22#24#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact \n"}nil_first --> [4, 9, 25]
nil_second --> [3]
--------------------------
{1=>"Choosing a small number of bins , however , might not accurately approximate the real densities of the data distribution and could influence the selection of the best weak classifier .\n", 2=>"Meanwhile choosing a small number of bins might not well approximate the real densities of the data distribution and thus influence selection of the best weak classifier .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#9#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#20#22#exact -1#21,22#23,24,25#para -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact \n"}nil_first --> [6, 7, 8, 11, 21]
nil_second --> [0, 9, 19]
--------------------------
{1=>"A deterministic method is therefore needed to automatically and optimally choose the number of bins .\n", 2=>"It is therefore necessary to have a deterministic method to choose this number of bins automatically and optimally .\n", 3=>"-1#6#0#lc -1#7#1#exact -1#8#2#exact -1#1#3#exact -1#2#4#exact -1#3,4#5,6#para -1#15#7#exact -1#16#8#exact -1#17#9#exact -1#10#10#exact -1#11,12#11,12#para -1#13#13#exact -1#14#14#exact -1#18#15#exact \n"}nil_first --> []
nil_second --> [0, 5, 9]
--------------------------
{1=>"This problem can be formulated as a discretization problem in which subspace boundaries are found by some criteria . //[some criteria?This sounds a bit vague . Could you be more specific?]\n", 2=>"This problem can be formulated as a discretization problem in which subspace boundaries are found by some criteria .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
nil_second --> []
--------------------------
{1=>"Among discretization methods , the entropy-based method [19] has been proved most efficient . Hence , we propose using it to solve the problem .\n", 2=>"Among discretization methods , the entropy based method [19] has been proved most efficiently ; hence , we propose using it to solve the problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#stem -1#25#13#exact -1#15#14#lc -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#22,23,24#20,21,22,23#para \n"}nil_first --> [5, 24]
nil_second --> [5, 6, 14, 21]
--------------------------
{1=>"The entropy-based discretization method is an algorithm that automatically selects appropriate thresholds to split feature values into optimal bins by using entropy measurement .\n", 2=>"The entropy based discretization method is an algorithm that automatically selects appropriate thresholds to split feature values into optimal bins by using entropy measurement .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> [1]
nil_second --> [1, 2]
--------------------------
{1=>"It is a supervised discretization method that takes into account class information and data distribution , so it is generic and can be applied to any kind of input data .\n", 2=>"It is a supervised discretization method which takes into account class information and data distribution , so it is generic and can be applied for any kinds of input data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#25#24,25#para -1#26,27#26,27#para -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> []
nil_second --> [24]
--------------------------
{1=>"Furthermore , many studies have shown that the discretization process might help to improve performance in induction tasks [18] and it can also work with a weighted data distribution . Therefore , it is most appropriate for boosting-based methods .\n", 2=>"Furthermore , many studies have been shown that discretization process might help to improve performance in induction tasks [18] , it can also work with a weighted data distribution ; therefore , it is most appropriate for boosting-based methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#39#29#exact -1#30#30#lc -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact \n"}nil_first --> [7, 19, 39]
nil_second --> [5, 19, 29]
--------------------------
{1=>"Besides learning weak classifiers , selecting the best weak classifier in the large set of weak classifiers in each round of boosting is also important .\n", 2=>"Besides learning weak classifiers , selecting the best weak classifier in the large weak classifier set in each round of boosting is also important .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#15#13#exact -1#19#14#exact -1#13#15#exact -1#14#16#stem -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"Following the method used in [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples . // [used / proposed?]\n", 2=>"Adopting [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples .\n", 3=>"-1#8#1#exact -1#1#5#exact -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#9#12,13#para -1#10#14#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact \n"}nil_first --> [0, 2, 3, 4, 31, 32, 33, 34]
nil_second --> [0]
--------------------------
{1=>"Originally , Discrete AdaBoost , proposed by Freund and Schapire [16] , was a learning method of combining weak classifiers to form a strong classier .\n", 2=>"Originally , Discrete AdaBoost proposed by Freund and Schapire [16] is a learning method of combining weak classifiers to a strong classier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10,11#12,13#para -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact \n"}nil_first --> [4, 11, 21]
nil_second --> []
--------------------------
{1=>"Given a training set \\MATH , where \\MATH and \\MATH , a weak classifier \\MATH has the form \\MATH .\n", 2=>"Given a training set \\MATH where \\MATH and \\MATH , a weak classifier \\MATH has the form \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#9#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Therefore , in many applications [4] , [5] , [7] , it is simplified by associating with one feature \\MATH .\n", 2=>"Therefore , in many applications [4] ,[5] ,[7] , it is simplified by associating to one feature \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#6#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [7, 8, 9, 10, 16]
nil_second --> [6, 7, 14]
--------------------------
{1=>"Through boosting processing , weak classifiers are combined into a strong classifier \\MATH where \\MATH are values that measure the performance of the selected weak classifier .\n", 2=>"Through boosting processing , weak classifiers are combined into a strong classifier \\MATH where \\MATH are values that measure performance of the selected weak classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19,20,21,22#para -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> []
nil_second --> [20, 21]
--------------------------
{1=>"In the boosting process , a distribution \\MATH or set of weights over the training samples are maintained and updated so that subsequent weak classifiers focus on the strong-classified samples . //[hard / strong?]\n", 2=>"In boosting process , a distribution \\MATH or set of weights over the training samples are maintained and updated so that subsequent weak classifiers focus on the hard classified samples .\n", 3=>"-1#0#0#exact -1#12#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#26#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [27, 28, 31, 32, 33]
nil_second --> [27, 28]
--------------------------
{1=>"Real AdaBoost [3] is a generalized version of Discrete AdaBoost in that weak classifiers are real-valued functions instead of binary ones and \\MATH is found numerically instead of by predescription . //[This method also involves?NOTE : A method cannot propose something .\n", 2=>"Real AdaBoost [3] is a generalized version of Discrete AdaBoost in that weak classifiers are real-valued functions instead of binary ones and \\MATH is found numerically in general instead of predescription .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#28#26#exact -1#29#27#exact -1#30#29#exact -1#31#30#exact \n"}nil_first --> [28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
nil_second --> [26, 27]
--------------------------
{1=>"Do you mean that the creators of this system proposed this?] designing weak classifiers that partition the input space into subspaces so that the predictions are unique in each subspace .\n", 2=>"This method also proposes designing weak classifiers that partition the input space into subspaces so that its predictions are unique in each subspace .\n", 3=>"-1#18#0,1#para -1#15#3#exact -1#9#4#exact -1#0#7#lc -1#4#11#exact -1#5#12#exact -1#6#13#exact -1#7#14#exact -1#8#15#exact -1#10#17#exact -1#11#18#exact -1#12#19#exact -1#13#20#exact -1#14#21#exact -1#16,17#23,24#para -1#19#25,26#para -1#20#27#exact -1#21#28#exact -1#22#29#exact -1#23#30#exact \n"}nil_first --> [2, 5, 6, 8, 9, 10, 16, 22]
nil_second --> [1, 2, 3]
--------------------------
{1=>"Such weak classifiers are used widely in current state-of-the-art object detection systems [5] , [8] , [17] .\n", 2=>"Such weak classifiers are used widely in current state of the art object detection systems [5] ,[17] ,[8] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#18#17#exact \n"}nil_first --> [8, 13, 14, 15, 16]
nil_second --> [8, 9, 10, 11, 16, 17]
--------------------------
{1=>"It is proven in [3] that the most appropriate choice for the prediction of the weak classifier on block \\MATH to maximize the margin is \\MATH where \\MATH is a smoothed value in order to handle cases in which \\MATH is very small or even zero .\n", 2=>"It is proved in [3] that the most appropriate choice for the prediction of the weak classifier on block \\MATH to maximize the margin is \\MATH where \\MATH is a smoothed value in order to handle cases that \\MATH is very small or even zero .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#6,7,8#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact \n"}nil_first --> [37, 38]
nil_second --> [5, 37]
--------------------------
{1=>"Real AdaBoost is easy to implement , but in practical applications , designing and learning weak classifiers depend on specific applications .\n", 2=>"Real AdaBoost is easy to implement ; however , in practical applications , designing and learning weak classifiers depend on specific applications .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#6#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> [7]
nil_second --> [6, 7]
--------------------------
{1=>"In such face detection systems as [those described in?] [5] , [6] , [8] , and [17] , weak classifiers are usually associated with one feature .\n", 2=>"In such face detection systems as [5] ,[6] ,[17] ,[8] , weak classifiers are usually associated with one feature .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#9#exact -1#10#10#exact -1#11#18#exact -1#12#19#exact -1#13#20#exact -1#14#21#exact -1#15#22#exact -1#16#23#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact \n"}nil_first --> [6, 7, 8, 11, 12, 13, 14, 15, 16, 17]
nil_second --> [7, 8, 9]
--------------------------
{1=>"With a very large number of available features ? hundreds of thousands ? [there are many candidates from which to / many choices must be made to?] select one weak classifier for each round of boosting .\n", 2=>"With a very large number of available features , hundreds of thousands , there are a lot of choices to choose one weak classifier for each round of boosting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5#2,3,4#para -1#10#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#17#10#exact -1#11#11#exact -1#14#14#exact -1#19#19#exact -1#18#22#exact -1#20#27#syn -1#21#28#exact -1#22#29#exact -1#23#30#exact -1#24#31#exact -1#25#32#exact -1#26#33#exact -1#27#34#exact -1#28#35#exact -1#29#36#exact \n"}nil_first --> [8, 12, 13, 15, 16, 17, 18, 20, 21, 23, 24, 25, 26]
nil_second --> [8, 12, 13, 15, 16]
--------------------------
{1=>"Optimally selecting the suitable weak classifier makes the final strong classifier more robust and efficient .\n", 2=>"Generally , optimally selecting the suitable weak classifier will make the final strong classifier more robust and efficient .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#9#6#stem -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact \n"}nil_first --> []
nil_second --> [0, 1, 8]
--------------------------
{1=>"Furthermore , optimal selection can reduce the number of boosting rounds , thus directly shortening training time .\n", 2=>"Furthermore , it can reduce the number of boosting rounds that directly shorten training time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#11#13#exact -1#12#14#stem -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [2, 3, 11, 12]
nil_second --> [2, 10]
--------------------------
{1=>"Most studies so far have been focused on how to measure the discriminant power of weak classifiers in order to select the best weak classifier .\n", 2=>"So far , most current studies have been focused on how to measure the discriminant power of weak classifiers in order to select the best weak classifier .\n", 3=>"-1#3#0#lc -1#5#1#exact -1#0#2#lc -1#1#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact \n"}nil_first --> []
nil_second --> [2, 4]
--------------------------
{1=>"Many measurements have been proposed , for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] , and recently , Jensen-Shannon divergence [8] and mutual information [9] ( Table 1 ) .\n", 2=>"Many measurements have been proposed ; for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] and , recently , Jensen-Shannon divergence [8] and mutual information [9] ( cf . Table 1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#5#exact -1#6#6#exact -1#7#7#exact -1#12#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#17#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#19#16#exact -1#16#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#30#28#exact -1#31#29#exact -1#29#31#exact \n"}nil_first --> [19, 30]
nil_second --> [5, 28, 32]
--------------------------
{1=>"Meanwhile , few studies have been made on efficiently partitioning subspaces .\n", 2=>"Meanwhile , few studies have been made for efficiently partitioning subspaces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by the above measures give similar performances . //[measurements / measures?]\n", 2=>"As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by above measurements give comparable performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#stem -1#19#20#exact -1#20#21#para -1#21#22#stem -1#22#23#exact \n"}nil_first --> [17, 24, 25, 26]
nil_second --> []
--------------------------
{1=>"However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .\n", 2=>"However , it will be shown in section 5 , these performances are affected seriously if different subspace splitting methods are used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#3#exact -1#8#4#exact -1#3#5#exact -1#4,5#6#para -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12,13#para -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact \n"}nil_first --> [2]
nil_second --> [2, 6, 15]
--------------------------
{1=>"The proposed boosting scheme , Ent-Boost , is an integration of adaptive entropy-based subspace splitting and the symmetric KL divergence-based weak classifier selection .\n", 2=>"The proposed boosting scheme Ent-Boost is an integration of adaptive entropy-based subspace splitting and the symmetric KL divergence-based weak classifier selection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [4, 6]
nil_second --> []
--------------------------
{1=>"In Ent-Boost , each weak classifier is constructed from one feature and trained on weighted training samples similar to [those used in?] Real AdaBoost .\n", 2=>"In Ent-Boost , each weak classifier is constructed from one feature and trained on the weighted training samples similar to Real AdaBoost .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [19, 20, 21]
nil_second --> [14]
--------------------------
{1=>"However , instead of using the equal-width binning method used in Real AdaBoost [6] , [17] which has a hard time predicting the suitable number of bins in advance , we use the entropy-based discretization method [19] to split the input space into subspaces .\n", 2=>"However , instead of using equal-width binning method like Real AdaBoost [6] ,[17] which is hard to know the suitable number of bins in advance , we use entropy-based discretization method [19] to split the input space into subspaces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#18#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#27#9,10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#25#14#exact -1#13#16#exact -1#14#17,18#para -1#15#19#exact -1#34#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#26#30#exact -1#35#31#para -1#28#33#exact -1#29#34#exact -1#30#35#exact -1#31#36#exact -1#32#37#exact -1#33#38#exact -1#36#41#exact -1#37#42#exact -1#38#43#exact -1#39#44#exact \n"}nil_first --> [15, 20, 21, 29, 32, 39, 40]
nil_second --> [8, 12, 16, 17]
--------------------------
{1=>"This subspace splitting process is totally automatic ; the stopping criteria of the splitting process are determined using minimum description length principles ( MDLP ) . This process will be described in greater detail in the next section .\n", 2=>"This subspace splitting process is totally automatically in which the stopping criteria of splitting process is determined through using Minimum Description Length Principles ( MDLP ) ( see the next section ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#28#12#exact -1#13#13#exact -1#14#14#exact -1#15,16#15,16#para -1#18#17#exact -1#19#18#lc -1#20#19#lc -1#21#20#lc -1#22#21#lc -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#32#25#exact -1#7#31#exact -1#29#34,35,36#para -1#30#37#exact \n"}nil_first --> [7, 26, 27, 28, 29, 30, 32, 33, 38]
nil_second --> [8, 17, 26, 27, 31]
--------------------------
{1=>"To select the best weak classifier from the input weak classifier set , we use symmetric KL divergence as in [5] , which measures the distance between two distributions as follows : \\MATH where \\MATH and \\MATH are probability distributions of a discrete random variable .\n", 2=>"To select the best weak classifier from the input weak classifier set , we use symmetric KL divergence as in [5] which measures the distance between two distributions as follows : \\MATH where \\MATH and \\MATH are probability distributions of a discrete random variable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"This formula can be rewritten in entropy terms : \\MATH or \\MATH where \\MATH and \\MATH are entropy and \\MATH is cross entropy of \\MATH and \\MATH .\n", 2=>"This formula can be rewritten in entropy terms : \\MATH or \\MATH where \\MATH and \\MATH are entropy , and \\MATH is cross entropy of \\MATH and \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact \n"}nil_first --> []
nil_second --> [18]
--------------------------
{1=>"Note that the discretization process is performed in every round of boosting to adapt to new distributions of samples .\n", 2=>"Note that the discretization process is performed in every round of boosting to adapt to new distributions of samples .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13,14,15#12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"As a result , the number of intervals of the selected weak classifier varies . //[classifier varies / classifiers vary?]\n", 2=>"As a result , the number of intervals of selected weak classifier varies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [9, 15, 16, 17, 18, 19]
nil_second --> []
--------------------------
{1=>"This is different from previous methods , which fix the number of equal-width intervals in advance .\n", 2=>"This is different from previous methods that fix the number of equal-width intervals in advance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [6, 7]
nil_second --> [6]
--------------------------
{1=>"This section briefly describes automatic subspace splitting using entropy-based discretization .\n", 2=>"This section gives a brief introduction on automatic subspace splitting using entropy-based discretization .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3,4#2#para -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact \n"}nil_first --> [3]
nil_second --> [2, 5, 6]
--------------------------
{1=>"Discretization is a quantizing process that converts continuous values into discrete values . It typically consists of four steps [18] .\n", 2=>"Basically , discretization is a quantizing process that converts continuous values into discrete values ; it typically consists of four steps [18] :\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#15#13#lc -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact \n"}nil_first --> [12, 20]
nil_second --> [0, 1, 14, 22]
--------------------------
{1=>"A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold .\n", 2=>"A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#33,34,35#32,33,34,35#para -1#36#36#exact -1#37#37#exact \n"}nil_first --> []
nil_second --> [32]
--------------------------
{1=>"The stopping criteria are usually selected by considering a trade-off between lower arity ( the number of intervals or the number of bins ) and its effect on the accuracy of classification tasks .\n", 2=>"The stopping criteria are usually selected according to a trade-off between lower arity ( the number of intervals or the number of bins ) and its effect on the accuracy of classification tasks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27,28,29,30#27,28,29#para -1#31#30,31#para -1#32#32#exact -1#33#33#exact \n"}nil_first --> [6, 7]
nil_second --> [6, 7]
--------------------------
{1=>"A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .\n", 2=>"A higher arity can make the understanding of an attribute more difficult , while a very low arity may affect predictive accuracy negatively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#11#6,7#para -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#23#22#exact \n"}nil_first --> [19]
nil_second --> [10, 19, 22]
--------------------------
{1=>"For each cut-point \\MATH that splits set \\MATH into two subsets \\MATH , the class entropy of a subset \\MATH is defined as \\MATH where \\MATH is the number of classes \\MATH , and \\MATH is the proportion of examples in \\MATH that have class \\MATH .\n", 2=>"For each cut-point \\MATH that splits set \\MATH into two subsets \\MATH , the class entropy of a subset \\MATH is defined as \\MATH where \\MATH is the number of classes \\MATH , and \\MATH is the proportion of examples in \\MATH that have class \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#36,37,38#35,36,37,38#para -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact \n"}nil_first --> []
nil_second --> [35]
--------------------------
{1=>"Given set S and a potential binary partition \\MATH , specified on S by the given cut-point \\MATH , a stopping criteria is used to decide whether or not this partition should be accepted .\n", 2=>"Given set S and a potential binary partition , \\MATH , specified on S by the given cut-point \\MATH , a stopping criteria is used to decide whether or not this partition should be accepted .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25,26#24,25,26,27#para -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> []
nil_second --> [8, 27, 28]
--------------------------
{1=>"The minimum description length of an object is defined as the minimum number of bits required to uniquely specify that object out of the universe of all objects .\n", 2=>"Originally , the minimum description length of an object is defined as the minimum number of bits required to uniquely specify that object out of the universe of all objects .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12,13#10,11,12,13#para -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact \n"}nil_first --> []
nil_second --> [0, 1, 14, 15]
--------------------------
{1=>"The sender needs to convey needed information for the proper class labeling of the example set to the receiver .\n", 2=>"The sender needs to convey to proper class labeling of the example set to the receiver .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#14#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#15#18#exact -1#16#19#exact \n"}nil_first --> [5, 6, 7, 17]
nil_second --> [5]
--------------------------
{1=>"where \\MATH and \\MATH where\\MATH is the number of classes in \\MATH Extensive experiments [18] , [19] recommended that this method should be the first choice for variable discretization because it gives a small number of cut-points while maintaining consistency .\n", 2=>"where \\MATH and \\MATH \\MATH is the number of classes in \\MATH Extensive experiments [19] ,[18] recommended that this method should be the first choice for variable discretization because it gives small number of cut-points while maintaining consistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32,33#para -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact \n"}nil_first --> [4, 14, 15]
nil_second --> [4, 15]
--------------------------
{1=>"For our experiments , face and non-face patterns were of size 24x24 . //[what is the unit here?]\n", 2=>"For experiments , face and non-face patterns are of size 24x24 .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7,8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [1, 13, 14, 15, 16, 17]
nil_second --> []
--------------------------
{1=>"Haar wavelet features , which have been used in many face detection systems [4] , [6] , [14] , were used in our experiments .\n", 2=>"Haar wavelet feature that has been widely used in many face detection systems [4] ,[6] ,[14] is used in our experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#7,8#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#4,5#19#para -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact \n"}nil_first --> [3, 4, 14, 15, 16, 17, 18]
nil_second --> [3, 6, 14, 15, 16]
--------------------------
{1=>"These consisted of four kinds of features modeled from adjacent basic rectangles with the same size and shape .\n", 2=>"It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .\n", 3=>"-1#1,2#1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14,15,16#13,14,15,16#para -1#17#17#exact -1#18#18#exact \n"}nil_first --> [0]
nil_second --> [0, 13]
--------------------------
{1=>"The feature value was defined as the difference of the sum of the pixels within rectangles ( Figure 3 ) .\n", 2=>"The feature value is defined as the difference of sum of the pixels within rectangles ( cf . Figure 3 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> [12]
nil_second --> [16, 17]
--------------------------
{1=>"Figure 4 shows a comparison of the performances of strong classifiers trained by the different boosting schemes : AdaBoost [4] , Real AdaBoost [17] , and Ent-Boost .\n", 2=>"Figure 4 shows a comparison of performances of strong classifiers trained by different boosting schemes that are AdaBoost [4] , Real AdaBoost [17] and Ent-Boost .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13,14#para -1#13#15#exact -1#14#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [6, 17, 24]
nil_second --> [15, 16]
--------------------------
{1=>"For Real AdaBoost , subspace splitting is done by equal-width binning in which the number of bins is arbitrarily selected to be 64 and 128 .\n", 2=>"As for Real AdaBoost , the subspace splitting is done by equal width binning in which the number of bins is arbitrarily selected to be 64 and 128 .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact \n"}nil_first --> [9]
nil_second --> [0, 5, 11, 12]
--------------------------
{1=>"Overall , Ent-Boost produced the best result .\n", 2=>"Overall , Ent-Boost has the best result .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact \n"}nil_first --> [3]
nil_second --> [3]
--------------------------
{1=>"As for storage space , the Ent-Boost-based classifier only uses 6 .79 bins on average , which is much fewer than the number used by Real AdaBoost-based classifiers .\n", 2=>"As for storage space , the Ent-Boost based classifier only employs 6 .79 bins on average which is much smaller than that of Real AdaBoost-based classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#7#exact -1#9#8#exact -1#10#9#syn -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20,21#20,21,22#para -1#23#24,25#para -1#24#26#exact -1#25#27#exact -1#26#28#exact \n"}nil_first --> [6, 15, 19, 23]
nil_second --> [6, 7, 19, 22]
--------------------------
{1=>"It was a cascade of Ent-Boost-based classifiers that were trained [through a process similar to that used in] [4] .\n", 2=>"It was a cascade of Ent-Boost based classifiers that were trained similar to [4] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#13#exact -1#12#14#exact -1#13#18#exact -1#14#19#exact \n"}nil_first --> [5, 10, 11, 12, 15, 16, 17]
nil_second --> [5, 6]
--------------------------
{1=>"The performances of the AdaBoost-based face detector [4] and our Ent-Boost-based face detector on the MIT+CMU test set [1] confirmed the effectiveness of our proposed boosting scheme ( Table 2 ) .\n", 2=>"Performances of AdaBoost-based face detector [4] and Ent-Boost based face detector on MIT+CMU test set [1] shown in Table 2 has confirmed the effectiveness of our proposed boosting scheme .\n", 3=>"-1#22#0#lc -1#0#1#lc -1#1#2#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#25#9#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#21#19#exact -1#23,24#20,21,22#para -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#18#28#exact -1#19#29#exact -1#29#31#exact \n"}nil_first --> [3, 10, 14, 23, 27, 30]
nil_second --> [7, 8, 16, 17, 20]
--------------------------
{1=>"We have described Ent-Boost , a variant of AdaBoost , which uses entropy measures for automatic subspace splitting and optimal weak classifier selection .\n", 2=>"We have presented Ent-Boost , a variant of AdaBoost , which uses entropy measure for automatic subspace splitting and optimal weak classifier selection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14#13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"The resultant strong classifier has good performance and achieves compact storage .\n", 2=>"The resulted strong classifier has good performance and compact storage .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"Furthermore , this new boosting scheme overcomes the main limitation of Real AdaBoost , which is difficulty in determining the suitable number of bins for subspace splitting .\n", 2=>"Furthermore , it overcomes the main limitation of Real AdaBoost which is hard to determine the suitable number of bins for subspace splitting .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#14#exact -1#11#15#exact -1#13,14#17,18#para -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact \n"}nil_first --> [2, 3, 4, 5, 13, 16]
nil_second --> [2, 12]
--------------------------
{1=>"Because it considers the class information and the distribution of the input data in the splitting process , this method is generic and can be used for other applications .\n", 2=>"By considering the class information and the distribution of the input data in splitting process , this method is generic and can be applied to other applications .\n", 3=>"-1#1#2#stem -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14,15#para -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21,22#23,24,25,26#para -1#25#27#exact -1#26#28#exact -1#27#29#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 23, 24]
--------------------------
{1=>"This paper describes an efficient feature selection method which that quickly selects a small subset out of a given huge feature set ; the proposed method for will be useful for building robust object detection systems .\n", 2=>"This paper describes an efficient feature selection method which quickly selects a small subset out of a given huge feature set for building robust object detection systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#26#exact -1#22#31#exact -1#23#32#exact -1#24#33#exact -1#25#34#exact -1#26#35#exact -1#27#36#exact \n"}nil_first --> [22, 23, 24, 25, 27, 28, 29, 30]
nil_second --> []
--------------------------
{1=>"In this filter-based method , features are selected so that not only to maximizeing their relevance with the target class but also to minimizeing their mutual dependency .\n", 2=>"In this filter-based method , features are selected so that not only maximizing their relevance with the target class but also minimizing their mutual dependency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#stem -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#23#stem -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [12, 22]
nil_second --> []
--------------------------
{1=>"As a result , the selected feature set only contains only highly informative and non-redundant features , which significantly improve classification performance when combined together , significantly improve classification performance .\n", 2=>"As a result , the selected feature set only contains highly informative and non-redundant features which when combined together , significantly improve classification performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#19#16#exact -1#15#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#16#22#exact -1#17#23#exact -1#18#24#exact -1#24#30#exact \n"}nil_first --> [25, 26, 27, 28, 29]
nil_second --> []
--------------------------
{1=>"The relevance and mutual dependency of features are measured by using conditional mutual information ( CMI ), in which features and classes are treated as discrete random variables . //[ ,?<--A comma can be used here if the following describes CMI in general .]\n", 2=>"The relevance and mutual dependency of features are measured by using conditional mutual information ( CMI ) in which features and classes are treated as discrete random variables .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [16, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
nil_second --> [16]
--------------------------
{1=>"Experiments on different huge feature sets have shown that the proposed CMI-based feature selection can both reduce significantly the training time significantly and achieve high accuracy .\n", 2=>"Experiments on different huge feature sets have shown that the proposed CMI-based feature selection can both reduce significantly the training time and achieve high accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [21]
nil_second --> []
--------------------------
{1=>"One of the fundamental research issues in pattern recognition is feature selection , which is the task of finding a small subset out of a given large set of features .\n", 2=>"One of the fundamental research issues in pattern recognition is feature selection which is the task of finding a small subset out of a given large set of features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"Improving the method of accomplishing this task is important due to the following three reasons .\n", 2=>"It is significant due to the following three reasons .\n", 3=>"-1#5#1#exact -1#1#7#exact -1#2#8#syn -1#3#9#exact -1#4#10#exact -1#6#11,12#para -1#7#13#exact -1#8#14#exact -1#9#15#exact \n"}nil_first --> [0, 2, 3, 4, 5, 6]
nil_second --> [0]
--------------------------
{1=>"First , there are many ways can be used to represent a target object , and this variety leadsleading to a huge feature set .\n", 2=>"First , there are many ways to represent a target object , leading to a huge feature set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#13#19#exact -1#14#20#exact -1#15#21#exact -1#16#22#exact -1#17#23#exact -1#18#24#exact \n"}nil_first --> [6, 7, 8, 15, 16, 17, 18]
nil_second --> [12]
--------------------------
{1=>"Second , a huge feature set usually includes many irrelevant and redundant features that can degrade the generalization performance of classifiers , waste storage space , and increase training time [2 , 3] .\n", 2=>"Second , a huge feature set usually includes many irrelevant and redundant features that can degrade the generalization performance of classifiers , waste storage space and increase training time [2 , 3] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#30#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [31]
nil_second --> []
--------------------------
{1=>"In face detection , the success of systems such as those in [1 , 4] comes mainly from efficient feature selection methods .\n", 2=>"In face detection , the success of systems such as those in [1 , 4] comes mainly from efficient feature selection methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7,8,9#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"Generally , feature selection methods can be categorized into two kinds : the filter-based approach and the wrapper-based approach [5] .\n", 2=>"Generally , feature selection methods can be categorized into two kinds : filter-based approach and wrapper-based approach [5] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [12, 16]
nil_second --> []
--------------------------
{1=>"The filter-based approach is independent of any induction algorithm , while but the wrapper-based approach is associated with a specific induction algorithm to evaluate the quality of the selected feature subset . //[goodness / quality / appropriateness?<--If \" goodness \" is the word you would usually use in your field for this , it is fine , but I would suggest a different word choice otherwise . \" Goodness \" seems vague , so in what sense do you mean \" good \" ?]\n", 2=>"The filter-based approach is independent of any induction algorithm while the wrapper-based approach is associated with a specific induction algorithm to evaluate the goodness of the selected feature subset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#23#38#exact \n"}nil_first --> [9, 11, 25, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
nil_second --> []
--------------------------
{1=>"In the filter-based approach , features are normally selected based on their individual predictive power . This power is measured by Fisher scores , Pearson correlation [6] , or mutual information [7] .\n", 2=>"In the filter-based approach , features are normally selected based on their individual predictive power which is measured by Fisher scores , Pearson correlation [6] or mutual information [7] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#29#15#exact -1#16#17,18#para -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact \n"}nil_first --> [16, 27, 32]
nil_second --> [15]
--------------------------
{1=>"The major advantage of these measurement methods is their speed and ability to scale to huge feature sets .\n", 2=>"The major advantage of these methods is their speed and ability to scale to huge feature sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"However , because the mutual relationships between features is are often not taken into account , leading the selected features might be highly redundant and less informative because two features with high individual predictive power , when combined together , might not bring significant performance improvement . Combining compared with two features of which one of them has low predictive power but is useful when combined with others would thus be more effective for improving performance .\n", 2=>"However , the mutual relationships between features is often not taken into account , leading selected features might be highly redundant and less informative because two features with high individual predict power when combined together might not bring significant performance improvement compared with two features which one of them has low predictive power but is useful when combined with others .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#24#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8,9#9,10,11#para -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#51#33#exact -1#52#34#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#40#exact -1#36#41#exact -1#37#42#exact -1#38#43#exact -1#39#44#exact -1#40#45#exact -1#60#46#exact -1#57#47#stem -1#41#48#exact -1#42#49#exact -1#43#50#exact -1#44#51#exact -1#47#52#exact -1#45#53#exact -1#46#54#exact -1#48#55,56#para -1#49#57#exact -1#50#58#exact -1#30#59#stem -1#31#60#exact -1#53#61#exact -1#54#62#exact -1#55#63#exact -1#56#64#exact -1#58#66#exact -1#59#67#exact \n"}nil_first --> [17, 27, 35, 39, 65, 68, 69, 70, 71, 72, 73, 74, 75, 76]
nil_second --> []
--------------------------
{1=>"Since wrapper-based feature selection methods use machine learning algorithms as a black box in the selection process , they can suffer from over-fitting in situations of when applied to small training sets . //[when used with / when applied to?]\n", 2=>"Since wrapper-based feature selection methods use machine learning algorithms as a black box in selection process , they can suffer from over-fitting in situations of small training sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#28,29#para -1#26#30#exact -1#27#31#exact -1#28#32#exact \n"}nil_first --> [26, 27, 33, 34, 35, 36, 37, 38, 39]
nil_second --> []
--------------------------
{1=>"Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands of features , so using wrapper-based methods is obviously inefficient because of the very high computation costs they incur .\n", 2=>"Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands features , using wrapper-based methods is obviously inefficient because of very high computation cost .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18,19,20#18,19,20,21#para -1#21#22#exact -1#22#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33,34#para -1#32#35#exact -1#33#36#exact -1#34#37#stem -1#35#40#exact \n"}nil_first --> [24, 38, 39]
nil_second --> []
--------------------------
{1=>"For example , in the state- of- the- art face detection system in [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by using AdaBoost has takentook several weeks . //[by using / generated by?]\n", 2=>"For example , in the state of the art face detection system [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by AdaBoost has taken several weeks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#29#exact -1#28#30#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact \n"}nil_first --> [5, 6, 7, 12, 28, 31, 35, 36, 37, 38, 39]
nil_second --> [5, 6, 7, 29]
--------------------------
{1=>"Consequently , feature selection methods based on conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of the above approaches for handling large scale feature sets .\n", 2=>"Consequently , conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of above approaches for handling large scale feature sets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#10#3#exact -1#11#4#exact -1#8#5#exact -1#2#7#exact -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6#11#exact -1#7#12#exact -1#33#14#exact -1#28#16#para -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact -1#19#24#exact -1#20#25#exact -1#21#26#exact -1#22#27#exact -1#23#28#exact -1#24#29#exact -1#25#30#exact -1#26#31#exact -1#27#33#exact -1#29#35#exact -1#30#36#exact -1#31#37#exact -1#32#38#exact -1#34#40#exact -1#35#41#exact \n"}nil_first --> [6, 13, 15, 32, 34, 39]
nil_second --> []
--------------------------
{1=>"The main goal of these CMI-based methods is to select features which that maximize their relevance with the target class and to simultaneously minimize mutual dependency between selected ones . //[idea / goal?]\n", 2=>"The main idea of CMI-based methods is to select features which maximize their relevance with the target class and simultaneously minimize mutual dependency between selected ones .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#15,16#2#para -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact \n"}nil_first --> [4, 12, 17, 18, 21, 30, 31, 32]
nil_second --> [2]
--------------------------
{1=>"It doesThese methods do not select a feature similar to ones already selected ones , even if itthe feature is individually powerful , as because selecting it might not do much to increase much information about the target class [7] .\n", 2=>"It does not select a feature similar to already selected ones , even if it is individual powerful , as selecting it might not increase much information about the target class [7] .\n", 3=>"-1#14#0#lc -1#1,2#3,4#para -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#10#10#exact -1#8#11#exact -1#9#12#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#15#19#exact -1#16#20#stem -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#25#exact -1#21#26#exact -1#22#27#exact -1#23#28#exact -1#25#29,30#para -1#24#31,32#para -1#26#33,34#para -1#27#35#exact -1#28#36#exact -1#29#37#exact -1#30#38#exact -1#31#39#exact -1#32#40#exact \n"}nil_first --> [1, 2, 13, 17, 18, 24]
nil_second --> [0]
--------------------------
{1=>"One of the important tasks in using CMI-based methods is mutual information estimation , which involves to computecomputing the probability densities of continuous random variables .\n", 2=>"One of the important tasks in using CMI-based methods is mutual information estimation which involves to compute probability densities of continuous random variables .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [13, 17]
nil_second --> [16]
--------------------------
{1=>"In [9] , Kwak and Choi used a Parzen windows -based density estimation method in which many parameters such as kernel function and window width are complicated to determine .\n", 2=>"In [9] , Kwak and Choi used Parzen windows based density estimation method in which many parameters such as kernel function and window width are complicated to determine .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [7, 10]
nil_second --> [9]
--------------------------
{1=>"For simplification , discretizing features is often used on the features . //[discretizing features is often used on the features / the features are often discretized?]\n", 2=>"For simplification , discretizing features is often used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#11#exact \n"}nil_first --> [8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
nil_second --> []
--------------------------
{1=>"So far , in object detection systems like [8 , 7] treat , features are treated as binary random variables by choosing appropriate thresholds .\n", 2=>"So far , in object detection systems like [8 , 7] , features are treated as binary random variables by choosing appropriate thresholds .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#14#11#stem -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"However , binarizing features is not a suitable way to handle highly complex data for which it is hard to finding the best threshold is difficult .\n", 2=>"However , binarizing features is not a suitable way to handle highly complex data for which it is hard to find the best threshold .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17,18#16,17,18,19#para -1#19,20,21#20,21#para -1#22#22#exact -1#23#23#exact -1#24#26#exact \n"}nil_first --> [24, 25]
nil_second --> []
--------------------------
{1=>"Using multiple thresholds to discretize data is better than using a binary approach .\n", 2=>"It is better if multiple thresholds are used to discretize data .\n", 3=>"-1#4#1#exact -1#5#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#1#6#exact -1#2#7#exact -1#11#13#exact \n"}nil_first --> [0, 8, 9, 10, 11, 12]
nil_second --> [0, 3, 6, 7]
--------------------------
{1=>"Such a simple method is equal-width binning , which divides the range of feature values into m equally sized bins , where m must be known in advance .\n", 2=>"Such a simple method is equal-width binning which divides the range of feature values into m equal sized bins , where m must be known in advance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#19#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#stem -1#17#18#exact -1#18#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"However , the method�fs main distinguishing point is that it employs the entropy-based discretization method [11] to discretize features . //[distinguishing / unique?]\n", 2=>"However , the main distinguished point is that it employs the entropy-based discretization method [11] to discretize features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#stem -1#5,6,7#6,7,8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [3, 20, 21, 22]
nil_second --> [8]
--------------------------
{1=>"This discretization method is simpler than the Parzen window-s based density estimation method and is more efficient than binary discretization .\n", 2=>"This discretization method is simpler than Parzen windows based density estimation method and more efficient than binary discretization .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [6, 8, 14]
nil_second --> [7]
--------------------------
{1=>"Furthermore , contrary to equal-width binning , it can automatically evaluate the optimal number of bins based on data distribution . //[evaluate / determine?]\n", 2=>"Furthermore , contrary to equal-width binning , it can automatically evaluate the optimal number of bins based on data distribution .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [21, 22, 23]
nil_second --> []
--------------------------
{1=>"Experiments show that the proposed method can well capably handle huge feature sets of data such as Haar wavelets [1] and Gabor wavelets [12] for face detection , significantly reducinge the training time while maintaining high classification performance .\n", 2=>"Experiments show that the proposed method can well handle huge feature sets for face detection such as Haar wavelets [1] and Gabor wavelets [12] , significantly reduce the training time while maintaining high classification performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#12#24#exact -1#13#25#exact -1#14#26#exact -1#24#27#exact -1#25#28#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact -1#35#38#exact \n"}nil_first --> [8, 13, 14, 29]
nil_second --> [26]
--------------------------
{1=>"Huge feature sets usually contain four kinds of features : ( i ) irrelevant features , ( ii ) weakly relevant and redundant features , ( iii ) weakly relevant but non-redundant features , and ( iv ) strongly relevant features ; in which ( iii ) and ( iv ) are the objectives of feature selection methods [13] .\n", 2=>"Huge feature sets usually contain four kinds of features : ( i ) irrelevant features , ( ii ) weakly relevant and redundant features , ( iii ) weakly relevant but non-redundant features and ( iv ) strongly relevant features in which ( iii ) and ( iv ) are the objective of feature selection methods [13] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#51#53#stem -1#52#54#exact -1#53#55#exact -1#54#56#exact -1#55#57#exact -1#56#58#exact -1#57#59#exact \n"}nil_first --> [33, 41]
nil_second --> []
--------------------------
{1=>"To measure the relevance of a feature , an entropy-based measure , which quantifies the uncertainty of random variables , is normally used .\n", 2=>"To measure relevance of a feature , the entropy-based measure which quantifies the uncertainty of random variables is normally used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact \n"}nil_first --> [8, 11, 19]
nil_second --> []
--------------------------
{1=>"The mutual dependence between two random variables is measured by mutual information : \\MATH .\n", 2=>"The mutual dependence between two random variables is measured by mutual information \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"The conditional mutual information is defined as : \\MATH .\n", 2=>"The conditional mutual information is defined as : \\MATH\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"In the first step , the most relevant feature F1 , which has the highest largest amount of mutual information , is selected .\n", 2=>"In the first step , the most relevant feature F1 which has the highest mutual information is selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#17,18#para -1#15#19#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact \n"}nil_first --> [10, 15, 16, 20]
nil_second --> []
--------------------------
{1=>"However , iIn the second step , however , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .\n", 2=>"However , in the second step , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#19#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#32,33#32,33,34,35#para -1#34#36#exact -1#35#37#exact \n"}nil_first --> [2, 7, 21]
nil_second --> [2, 30, 31]
--------------------------
{1=>"Therefore , F2 is selected so that maximizingas to maximize the information it can add :\\MATH .\n", 2=>"Therefore , F2 is selected so that maximizing :\\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#9#stem -1#8#15#exact -1#9#16#exact \n"}nil_first --> [7, 8, 10, 11, 12, 13, 14]
nil_second --> []
--------------------------
{1=>"Following the same scheme, we iteratively add the feature that brings the highest increase of the information content contained in the current selected feature set . //[the / an?<-- \" An \" is correct if there is more than one such measure .]\n", 2=>"Following the same scheme , we iteratively add the feature that brings the highest increase of information content contained in current selected feature set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15,16#para -1#17#17#exact -1#18,19,20#18,19,20,21#para -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
nil_second --> [3, 4]
--------------------------
{1=>"To simply estimate mutual information , the easiest way is to discretize features are discretized in binary values by specifying thresholds [8 , 7] .\n", 2=>"In order to simply estimate mutual information , the easiest way is features are discretized in binary values by specifying thresholds [8 , 7] .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [10, 11]
nil_second --> [0, 1]
--------------------------
{1=>"However , for complex data , doing thisit is not efficient ; therefore , we use the entropy-based method proposed by Fayyad and Irani [11] for discretization .\n", 2=>"However , for complex data , it is not efficient ; therefore , we use entropy-based method proposed by Fayyad and Irani [11] for discretization .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [6, 7, 16]
nil_second --> [6]
--------------------------
{1=>"Discretization is essentially a quantizing process that converts continuous values into discrete values .\n", 2=>"Basically , discretization is a quantizing process that converts continuous values into discrete values .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> [2]
nil_second --> [0, 1]
--------------------------
{1=>"Suppose that we are given a set of instances S , a feature A , and a cut-point T . ( A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold . ) .\n", 2=>"Suppose that we are given a set of instances S , a feature A and a cut-point T ( a cutpoint is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#46#14#exact -1#47#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#57#19#exact -1#18#20#exact -1#19#21#lc -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact -1#14#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#52,53,54#53,54,55,56#para -1#55#57#exact -1#56#59#exact \n"}nil_first --> [22, 48, 58, 60]
nil_second --> [20, 51]
--------------------------
{1=>"Among candidate cut-points , the best candidate cut-point Tmin , which minimizes the entropy function \\MATH , is selected to split \\MATH into two partitions \\MATH and \\MATH .\n", 2=>"Among candidate cut-points , the best candidate cut-point Tmin which minimizes the entropy function \\MATH is selected to split \\MATH into two partitions \\MATH and \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact \n"}nil_first --> [9, 16]
nil_second --> []
--------------------------
{1=>"This process can then be repeated recursively forto \\MATH and \\MATH until some stopping condition is satisfied , thus creating multiple intervals on the feature \\MATH .\n", 2=>"This process can then be repeated recursively to \\MATH and \\MATH until some stopping condition is satisfied , thus creating multiple intervals on the feature \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"Using MDLP , the stopping criteria is was proposed by Fayyad and Irani [11] as follows :\n", 2=>"Using MDLP , the stopping criteria is proposed by Fayyad and Irani [11] as follows :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14#7#syn -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \\MATH wWhere \\MATH ,where \\MATH , \\MATH , and \\MATH is are the numbers of classes in \\MATH , \\MATH , and \\MATH , respectively .\n", 2=>"MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \\MATH Where \\MATH ,where \\MATH , \\MATH , \\MATH is the number of classes in \\MATH , \\MATH , \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#29#exact -1#29#30#exact -1#30#32#exact -1#31,32#33,34#para -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#42#exact -1#40#45#exact \n"}nil_first --> [21, 28, 31, 41, 43, 44]
nil_second --> [21]
--------------------------
{1=>"Extensive experiments [11 , 14] have shown that this method is one of the best in variable discretization one because it gives a small number of cut-points while maintaining consistency .\n", 2=>"Extensive experiments [11 , 14] have shown that this method is one of the best variable discretization one because it gives small number of cut-points while maintaining consistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11,12,13,14#10,11,12,13,14#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22,23#para -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [15]
nil_second --> [10]
--------------------------
{1=>"Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .\n", 2=>"Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#51#34,35#para -1#52#36#exact -1#53#37#exact -1#34#41#exact -1#35#43#exact -1#36#44#exact -1#37#45#exact -1#38#46#exact -1#39#47#exact -1#40#48#exact -1#41#49#exact -1#42#50#exact -1#43#51#exact -1#44#52#exact -1#45#53#exact -1#46#54#exact -1#47#55#exact -1#48#56#exact -1#49#57#exact -1#50#58#exact -1#54#62#exact \n"}nil_first --> [38, 39, 40, 42, 59, 60, 61]
nil_second --> []
--------------------------
{1=>"Two types of features ?that are Haar wavelet features and Gabor wavelet features ? were used in our experiments .\n", 2=>"Two types of features that are Haar wavelet feature and Gabor wavelet feature were used in experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [4, 13, 17]
nil_second --> [4]
--------------------------
{1=>"They consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape . //I�fm not 100 percent clear on what \" they \" points to here . \" These Haar wavelet features , \" perhaps? But can features consist of other kinds of features? You may want to clarify here .]\n", 2=>"It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14,15,16#13,14,15,16#para -1#17#17#exact -1#18#18#exact \n"}nil_first --> [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]
nil_second --> [0, 13]
--------------------------
{1=>"The feature value is defined as the difference of the sum of the pixels within the rectangles .\n", 2=>"The feature value is defined as the difference of sum of the pixels within rectangles .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [12, 15]
nil_second --> []
--------------------------
{1=>"Gabor wavelet features have also often been used often in face recognition systems [12] and are defined as : \\MATH , where \\MATH and \\MATH define the orientation and scale of the Gabor kernels respectively , \\MATH , and the wave vector \\MATH , is defined as : \\MATH where \\MATH , \\MATH and \\MATH .\n", 2=>"Gabor wavelet features have also often been used in face recognition systems [12] and are defined as : \\MATH where \\MATH and \\MATH define the orientation and scale of the Gabor kernels respectively , \\MATH , and the wave vector \\MATH , is defined as : \\MATH where \\MATH , \\MATH \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#33#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#41#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#49#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#50#52#exact -1#51#54#exact -1#52#55#exact \n"}nil_first --> [8, 51, 53]
nil_second --> []
--------------------------
{1=>"Let \\MATH be the face image ; , its convolution with a Gabor filter �� ,_( z ) is defined as : \\MATH where \\MATH denotes the convolution operator .\n", 2=>"Let \\MATH be the face image , its convolution with a Gabor filter �� ,_( z ) is defined as : \\MATH where \\MATH denotes the convolution operator .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"Similar to [12] , Gabor kernels at five scales , \\MATH , and eight orientations , \\MATH , were used .\n", 2=>"Similar to [12] , Gabor kernels at five scales \\MATH and eight orientations \\MATH were used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#16#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact \n"}nil_first --> [9, 11, 15, 17]
nil_second --> []
--------------------------
{1=>"As a result , one \\MATH training sample hasthere are \\MATH Gabor features for one 24x24 training sample .\n", 2=>"As a result , \\MATH there are \\MATH Gabor features for one 24x24 training sample .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#11#4#exact -1#4#5#exact -1#13#6#exact -1#14#7#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#12#15#exact -1#15#18#exact \n"}nil_first --> [8, 14, 16, 17]
nil_second --> [5]
--------------------------
{1=>"To prove the effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods ?that are forward feature selection ( FFS ) [16] and a CMI-based methods using binary features ( CMI-Binary ) [8 , 7] ? on the data set and feature sets mentioned described above .\n", 2=>"In order to show effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods that are forward feature selection ( FFS ) [16] and CMI-basedmethod using binary features ( CMIBinary ) [8 , 7] on the data set and feature setsmentioned above .\n", 3=>"-1#2#0#lc -1#3#1#syn -1#4,5#2,3,4#para -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#46#exact -1#45#47#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#51#55#exact -1#52#56#exact \n"}nil_first --> [23, 33, 34, 35, 40, 45, 52, 53, 54]
nil_second --> [0, 1, 24, 34, 39, 50]
--------------------------
{1=>"We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results , when not only reducing significantly the training time of the AdaBoost-based face detection systems [1] by ( about 100 times , ) but also maintaining comparable performance .\n", 2=>"We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results when not only reducing significantly the training time of AdaBoost-based face detection system [1] ( about 100 times ) but also maintaining comparable performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#stem -1#31#33#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact -1#35#38#exact -1#36#40#exact -1#37#41#exact -1#38#42#exact -1#39#43#exact -1#40#44#exact -1#41#45#exact -1#42#46#exact \n"}nil_first --> [18, 28, 34, 39]
nil_second --> []
--------------------------
{1=>"The figureIt indicates that , the proposed method , CMI-Multi , outperforms the others while the performances of FFS and CMI-Binary have were comparable performanceto one another .\n", 2=>"It indicates that , the proposed method CMI-Multi outperforms the others while FFS and CMI-Binary have comparable performance .\n", 3=>"-1#9#0#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#9#exact -1#8#11#exact -1#10#12,13#para -1#11#14#exact -1#12#18#exact -1#13#19#exact -1#14#20#exact -1#15#21#exact -1#16#23#exact -1#18#27#exact \n"}nil_first --> [1, 8, 10, 15, 16, 17, 22, 24, 25, 26]
nil_second --> [0, 17]
--------------------------
{1=>"The A similar result is was also shown when the three feature selection methods were tested on Gabor wavelet features .\n", 2=>"The similar result is also shown when tested on Gabor wavelet features .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7,8#14#para -1#9#17#exact -1#10#18#exact -1#11#19#exact -1#12#20#exact \n"}nil_first --> [1, 5, 9, 10, 11, 12, 13, 15, 16]
nil_second --> []
--------------------------
{1=>"In this case , CMI-based feature selection methods obviously clearly outperformed FFS , and CMI-Multi is was confirmed to be more efficient than CMI-Binary .\n", 2=>"In this case , CMI-based feature selection methods obviously outperform FFS and CMI-Multi is confirmed to be more efficient than CMI-Binary .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#stem -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16,17#para -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact \n"}nil_first --> [9, 12]
nil_second --> []
--------------------------
{1=>"Because our proposed method uses same principle as FFS , which only trains weak classifiers once , it is extremely fast compared with AdaBoost [1] .\n", 2=>"Because our proposed method uses same principle as FFS which only trains weak classifiers once , it is extremely fast compared with AdaBoost [1] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#15#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"Testing on the standard benchmark MIT+CMU test set , they hadve comparable performance .\n", 2=>"Testing on the standard benchmark MIT+CMU test set , they have comparable performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"However , CMI-Multi wasis trained faster than was AdaBoost by approximately 70 times .\n", 2=>"However , CMI-Multi is trained faster than AdaBoost approximately 70 times .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#3#7#syn -1#7#8#exact -1#8#9,10#para -1#9#11#exact -1#10#12#exact -1#11#13#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"The estimation of mutual information is simplified by using an MDLP- based discretization method .\n", 2=>"The estimation of mutual information is simplified by using MDLP based discretization method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [9, 10]
nil_second --> [9]
--------------------------
{1=>"Integrated into AdaBoost-based object detection systems , our proposed methodit can not only reduces the training time significantly , but also achieves high classification performance .\n", 2=>"Integrated into AdaBoost-based object detection systems , it can not only reduce the training time significantly but also achieve high classification performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11,12#13,14#para -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#19#exact -1#17#20#exact -1#18#21#stem -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact \n"}nil_first --> [7, 8, 9, 18]
nil_second --> [7]
--------------------------
{1=>"Experiments on two popular feature sets have demonstrated the effectiveness of the proposed method . //[Please note : I am not sure which of the following you mean .--> one composed of such as Haar wavelets and the other composed of Gabor wavelets / ? Haar wavelets and Gabor wavelets ?]\n", 2=>"Experiments on two popular feature sets such as Haar wavelets and Gabor wavelets have demonstrated the effectiveness of the proposed method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#13#6#exact -1#14#7#exact -1#15,16,17,18#8,9,10#para -1#19#12#exact -1#20#13#exact -1#21#14#exact -1#6#32#exact -1#7#33#exact -1#8#34#exact -1#9#35#exact -1#10#36#exact -1#11#41#exact -1#12#42#exact \n"}nil_first --> [11, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 49, 50]
nil_second --> []
--------------------------
{1=>"A multi-stage approach that is fast , robust , and easy to train is proposed for a face-detection system .\n", 2=>"A multi-stage approach --- which is fast , robust and easy to train --- for a face-detection system is proposed .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4,5#3,4#para -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#18#13#exact -1#19#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#20#19#exact \n"}nil_first --> [8]
nil_second --> [3, 13]
--------------------------
{1=>"However , our [system / approach?] is distinguished from previous work by two features .\n", 2=>"However , it is distinguished from previous work by two features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact \n"}nil_first --> [2, 3, 4, 5]
nil_second --> [2]
--------------------------
{1=>"Second , support vector machine ( SVM ) classifiers are used instead of AdaBoost classifiers in the last stage , and Haar wavelet features selected by the previous stage are reused for the SVM classifier robustly and efficiently .\n", 2=>"Second , SVM classifiers are used instead of AdaBoost classifiers in the last stage , and Haar wavelet features selected by the previous stage are reused for the SVM classifier robustly and efficiently .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#6#exact -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6#11#exact -1#7#12#exact -1#8#13#exact -1#9#14#exact -1#10#15#exact -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact -1#19#24#exact -1#20#25#exact -1#21#26#exact -1#22#27#exact -1#23#28#exact -1#24#29#exact -1#25#30#exact -1#26#31#exact -1#27#32#exact -1#28#33#exact -1#29#34#exact -1#30#35#exact -1#31#36#exact -1#32#37#exact -1#33#38#exact \n"}nil_first --> [2, 3, 4, 5, 7]
nil_second --> []
--------------------------
{1=>"For example , face detection is combined with other modules to identify a person in a video sequence [2] .\n", 2=>"For example , face detection is combined with other modules to identify who a person in a video sequence is [2] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13,14#12,13,14,15#para -1#17#16#exact -1#18#17#exact -1#20#18#exact -1#21#19#exact \n"}nil_first --> []
nil_second --> [12, 15, 16, 19]
--------------------------
{1=>"Face locations , the results of a face detection system , can be used for applications such as face recognition and video indexing [3] .\n", 2=>"Face locations , the results of a face detection system , can be used for applications such as face recognition and video indexing [3] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4,5,6#3,4,5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12#11,12,13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> []
nil_second --> [3, 13, 14]
--------------------------
{1=>"Although this area has been studied for more than 30 years , developing a fast and robust face detection system that can handle the variations found in different faces in real applications , such as facial expressions , pose changes , illumination changes , complex backgrounds , and low resolutions , is still a challenging research target [4] .\n", 2=>"Although it has been studied for more than 30 years , developing a fast and robust face detection system that can handle the variations found in different faces in real applications , such as facial expressions , pose changes , illumination changes , complex backgrounds , and low resolutions , is still a challenging research target [4] .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#55#56#exact -1#56#57#exact -1#57#58#exact \n"}nil_first --> [1, 2]
nil_second --> [1]
--------------------------
{1=>"Recently , with advances in machine learning research , neural networks [5] , [6] , support vector machines ( SVM ) [7] , [8] , [9] and AdaBoost [1] , [10] , [11] , [12] , [13] are typical choices for building robust face detectors .\n", 2=>"Recently , with advances in machine learning research , Neural Network [5] ,[6] , Support Vector Machines ( SVM ) [7] ,[8] ,[9] and AdaBoost [1] ,[10] ,[11] ,[12] ,[13] are typical choices for building robust face detectors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#lc -1#10#10#stem -1#11#11#exact -1#13#12#exact -1#14#15#lc -1#15#16#lc -1#16#17#lc -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#30#37#exact -1#31#38#exact -1#32#39#exact -1#33#40#exact -1#34#41#exact -1#35#42#exact -1#36#43#exact -1#37#44#exact -1#38#45#exact \n"}nil_first --> [13, 14, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36]
nil_second --> [12, 21, 22, 26, 27, 28, 29]
--------------------------
{1=>"Generally , to classify an input pattern of intensities as a face or non-face , features must be extracted and normalized before passing [the image / the pattern / the results?] to a classifier [14] .\n", 2=>"Generally , to classify an input pattern of intensities as a face or non-face , features must be extracted and normalized before passing to a classifier [14] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#31#exact -1#24#32#exact -1#25#33#exact -1#26#34#exact -1#27#35#exact \n"}nil_first --> [23, 24, 25, 26, 27, 28, 29, 30]
nil_second --> []
--------------------------
{1=>"Many kinds of features have been used , ranging from simple ones such as intensity values [7] , [5] and eigenspace [15] to complex ones such as wavelets [16] , [1] , [12] , edge orientation histograms [17] , [18] , and Bayesian discriminating features ( BDF ) [19] .\n", 2=>"There are many kinds of features that have been used ranging from simple features such as intensity values [7] ,[5] and eigenspace [15] to complex features such as wavelets [16] ,[1] ,[12] , edge orientation histograms [17] ,[18] and Bayesian discriminating features ( BDF ) [19] .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#32#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#44#47#exact -1#45#48#exact -1#46#49#exact \n"}nil_first --> [11, 17, 18, 24, 29, 30, 31, 32, 33, 38, 39, 40]
nil_second --> [0, 1, 6, 13, 19, 25, 30, 31, 37]
--------------------------
{1=>"In a typical face detector that is scale- and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .\n", 2=>"In a typical face detector which is scale-free and location-free , the number of analyzed patterns is usually very large ( 160 ,000 patterns for a 320x240 pixel image ) because the face classifier has to scan over the input image at every location and every scale .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"However , the vast majority of the analyzed patterns are non-face .\n", 2=>"However , the huge majority of the analyzed patterns are non-face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4,5#3,4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"Face detectors based on single classifiers such as SVM [7] , [8] , [9] and neural networks [6] , [5] are usually slow because they equally process non-face and face regions in the input image .\n", 2=>"Face detectors based on single classifiers such as SVM [7] ,[8] ,[9] and Neural Network [6] ,[5] are usually slow because they process non-face regions and face regions in the input image equally .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#14#exact -1#13#15#lc -1#14#16#stem -1#15#17#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#32#25#exact -1#22#26#exact -1#23#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#33#35#exact \n"}nil_first --> [10, 11, 12, 13, 18, 19]
nil_second --> [10, 11, 16, 24]
--------------------------
{1=>"To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers has been proposed [8] , [1] , [9] , [20] , [21] , [11] .\n", 2=>"To deal with the problem of processing a large number of patterns , a combination of simple-to-complex classifiers is proposed [8] ,[1] ,[9] ,[20] ,[21] ,[11] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9,10#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18,19#18,19,20#para -1#20#21#exact -1#26#32#exact \n"}nil_first --> [22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
nil_second --> [7, 21, 22, 23, 24, 25]
--------------------------
{1=>"In particular , fast and simple classifiers are [recommended to be?] used as filters at the earliest stages to quickly reject a large number of non-face patterns and a slower yet more accurate classifier is then recommended to be used for classifying face-like patterns .\n", 2=>"In particular , fast and simple classifiers are used as filters at the earliest stages to quickly reject a large number of non-face patterns and a slower yet more accurate classifier is then used for classifying face-like patterns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#15#9#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#16#19#exact -1#17#20#exact -1#19,20,21#21,22,23,24#para -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#37,38,39,40#para -1#35#41#exact -1#36#42#exact -1#37#43#exact -1#38#44#exact \n"}nil_first --> [8, 10, 18, 36]
nil_second --> [18, 34]
--------------------------
{1=>"In this way , the complexity of classifiers can be adapted corresponding to the difficulty in the input patterns . / / [is / can be?]\n", 2=>"By this way , the complexity of classifiers is adapted corresponding to the difficulty in the input patterns .\n", 3=>"-1#14#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [15, 20, 21, 22, 23, 24, 25]
nil_second --> [0]
--------------------------
{1=>"In [8] , nonlinear SVM classifiers using pixel-based features were arranged into a sequence by increasing the number of support vectors , while in [9] , linear SVM classifiers trained at different resolutions were used for rejection and a reduced set of principle component analysis ( PCA )-based features were used with the nonlinear SVM at the classification stage in order to reduce computation time .\n", 2=>"In [8] , non linear SVM classifiers using pixel-based features are arranged into a sequence with increasing number of support vectors , or in [9] , linear SVM classifiers trained at different resolutions are used for rejection and a reduced set of principle component analysis ( PCA )-based features are used with the non linear SVM at the classification stage in order to reduce computation time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#syn -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#16#14,15#para -1#52#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33,34#33,34#para -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49,50#49,50#para -1#51#51#exact -1#57#52#exact -1#55#54#exact -1#56#55#exact -1#58#56,57#para -1#59#58#exact -1#61,62,63#59,60,61,62#para -1#64#63#exact -1#65#64#exact -1#66#65#exact \n"}nil_first --> [3, 22, 53]
nil_second --> [3, 4, 15, 22, 53, 54, 60]
--------------------------
{1=>"In [1] , AdaBoost-based classifiers were arranged in a degeneration decision tree or a cascade .\n", 2=>"In [1] , AdaBoost based classifiers are arranged in a degeneration decision tree or a cascade .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#syn -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact \n"}nil_first --> [3]
nil_second --> [3, 4]
--------------------------
{1=>"Using about 10 features of the first two layers , more than 90\\% of non-face patterns were rejected .\n", 2=>"Using about 10 features of the first two layers , more than 90\\% of non-face patterns are rejected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6,7#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17#16,17#para -1#18#18#exact \n"}nil_first --> []
nil_second --> [4, 5]
--------------------------
{1=>"Recently , a boosting chain [20] and a nested cascade [11] have also been proposed for improvements .\n", 2=>"Recently , boosting chain [20] and nested cascade [11] have also been proposed for improvements .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [2, 7]
nil_second --> []
--------------------------
{1=>"It is believed that the cascade structure of classifiers is the key factor in enhancement of current real-time face detectors . / / It is believed?This sounds vague?who believes this? \" May researchers believe , \" for example , would be clearer and sound more believable .]\n", 2=>"It is believed that the cascade structure of classifiers is the key factor in enhancement of current real-time face detectors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11,12,13#10,11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]
nil_second --> [10]
--------------------------
{1=>"This work is motivated by Viola and Jones [1] , who proposed a framework for fast and robust face detection .\n", 2=>"This work is motivated by Viola and Jones [1] who proposed a framework for fast and robust face detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"-The cascaded structure of simple-to-complex classifiers reduces computation time dramatically .\n", 2=>"-Firstly , the cascaded structure of simple-to-complex classifiers reduces computation time dramatically ( as mentioned above ) .\n", 3=>"-1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#17#10#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 2, 12, 13, 14, 15, 16]
--------------------------
{1=>"-AdaBoost is used to select discriminative and significant features from a pool of a very large number of features and then construct the classifier .\n", 2=>"-Secondly , AdaBoost is used to select discriminative and significant features from a pool of a very large number of features and then construct the classifier .\n", 3=>"-1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15,16,17,18,19#13,14,15,16#para -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> [0, 17]
nil_second --> [0, 1, 2]
--------------------------
{1=>"Compared to SVM-based classifiers or neural network-based classifiers , AdaBoost-based classifiers are hundreds of times faster .\n", 2=>"Compared to SVM-based classifiers or neural network-based classifiers , AdaBoost based classifiers are hundreds of times faster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> [9]
nil_second --> [9, 10]
--------------------------
{1=>"-Haar wavelet features used for all stages are informative [22] and can be evaluated extremely quickly due to the introduction of the integral image .\n", 2=>"-Thirdly , Haar-wavelet features used for all stages are informative [22] and evaluated extremely fast due to the introduction of the integral image .\n", 3=>"-1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#12,13#para -1#13#14#exact -1#14#15#para -1#17,18#16,17,18,19#para -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [0, 1, 11]
nil_second --> [0, 1, 2, 15, 16]
--------------------------
{1=>"This need is apparent because when face and non-face patterns become hard to distinguish , weak classifiers are too weak to boost [22] .\n", 2=>"This need is apparent when face and non-face patterns become hard to distinguish , weak classifiers are too weak to boost [22] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"With the first several layers in our experiment ( cf . Figure 1 ) , using some 800 weak classifiers , more than \\MATH of non-face patterns were rejected .\n", 2=>"With the first several layers in our experiment ( cf. Figure 1 ) , using some 800 weak classifiers , more than \\MATH of non-face patterns are rejected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#syn -1#28#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26,27#27,28#para \n"}nil_first --> [29]
nil_second --> []
--------------------------
{1=>"However , enabling the later layers to robustly classify a smaller number of remaining patterns requires many more weak classifiers ( around 5 ,660 ) , thus making the training task much more complicated .\n", 2=>"However , turning the later layers into robustly classifying a smaller number of remaining patterns , it requires a lot more , e.g. , 5 ,660 , weak classifiers , thus making the training task much more complicated .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#15#exact -1#20#17#exact -1#27#18#exact -1#28#19#exact -1#24#22#exact -1#25#23#exact -1#15#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34#30#exact -1#35#31#exact -1#36#32#exact -1#37#33#exact -1#38#34#exact \n"}nil_first --> [2, 6, 16, 20, 21, 24]
nil_second --> [2, 6, 16, 18, 19, 21, 22, 23, 26, 29]
--------------------------
{1=>"It requires a long time because the training time is proportional to the number of features in the input feature set ( which is normally hundreds of thousands ) and the number of training samples ( which is generally tens of thousands ) .\n", 2=>"Firstly , it requires a long training time because the training time is proportional to the number of features in the input feature set ( which is normally hundreds of thousands ) and the number of training samples ( which is generally tens of thousands ) .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#38#35#exact -1#39#36#exact -1#40#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#46#43#exact \n"}nil_first --> []
nil_second --> [0, 1, 6]
--------------------------
{1=>"Another thing that complicates the training process is that AdaBoost-based classifiers are constructed by adding features after each round of boosting , so several training parameters must be tuned manually while training .\n", 2=>"Secondly , AdaBoost-based classifiers are constructed by adding features after each round of boosting , so several training parameters must be tuned manually while training .\n", 3=>"-1#24#5#exact -1#2#9#exact -1#3#10#exact -1#4#11#exact -1#5#12#exact -1#6#13#exact -1#7#14#exact -1#8#15#exact -1#9#16#exact -1#10#17#exact -1#11#18#exact -1#12#19#exact -1#13#20#exact -1#14#21#exact -1#15#22#exact -1#16#23#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact -1#20#27#exact -1#21#28#exact -1#22#29#exact -1#23#30#exact -1#25#32#exact \n"}nil_first --> [0, 1, 2, 3, 4, 6, 7, 8, 31]
nil_second --> [0, 1]
--------------------------
{1=>"Because the complexity of the training sets varies throughout the layers in the cascade , a way to choose these parameters automatically and optimally has not been determined .\n", 2=>"Because the complexity of the training sets varies through layers in the cascade , it is undetermined how to choose these parameters automatically and optimally .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#para -1#11#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#17,18#15,16,17#para -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#15#24#para -1#25#28#exact \n"}nil_first --> [12, 25, 26, 27]
nil_second --> [14, 16]
--------------------------
{1=>"Specifically , for quick rejection of non-face patterns , we have reused two key ingredients of Viola and Jones' system , that is , the cascaded structure of simple-to-complex classifiers and AdaBoost trained with Haar wavelet features .\n", 2=>"Specifically , for quick rejection of non-face patterns , we reuse two key ingredients of Viola and Jones' system , that is , the cascaded structure of simple-to-complex classifiers and AdaBoost trained with Haar-wavelet features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#34#10#syn -1#10#11#stem -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#35#37#exact \n"}nil_first --> [34, 35, 36]
nil_second --> [33]
--------------------------
{1=>"The contribution of this approach is threefold :\n", 2=>"The contribution of this approach is three fold :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#para -1#8#7#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"We use 36 x 36-pixel window-based classifiers with a moving step size of 12 pixels to quickly estimate the candidate face regions .\n", 2=>"We use 36 x 36-pixel window-based classifiers with a moving step size of 12 pixels , to quickly estimate the candidate face regions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [15]
--------------------------
{1=>"To improve speed while maintaining high accuracy , our approach takes advantage of the combination of the Haar wavelet features and the AdaBoost learning for fast and robust evaluation .\n", 2=>"To improve speed while maintaining high accuracy , our approach takes advantage of the combination of the Haar wavelet features and the AdaBoost learning for fast and robust evaluation\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15,16#13,14,15#para -1#21#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [21, 29]
nil_second --> []
--------------------------
{1=>"Second , we have investigated how to efficiently reuse the features selected by AdaBoost in the previous stage for the SVM classifiers of the last stage .\n", 2=>"Second , how to efficiently reuse the features selected by AdaBoost in the previous stage , for the SVM classifiers of the last stage , is investigated .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#26#4#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#27#26#exact \n"}nil_first --> [2, 3]
nil_second --> [15, 24, 25]
--------------------------
{1=>"Reusing these features brings two advantages : ( i ) Haar wavelet features are very fast in being evaluated and normalized [1] .\n", 2=>"Reusing these features brings to two advantages : ( i ) Haar wavelet features are very fast in evaluating and normalizing [1] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#18#stem -1#19#19#exact -1#20#20#stem -1#21#21#exact -1#22#22#exact \n"}nil_first --> [17]
nil_second --> [4]
--------------------------
{1=>"Furthermore , these features do not need to be re-evaluated because they have already been evaluated .\n", 2=>"Furthermore , it is unnecessary to re-evaluate these features because they have been previously evaluated .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#7#2#exact -1#8#3#exact -1#3,4#4,5,6#para -1#5#7#exact -1#6#9#stem -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13,14#para -1#14#15#exact -1#15#16#exact \n"}nil_first --> [8]
nil_second --> [2, 13]
--------------------------
{1=>"( ii ) By using SVM classifiers with powerful generalization , using too many features in the cascade is avoided , with the important results of saving training time and avoiding over-fitting .\n", 2=>"( ii ) By using SVM classifiers with powerful generalization , using too many features in the cascade is avoided , therefore importantly training time is saved and over-fitting is avoided .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#23#stem -1#23#27#exact -1#24#28#exact -1#27#29#exact -1#28#31#exact -1#31#32#exact \n"}nil_first --> [21, 22, 24, 25, 26, 30]
nil_second --> [21, 25, 26, 29, 30]
--------------------------
{1=>"Third , the training time of AdaBoost classifiers has been shortened by using simple sampling techniques to reduce the number of features in the feature set .\n", 2=>"Third , the training time of AdaBoost classifiers is shortened by using simple sampling techniques to reduce the number of features in the feature set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16,17,18,19#16,17,18,19,20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> []
nil_second --> [15]
--------------------------
{1=>"Experiments showed that for rejection , the performance gained by using a sampled feature set was comparable to that of a full feature set .\n", 2=>"Experiments will show that for rejection , using a full feature set and a sampled feature set gives the comparable performance .\n", 3=>"-1#0#0#exact -1#2#1#stem -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#18#6#exact -1#20#7#exact -1#7,8#9,10,11#para -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#19#16#exact -1#13#20#exact -1#9#21#exact -1#10#22#exact -1#11#23#exact -1#21#24#exact \n"}nil_first --> [8, 15, 17, 18, 19]
nil_second --> [1, 12, 17]
--------------------------
{1=>"Several studies have worked on addressing the drawbacks of Viola and Jones' system .\n", 2=>"There have been several studies working on how to handle the drawbacks of Viola and Jones' system .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#1#2#exact -1#5#3#stem -1#6#4#exact -1#9#5#syn -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact \n"}nil_first --> []
nil_second --> [0, 2, 7, 8]
--------------------------
{1=>"This process is very time consuming because all weak classifiers must be trained every time one feature is selected .\n", 2=>"It is therefore very time consuming because all weak classifiers must be trained every time one feature is selected .\n", 3=>"-1#1#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 2]
--------------------------
{1=>"With the new proposal of Wu et al. , weak classifiers are trained only once and features are selected by the direct feature selection method , which directly maximizes the learning objective of the output classifier .\n", 2=>"With their new proposal , weak classifiers are trained only once and features are selected by the direct feature selection method that directly maximizes the learning objective of the output classifier .\n", 3=>"-1#0#0#exact -1#16#1#exact -1#2#2#exact -1#3#3#exact -1#27#4#exact -1#4#8#exact -1#5#9#exact -1#6#10#exact -1#7#11#exact -1#8#12#exact -1#9#13#exact -1#10#14#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#24#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21,22#26,27#para -1#23#28#exact -1#28#29#exact -1#25#30#exact -1#26#31#exact -1#29#33,34#para -1#30#35#exact -1#31#36#exact \n"}nil_first --> [5, 6, 7, 25, 32]
nil_second --> [1]
--------------------------
{1=>"Another direction is to optimally build the cascade to improve its overall performance .\n", 2=>"Another direction is to optimally build the cascade to improve the overall performance of the cascade .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#11#11#exact -1#12#12#exact -1#16#13#exact \n"}nil_first --> []
nil_second --> [13, 14, 15]
--------------------------
{1=>"However , their approach is somewhat complicated and is not easy to implement .\n", 2=>"However , their approach is somewhat complicated and is not easy to implement .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10#8,9,10,11#para -1#12#12#exact -1#13#13#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"Xiao et al. [20] and Huang et al. [11] proposed a boosting chain structure in which subsequent layers utilize the historical information of the previous layers .\n", 2=>"Xiao et al. [20] and Huang et al. [11] propose the boosting chain structure in which subsequent layers utilize historical information of previous layers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#10#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23,24#para -1#23#25#exact -1#24#26#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Studies based on RealBoost [26] , such as [12] , [10] , [27] , and [11] , introduced new kinds of weak classifiers that are stronger than binary weak classifiers .\n", 2=>"Studies based on RealBoost [26] , such as [12] ,[10] ,[27] ,[11] , introduced new kinds of weak classifiers that are stronger than binary weak classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#12#9#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact \n"}nil_first --> [10, 11, 12, 13, 14, 15, 16]
nil_second --> [9, 10, 11]
--------------------------
{1=>"These new real-valued weak classifiers can effectively discriminate face and non-face distributions , so the total number of features used is also reduced dramatically .\n", 2=>"New real-valued weak classifiers can effectively discriminate face and non-face distributions and , in consequence , the total number of features used also reduces dramatically .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#12#exact -1#16,17,18,19#14,15,16#para -1#20#18#exact -1#21#19#exact -1#22#20,21#para -1#23#22#stem -1#24#23#exact -1#25#24#exact \n"}nil_first --> [0, 13, 17]
nil_second --> [11, 13, 14, 15]
--------------------------
{1=>"Face detection systems such as [27] and [11] only use around 800 features .\n", 2=>"Face detection systems such as [27] ,[11] only use around 800 features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [6, 7]
nil_second --> [6]
--------------------------
{1=>"A small number of bins might not accurately approximate the real distribution , while a large number of bins might cause over-fitting , increase computation time , and waste storage space .\n", 2=>"Small number of bins might not well approximate the real distribution while large number of bins might cause over-fitting , increase computation time and waste storage space .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#19#12#exact -1#11#13#exact -1#12,13,14#14,15,16,17#para -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact \n"}nil_first --> [0, 7, 22, 26]
nil_second --> [6]
--------------------------
{1=>"However , our system can benefit from this approach when building the rejection stage and can thus reduce the training time even further .\n", 2=>"Actually , our system can benefit from this approach when building the rejection stage and thus also reduce the training time much more .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#16#22#para -1#23#23#exact \n"}nil_first --> [0, 15, 21]
nil_second --> [0, 21, 22]
--------------------------
{1=>"The proposed face detection system consists of three stages that classify a 24x24-pixel window as either a face or a non-face .\n", 2=>"The proposed face detection system consists of three stages that classify a 24x24 pixel window as either a face or a non-face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> [12]
nil_second --> [12, 13]
--------------------------
{1=>"To detect faces of different sizes and locations , the detector is applied at every location and scale in the input image with a scale factor of 1 .2 , which is similar to other approaches [5] , [6] , [9] .\n", 2=>"To detect faces of different sizes and locations , the detector is applied at every location and scale in the input image with a scale factor of 1 .2 , which is similar to the other approaches [5] ,[6] ,[9] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31,32,33,34#30,31,32,33#para -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#40#41#exact \n"}nil_first --> [37, 38, 39, 40]
nil_second --> [30, 38, 39]
--------------------------
{1=>"If a 36x36-pixel window is detected as the existence of a face , 144 ( i.e. , 12x12 ) likely face positions are collected and passed to the next stage .\n", 2=>"If a 36x36 window is detected as the existence of a face , 144 ( i.e. 12x12 ) likely face positions are collected and passed to the next stage .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [2, 16]
nil_second --> [2, 10]
--------------------------
{1=>"The main purpose of designing these two stages is trying to filter out a large number of non-face patterns as quickly as possible before passing complex patterns to the final stage classifier .\n", 2=>"The main purpose of designing these two stages is trying to filter out a large number of non-face patterns as quick as possible before passing complex patterns to the final stage classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14,15,16#13,14,15,16#para -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#stem -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"This is done by taking advantage of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers enable extremely fast computation .\n", 2=>"This is done by taking advantages of Viola and Jones' approach [1] , in which Haar wavelet features and the cascaded AdaBoost classifiers are extremely fast in computation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#24#24#exact -1#25#25#exact -1#27#26#exact -1#28#27#exact \n"}nil_first --> [23]
nil_second --> [23, 26]
--------------------------
{1=>"Although the cascade of \\MATH AdaBoost classifiers rejects non-face patterns rapidly , it is still influenced by the large number of \\MATH patterns that it must process .\n", 2=>"Although the cascade of \\MATH AdaBoost classifiers rejects non-face patterns rapidly , it is still influenced by the large number of \\MATH patterns that it must process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17,18,19#17,18,19,20#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> []
nil_second --> [20]
--------------------------
{1=>"For this reason , the first stage , which is a cascade of \\MATH classifiers , is added is to decrease the number of analyzed patterns .\n", 2=>"The reason why the fist stage , which is a cascade of \\MATH classifiers , is added is to decrease the number of analyzed patterns .\n", 3=>"-1#1#2#exact -1#14#3#exact -1#3#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [0, 1, 5, 15]
nil_second --> [0, 2, 4]
--------------------------
{1=>"The \\MATH window is chosen in accordance with the idea in [5] that the classifier can be trained to be invariant to translation by up to \\MATH of the original window size .\n", 2=>"The \\MATH window is chosen in accordance with the idea from [5] stated that the classifier can be trained to be invariant to translation by up to \\MATH of original window size .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8#5,6,7#para -1#14#8#exact -1#9,10#9,10#para -1#11#11#exact -1#13#12#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28,29#para -1#30#30#exact -1#31#31#exact -1#32#32#exact \n"}nil_first --> [13]
nil_second --> [12]
--------------------------
{1=>"With this flexible classifier , the moving step size can be increased up to 12 pixels to dramatically reduce the number of analyzed patterns .\n", 2=>"With this flexible classifier , the moving step size can be increased up to 12 pixels that reduce dramatically number of analyzed patterns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#18#17#exact -1#17#18#exact -1#19,20#19,20,21#para -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [16]
nil_second --> [16]
--------------------------
{1=>"The efficiency of this stage will be discussed further in section 6 .3 .\n", 2=>"Efficiency of this stage will be discussed further in section 6 .3 .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [0]
nil_second --> []
--------------------------
{1=>"The last stage is a cascade of nonlinear SVM classifiers that reuses features that have been selected by AdaBoost in the second stage classifier .\n", 2=>"The last stage is a cascade of non-linear SVM classifiers that reuses features that have been selected by AdaBoost in the second stage classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [7]
nil_second --> [7]
--------------------------
{1=>"In our experiments , only 100 features were used , making classification faster than it would have been using pixel-based SVM classifiers [8] , [9] .\n", 2=>"In our experiments , only 100 features are used and hence it is faster than using any pixel-based SVM classifiers [8] ,[9] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#13#12#exact -1#14#13#exact -1#11#14#exact -1#12#17#syn -1#15#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#22#25#exact \n"}nil_first --> [9, 10, 11, 15, 16, 23, 24]
nil_second --> [9, 10, 16, 21]
--------------------------
{1=>"The same feature set proposed in [1] was used ( cf . Figure 4 ) .\n", 2=>"The same feature set as proposed in [1] is used ( cf. Figure 4 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8,9#7,8#para -1#10#9#exact -1#11#10#syn -1#15#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [15]
nil_second --> [4]
--------------------------
{1=>"It consists of four kinds of features modeled from adjacent basic rectangles of the same size and shape .\n", 2=>"It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13,14,15#12,13,14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"The feature value is defined as the difference of the sum of the pixels within rectangles .\n", 2=>"The feature value is defined as the difference of sum of the pixels within rectangles .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"Each feature is parameterized by four parameters : the position within the window \\MATH , the width \\MATH , and the height \\MATH ( cf . Figure 5 ) .\n", 2=>"Each feature is parameterized by four parameters : the position within the window \\MATH , width \\MATH and height \\MATH ( cf. Figure 5 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20,21#para -1#19#22#exact -1#20#23#exact -1#21#24#syn -1#25#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact \n"}nil_first --> [15, 18, 29]
nil_second --> []
--------------------------
{1=>"By using integral image definition [1] , the feature values of these rectangles can be computed extremely quickly .\n", 2=>"By using integral image definition [1] , these rectangle feature values can be computed extremely quickly .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#8#exact -1#10#9#exact -1#7#11#exact -1#8#12#stem -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [7, 10]
nil_second --> []
--------------------------
{1=>"Given \\MATH weak classifiers \\MATH learned through \\MATH rounds of boosting , the strong classifier is formed by a linear combination : \\MATH , where \\MATH are coefficients found in the boosting process .\n", 2=>"Given \\MATH weak classifiers \\MATH learned through \\MATH rounds of boosting , the strong classifier is formed by a linear combination : \\MATH where \\MATH are coefficients found in the boosting process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [23]
nil_second --> []
--------------------------
{1=>"Each weak classifier \\MATH is associated with a feature \\MATH and a threshold \\MATH such that the number of incorrectly classified examples corresponding to the weak classifier is minimized : \\MATH , where polarity \\MATH indicates the direction of the inequality sign .\n", 2=>"Each weak classifier \\MATH is associated with a feature \\MATH and a threshold \\MATH such that the number of incorrect classified examples corresponding to this weak classifier is minimized : \\MATH , where polarity \\MATH indicates the direction of the inequality sign .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16,17,18#15,16,17,18#para -1#19#19#stem -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#36#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#37,38,39#36,37,38,39#para -1#40#40#exact -1#41#41#exact -1#42#42#exact \n"}nil_first --> []
nil_second --> [15, 24]
--------------------------
{1=>"The error of each weak classifier is measured with respect to the set of weights over each example of the training set \\MATH , where \\MATH and \\MATH are the respective weight and label of the training example \\MATH .\n", 2=>"The error of each weak classifier is measured with respect to the set of weights over each example of the training set \\MATH , where \\MATH and \\MATH are the weight and the label of the training example \\MATH , respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#40#30#stem -1#30#31#exact -1#31#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#41#39#exact \n"}nil_first --> []
nil_second --> [32, 39]
--------------------------
{1=>"After each round , these weights are updated such that the weak learner will focus much more on the hard examples in the next round .\n", 2=>"After each round , these weights are updated such that the weak learner will focus much more on the hard examples in the next round .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22,23,24#21,22,23,24#para -1#25#25#exact \n"}nil_first --> []
nil_second --> [21]
--------------------------
{1=>"Training cascaded classifiers that can achieve both good detection rates and less computation time is quite complex ; a higher detection rate requires more features , but more features correspond to more time needed for evaluation .\n", 2=>"Training cascaded classifiers that can achieve both good detection rate and less computation time is quite complex , because a higher detection rate requires more features , but more features are correspondent to more time for evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9,10#9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#para -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact \n"}nil_first --> [17, 33]
nil_second --> [17, 18, 31]
--------------------------
{1=>"Viola and Jones [1] stated that , if the layer classifier has achieved the predefined target goals after 200 features are used , the training process will stop and a new layer will be added .\n", 2=>"Viola and Jones [1] stated that , if the layer classifier could achieve the predefined target goals after 200 features are used , the training process will stop and a new layer will be added .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#19#11#syn -1#12#12#stem -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> [19]
nil_second --> [11]
--------------------------
{1=>"It has been very efficiently proven in many pattern recognition applications [29] , [8] , [9] .\n", 2=>"It has been very efficiently proved in many pattern recognition applications [29] ,[8] ,[9] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#syn -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#14#16#exact \n"}nil_first --> [12, 13, 14, 15]
nil_second --> [12, 13]
--------------------------
{1=>"The form of SVM classifiers is : \\MATH where \\MATH is the d-dimensional vector of an observation example , \\MATH is a class label , and \\MATH is the vector of the \\MATH training example .\n", 2=>"The form of SVM classifiers is : \\MATH where : \\MATH is the d-dimensional vector of an observation example , \\MATH is a class label , and \\MATH is the vector of the \\MATH training example .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"It is important to choose the appropriate kernel and parameter \\MATH in order to obtain the robust SVM classifier .\n", 2=>"It is important to choose the appropriate kernel and parameter \\MATH in order to to obtain the robust SVM classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#14,15#11,12,13,14#para -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> []
nil_second --> [11, 12, 13]
--------------------------
{1=>"Although many kernels have been introduced by researchers , the following four kernels are commonly used : \\MATH where \\MATH , and \\MATH are kernel parameters .\n", 2=>"Although many kernels have been introduced by researchers , the following four kernels are commonly used : \\MATH where \\MATH and \\MATH are kernel parameters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"Compared to AdaBoost classifiers , SVM classifiers run much more slowly because of the large number of support vectors and the heavy kernel computation .\n", 2=>"Compared to AdaBoost classifiers , SVM classifiers run much slower in running because of the large number of support vectors and heavy kernel computation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9,10#para -1#12#11#exact -1#14,15,16,17#12,13,14,15,16#para -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20,21#para -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> []
nil_second --> [10, 11, 13]
--------------------------
{1=>"For training , we collected 7 ,500 , 24x24-size face patterns from the Internet . / / size / pixel?\n", 2=>"For training , we collected 7 ,500 , 24x24-size face patterns from the Internet .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [15, 16, 17, 18, 19]
nil_second --> []
--------------------------
{1=>"Non-face patterns of the training and the validating sets of the first layer in the cascade were selected randomly .\n", 2=>"Non-face patterns of the training and the validating sets of the first layer in the cascade are selected randomly .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#14#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#16,17#16,17#para -1#18#18#exact -1#19#19#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"To compare the performance of classifiers , we implemented a full cascade of classifiers trained by AdaBoost , similar to that used by Viola and Jones [1] .\n", 2=>"To compare the performance of classifiers , we have implemented a fully cascade of classifiers trained by AdaBoost , similar to that used by Viola and Jones [1] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#syn -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"The minimum of the detection rate was \\MATH , the maximum of the false positive rate was \\MATH , and the maximum of the number of features in each layer was 200 .\n", 2=>"The minimum of the detection rate is \\MATH , the maximum of the false positive rate is \\MATH and the maximum of the number of features in each layer is 200 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#15,16#6#para -1#17#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#6#15,16#para -1#7#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21,22,23,24#22,23,24#para -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#syn -1#30#31#exact -1#31#32#exact \n"}nil_first --> [18, 25]
nil_second --> []
--------------------------
{1=>"If \\MATH is the number of Haar wavelet features and \\MATH is the number of training patterns , the learning time of AdaBoost to train \\MATH weak classifiers is roughly [1] .\n", 2=>"If \\MATH is the number of Haar wavelet features and \\MATH is the number of training patterns , the learning time of AdaBoost to train \\MATH weak classifiers is roughly[1] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#30#31#exact \n"}nil_first --> [29, 30]
nil_second --> [29]
--------------------------
{1=>"In our approach , the cascaded classifiers are only used for efficient rejection , so we can reduce both of these numbers in order to keep the training time for the full system reasonable .\n", 2=>"In our approach , the cascaded classifiers are only used for efficient rejection , so we can reduce both these numbers in order to keep training time for the full system reasonable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#23,24#22,23,24,25#para -1#28#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#29#30,31#para -1#30#32#exact -1#31#33#exact -1#32#34#exact \n"}nil_first --> [19]
nil_second --> [21, 22]
--------------------------
{1=>"As mentioned in section 4 .1 , each feature is parameterized by a tuple of four parameters \\MATH . / / If this ( and other places ) do not display with spaces after the commas , spaces must be insert . A comma should always be followed by a space . I recommend checking this carefully throughout .]\n", 2=>"As mentioned in section 4 .1 , each feature is parameterized by a tuple of four parameters \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]
nil_second --> []
--------------------------
{1=>"A feature set , on the other hand , is parameterized by \\MATH .\n", 2=>"In the other hand , a feature set is parameterized by \\MATH .\n", 3=>"-1#5#0#lc -1#6#1#exact -1#7#2#exact -1#4#3#exact -1#1#5#exact -1#2#6#exact -1#3#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [4, 8]
nil_second --> [0]
--------------------------
{1=>"We carried out experiments to compare the performance of classifiers trained on these two feature sets : the full feature set \\MATH , containing 134 ,736 features and the reduced feature set \\MATH , containing 14 ,807 features ( excluding features of small size ) .\n", 2=>"We carried out experiments to compare the performance of classifiers trained on these two feature sets : the full feature set \\MATH containing 134 ,736 features and the reduced feature set \\MATH containing 14 ,807 features ( excluding features with the small size ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#40,41#41,42#para -1#42#43#exact -1#43#44#exact -1#44#45#exact \n"}nil_first --> [22, 33]
nil_second --> [39]
--------------------------
{1=>"As a result , by using the reduced feature set , the training time can be shortened to approximately one-ninth .\n", 2=>"As a result , by using the reduced feature set , the training time can be shortened approximately to one ninth .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#17#exact -1#17#18#exact -1#20#19#syn -1#21#20#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"Another experiment we conducted showed that , for similar performance , an AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than one trained on the full feature set . / / [Do you need a reference here , or is this still talking about the experiments you report in this paper?]\n", 2=>"Our another experiment has shown that , for similar performance , the AdaBoost classifier trained on the reduced feature set that uses larger sampling step sizes requires more features than that trained on the full feature set .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3,4#4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#11#53#exact \n"}nil_first --> [2, 3, 11, 30, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59]
nil_second --> [0, 30]
--------------------------
{1=>"By taking advantage of simplification in training classifiers only for rejection , as demonstrated in section 6 .2 , training this cascade only uses the feature set generated from a 36x36 window with sampling parameters \\MATH .\n", 2=>"By taking advantage of simplification in training classifiers only for rejection demonstrated in section 6 .2 , training this cascade only uses the feature set generated from a 36x36 window with sampling parameters \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#16#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact \n"}nil_first --> [12, 18]
nil_second --> []
--------------------------
{1=>"Since a 36x36 face sample contains a large proportion of background outside the 24x24 face region and the classifier is required to be fast and to keep all possible face regions , a minimum detection rate of \\MATH and a maximum of false positive rate of \\MATH were set as the training parameters .\n", 2=>"Since a 36x36 face sample contains a lot of background outside the 24x24 face region while the classifier is required to be fast and to keep all possible face regions , training parameters are set as follows : the minimum detection rate of \\MATH and maximum of false positive rate of \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#42#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#23#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#44#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#38,39#32,33#para -1#40#34#exact -1#41#35#exact -1#46#36#exact -1#43#37#exact -1#45#39,40#para -1#50#41#exact -1#47#42#exact -1#48#43#exact -1#49#44#exact -1#51#46#exact -1#34#47,48#para -1#35#49#exact -1#31#50,51#para -1#32#52#exact -1#52#53#exact \n"}nil_first --> [38, 45]
nil_second --> [15, 33, 36, 37]
--------------------------
{1=>"In our experiments , after reaching 50 features , the classifier 's performance did not significantly increase , so the maximum number of features for each layer is set to 50 .\n", 2=>"In our experiments , after reaching 50 features , the classifier 's performance does not significantly increase anymore , so the maximum number of features for each layer is set to 50 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13,14#para -1#15#15#exact -1#16#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact \n"}nil_first --> []
nil_second --> [13, 17]
--------------------------
{1=>"They are somehow similar to the features of the first 24x24 layer classifier as shown in Figure 11( b ) . / / [somehow?This sounds vague . How are they similar?]\n", 2=>"It is somehow similar to features of the first 24x24 layer classifier as shown in Figure 11( b ) .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [8, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
nil_second --> []
--------------------------
{1=>"Two main issues surrounding the reuse of features selected by AdaBoost are : ( i ) which layer�fs features should be reused for SVM and ( ii ) how many features should be used .\n", 2=>"Two main issues surrounding the reuse of features selected by AdaBoost are : ( i ) which layer whose features will be reused for SVM is the best? and ( ii ) How many features should be used?\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#19#18#exact -1#35#19#exact -1#36#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#lc -1#33#29#exact -1#34#30#exact -1#21#32#exact \n"}nil_first --> [17, 31, 33, 34]
nil_second --> [17, 18, 20, 25, 26, 27, 37]
--------------------------
{1=>"These parameters were found by using a cross-validation test .\n", 2=>"These parameters were found by using cross-validation test .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"As a result , the total training time of the system can easily be controlled .\n", 2=>"As a result , total training time of the system can be easily controlled .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#10,11#9,10,11#para -1#12#12#exact -1#13#13,14#para -1#14#15#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"To determine the number of features is that would be sufficiently robust , we used the 200-feature set selected in layer 17 to generate different subsets of features with different numbers of features .\n", 2=>"To determine how many features is robust enough , we used the 200-feature set selected in layer 17 to generate different subsets of features with different number of features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#11#2#exact -1#26#3#exact -1#27#4#exact -1#28#5#exact -1#5#6#exact -1#6#11#exact -1#8#12#exact -1#9#13#exact -1#10#14#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#4#32#exact -1#29#33#exact \n"}nil_first --> [7, 8, 9, 10, 15, 30, 31]
nil_second --> [2, 3, 7]
--------------------------
{1=>"Features in each set were selected in the order in which they were added in the training process .\n", 2=>"Features in each set were selected in the order that they were added in the training process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8,9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"For example , a 25-feature set consists of the first 25 features selected by AdaBoost when training layer 17 .\n", 2=>"For example , a 25-feature set consists of first 25 features selected by AdaBoost when training layer 17 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"The results shown in Figure 14 indicate that with more than 100 features , the performance of the classifiers was comparable . / / [to what?]\n", 2=>"The results shown in Figure 14 indicate that with more than 100 features , the performance of classifiers is comparable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16#14,15,16,17#para -1#17#18#exact -1#18#19#syn -1#19#20#exact -1#20#21#exact \n"}nil_first --> [22, 23, 24, 25]
nil_second --> []
--------------------------
{1=>"Figure 15 shows the processing speed of SVM classifiers using different subsets of features .\n", 2=>"Figure 15 shows the processing speed of SVM classifiers that uses different subsets of features .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9,10#para -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> []
nil_second --> [9, 10]
--------------------------
{1=>"The SVM classifier using 25 features ran the fastest , while the SVM classifier using 200 features was the slowest .\n", 2=>"The SVM classifier using 25 features run fastest while the SVM classifier using 200 features run slowest .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#syn -1#9#7#exact -1#7#8#exact -1#8#10#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#18,19#para -1#17#20#exact \n"}nil_first --> [9, 11, 17]
nil_second --> [15]
--------------------------
{1=>"The speeds of SVM classifiers using 100 , 125 , and 175 features were not importantly different because their difference in terms of number of features and number of support vectors were not large enough to have a significant impact .\n", 2=>"The speeds of SVM classifiers using 100 , 125 and 175 features are not importantly different because their difference in terms of number of features and number of support vectors is inconsiderable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12,13#13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#syn -1#32#40#exact \n"}nil_first --> [9, 32, 33, 34, 35, 36, 37, 38, 39]
nil_second --> [31]
--------------------------
{1=>"Therefore , 100 features might be the best trade-off between speed and performance .\n", 2=>"Therefore , 100 features might be the best trade-off between the speed and the performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11,12#10,11#para -1#14#12#exact -1#15#13#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"The remaining 34 ,000 non-face patterns and other 2 ,450 face patterns were used to compare the accuracy of the classifiers .\n", 2=>"The remaining 34 ,000 non-face patterns and other 2 ,450 face patterns were used to compare the accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#21#exact \n"}nil_first --> [18, 19, 20]
nil_second --> []
--------------------------
{1=>"The RBF SVM classifier reused 100 features selected by the last layer of CAB17 as the feature vector and was trained by an RBF kernel whose parameter \\MATH is \\MATH .\n", 2=>"The RBF SVM classifier reused 100 features selected by the last layer of CAB17 as the feature vector and was trained by a RBF kernel whose parameter \\MATH is \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [22]
nil_second --> [22]
--------------------------
{1=>"These parameters were found by using a cross-validation test .\n", 2=>"These parameters are found by using cross-validation test .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"The result shown in Figure 16 demonstrates that with hard classified patterns that later layers of the cascade will process , the single SVM classifier can achieve higher accuracy than the cascade of AdaBoost classifiers trained by roughly predefined training parameters . / / ?NOTE : I believe that I hyphenated this term in your previous document , but after seeing it used here , I would say that it does not need to be hyphenated. My apologies for any confusion . A better way to express this , however , might be \" patterns that have been classified as difficult \" or \" patterns shown to be difficult to classify .]\n", 2=>"The result shown in Figure 16 demonstrates that with hard classified patterns that later layers of the cascade will process , the single SVM classifier can achieve higher accuracy than the cascade of AdaBoost classifiers trained by roughly predefined training parameters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact \n"}nil_first --> [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111]
nil_second --> []
--------------------------
{1=>"Furthermore , the training time of a single SVM ( which takes several hours ) is much shorter than that of a cascade of AdaBoost classifiers ( which might take several weeks ) .\n", 2=>"Furthermore , the training time of a single SVM ( which takes several hours ) is much smaller than that of a cascade of AdaBoost classifiers ( which might take everal weeks ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#31#30,31#para -1#32#32#exact -1#33#33#exact \n"}nil_first --> [17]
nil_second --> [17, 30]
--------------------------
{1=>"In the first stage , the cascaded 36x36 classifiers consist of three layers , making for a total of 120 features .\n", 2=>"In the first stage , the cascaded 36x36 classifiers consist of three layers , making a total number of features used of 120 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#21#18#exact -1#22#19#exact -1#19#20#exact -1#23#21#exact \n"}nil_first --> [15]
nil_second --> [17, 18, 20]
--------------------------
{1=>"Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and can thus save significant training time ; the training time needed using the new system is approximately 27 times shorter / approximately 27 rounds of training are needed in the new system . / / <--I think that the first choice here is your intended meaning , but please check carefully .\n", 2=>"Compared to the system with 6 ,061 features used in [1] , our system uses fewer features and , thus , can save significant training time ( which is approximate 27 times in total ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#21#18#exact -1#19#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#31,32#27#para -1#28#32,33#para -1#29#34#stem -1#30#35#exact -1#35#50#exact -1#18#64#exact \n"}nil_first --> [24, 25, 26, 28, 29, 30, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69]
nil_second --> [20, 26, 27, 33, 34]
--------------------------
{1=>"The 7 ,500 non-face patterns used to train the first SVM classifier were selected randomly from the 40 ,000 non-face patterns .\n", 2=>"7 ,500 non-face patterns used to train the first SVM classifier were selected randomly from the 40 ,000 non-face patterns .\n", 3=>"-1#15#0#lc -1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"All these parameters were found by using a cross-validation test tool provided by LibSVM [31] . / / ?NOTE : I believe that I hyphenated this term in your previous document , but after seeing it used here , I would say that it does not need to be hyphenated\n", 2=>"All these parameters were found by using cross-validation test tool provided by LibSVM [31]} .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact \n"}nil_first --> [7, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
nil_second --> [13]
--------------------------
{1=>"This training procedure yielded three SVM classifiers whose numbers of support vectors are 4 ,725 , 5 ,043 , and 4 ,847 .\n", 2=>"This training procedure resulted three SVM classifiers whose the numbers of support vectors are 4 ,725 , 5 ,043 , and 4 ,847 respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#24#22#exact \n"}nil_first --> [3]
nil_second --> [3, 8, 23]
--------------------------
{1=>"We tested our system on the MIT+CMU frontal-face standard test set [5] , which consists of 124 images with 480 frontal faces ( excluding images containing hand-drawn , cartoon , and small faces ) .\n", 2=>"We tested our system on the MIT+CMU frontal-face standard test set [5] which consists of 124 images with 480 frontal faces ( excluding images containing hand-drawn , cartoon and small faces ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#26#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#27#28#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact \n"}nil_first --> [27, 29]
nil_second --> []
--------------------------
{1=>"The first row presents the number of features of each layer and the second row shows the fraction of the remaining patterns after each layer were processed . / / [fraction / percentage?<--Here and after , you use \" percentage \" in the graph , so you may want to keep the same term here .]\n", 2=>"The first row presents the number of features of each layer , and the second row shows the fraction of the remaining patterns after each layer processing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#26#stem -1#27#27#exact -1#11#35#exact \n"}nil_first --> [25, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
nil_second --> []
--------------------------
{1=>"The last row indicates the fraction of time that each layer consumed . / / [fraction / percentage?]\n", 2=>"The last row indicates the fraction of time that each layer consumes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#stem -1#12#12#exact \n"}nil_first --> [13, 14, 15, 16, 17]
nil_second --> []
--------------------------
{1=>"All these statistics were extracted by running the classifiers on the MIT+CMU test set .\n", 2=>"All these statistics are extracted from running the classifiers on the MIT+CMU test set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#syn -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [5]
nil_second --> [5]
--------------------------
{1=>"When the first 24x24 layer classifier was added to the cascade of 36x36 classifiers , this combination rejected 85 .91\\% of analyzed patterns compared to \\MATH of using only the first layer of the single cascade of 24x24 classifiers .\n", 2=>"If the first 24x24 layer classifier is added to the cascade of 36x36 classifiers , this combination rejects 85 .91\\% of analyzed patterns compared to \\MATH of using only the first layer of the single cascade 24x24 classifiers .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#stem -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact \n"}nil_first --> [0, 36]
nil_second --> [0, 6]
--------------------------
{1=>"Furthermore , the rejection of this very large number of patterns was done extremely quickly , only using \\MATH of the total processing time . / / [the total / the standard?]\n", 2=>"Furthermore , the rejection of this very large number of patterns is done extremely quickly , only using \\MATH of processing time .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9#6,7,8#para -1#19#9#exact -1#10#10#exact -1#11#11#syn -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [19, 20, 21, 25, 26, 27, 28, 29, 30, 31]
nil_second --> []
--------------------------
{1=>"The detection rate and speed of the classifiers with ten false positives are listed in Table 3 .\n", 2=>"Detection rate and speed of classifiers with ten false positives are listed in Table 3 .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [0, 6]
nil_second --> []
--------------------------
{1=>"It is clear that our multi-stage system ran faster than the single cascade of 24x24 AdaBoost classifiers while achieving comparable detection rates .\n", 2=>"It is clear that our multi-stage system runs faster than the single cascade of 24x24 AdaBoost classifiers while detection rates are comparable .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#21#19#exact -1#18#20#exact -1#19#21#exact -1#22#22#exact \n"}nil_first --> [18]
nil_second --> [20]
--------------------------
{1=>"This performance was possible for three reasons .\n", 2=>"This performance is possible because of the three following reasons :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#7#4,5#para -1#9#6#exact \n"}nil_first --> [7]
nil_second --> [4, 5, 6, 8, 10]
--------------------------
{1=>"First , the cascade of 36x36 AdaBoost classifiers rejected many of non-face patterns extremely quickly , while slow SVM classifiers only processed a very small number of the remaining patterns .\n", 2=>"First , the cascade of 36x36 AdaBoost classifiers rejects a lot of non-face patterns extremely fast while slow SVM classifiers only process a very small number of the remaining patterns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#stem -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [9, 15]
nil_second --> [9, 10]
--------------------------
{1=>"Second , many images in the MIT+CMU test set contain large portion of background , which [9] mentioned has a ratio of non-face to face patterns of about 50 ,000 to 1 .\n", 2=>"Second , many images in the MIT+CMU test set contain large portion of background which was mentioned in [9] which said the ratio of non-face to face patterns is about 50 ,000 to 1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#18#16#exact -1#16#17#exact -1#28#18,19#para -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#29#26,27#para -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact \n"}nil_first --> [14]
nil_second --> [15, 17, 19, 20, 21]
--------------------------
{1=>"Experimental results showed that the AdaBoost+SVM system ran faster than that of the original AdaBoost on \\MATH of the total number of images in this test set .\n", 2=>"Experimental results showed that the AdaBoost+SVM system runs faster than that of the original AdaBoost on \\MATH of total number of images in this test set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17,18#17,18,19,20#para -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers did not severely affect the final performance because they might also be rejected by 24x24 classifiers in later layers .\n", 2=>"Third , at a small number of false positives , some true face candidate regions rejected by 36x36 classifiers do not affect so much in final performance because it might also be rejected by 24x24 classifiers in later layers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20#19,20#para -1#21#22#exact -1#25#23,24#para -1#26#25#exact -1#27#26#exact -1#29#27,28#para -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact \n"}nil_first --> [21]
nil_second --> [22, 23, 24, 28]
--------------------------
{1=>"The cascaded structure of AdaBoost-based classifiers in the two first stages allows the system to best adapt to various complexities of input patterns , while nonlinear SVM classifiers at the final stage are robust enough to achieve good results .\n", 2=>"The cascaded structure of AdaBoost-based classifiers in two first stages allows to best adapt to various complexities of input patterns ,while non linear SVM classifiers at the final stage are robust enough to achieve good results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#26#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#27#29,30#para -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact -1#35#38#exact -1#36#39#exact \n"}nil_first --> [12, 13, 23, 24, 25]
nil_second --> [20, 21, 22]
--------------------------
{1=>"Extensive experiments demonstrated that a significant computation time is devoted to potential face regions because almost all non-face patterns are rejected quickly by the two first stages , and only a very small number of face-like patterns are processed by the slow SVM classifiers . / / [are / need to be?]\n", 2=>"Extensive experiments demonstrated that a significant computation time is devoted to potential face regions because almost all non-face patterns are rejected quickly by the two first stages , and only a very small number of face-like patterns is processed by the slow SVM classifiers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#syn -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact \n"}nil_first --> [45, 46, 47, 48, 49, 50, 51]
nil_second --> []
--------------------------
{1=>"Discriminant Haar wavelet features selected from AdaBoost are used for all stage classifiers to take advantage of their efficient representation and fast evaluation .\n", 2=>"Discriminant Haar wavelet features selected from AdaBoost are used for all stage classifier to take advantages from their efficient representation and fast evaluation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13,14#13,14,15,16#para -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> []
nil_second --> [15, 16]
--------------------------
{1=>"It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing .\n", 2=>"to improve the retrieval performance of image search engines that use textual information for indexing , it is necessary to utilize visual information .\n", 3=>"-1#16#0#lc -1#17#1#exact -1#18#2#exact -1#19#3#exact -1#20#4#exact -1#21#5#exact -1#22#6#exact -1#0,1,2#7,8,9,10,11#para -1#3#12#exact -1#8#15#exact -1#9#16#exact -1#10#17#exact -1#11#18#exact -1#12#19#exact -1#13#20#exact -1#14#21#exact -1#23#22#exact \n"}nil_first --> [13, 14]
nil_second --> [4, 5, 6, 7, 15]
--------------------------
{1=>"Most state-of-the-art methods of learning visual consistency usually learn one specific classifier for each query to re-rank the returned images .\n", 2=>"Most of the state of the art methods for learning the visual consistency usually learn one specific classifier for each query for re-ranking the returned images .\n", 3=>"-1#0#0#exact -1#7#2#exact -1#1#3#exact -1#9#4#exact -1#11#5#exact -1#12#6#exact -1#13#7#exact -1#14#8#exact -1#15#9#exact -1#16#10#exact -1#17#11#exact -1#18#12#exact -1#19#13#exact -1#20#14#exact -1#22#16#stem -1#23#17#exact -1#24#18#exact -1#25#19#exact -1#26#20#exact \n"}nil_first --> [1, 15]
nil_second --> [2, 3, 4, 5, 6, 8, 10, 21]
--------------------------
{1=>"The main drawback with these methods is that they require computational cost and processing time that are unsuitable for handling a large number of queries .\n", 2=>"The drawback of these methods is it requires computational cost and processing time that are unsuitable for handling a large number of queries .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#13#7#exact -1#7#9#stem -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#14#15,16#para -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#19,20,21#20,21,22,23#para -1#22#24#exact -1#23#25#exact \n"}nil_first --> [1, 3, 8]
nil_second --> [2, 6, 18]
--------------------------
{1=>"Different from query-specific based methods that learn classifiers for recognition concepts encoded in each query , the generic classifier in our method learns relevance between images and the query for re-ranking purposes .\n", 2=>"Different from query-specific based methods that learn classifiers for recognition concepts encoded in each query , the generic classifier of our method learns relevancy between images and the query for re-ranking purpose .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#stem -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#stem -1#32#32#exact \n"}nil_first --> [19]
nil_second --> [19]
--------------------------
{1=>"The key contribution of this research is to introduce a query-dependent feature to represent this relevance and an unsupervised method of collecting training samples to learn the generic classifier .\n", 2=>"The key contribution of this paper is to introduce a query-dependent feature to represent this relevancy and an unsupervised method to collect training samples for learning the generic classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#stem -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20,21#20,21#para -1#22#22#exact -1#23#23#exact -1#25#24,25#para -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> []
nil_second --> [5, 24]
--------------------------
{1=>"The generic classifier is built automatically and is independent of existing ranking algorithms for input search engines .\n", 2=>"The generic classifier is built automatically and independent with existing ranking algorithms of input search engines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7,8#para -1#12#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [13]
nil_second --> [8]
--------------------------
{1=>"The experimental results demonstrated that the proposed method performed very well in various datasets .\n", 2=>"experimental results show that the proposed method achieves good performance in various datasets .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#syn -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#10#syn -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [0, 8, 9]
nil_second --> [7, 9]
--------------------------
{1=>"Most existing image-search engines usually use text information to determine relevance , resulting in poor precision .\n", 2=>"Most of existing image search engines usually use text information for judging relevancy , resulting low precision performance .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10,11#8,9#para -1#12#10#stem -1#13#11#exact -1#14#12#exact -1#16#15#exact -1#18#16#exact \n"}nil_first --> [2, 13, 14]
nil_second --> [1, 3, 4, 15, 17]
--------------------------
{1=>"To improve the accuracy of retrieval , it is necessary to use visual information from images to re-rank them .\n", 2=>"To improve the retrieval performance , it is necessary to use visual information of images for re-ranking .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#13#4#exact -1#3#5#exact -1#5#6#exact -1#6,7,8#7,8,9,10#para -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact -1#9#16#exact -1#16#17#stem -1#17#19#exact \n"}nil_first --> [3, 14, 18]
nil_second --> [4, 15]
--------------------------
{1=>"In addition , using visual information requires much greater computational cost than using text .\n", 2=>"In addition , using visual information requires huge computational cost compared with using text .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#9#exact -1#9#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [7, 8, 11]
nil_second --> [7, 10, 11]
--------------------------
{1=>"One popular approach \\CITE combining both text and visual information has been to use text information to quickly retrieve a set of candidates and then do post-processing ( i . e . , re-ranking ) on this set to improve precision .\n", 2=>"One popular approach \\CITE combining both text and visual information is to use text information to quickly retrieve a set of candidates and then do post-processing (i . e . re-rank) on this set to improve the precision .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#37#40#exact -1#38#41#exact \n"}nil_first --> [27, 28, 32, 33, 34]
nil_second --> [26, 30, 36]
--------------------------
{1=>"There are two ways of doing post-processing : The first \\CITE has been to build a ranker or a classifier specific to the given query using the returned images .\n", 2=>"There are two ways for post-processing : The first way \\CITE is to build a ranker or a classifier specific to the given query using the returned images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#10#exact -1#11,12#11,12,13#para -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [4, 5]
nil_second --> [4, 9]
--------------------------
{1=>"As a result , this way is not scalable for applications that process very large numbers of queries .\n", 2=>"As a result , this way is not scalable for applications processing very large number of queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#stem -1#12#13#exact -1#13#14#exact -1#14,15#15,16#para -1#16#17#exact -1#17#18#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"This is more scalable and can be used for practical applications such as meta-search engines .\n", 2=>"This way is more scalable and can be used for practical applications such as meta search engines .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6,7#5,6,7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#16#14#exact -1#17#15#exact \n"}nil_first --> [13]
nil_second --> [1, 8, 9, 14, 15]
--------------------------
{1=>"We pursued the latter way to solve the problem with face retrieval in which the system enables users to search people's appearances by their names .\n", 2=>"We follow the latter way for the problem of face retrieval in which the system enables users to search persons's appearance by their names .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#17#5#exact -1#6#7#exact -1#7,8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#18#18,19#para -1#20#21#stem -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [6, 20]
nil_second --> [5, 19]
--------------------------
{1=>"Our system re-ranks the faces returned by text-based search engines with a generic classifier that is trained in advance using visual information before returning them to the user .\n", 2=>"Our system re-ranks the faces returned by text-based search engines by a generic classifier that is trained in advance using visual information before returning to the user .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [10, 24]
nil_second --> [10]
--------------------------
{1=>"Building such generic classifiers requires two problems to be solved : finding a good query-relative representation of faces and collecting a large labeled dataset to train the classifier .\n", 2=>"Building such generic classifiers requires solving two problems : finding good query-relative representation of faces and collecting a large labeled dataset for training the classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#5#9#stem -1#8#10#exact -1#9#11#exact -1#17#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#18#20,21#para -1#19#22#exact -1#20#23#exact -1#21,22#24,25#para -1#23#26#exact -1#24#27#exact -1#25#28#exact \n"}nil_first --> [7, 8]
nil_second --> []
--------------------------
{1=>"Our contribution by addressing these problems is two-fold :\n", 2=>"By addressing these problems , Our contribution is two-fold :\n", 3=>"-1#5#0#exact -1#6#1#exact -1#0#2#lc -1#1#3#exact -1#2#4#exact -1#3#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not in this framework .\n", 2=>"In this framework , We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#16,17,18#11,12,13,14#para -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#0#19#lc -1#1#20#exact -1#2#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [3, 15]
--------------------------
{1=>"The more relevant a face is to the query , the higher score is .\n", 2=>"The more relevant a face to the query , the higher score is .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"This approach is different from existing ones \\CITE that learn a classifier to recognize the identity of the returned faces .\n", 2=>"This approach is different from existing approaches such as \\CITE that learn a classifier to recognize the identity of the returned faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact \n"}nil_first --> [6]
nil_second --> [6, 7, 8]
--------------------------
{1=>"As this classifier is independent of the identity of faces , it can be shared for multiple queries ( cf . Figure \\REF ) .\n", 2=>"this classifier is independent with the identity of faces , so it can be shared for multiple queries (cf . Figure \\REF) .\n", 3=>"-1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#7#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#20#exact -1#20#21#exact -1#22#24#exact \n"}nil_first --> [0, 8, 18, 19, 22, 23]
nil_second --> [4, 10, 18, 21]
--------------------------
{1=>"We propose a novel representation for each face that models the relevance between that face and the query .\n", 2=>"We propose a novel representation for each face that models relevance between that face and the query .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#15#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"Once this query-dependent feature for each face is extracted , one relevance classifier can be shared by the faces of various queries .\n", 2=>"Once this query-dependent feature for each face is extracted , one relevance classifier can be shared by faces of various queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"The experimental results demonstrated that the relevance classifier that is independent of the underlying ranking algorithms of existing search engines could significantly boost performance .\n", 2=>"experimental results show that the relevance classifier that is independent with underlying ranking algorithm of existing search engines can significantly boost the performance .\n", 3=>"-1#21#0#lc -1#0#1#exact -1#1#2#exact -1#2#3#syn -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#14#11#exact -1#11#12,13#para -1#12#14#exact -1#13#15#stem -1#15#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#para -1#19#21#exact -1#20#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"-We propose a simple yet efficient mining technique of automatically collecting labeled data to train the generic classifier .\n", 2=>"-We propose a simple yet efficient mining technique for automatically collecting labeled data for training the generic classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14#13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"We specifically detected and grouped faces of people appearing in video programs in face tracks in which each face track contained the faces of one person .\n", 2=>"Specifically , We detect and group faces of persons appearing in video programs in face tracks in which each face track contains of the faces of one person .\n", 3=>"-1#2#0#exact -1#0#1#lc -1#3#2#stem -1#4#3#exact -1#5#4#stem -1#6#5#exact -1#7#6#exact -1#8#7#para -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#stem -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact \n"}nil_first --> []
nil_second --> [1, 22]
--------------------------
{1=>"To distinguish the face tracks of different people , we assumed that if multiple faces were detected at different locations in one frame , they would be of different people ( cf .\n", 2=>"To distinguish face tracks of different persons , we assume that if multiple faces are detected at different locations in one frame , they are of different persons (cf . Figure \\REF) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#para -1#7#8#exact -1#8#9#exact -1#9,10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#23,24#15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#14#25,26#para -1#25#27#exact -1#26#28#exact -1#27#29#para -1#29#32#exact \n"}nil_first --> [2, 24, 30, 31]
nil_second --> [28, 30, 31, 32]
--------------------------
{1=>"Using this assumption , we collected face tracks whose faces were detected in the same frames to guarantee that each face track was associated with one unique person .\n", 2=>"Using this assumption , we collect the face tracks whose faces are detected in the same frames to guarantee that each face track is associated to one unique person .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#12,13#10,11,12#para -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#syn -1#24#23#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [24]
nil_second --> [6, 11, 25]
--------------------------
{1=>"We used video programs from multiple genres and channels to increase the number of such face tracks .\n", 2=>"To enlarge the number of such face tracks , We use video programs of multiple genres and channels .\n", 3=>"-1#9#0#exact -1#10#1#stem -1#11#2#exact -1#12#3#exact -1#14#5#exact -1#15#6#exact -1#16#7#exact -1#17#8#exact -1#0#9#lc -1#1,2#10,11#para -1#3#12#exact -1#4#13#exact -1#5#14#exact -1#6#15#exact -1#7#16#exact -1#18#17#exact \n"}nil_first --> [4]
nil_second --> [8, 13]
--------------------------
{1=>"We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .\n", 2=>"From these faces , We can artificially generate face sets similar to the sets returned by search engines given person names .\n", 3=>"-1#4#0#exact -1#5#1#para -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#0#6#lc -1#1#7#exact -1#2#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> [18]
nil_second --> [3, 19]
--------------------------
{1=>"Since we knew the relevance of these faces to the artificial sets , the labels for each face could be easily generated and no human intervention was needed in this process .\n", 2=>"Since we know the relevance of these faces to the artificial sets , the labels of each face can be easily generated and no human intervention is needed for this process .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15,16#15,16#para -1#17#17#exact -1#18,19#18,19#para -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26,27#26,27#para -1#29,30#28,29,30#para -1#31#31#exact \n"}nil_first --> []
nil_second --> [28]
--------------------------
{1=>"Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .\n", 2=>"Note that the label of faces in our approach is not identity of that face . It is the relevance between the face and the associated query .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#18#2#exact -1#2,3#3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#9,10#para -1#11#11#exact -1#14#13#stem -1#21#15#exact -1#19#16#exact -1#20#17#exact -1#24#18#exact -1#22,23#19,20#para -1#25#21,22#para -1#26#23#exact -1#27#24#exact \n"}nil_first --> [12, 14]
nil_second --> [9, 12, 13, 15, 16, 17]
--------------------------
{1=>"Collecting training sets from such external sources as video archives is easy and efficient because , first , a large number of videos can be easily obtained .\n", 2=>"Collecting training sets from such external sources as video archives is easy and efficient because : firstly , a large number of videos can be easy to obtain .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#15#exact -1#19,20,21#18,19,20,21#para -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25,26#25#para -1#27#26#stem -1#28#27#exact \n"}nil_first --> [16, 17]
nil_second --> [15, 16, 18]
--------------------------
{1=>"For example , people can record broadcast videos from different channels within a certain period .\n", 2=>"For example , people can record broadcast videos of different channels in a certain period .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9#para -1#10#10#exact -1#12,13,14#11,12,13,14#para -1#15#15#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"Second , a huge number of faces can be obtained by applying a face detector to all frames .\n", 2=>"Secondly , a huge number of faces can be obtained by applying the face detector in every frame .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#17#17#stem -1#18#18#exact \n"}nil_first --> [12, 15, 16]
nil_second --> [12, 15, 16]
--------------------------
{1=>"In addition , the faces of one person appearing in consecutive frames can be automatically grouped with a high degree of accuracy using temporal information .\n", 2=>"In addition , using temporal information , faces of one person appearing in consecutive frames can be automatically grouped with high accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17,18,19,20#para -1#21#21#exact -1#3#22#exact -1#4#23#exact -1#5#24#exact -1#22#25#exact \n"}nil_first --> [3]
nil_second --> [6]
--------------------------
{1=>"It is essential for image-search engines to find relevant images with a high degree of precision given queries described by text , e.g. , 'airplane' or 'George Bush' .\n", 2=>"given a query described by text , for example , 'airplane' or 'George Bush' , finding relevant images with high precision is essential for image search engines .\n", 3=>"-1#21#1#exact -1#22#2#exact -1#23#3#exact -1#26#5#exact -1#15#6,7#para -1#16#8#exact -1#17#9#exact -1#18#10#exact -1#1#11#exact -1#19#12#exact -1#20#15#exact -1#0#16#exact -1#2#17#stem -1#3#18#exact -1#4#19#exact -1#5#20#exact -1#6#21#exact -1#9#23#exact -1#10#24#exact -1#11#25#exact -1#12#26#exact -1#13#27#exact -1#27#28#exact \n"}nil_first --> [0, 4, 13, 14, 22]
nil_second --> [7, 8, 14, 24, 25]
--------------------------
{1=>"Existing image-search engines usually use textual information associated with images such as filenames , image captions , and surrounding text for ranking that leads to poor precision .\n", 2=>"Existing image search engines usually use textual information associated with the images such as filename , image caption , and surrounding text for ranking that leads to poor precision .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#stem -1#15#13#exact -1#16#14#exact -1#17#15#stem -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact \n"}nil_first --> [1]
nil_second --> [1, 2, 10]
--------------------------
{1=>"To improve precision , visual information is used to re-rank the returned images .\n", 2=>"To improve the precision , visual information is used to re-rank the returned images .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"There have been different approaches \\CITE to re-ranking images containing general objects and faces returned from text-based search engines .\n", 2=>"There are different approaches described in \\CITE for re-ranking images containing general objects and faces returned from text-based search engines .\n", 3=>"-1#0#0#exact -1#1#1,2#para -1#2#3#exact -1#3#4#exact -1#6#5#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [6]
nil_second --> [4, 5, 7]
--------------------------
{1=>"Work \\CITE has extended to topics on models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or the Hierarchical Dirichlet Process to learn generative model-based classifiers .\n", 2=>"Work such as \\CITE extend topics models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or Hierarchical Dirichlet Process to learn generative model based classifiers .\n", 3=>"-1#0#0#exact -1#3#1#exact -1#21#4#exact -1#5#5#exact -1#6#6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#22#23,24#para -1#23#25#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [2, 3, 19, 26]
nil_second --> [1, 2, 4, 24, 25]
--------------------------
{1=>"However , they have many parameters that need to be tuned such as the number of topics and feature configurations .\n", 2=>"However , they have many parameters needed to be tuned such as number of topics and feature configurations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7,8#6,7,8,9#para -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12,13#13,14,15#para -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"In addition , how the best topic is selected associated with the input query to identify the target label is still a difficult issue \\CITE .\n", 2=>"In addition , how to select the best topic associated with the input query for identifying target label is still challenging \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#18#7#exact -1#5#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#4#14#exact -1#15#15,16#para -1#16#17#exact -1#17#18#exact -1#19#19,20#para -1#20#22#para -1#21#24#exact -1#22#25#exact \n"}nil_first --> [21, 23]
nil_second --> [14]
--------------------------
{1=>"Textual information has been used to build a text ranker to re-rank the returned images \\CITE .\n", 2=>"In \\CITE , Textual information is used to build a text ranker to re-rank the returned images \\CITE .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2,3#para -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact \n"}nil_first --> []
nil_second --> [0, 1, 2]
--------------------------
{1=>"The top images in this ranked list were used as positive samples to train visual classifiers using SVM ( Support vector machines ) .\n", 2=>"The top images in this ranked list are used as positive samples to train visual classifiers using SVM (Support vector machines) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#20#exact -1#21#23#exact \n"}nil_first --> [18, 19, 21, 22]
nil_second --> [18, 20]
--------------------------
{1=>"This method made the training data cleaner and led to improved performance .\n", 2=>"This method makes the training data cleaner that leads to performance improvement .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8,9#8,9#para -1#10#10,11#para -1#12#12#exact \n"}nil_first --> [7]
nil_second --> [7, 11]
--------------------------
{1=>"A multiple-instance learning framework has been used to learn category models from images associated with keywords \\CITE .\n", 2=>"In \\CITE , A multiple instance learning framework is used to learn category models from images associated with keywords \\CITE .\n", 3=>"-1#3#0#exact -1#6#2#exact -1#7#3#exact -1#8#4,5#para -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact \n"}nil_first --> [1]
nil_second --> [0, 1, 2, 4, 5]
--------------------------
{1=>"These researchers re-ranked images containing general objects .\n", 2=>"The work mentioned above are for re-ranking images containing general objects .\n", 3=>"-1#6#2#stem -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 1, 2, 3, 4, 5]
--------------------------
{1=>"Gaussian mixture models have been used for re-ranking faces to build face recognizers and apply these recognizers back to the input faces for re-ranking \\CITE .\n", 2=>"For re-ranking faces , work described in \\CITE use Gaussian mixture models to build face recognizers and apply these recognizers back to the input faces for re-ranking \\CITE .\n", 3=>"-1#9#0#exact -1#10#1#exact -1#11#2#exact -1#8#4,5#para -1#0#6#lc -1#1#7#exact -1#2#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact \n"}nil_first --> [3]
nil_second --> [3, 4, 5, 6, 7]
--------------------------
{1=>"Discriminative-approach-based models such as SVM and linear discriminant analysis have been used instead of Gaussian mixture models \\CITE .\n", 2=>"In \\CITE , Discriminative approach based models such as SVM and linear discriminant analysis are used instead of Gaussian mixture models \\CITE .\n", 3=>"-1#6#1#exact -1#7#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#14,15#9,10,11#para -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact \n"}nil_first --> [0]
nil_second --> [0, 1, 2, 3, 4, 5]
--------------------------
{1=>"A densest-graph-based method has been used for finding the face group relevant to the query \\CITE .\n", 2=>"In \\CITE , A densest graph based method is used for finding the face group relevant to the query \\CITE .\n", 3=>"-1#3#0#exact -1#7#2#exact -1#8#3,4#para -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact \n"}nil_first --> [1]
nil_second --> [0, 1, 2, 4, 5, 6]
--------------------------
{1=>"One specific classifier is built for each query in these approaches .\n", 2=>"As for these approaches , One specific classifier is built for each query .\n", 3=>"-1#5#0#exact -1#6#1#exact -1#7#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#2#9#exact -1#3#10#exact -1#13#11#exact \n"}nil_first --> [8]
nil_second --> [0, 1, 4]
--------------------------
{1=>"Therefore , many classifiers must be built , which are not suitable in practice , to handle a large number of queries .\n", 2=>"Therefore , to handle a large number of queries , many classifiers must be built which are not suitable in practice .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#10#2#exact -1#11#3#exact -1#12#4#exact -1#13#5#exact -1#14#6#exact -1#9#7#exact -1#15#8#exact -1#16#9#exact -1#17#10#exact -1#18#11#exact -1#19#12#exact -1#20#13#exact -1#2#15#exact -1#3#16#exact -1#5,6,7#17,18,19,20#para -1#8#21#exact -1#21#22#exact \n"}nil_first --> [14]
nil_second --> [4]
--------------------------
{1=>"Only one generic classifier has been built in advance \\CITE and then used for all queries .\n", 2=>"In \\CITE{Krapac10CVPR} , Only one generic classifier is built in advance \\CITE and then used for all queries .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4,5#para -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact \n"}nil_first --> []
nil_second --> [0, 1, 2]
--------------------------
{1=>"Each image for specific classifiers is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , e.g. , 'airplane' .\n", 2=>"As for specific classifiers , Each image is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , for example , 'airplane' .\n", 3=>"-1#5#0#exact -1#6#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#4#23#exact -1#27#24#exact -1#28#25#exact \n"}nil_first --> [22]
nil_second --> [0, 24, 25, 26]
--------------------------
{1=>"Each image in a generic classifier is classified as relevant or irrelevant to the query .\n", 2=>"In generic classifier , Each image is classified as relevant or irrelevant to the query .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#0#2#lc -1#1#4#exact -1#2#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [3]
nil_second --> [3]
--------------------------
{1=>"Therefore , it is independent of class labels and can be used for any query .\n", 2=>"Therefore , it is independent to class labels and can be used for any query .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5,6#para -1#7#7#exact -1#8#8#exact -1#9,10#9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> []
nil_second --> [5, 11, 12]
--------------------------
{1=>"This method works well for objects such as cars and flags , but fails to handle faces .\n", 2=>"This method works well for objects such as car , flag , but fails to handle faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#10#10#stem -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"Our method was inspired by the generic-classifier-based approach .\n", 2=>"Our method is inspired by the generic classifier based approach .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#9#7#exact -1#10#8#exact \n"}nil_first --> [6]
nil_second --> [6, 7, 8]
--------------------------
{1=>"We extended it in two ways : first , query-dependent features specific to faces are proposed , and second , the training data for learning the generic classifier are collected automatically by mining video archives .\n", 2=>"We extend it by two means : first , query-dependent features specific for faces are proposed , and second , the training data for learning the generic classifier is collected automatically by mining video archives .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3,4#3,4#para -1#5#5#syn -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#syn -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> [12]
nil_second --> [12]
--------------------------
{1=>"Given a set of faces returned by any search engine for a queried person ( e.g. , 'George Bush' ) , our task is to re-rank these faces to improve precision .\n", 2=>"Given a set of faces returned by any search engine for a queried person ( e .g . 'George Bush' ) , our task is to re-rank these faces to improve the precision .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#21#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#22,23,24,25#21,22,23#para -1#29#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#30#28,29#para -1#32#30#exact -1#33#31#exact \n"}nil_first --> [15, 20]
nil_second --> [15, 16, 17, 31]
--------------------------
{1=>"This approach is different from the existing approaches \\CITE shown in Figure \\REF( a ) in which one specific classifier is built for each query .\n", 2=>"This approach is different from existing approaches such as \\CITE as shown in Figure \\REF( a ) in which one specific classifier is built for each query .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#9#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact \n"}nil_first --> [5]
nil_second --> [7, 8, 10]
--------------------------
{1=>"To build a specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by a query-independent feature such as pixel intensity around facial features such as the eyes , nose , and mouth \\CITE .\n", 2=>"To build the specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by the query-independent feature such as pixel intensity around facial features such as eyes , nose , and mouth \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#2#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact \n"}nil_first --> [2, 20]
nil_second --> [20]
--------------------------
{1=>"Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .\n", 2=>"Meanwhile , to build the generic classifier which is independent with any \\textit{'personX'} , each face is represented by the query-dependent feature .\n", 3=>"-1#13#1#exact -1#14#2#exact -1#15#3#exact -1#16#4#exact -1#17#5#exact -1#18#6#exact -1#19#7#exact -1#20#8#exact -1#21#9#exact -1#2#10#exact -1#3#11#exact -1#5#13#exact -1#6#14#exact -1#7,8#15,16#para -1#9#17#exact -1#11#18,19#para -1#22#21#exact \n"}nil_first --> [0, 12, 20]
nil_second --> [0, 1, 4, 10, 12]
--------------------------
{1=>"Query-dependent features using textual information has been proposed \\CITE .\n", 2=>"In \\CITE , the Query-dependent features using textual information are proposed \\CITE .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#10#5,6,7#para -1#11#8#exact -1#12#9#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3, 9]
--------------------------
{1=>"Each feature was treated as binary indicating the presence or absence of query terms in the textual data associated with the input image , e.g. , filenames , image titles , and nearby text .\n", 2=>"Each feature is treated as binary indicating the presence or absence of the query terms in textual data associated with the input image , for example , filename , image title , and nearby text .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#12#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#26#25#exact -1#27#26#stem -1#28#27#exact -1#29#28#exact -1#30#29#stem -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> [24]
nil_second --> [24, 25]
--------------------------
{1=>"Extending this query-dependent feature to use visual information is not trivial since we cannot compute the presence or absence of query terms such as 'George Bush' in each face .\n", 2=>"Extending this query-dependent feature for using visual information is not trivial since we can not compute the presence and absence of the query term such as 'George Bush' in each face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14#13#para -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#19#18#exact -1#20#19#exact -1#22#20#exact -1#23#21#stem -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact \n"}nil_first --> [17]
nil_second --> [18, 21]
--------------------------
{1=>"Each image in \\CITE is represented as a set of visual words .\n", 2=>"In \\CITE , Each image \\CITE is represented as a set of visual words .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#0#2#lc -1#1#3#exact -1#6#4#exact -1#7#5#exact -1#8,9,10,11#6,7,8#para -1#12#10#exact -1#13#11#exact -1#14#12#exact \n"}nil_first --> [9]
nil_second --> [2, 5]
--------------------------
{1=>"The top- \\MATH visual words that are strongly associated with the set of returned images for the query are selected .\n", 2=>"The top- \\MATH visual words that are strongly associated with the set of the returned images for the query are selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"Since this method is suitable for general objects rather than faces , we propose another method of extracting query-dependent features to train the generic classifier that is described below .\n", 2=>"Since this method is suitable for general objects rather than faces , we proposed another method described below for extracting query-dependent features to train the generic classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#stem -1#14#14#exact -1#15#15#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#16#26,27#para -1#17#28#exact -1#27#29#exact \n"}nil_first --> [16, 25]
nil_second --> [18]
--------------------------
{1=>"We assumed that there would be visual consistency between faces returned by search engines for a query to be able to model the relevance between a face and that given query .\n", 2=>"To be able to model the relevance between a face and the given query , We assume that there is visual consistency among faces returned by search engines for that query .\n", 3=>"-1#15#0#exact -1#16#1#stem -1#17#2#exact -1#18#3#exact -1#19#4,5#para -1#20#6#exact -1#21#7#exact -1#7#8#exact -1#23#9#exact -1#24#10#exact -1#25#11#exact -1#26#12#exact -1#27#13#exact -1#28#14#exact -1#8#15#exact -1#13#16#exact -1#1,2,3#17,18,19,20#para -1#4#21#exact -1#5#22#exact -1#6#23#exact -1#9#26#exact -1#10#27#exact -1#29#28#exact -1#12#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [24, 25]
nil_second --> [0, 11, 14, 22]
--------------------------
{1=>"In the other words , we assumed faces that were relevant to the query would form the largest cluster .\n", 2=>"In the other word , we assume faces that are relevant to the query form the largest cluster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#syn -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"This assumption is widely accepted in most of the work in this field \\CITE .\n", 2=>"This assumption is widely accepted in most of the work of this field \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7,8#5,6#para -1#10#7#exact -1#9#8,9#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"We consider the problem of finding relevant and irrelevant faces in the input set to be the problem of outlier detection \\CITE that is popular in the data-mining community .\n", 2=>"We consider the problem of finding relevant and irrelevant faces in the input set as the problem of outlier detection \\CITE that is popular in data mining community .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#22#15#syn -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#23#24#exact -1#24#25#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [14, 23, 26, 27]
nil_second --> [14, 25, 26]
--------------------------
{1=>"We first describe several distance-based methods of outlier detection that use the distance to the \\MATH -nearest neighbors to determine observations as outliers or non-outliers .\n", 2=>"We first describe several distance based outlier detection methods that use the distance to the \\MATH -nearest neighbors to determine observations as outliers or non-outliers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#5#exact -1#6#7#exact -1#7#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [4, 6]
nil_second --> [4, 5]
--------------------------
{1=>"Then , adaptation is proposed to form a query-dependent feature .\n", 2=>"Then the adaptation is proposed to form the query-dependent feature .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [1, 7]
nil_second --> [1, 7]
--------------------------
{1=>"Given threshold \\MATH , for each point \\MATH , we examine the number of points \\MATH so that \\MATH , where \\MATH is the distance ( e.g. , Euclidean distance ) between \\MATH and \\MATH in the feature space .\n", 2=>"Given a threshold \\MATH , for each point \\MATH , we examine number of points \\MATH so that \\MATH , where \\MATH is the distance ( e .g . Euclidean distance ) between \\MATH and \\MATH in the feature space .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12,13#11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact \n"}nil_first --> [26, 27]
nil_second --> [1, 26, 27, 28]
--------------------------
{1=>"This number of points \\MATH is called the neighborhood score of \\MATH and is defined as : \\MATH where \\MATH is the total number of points in the input dataset .\n", 2=>"This number of points \\MATH is called the neighborhood score of \\MATH and is defined as follows : \\MATH where \\MATH is the total number of points of the input dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22,23,24,25#21,22,23#para -1#27#24#exact -1#26#25#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact \n"}nil_first --> [26]
nil_second --> [16]
--------------------------
{1=>"A low value for \\MATH indicates \\MATH is a candidate of outliers , while a high value for \\MATH indicates \\MATH is a member of one strong association cluster .\n", 2=>"A low value of \\MATH indicates \\MATH is a candidate of outliers , while a high value of \\MATH indicates \\MATH is a member of one strong association cluster .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17#16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22,23,24#21,22,23,24#para -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> []
nil_second --> [21]
--------------------------
{1=>"In practice , it is difficult to know \\MATH because this depends on the underlying distribution of the input dataset .\n", 2=>"In practice , it is difficult to know \\MATH because it depends on underlying distribution of the input dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5,6#3,4,5#para -1#7#6,7#para -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#16#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"For each point \\MATH , find its \\MATH -nearest neighbors \\MATH ; the distance score of \\MATH is the sum of the distances between \\MATH and its \\MATH -nearest neighbors \\MATH and is defined as : \\MATH\n", 2=>"For each point \\MATH , find its \\MATH -nearest neighbors \\MATH , the distance score of \\MATH is the sum of the distances between \\MATH and its \\MATH -nearest neighbors \\MATH and is defined as follows : \\MATH\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#36#35#exact -1#37#36#exact \n"}nil_first --> [11]
nil_second --> [11, 35]
--------------------------
{1=>"Points with larger values for \\MATH have sparser neighborhoods and are more likely outliers than points belonging to dense clusters , which usually have lower values for \\MATH .\n", 2=>"Points with larger values for \\MATH have more sparse neighborhoods and are likely outliers than points belonging to dense clusters which usually have lower values of \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#syn -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#7#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [20, 26]
nil_second --> [25]
--------------------------
{1=>"We consider the generic classifier as an outlier classifier that classifies an input sample as an outlier or a non-outlier .\n", 2=>"We consider the generic classifier as an outlier classifier that classifies an input sample as outlier or non-outlier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [15, 18]
nil_second --> []
--------------------------
{1=>"Each face in our framework is a sample , and non-outliers / outliers mean faces are relevant / irrelevant to the query ( i.e. , target person ) .\n", 2=>"In our framework , Each face is an sample , and non-outliers / outliers mean faces relevant / irrelevant to the query ( i .e . target person ) .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#0#2#lc -1#1#3#exact -1#2#4#exact -1#6,7#5,6#para -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#3#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [15, 23]
nil_second --> [23, 24, 25]
--------------------------
{1=>"As described above , the \\MATH and \\MATH of outliers and non-outliers might have the distributions in Figure \\REF ; these scores can be used as feature values to discriminate non-outliers from outliers .\n", 2=>"As described above , \\MATH and \\MATH of outliers and non-outliers might have distributions shown in Figure \\REF , these scores can be used as feature values to discriminate non-outliers and outliers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#19#20#exact -1#20#21#exact -1#21,22#22,23,24,25#para -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [4, 14, 19, 31]
nil_second --> [14, 18, 23, 24, 30]
--------------------------
{1=>"It requires a sufficient number of training samples to train the relevance classifier using supervised learning methods such as SVM .\n", 2=>"In order to train the relevance classifier using supervised learning methods such as SVM , it requires a sufficient number of training samples .\n", 3=>"-1#15#0#lc -1#16#1#exact -1#17#2#exact -1#18#3#exact -1#19#4#exact -1#20#5#exact -1#21#6#exact -1#22#7#exact -1#2#8#exact -1#3#9#exact -1#4#10#exact -1#5#11#exact -1#6#12#exact -1#7#13#exact -1#8#14#exact -1#9#15#exact -1#10#16#exact -1#11#17#exact -1#12#18#exact -1#13#19#exact -1#23#20#exact \n"}nil_first --> []
nil_second --> [0, 1, 14]
--------------------------
{1=>"The simplest way \\CITE of collecting training samples is to pick many names , and pass them to search engines .\n", 2=>"To collect training samples , The simplest way \\CITE is we pick many names , and pass them to search engines .\n", 3=>"-1#5#0#exact -1#6#1#exact -1#7#2#exact -1#8#3#exact -1#0,1#4,5#para -1#2#6#exact -1#3#7#exact -1#9#8#exact -1#18#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#19#17,18#para -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [4, 10]
--------------------------
{1=>"This is a tedious task and involves a human-labor cost .\n", 2=>"It is a tedious task and requires human labor cost .\n", 3=>"-1#0,1#0,1#para -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#syn -1#9#9#exact -1#10#10#exact \n"}nil_first --> [7, 8]
nil_second --> [7, 8]
--------------------------
{1=>"This approach consists of two steps .\n", 2=>"This approach consists of two steps :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"First , by mining video archives , we automatically collect a set of faces of \\MATH different people \\MATH , where \\MATH is the set of faces of person \\MATH , and \\MATH is the number of people .\n", 2=>"First , by mining video archives , we automatically collect a set of faces of \\MATH different persons \\MATH , where \\MATH is the set of faces of person \\MATH , and \\MATH is the number of persons; and\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact \n"}nil_first --> [37, 38]
nil_second --> [37, 38]
--------------------------
{1=>"The restriction is that the assumption of visual consistency is satisfied .\n", 2=>"The restriction is the assumption of visual consistency is satisfied .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact \n"}nil_first --> [3]
nil_second --> []
--------------------------
{1=>"In other words , as seen in Figure \\REF , \\MATH might have several face clusters and the largest cluster is equivalent to the faces relevant to the query if they are returned by a search engine .\n", 2=>"In other words , as shown in Figure \\REF , \\MATH might have several face clusters and the largest cluster is equivalent to the faces relevant to the query if returning by a search engine .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#32#stem -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [5, 30, 31]
nil_second --> [5]
--------------------------
{1=>"As a result , this method can be used to stimulate face sets returned by search engines using many names as mentioned above .\n", 2=>"As a result , this method can stimulate face sets returned by search engines using many names mentioned above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14#8#stem -1#7#9,10#para -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#15#18#exact -1#16#19#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact \n"}nil_first --> [7, 17, 20]
nil_second --> []
--------------------------
{1=>"We specifically use the following heuristics to pick a set of different people appearing in video archives :\n", 2=>"Specifically , We use the following heuristics to pick a set of different persons appearing in video archives :\n", 3=>"-1#2#0#exact -1#0#1#lc -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#para -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .\n", 2=>"-If there are more than one face appearing in different locations in one frame , they likely belong to different persons .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16,17#para -1#17,18#18,19#para -1#19#21#exact -1#20#22#para -1#21#23#exact \n"}nil_first --> [20]
nil_second --> []
--------------------------
{1=>"Figure \\REF shows an example where this has occurred .\n", 2=>"Figure \\REF shows an example of this case .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#8#9#exact \n"}nil_first --> [5, 7, 8]
nil_second --> [5, 7]
--------------------------
{1=>"-If two people appear in video programs broadcast by different broadcast stations ( e.g. , CNN , MSNBC , and CCTV ) , they are likely to be different .\n", 2=>"-If two persons appear in video programs broadcast by different broadcast stations ( e .g . , CNN , MSNBC , and CCTV ) , they are likely different .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25,26,27#23,24,25,26#para -1#28#27,28#para -1#29#29#exact \n"}nil_first --> [13]
nil_second --> [13, 14, 15]
--------------------------
{1=>"If we have large video archives , we can collect a sufficient number of training samples to learn the relevance classifier by using these heuristics .\n", 2=>"If we have large video archives , using these heuristics we can collect a sufficient number of training samples for learning the relevance classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#20#16,17#para -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#7#21,22#para -1#8#23#exact -1#9#24#exact -1#24#25#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"We form face set Generating \\MATH by picking a subset of faces of Generating \\MATH and randomly adding faces from other sets Generating \\MATH .\n", 2=>"We form a face set Generating \\MATH by picking a subset of faces of Generating \\MATH and adding randomly faces from other sets Generating \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#18#16#exact -1#17#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"To keep satisfying the assumption of visual consistency , the number of faces selected in each set Generating \\MATH must be smaller than the number of faces in set Generating \\MATH .\n", 2=>"To keep the assumption of visual consistency satisfied , the number of faces selected in each set Generating \\MATH must be smaller than the number of faces in set Generating \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#8#exact -1#22,23,24,25#9,10,11#para -1#26#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#9,10,11#22,23,24,25#para -1#12#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [2]
nil_second --> [7]
--------------------------
{1=>"Once the training samples are collected , we use SVM with a linear kernel to learn the relevance classifier .\n", 2=>"Once the training samples are collected , we use SVM with linear kernel to learn the relevance classifier .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"TRECVID dataset : We collected all video programs from the TRECVID 2006 dataset \\CITE .\n", 2=>"TRECVID dataset : We collected all video programs of TRECVID 2006 dataset \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [8, 9]
nil_second --> [8]
--------------------------
{1=>"There were 527 video programs broadcast on seven channels in three languages including English , Chinese , and Arabic .\n", 2=>"There are 527 video programs broadcast on 7 channels in 3 languages including English , Chinese and Arabic .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#syn -1#8#8#exact -1#9#9#exact -1#10#10#syn -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"We extracted faces from these video programs and grouped faces belonging to one person in each shot in one face track using a similar method to that described in \\CITE .\n", 2=>"We extracted faces from these video programs and grouped faces belonging to one person in each shot in one face track using a similar method described in \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [25, 26]
nil_second --> []
--------------------------
{1=>"We scanned all face tracks for each channel extracted from the videos broadcast by this channel , and picked face tracks extracted from key frames where several faces were detected at different locations .\n", 2=>"For each channel , We scanned all face tracks extracted from the videos broadcast by this channel , and picked face tracks extracted from keyframes that several faces were detected at different locations .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#0#5#lc -1#1#6#exact -1#2#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact \n"}nil_first --> [23, 24, 25]
nil_second --> [3, 24, 25]
--------------------------
{1=>"To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .\n", 2=>"To guarantee selected face tracks representing different persons , for one channel , only face tracks of one shot was picked .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#para -1#8#8#exact -1#13#9#exact -1#14#10,11#para -1#15#12#exact -1#10#14#exact -1#18#15#exact -1#19#16,17#para -1#9#18#exact -1#17#19#exact -1#11#20#exact -1#21#21#exact \n"}nil_first --> [13]
nil_second --> [12, 16, 20]
--------------------------
{1=>"Note that the system did not know the identity of these faces .\n", 2=>"Note that , the system does not know the identity of these faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5,6,7#4,5,6#para -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"The number of faces in these face tracks is shown in Figure \\REF .\n", 2=>"The number of faces of these face tracks is shown in Figure \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#10#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [10]
nil_second --> [4]
--------------------------
{1=>"We generated the 133 labeled sets described in Section \\REF using these face tracks and used them to train the relevance classifier .\n", 2=>"Using these face tracks , We generated 133 labeled sets described in Section \\REF and used them for training the relevance classifier .\n", 3=>"-1#5#0#exact -1#6#1#exact -1#19#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#0#10#lc -1#1#11#exact -1#2#12#exact -1#3#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17,18#17,18#para -1#20#19,20#para -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [4]
--------------------------
{1=>"Using people�fs names as queries , we applied a simple string search to the captions in this dataset to return a list of faces for each queried name .\n", 2=>"Using person names as queries , we applied simple string search to the captions this dataset to return a list of faces for each queried name .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#18#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#19#20,21#para -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact \n"}nil_first --> [1, 15]
nil_second --> [1]
--------------------------
{1=>"These names have widely been used in experiments \\CITE .\n", 2=>"These names are widely used in experiments such as \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#para -1#3#3#exact -1#4,5#4,5,6#para -1#6#7#exact -1#9#8#exact -1#10#9#exact \n"}nil_first --> []
nil_second --> [7, 8]
--------------------------
{1=>"A total of 9 ,136 faces were retrieved in which 3 ,909 faces were relevant .\n", 2=>"In total , 9 ,136 faces were retrieved in which 3 ,909 faces were relevant .\n", 3=>"-1#0,1#0,1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"The accuracy was \\MATH on average .\n", 2=>"On average , The accuracy was \\MATH .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#0#4#lc -1#1#5#exact -1#7#6#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"Google Images : We used the same set of people�fs names used in the Yahoo News Images dataset and input them into the Google Image Search Engine .\n", 2=>"Google Images : We used the same set of person names used in Yahoo News Images dataset and put to Google Image Search Engine .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact \n"}nil_first --> [9, 13, 19, 20, 21, 22]
nil_second --> [9, 18, 19]
--------------------------
{1=>"We crawled a maximum of 500 images from URLs returned by Google for each query .\n", 2=>"For each query , We crawled a maximum of 500 images from URLs returned by Google .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#0#12#lc -1#1#13#exact -1#2#14#exact -1#16#15#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"A total of 9 ,516 faces were extracted in which 5 ,816 faces were relevant .\n", 2=>"In total , 9 ,516 faces were extracted in which 5 ,816 faces were relevant .\n", 3=>"-1#0,1#0,1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"The accuracy was \\MATH on average .\n", 2=>"On average , The accuracy was \\MATH .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#0#4#lc -1#1#5#exact -1#7#6#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"The datasets for Yahoo News Images and Google Images , as shown in Figure \\REF , were used for testing .\n", 2=>"The datasets , Yahoo News Images and Google Images as shown in Figure \\REF , were used for testing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#17#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#2#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [18]
nil_second --> []
--------------------------
{1=>"We simply used a similar technique to that described in \\CITE to group faces belonging to one person in one video shot .\n", 2=>"To group faces belonging to one person in one video shot , We simply used a similar technique described in \\CITE .\n", 3=>"-1#12#0#exact -1#13#1#exact -1#14#2#exact -1#15#3#exact -1#16#4#exact -1#17#5#exact -1#4#6#exact -1#18#8#exact -1#19#9#exact -1#20#10#exact -1#0#11#lc -1#1#12#exact -1#2#13#exact -1#3#14#exact -1#5#16#exact -1#6#17#exact -1#7#18#exact -1#8#19#exact -1#9#20#exact -1#10#21#exact -1#21#22#exact \n"}nil_first --> [7, 15]
nil_second --> [11]
--------------------------
{1=>"Using prior knowledge that faces of the same person in consecutive frames do not change much in locations and appearance , the technique used tracked points to robustly associate these faces in face tracks with a precision of \\MATH .\n", 2=>"Using the prior knowledge that faces of the same person in consecutive frames do not change much in locations and appearance , the technique used tracked points to robustly associate these faces into face tracks with the precision of \\MATH .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact \n"}nil_first --> [31, 35]
nil_second --> [1, 32, 36]
--------------------------
{1=>"Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .\n", 2=>"Specifically , for each face , 9 facial feature points were detected , and 4 more facial feature points were inferred from these 9 points .\n", 3=>"-1#9#2#exact -1#10#3#exact -1#0#4#lc -1#11#5#exact -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#13#10#exact -1#14#11#syn -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23,24#20,21#para -1#25#22#exact \n"}nil_first --> [0, 1]
nil_second --> [1, 6, 7, 8, 12]
--------------------------
{1=>"There were a total of 13 feature points from which features were extracted .\n", 2=>"In total , There were 13 feature points from which features are extracted .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#0,1#2,3#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#syn -1#12#12#exact -1#13#13#exact \n"}nil_first --> [4]
nil_second --> [2]
--------------------------
{1=>"The features were intensity values lying within a circle with a radius of 15 pixels .\n", 2=>"The features are intensity values lying within the circle with radius of 15 pixels .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#syn -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Figure \\REF illustrates this feature .\n", 2=>"Figure \\REF shows illustration of this feature .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#para -1#5#3#exact -1#6#4#exact -1#7#5#exact \n"}nil_first --> []
nil_second --> [3, 4]
--------------------------
{1=>"We evaluated the efficiency of retrieval with measures that are commonly used in information retrieval , such as precision , recall , and average precision .\n", 2=>"We evaluated the retrieval performance with measures that are commonly used in information retrieval , such as precision , recall , and average precision .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [3, 4]
nil_second --> [4]
--------------------------
{1=>"Given a queried person and letting \\MATH be the total number of faces returned , \\MATH the number of relevant faces , and \\MATH the total number of relevant faces , recall and precision can be calculated as : \\MATH .\n", 2=>"Given a queried person and letting \\MATH be the total number of faces returned , \\MATH the number of relevant faces , and \\MATH the total number of relevant faces , recall and precision can be calculated as follows : \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10,11#8,9,10#para -1#18#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#24,25,26,27#16,17,18#para -1#28#19#exact -1#29#20#exact -1#30#21#exact -1#22#22#exact -1#23#23#exact -1#16#24#exact -1#17#25,26#para -1#19#28#exact -1#20#29#exact -1#21#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact \n"}nil_first --> [27]
nil_second --> [38]
--------------------------
{1=>"Precision and recall were only used to evaluate the quality of an unordered set of retrieved faces .\n", 2=>"Precision and recall only evaluate the quality of an unordered set of retrieved faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4,5#6,7,8#para -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact \n"}nil_first --> [3, 5]
nil_second --> []
--------------------------
{1=>"Average precision is usually used to evaluate ranked lists in which both recall and precision are taken into account .\n", 2=>"To evaluate ranked lists in which both recall and precision are taken into account , the average precision is usually used .\n", 3=>"-1#16#0#lc -1#17#1#exact -1#18#2#exact -1#19#3#exact -1#20#4#exact -1#0#5#lc -1#1#6#exact -1#2#7#exact -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6#11#exact -1#7#12#exact -1#8#13#exact -1#9#14#exact -1#10#15#exact -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#21#19#exact \n"}nil_first --> []
nil_second --> [14, 15]
--------------------------
{1=>"The average precision is computed by taking the average of the interpolated precision measured at 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .\n", 2=>"The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6,7,8#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> []
nil_second --> [5, 15]
--------------------------
{1=>"The interpolated precision , \\MATH , at a certain recall level , \\MATH , is defined as the highest precision found for any recall level \\MATH :\n", 2=>"The interpolated precision \\MATH at a certain recall level \\MATH is defined as the highest precision found for any recall level \\MATH :\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#12#exact -1#10#14#exact -1#11#15#exact -1#12#16#exact -1#13#17#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact \n"}nil_first --> [3, 5, 11, 13]
nil_second --> []
--------------------------
{1=>"In addition , we used the mean average precision to evaluate the performance of multiple queries , which is the mean of average precisions computed from queries .\n", 2=>"In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#11#3#exact -1#12#4#exact -1#19#5#exact -1#20#6#exact -1#14#7#exact -1#15#8#exact -1#3#9#exact -1#4#10#exact -1#5#11#exact -1#6#12#exact -1#7#13#exact -1#8#14#exact -1#9#15#exact -1#10#16#exact -1#17#17#exact -1#18#18#exact -1#13#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [19]
nil_second --> [16]
--------------------------
{1=>"We compared the performance of the Maximum A-Posteriori ( MAP ) algorithm in seven systems in this experiment by testing it on YahooNews Images :\n", 2=>"In this experiment , We compare the MAP performance of the following systems testing on YahooNews Images :\n", 3=>"-1#4#0#exact -1#5#1#stem -1#6#2#exact -1#8#3#exact -1#9#4#exact -1#10#5#exact -1#7#9#exact -1#0#12#lc -1#12#14#exact -1#1#16#exact -1#2#17#exact -1#13#19#exact -1#14#21#exact -1#15#22#exact -1#16#23#exact -1#17#24#exact \n"}nil_first --> [6, 7, 8, 10, 11, 13, 15, 18, 20]
nil_second --> [3, 11]
--------------------------
{1=>"-DistScore-TrainGoogleImages : The training set was the set of annotated faces returned by Google Images Search for 23 people�fs names .\n", 2=>"-DistScore-TrainGoogleImages : The training set is the set of annotated faces returned by Google Images Search for 23 person names .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [18]
nil_second --> [18]
--------------------------
{1=>"The training set was the set of annotated faces artificially generated with our method described in Section \\REF .\n", 2=>"The training set is the set of annotated faces artificially generated by our method described in Section \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [11]
nil_second --> [11]
--------------------------
{1=>"We re-implemented the method proposed by Krapac et al. \\CITE of extracting the query-dependent feature .\n", 2=>"We re-implemented the method proposed by Krapac et al . \\CITE for extracting query-dependent feature .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#exact -1#12#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [8, 10, 12]
nil_second --> [8, 9, 11]
--------------------------
{1=>"Since this method was proposed to handle images , not faces , we modified it to handle faces .\n", 2=>"Since this method was proposed to handle images , not for faces , we modified it for handling faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16,17#15,16#para -1#18#17#exact -1#19#18#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"Each face was specifically represented as a bag of visual words .\n", 2=>"Specifically , Each face is represented as a bag of visual words .\n", 3=>"-1#2#0#exact -1#3#1#exact -1#4#2#syn -1#0#3#lc -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"We used 13 facial-feature points detected in each face and their descriptors using pixel intensity as visual words .\n", 2=>"We used 13 facial feature points detected in each face and their descriptors using pixel intensity as visual words .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> [3]
nil_second --> [3, 4]
--------------------------
{1=>"The top-$k$ visual words strongly related to the returned faces of each query and the binary feature vector were computed as described in \\CITE .\n", 2=>"top-$k$ visual words strongly related to the returned faces of each query and the binary feature vector are computed as described in \\CITE .\n", 3=>"-1#6#0#lc -1#0#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#13#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#syn -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"-Mensink[15]-GaussianModels : This method proposed by Mensink et al. \\CITE modeled the returned faces by using two Gaussians , the first for the faces relevant to the target person and the second for the remaining faces .\n", 2=>"-Mensink[15]-GaussianModels : This method proposed by Mensink et al . \\CITE models the returned faces by using two Gaussians , one for the faces relevant to the target person and one for the remaining faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#exact -1#11#10#stem -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#22#19#exact -1#31#21#exact -1#32#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#21#32#exact -1#33#33,34#para -1#34#35#exact -1#35#36#exact \n"}nil_first --> [8, 20, 30, 31]
nil_second --> [8, 9, 20, 30]
--------------------------
{1=>"-Mensink[15]-Friends : This method proposed by Mensink et al. \\CITE used linear discriminant analysis to train a specific classifier for each query .\n", 2=>"-Mensink[15]-Friends : This method proposed by Mensink et al . \\CITE uses linear discriminant analysis to train a specific classifier for each query .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#exact -1#11#10#stem -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> [8]
nil_second --> [8, 9]
--------------------------
{1=>"This method used detected people�fs names in captions associated with faces for query expansion to model faces of the target person 's friends .\n", 2=>"This method uses detected person names in captions associated with faces for query expansion to model faces of the target person 's friends .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are state-of-the-art that learn a specific classifier for each query .\n", 2=>"The Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are the state of the art methods that learn a specific classifier for each query .\n", 3=>"-1#1#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#14#8#exact -1#15#9#exact -1#16#10#exact -1#17#11#exact -1#18#12#exact -1#19#13#exact -1#20#14#exact -1#21#15#exact -1#22#16#exact \n"}nil_first --> [7]
nil_second --> [0, 8, 9, 10, 11, 12, 13]
--------------------------
{1=>"Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then used for new queries .\n", 2=>"The method Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then is used for new queries .\n", 3=>"-1#2#0#exact -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19,20,21#17,18#para -1#22#19#exact -1#23#20#exact -1#24#21#exact \n"}nil_first --> []
nil_second --> [0, 1]
--------------------------
{1=>"Figure \\REF compares the performance of these systems when they were tested on the YahooNews Images dataset .\n", 2=>"Figure \\REF shows the performance comparison of these systems when testing on YahooNews Images dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#11#stem -1#11#12#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact \n"}nil_first --> [2, 9, 10, 13]
nil_second --> [2, 5]
--------------------------
{1=>"The curves plot the correlation between performance and the number of features for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID .\n", 2=>"As for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID , the curves show the correlation between the performance and the number of features .\n", 3=>"-1#14#0#lc -1#15#1#exact -1#17#3#exact -1#18#4#exact -1#19#5#exact -1#21#6#exact -1#22#7#exact -1#23#8#exact -1#24#9#exact -1#25#10#exact -1#26#11#exact -1#1#12#exact -1#2#13#exact -1#3#14#exact -1#4#15#exact -1#5#16#exact -1#6#17#exact -1#7#18#exact -1#8#19#exact -1#9#20#exact -1#10#21#exact -1#11#22#exact -1#12#23#exact -1#27#24#exact \n"}nil_first --> [2]
nil_second --> [0, 13, 16, 20]
--------------------------
{1=>"-DistScore performed significantly better than NNScore .\n", 2=>"-DistScore is significantly better than that of NNScore .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#5#exact -1#8#6#exact \n"}nil_first --> [1]
nil_second --> [1, 5, 6]
--------------------------
{1=>"-The performance of the system using training data generated artificially with our method was comparable to that of the system using training data returned by search engines .\n", 2=>"-The performance of the system using the training data generated artificially by our method is comparable with that of the system using the training data returned by search engines .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#16#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#syn -1#15#14#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact \n"}nil_first --> [15]
nil_second --> [6, 11, 22]
--------------------------
{1=>"-The method of DistScore-TrainTRECVID we propose performed comparably to the state-of-the-art method in the specific classifier-based approach of Mensink[15]-Friends .\n", 2=>"-Our proposed method DistScore-TrainTRECVID has comparable performance to the state of the art method in specific classifier-based approach Mensink[15]-Friends .\n", 3=>"-1#2#1#exact -1#10#2#exact -1#3#3#exact -1#1#4,5#para -1#4#6#para -1#5#7#stem -1#7#8#exact -1#8#9#exact -1#13#11#exact -1#14#12#exact -1#11#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [0, 10, 17]
nil_second --> [0, 6, 9, 12]
--------------------------
{1=>"It outperformed the method where only visual information was used , i.e. , Mensink[15]-GaussianModels .\n", 2=>"It outperforms the method using only visual information Mensink[15]-GaussianModels .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#4#9#stem -1#8#13#exact -1#9#14#exact \n"}nil_first --> [4, 8, 10, 11, 12]
nil_second --> []
--------------------------
{1=>"-Our proposed method DistScore-TrainTRECVID outperformed the method proposed by Krapac et al. , which was customized to handle faces .\n", 2=>"-Our proposed method DistScore-TrainTRECVID outperforms the method proposed by Krapac et al . customized for handling faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#stem -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#15#exact -1#14,15#16,17#para -1#16#18#exact -1#17#19#exact \n"}nil_first --> [11, 12, 13, 14]
nil_second --> [11, 12]
--------------------------
{1=>"As seen in Figure \\REF , DistScore-TrainTRECVID outperformed the original ranking of the Google Images Search Engine if from 20 to 50 features were used .\n", 2=>"As shown in Figure \\REF , DistScore-TrainTRECVID outperforms original ranking of Google Images Search Engine if using from 20 to 50 features .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#8#8,9#para -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#16#24#stem -1#22#25#exact \n"}nil_first --> [1, 12, 23]
nil_second --> [1]
--------------------------
{1=>"The results for DistScore-TrainTRECVID on the YahooNews Images set and Google Images set indicate that the relevance classifier with our proposed method was able to generalize well on different queries and was independent of underlying ranking algorithms used in search engines .\n", 2=>"The result of DistScore-TrainTRECVID on YahooNews Images set and Google Images set indicates that the relevance classifier of our proposed method is able to generalize well on different queries and independent with underlying ranking algorithms of search engines .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#3#3#exact -1#4#4#exact -1#14#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12,13#13,14#para -1#15#15,16#para -1#16#17#exact -1#31#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#syn -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#32#exact -1#2#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact \n"}nil_first --> [2, 31, 37, 38]
nil_second --> [17, 35]
--------------------------
{1=>"Figure \\REF shows an example of re-ranking results for the top-30 faces for the query John Paul , which is one of the most difficult cases in the YahooNews Images set .\n", 2=>"Figure \\REF shows an example of re-ranking result of top-30 faces for the query John Paul that is one of the most difficult cases of the YahooNews Images set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#stem -1#11#8#exact -1#12#9#exact -1#9#10#exact -1#10#11#exact -1#20#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16,17,18,19#18,19,20,21#para -1#25#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact \n"}nil_first --> [12, 17, 26, 27]
nil_second --> [8, 24]
--------------------------
{1=>"The results clearly demonstrate that our proposed method outperformed the other state-of-the-art methods .\n", 2=>"The result clearly shows that our proposed method outperforms the other state of the art methods .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#15#12#exact -1#16#13#exact \n"}nil_first --> [11]
nil_second --> [11, 12, 13, 14]
--------------------------
{1=>"Our query-dependent feature was based on the nearest neighbors of images in the returned image set that usually have a complexity of \\MATH , where \\MATH is the total number of images in the set .\n", 2=>"Our query-dependent feature is based on nearest neighbors of the images in the returned image set that usually have complexity of \\MATH , where \\MATH is the total number of images in the set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#9#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26,27,28,29#27,28,29#para -1#30#30,31#para -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"However , recent studies on indexing techniques such as \\MATH -d tree , locality sensitive hashing ( LSH ) , and a Self Adaptive Set of Histograms ( SASH ) \\CITE could significantly speed up the nearest neighbor search .\n", 2=>"However , recent studies on indexing techniques such as \\MATH -d tree , locality sensitive hashing ( LSH ) , and SASH \\CITE can speed up the nearest neighbor search significantly .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#28#exact -1#22#30#exact -1#23#31#para -1#30#32#exact -1#24#33#exact -1#25#34#exact -1#26#35#exact -1#27#36#exact -1#28#37#exact -1#29#38#exact -1#31#39#exact \n"}nil_first --> [21, 22, 23, 24, 25, 26, 27, 29]
nil_second --> []
--------------------------
{1=>"For example , the complexity of the fast lookup of $k$ approximate nearest neighbors is \\MATH \\CITE .\n", 2=>"For example , as described in \\CITE , the complexity of fast lookup of $k$ approximate nearest neighbors is \\MATH \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#9,10#3,4,5,6#para -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact \n"}nil_first --> []
nil_second --> [3, 4, 5, 6, 7, 8]
--------------------------
{1=>"Studying other techniques to speed up the process of query-feature extraction is our next step in future work .\n", 2=>"Studying other techniques to speedup the query-feature extraction process is our next step in future work .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#8#5,6,7,8#para -1#6#9#exact -1#7#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact \n"}nil_first --> [4]
nil_second --> [4, 5]
--------------------------
{1=>"Instead of training a specific classifier for each new query , we only trained one generic classifier and used it for ranking new queries .\n", 2=>"Instead of training a specific classifier for each new query , we train only one generic classifier and use it for ranking new queries .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#12#exact -1#12#13#stem -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18,19#para -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> []
nil_second --> [19]
--------------------------
{1=>"This helped make the ranking application more scalable .\n", 2=>"This helps to make the ranking application more scalable .\n", 3=>"-1#0#0#exact -1#1#1#stem -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"We propose a simple unsupervised method to train the generic classifier to obtain a large number of labeled faces from video archives .\n", 2=>"To train the generic classifier , We propose a simple unsupervised method to obtain a large number of labeled faces from video archives .\n", 3=>"-1#6#0#exact -1#7#1#exact -1#8#2#exact -1#9#3#exact -1#10#4#exact -1#11#5#exact -1#12#6#exact -1#1#7#exact -1#2#8#exact -1#3#9#exact -1#4#10#exact -1#0#11#lc -1#13#12#exact -1#15,16,17#13,14,15,16#para -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> []
nil_second --> [5, 14]
--------------------------
{1=>"Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .\n", 2=>"Experiments shown that although our method is unsupervised and independent with underlying algorithms of existing search engines but successfully learns visual consistency among returned faces to boosts retrieval performance .\n", 3=>"-1#0#0#exact -1#1,2#1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#13#10#exact -1#11#11#exact -1#12#12#exact -1#14#13,14#para -1#15#15#exact -1#16#16#exact -1#18#19#exact -1#19#20#stem -1#20#21#exact -1#21#22#exact -1#22#23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#stem -1#27#29,30#para -1#29#31#exact \n"}nil_first --> [17, 18, 28]
nil_second --> [10, 17, 28]
--------------------------
{1=>"We present a method to enhance the performance of a mathematical search system .\n", 2=>"We present a method to enhance the performance of a mathematical search system in this paper .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#16#13#exact \n"}nil_first --> []
nil_second --> [13, 14, 15]
--------------------------
{1=>"By targeting mathematical formulas that appear in natural language documents , we collect the names of formulas from the surrounding text and incorporate the correspondence into the search system 's database .\n", 2=>"Targeting to mathematical formulas that appear in natural language documents , we collect the names of formulas from the surrounding text , and incorpo-rate the correspondence to the search system 's database .\n", 3=>"-1#0#1#lc -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#21#exact -1#24#23#exact -1#25#24#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact \n"}nil_first --> [0, 22, 25]
nil_second --> [1, 21, 23, 26]
--------------------------
{1=>"The effectiveness of the approach is demonstrated through experiments using Wikipedia mathematical articles and Wolfram Functions Site data sets .\n", 2=>"E ectiveness of the proposed approach is shown through experiments using Wikipedia mathematical articles and Wolfram Functions Site data sets .\n", 3=>"-1#2#2#exact -1#3#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#syn -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 1, 4]
--------------------------
{1=>"The mathematical content being published on the Web is increasing day by day , and retrieving mathematical content has become an important issue for many users .\n", 2=>"In the current digital environment , the mathematical content being published on the Web is increasing day by day . While more and more mathematical contents being available on the Web , retrieving mathematical contents becomes an important issue for many users .\n", 3=>"-1#6#0#lc -1#7#1#exact -1#8#2#exact -1#9#3#exact -1#10#4#exact -1#11#5#exact -1#12#6#exact -1#13#7#exact -1#14#8#exact -1#15#9#exact -1#16#10#exact -1#17#11#exact -1#18#12#exact -1#5#13#exact -1#22#14#exact -1#32#15#exact -1#33#16#exact -1#34#17,18#para -1#35#19#stem -1#36#20#exact -1#37#21#exact -1#38#22#exact -1#39#23#exact -1#40#24#exact -1#41#25#exact -1#42#26#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3, 4, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31]
--------------------------
{1=>"Teachers , students , and researchers need better access to mathematical resources for teaching , studying , and obtaining information for research and development .\n", 2=>"Teach-ers , students , researchers do need to gain access to mathematical resources for teaching , studying , or obtaining updated information for research and development .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#24#4#exact -1#4#5#exact -1#6#6#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#19#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> [0, 7, 17, 22]
nil_second --> [0, 5, 7, 8, 18, 20]
--------------------------
{1=>"Moreover , users need specialized search systems to find formulas that are relevant to their needs .\n", 2=>"Therefore , users need specialized search systems to nd the formula that is relevant to their requirements .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#stem -1#11#10#exact -1#12,13#11,12#para -1#14#13#exact -1#15#14#exact -1#16#15#para -1#17#16#exact \n"}nil_first --> [0, 8]
nil_second --> [0, 8, 9]
--------------------------
{1=>"Internet search engines can detect particular keywords in mathematical formulas but they mostly fail at recognizing mathematical symbols and constructs such as integral and square root symbols , fractions , and matrices .\n", 2=>"Internet search engines are able to detect some particular keywords in mathematical formula but they mostly fail to recognize mathematical symbols and constructs such as integral sym-bols , square root symbols , fractions , or matrices .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4,5#3#para -1#6#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#stem -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#18#15#stem -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#35#31#exact -1#36#32#exact \n"}nil_first --> [14, 23, 30]
nil_second --> [7, 17, 26, 27, 34]
--------------------------
{1=>"There are some mathematically oriented search engines on the Internet .\n", 2=>"There exist some mathematical-dedicated search engines available on the Internet .\n", 3=>"-1#0#0#exact -1#1#1#syn -1#2#2#exact -1#4#5#exact -1#5#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact \n"}nil_first --> [3, 4]
nil_second --> [3, 6]
--------------------------
{1=>"Furthermore , these systems do not take into account the semantics of mathematical formulas as revealed by the surrounding natural language text , e.g. , the formula�fs name or the description of its variables .\n", 2=>"Furthermore , these systems do not take into account the semantics of mathematical formulas revealed by surrounding natural language text , like the name of the formula and its variables' descrip-tion .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5,6,7,8,9#4,5,6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#22#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#25#25#exact -1#23#27#exact -1#4#28#para -1#24#31#exact -1#28#32#exact -1#29#33#stem -1#31#34#exact \n"}nil_first --> [14, 23, 24, 26, 29, 30]
nil_second --> [21, 26, 27, 30]
--------------------------
{1=>"But even this site does not provide a full mathematical search .\n", 2=>"But full mathematical search is still not available .\n", 3=>"-1#0#0#exact -1#6#5#exact -1#1#8#exact -1#2#9#exact -1#3#10#exact -1#8#11#exact \n"}nil_first --> [1, 2, 3, 4, 6, 7]
nil_second --> [4, 5, 7]
--------------------------
{1=>"These systems , however , provide neither similarity structures nor semantic meanings of their formulas .\n", 2=>"These systems , however , provide neither similarity structures nor semantic meanings of the formulas .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [13]
nil_second --> [13]
--------------------------
{1=>"The Wolfram Functions Site [7] contains a large number of mathematical formulas and also provides a semantic search for them .\n", 2=>"The Wolfram Functions Site [7] contains large mathe-matical formulas and also provides a semantics search for mathematical formulas .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#12#6#exact -1#6#7#exact -1#16#10#exact -1#17#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#13#16#stem -1#14#17#exact -1#15#18#exact -1#18#20#exact \n"}nil_first --> [8, 9, 15, 19]
nil_second --> [7, 8]
--------------------------
{1=>"This site and some recent work done by Adeel et al. [2] and Yokoi and Aizawa [1] employ similarity search methods based on MathML but they do not make use of the semantics of the formulas' surrounding text . //[ ? ? propose is unclear in the sense of a website .]\n", 2=>"This site and some recent works done by Adeel et al. [2] and Yokoi and Aizawa [1] propose similarity search methods based on MathML but these works do not make use of the semantics of the formulas' surrounding text , which is considered to be important information sources .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#30,31#17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#27#25,26#para -1#28#27#exact -1#29#28#exact -1#34#30#exact -1#35#31#exact -1#33#32#exact -1#32#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#48#38#exact -1#17#42#exact -1#41#43#exact -1#40#47,48,49#para \n"}nil_first --> [29, 33, 39, 40, 41, 44, 45, 46, 50, 51]
nil_second --> [25, 26, 39, 42, 43, 44, 45, 46, 47]
--------------------------
{1=>"We describe our work toward creating a mathematical database that contains formulas , their names , variable descriptions , and other related information .\n", 2=>"We describe here in detail our work toward creating a mathematical database that contains for-mulas , their names , their variables' descriptions and other related information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#20#16#stem -1#21#17#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact \n"}nil_first --> [11, 18]
nil_second --> [2, 3, 4, 14, 19]
--------------------------
{1=>"We implemented a mathematical search system that uses this information as its base knowledge .\n", 2=>"We also implement a mathematical search system that use this information as its base knowledge .\n", 3=>"-1#0#0#exact -1#2#1#stem -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> []
nil_second --> [1]
--------------------------
{1=>"This information is very helpful when performing mathematical search by reducing the need for formula input and solving the notational variation problem where mathematically equivalent formulas follow different notations .\n", 2=>"This information is very helpful when performing mathematical search by reducing the need for formula input and solving the notational variation problem where mathematically equivalent formulas follow di erent notations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#29#28#exact -1#30#29#exact \n"}nil_first --> [27]
nil_second --> [27, 28]
--------------------------
{1=>"It also provides opportunities to make mathematics better understandable and usable for people with disabilities .\n", 2=>"It also provides opportunities to make mathematical better understandable and usable for di erent groups of people with disabilities .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact \n"}nil_first --> []
nil_second --> [12, 13, 14, 15]
--------------------------
{1=>"The remainder of this paper is organized as follows : we present an overview of our framework in section 2 .\n", 2=>"The remainder of this paper is organized as follow : In section 2 , we present an overview of the proposed framework .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#21#15,16#para -1#10#17#lc -1#11#18#exact -1#12#19#exact -1#22#20#exact \n"}nil_first --> []
nil_second --> [13, 19, 20]
--------------------------
{1=>"We then describe the results of our experiments in section 3 .\n", 2=>"We then describe the results of our experiments in section 3 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4,5,6#3,4,5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"Mathematical formulas on the Web have many different formats , e.g. , LaTeX and Mathematical Markup Language ( MathML ) [6] .\n", 2=>"Mathematical formulas on the Web has many di erent formats , some of them are LaTeX , and the Mathematical Markup Language ( MathML ) [6] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#9#8#exact -1#10#9#exact -1#16#11#exact -1#15#12#exact -1#17#13#exact -1#19#14#exact -1#20#15#exact -1#21#16#exact -1#22#17#exact -1#23#18#exact -1#24#19#exact -1#25#20#exact -1#26#21#exact \n"}nil_first --> [7, 10]
nil_second --> [7, 8, 11, 12, 13, 14, 18]
--------------------------
{1=>"This diversity makes searches more difficult .\n", 2=>"This makes the search more dif-cult .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#3#3#stem -1#4#4#exact -1#6#6#exact \n"}nil_first --> [1, 5]
nil_second --> [2, 5]
--------------------------
{1=>"In this paper , we shall use the MathML format for mathematical formulas .\n", 2=>"In this paper , we use the presentation MathML format for mathematical formulas .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [5]
nil_second --> [7]
--------------------------
{1=>"Formulas with other formats can be easily converted to MathML format by using freely available tools .\n", 2=>"Formulas with other formats can be easily converted to MathML format using existing freely available tools .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"For our work , we used LaTeXML Converter , which is freely available at \\URL .\n", 2=>"For our works , we use LaTeXML Converter which is freely available at \\URL .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"Figure 1 shows a page from a mathematical section on Wikipedia and the information we retrieved on this site , besides the mathematical formulas .\n", 2=>"Figure 1 shows a page on mathematical section on Wikipedia and the information we retrieved on this site besides the mathematical formulas .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [5, 19]
nil_second --> [5]
--------------------------
{1=>"We used heuristics to ensure adequate matching of mathematical formulas with their names .\n", 2=>"At this point , we use some heuristics to provide an adequate solution for matching mathematical formulas with their names .\n", 3=>"-1#4#0#lc -1#5,6#1#para -1#7#2#exact -1#8#3#exact -1#11#5#exact -1#14#6#exact -1#15#8#exact -1#16#9#exact -1#17#10#exact -1#18#11#exact -1#19#12#exact -1#20#13#exact \n"}nil_first --> [4, 7]
nil_second --> [0, 1, 2, 3, 9, 10, 12, 13]
--------------------------
{1=>"After collecting mathematical formulas from these resources , we extracted keywords for indexing .\n", 2=>"After collecting the mathematical formulas from these resources , we extract keywords for indexing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#stem -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"Our system allows two ways of searching : text content and formula content .\n", 2=>"Our system allows two ways of searching : text content search and formula content search .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#15#13#exact \n"}nil_first --> []
nil_second --> [10, 14]
--------------------------
{1=>"In a text content search , users search with extracted keywords , e.g. , \" sin \" , \" Pythagorean \" or \" trigonometric functions \" .\n", 2=>"In the rst case , users can use the extracted keywords for search , for example : \" sin \" , \" Pythagorean \" or \" trigonometric functions \" .\n", 3=>"-1#0#0#exact -1#12#4#exact -1#4#5#exact -1#5#6#exact -1#9#9#exact -1#10#10#exact -1#13#11#exact -1#20#13#exact -1#21#14#exact -1#18#15#exact -1#19#16#exact -1#17#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact \n"}nil_first --> [1, 2, 3, 7, 8, 12, 17]
nil_second --> [1, 2, 3, 6, 7, 8, 11, 14, 15, 16]
--------------------------
{1=>"In a formula content search , users directly input the formulas , for example : \\MATH .\n", 2=>"In the second case , users can input the mathematical formulas directly , for example : \\MATH .\n", 3=>"-1#0#0#exact -1#4#5#exact -1#5#6#exact -1#11#7#exact -1#7#8#exact -1#8#9#exact -1#10#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> [1, 2, 3, 4]
nil_second --> [1, 2, 3, 6, 9]
--------------------------
{1=>"If found , it will return other information related to that formula .\n", 2=>"If found , it will return other information related with that formula .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"If nothing matching is found , it looks for mathematical formulas which are similar to the input ( including formulas with a similar structure ) .\n", 2=>"Else , it just looks for mathematical formulas which are similar to the input ( including formulas with similar structure ) .\n", 3=>"-1#1#5#exact -1#2#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8,9,10,11#11,12,13#para -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21,22#para -1#19#23#exact -1#20#24#exact -1#21#25#exact \n"}nil_first --> [0, 1, 2, 3, 4, 14]
nil_second --> [0, 3]
--------------------------
{1=>"Evaluating a mathematical search system is not an easy task because we do not have any standard for this task .\n", 2=>"Evaluate a mathematical search system is not an easy task because we do not have any standard for this task .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> []
nil_second --> [12, 13]
--------------------------
{1=>"We consider that formulas with the same semantic meaning are relevant . //[The original is unclear the rewrite seems to be what you mean .]\n", 2=>"In our work , we manually consider formulas with the same semantic meaning are relevant .\n", 3=>"-1#4#0#lc -1#6#1#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact \n"}nil_first --> [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
nil_second --> [0, 1, 2, 3, 5]
--------------------------
{1=>"For example , while searching for sin( a ) , we also consider results containing arcsin or cosin .\n", 2=>"For example , while searching for sin( a ) , we also consider the results containing arcsin or cosin .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> []
nil_second --> [13]
--------------------------
{1=>"To show the effect of linking the formula with its name , we also set up an experimental search system without using the formula 's names .\n", 2=>"In order to show the e ect of linking the formula with its name , we also set up an experimental search system without using the formula 's names .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact \n"}nil_first --> [3]
nil_second --> [0, 1, 5, 6]
--------------------------
{1=>"Table 1 shows the top 5 search results for the query \" sin( a + b ) \" .\n", 2=>"Table 1 shows top 5 of the searching results for the query \\MATH .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#3#4#exact -1#4#5#exact -1#7#6#stem -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#13#18#exact \n"}nil_first --> [11, 12, 13, 14, 15, 16, 17]
nil_second --> [5, 12]
--------------------------
{1=>"As can be seen , when the system associates the formulas with their names , it can provide more useful information to the user .\n", 2=>"As can be seen from the table , when the system associates the formulas with their names , it can provide more useful information to the user .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact \n"}nil_first --> []
nil_second --> [4, 5, 6]
--------------------------
{1=>"Table 2 shows the top 10 results for the query \" Pythagorean \" .\n", 2=>"Table 2 shows top 10 results with the query \" Pythagorean \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#7#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [7, 8]
nil_second --> [6]
--------------------------
{1=>"We presented a new framework for mathematical searches where links between formulas and their names are automatically detected in the target documents and then utilized in the search .\n", 2=>"In this paper , we presented a new framework for mathematical search where links between formulas and their names are automatically detected from the target documents and then utilized in the search .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#stem -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#29#18#exact -1#30#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#0#25#lc -1#23#26#exact -1#31#27#exact -1#32#28#exact \n"}nil_first --> []
nil_second --> [1, 2, 3, 22]
--------------------------
{1=>"Due to unavailability of a standard corpora to evaluate mathematical search systems , our evaluation at this moment remains subjective and limited .\n", 2=>"Due to unavailability of the standard corpora to evaluate mathemat-ical search systems , our evaluation at this moment still remained subjective and limited .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#para -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact \n"}nil_first --> [4, 9]
nil_second --> [4, 9, 19]
--------------------------
{1=>"We believe that our approach of incorporating information other than the mathematical formulas themselves showed promising results .\n", 2=>"We believe that our approach , by incorporating information other than the mathematical formulas themselves , showed promising results .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact \n"}nil_first --> [5]
nil_second --> [5, 6, 15]
--------------------------
{1=>"The experimental results showed how helpful this information is to mathematical search users .\n", 2=>"The experimental results have shown how helpful this information provides to the users of mathematical search .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#para -1#10#9#exact -1#14#10#exact -1#15#11#exact -1#12#12#exact -1#16#13#exact \n"}nil_first --> []
nil_second --> [11, 13]
--------------------------
{1=>"However , this is only a first step ; many important issues are left for future study .\n", 2=>"However , this is only a rst step , some important issues are left for future study .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6,7#para -1#10,11#9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [8]
nil_second --> [6, 8, 9]
--------------------------
{1=>"Using a formula 's name is only one way of taking into account the semantic meaning of the formula ; we are considering other information such as the formula 's description and its variable 's description .\n", 2=>"Using formula 's name is one way of taking into account the semantic meaning of the formula , we are considering other information such as formula 's description and variable 's description .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5,6#6,7,8#para -1#7#9#exact -1#8,9,10,11#10,11,12#para -1#15#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#17,18#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27,28#para -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact \n"}nil_first --> [1, 19, 32]
nil_second --> [17]
--------------------------
{1=>"This paper explores the use of MathML Pallel Markup Corpora for automatic understanding of mathematical expressions , the task of which is formulated as a translation from Presentation to Content MathML Markups . // <the use of capitals implies that these are software applications like PowerPoint or Word . I assume this is the right idea .> .\n", 2=>"This paper explores the use of MathML Pallel Markup Corpora for mathematical expression understanding , the task of which is formulated as a translation from Presentation to Content MathML Markups in our approach .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#13#12#exact -1#17#13#exact -1#11#14#exact -1#12#15#stem -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#18#19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#33#32#exact \n"}nil_first --> [11, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]
nil_second --> [30, 31, 32]
--------------------------
{1=>"In contrast to previous research that mainly relied on manually encoded transformation rules , we use a statistical-machine-translation-based method to automatically extract translation rules from parallel markup corpora .\n", 2=>"In contrast to existing researches that mainly relied on manually encoded transformation rules , we adopt a Statistical-Machine-Translation-based method to automatically extract translation rules from parallel markup corpora .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#stem -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#lc -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [3, 15]
nil_second --> [3, 15]
--------------------------
{1=>"Our study shows that the structural features embedded in the MathML tree can be effectively exploited in the sub-tree alignment and the translation rules extracted from the alignment give a boost to the translation system .\n", 2=>"Our study shows that the structural features embedded in the MathML tree can be effectively exploited in the sub-tree alignment and the translation rules extracted from the alignment give boost to the translation system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [29]
nil_second --> []
--------------------------
{1=>"Experimental results on the Wolfram Function Site show that our approach is an improvement over prior rule-based systems . // <Note : It seems that where were two prior systems that were compared . If not , you can go back to using a prior system .> .\n", 2=>"Experimental results on the Wolfram Function Site show that our approach achieves an improvement against the prior rule-based system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12,13#11,12,13#para -1#16#15#exact -1#17#16#exact -1#18#17#stem -1#19#18#exact -1#14#32#para \n"}nil_first --> [14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
nil_second --> [11, 15]
--------------------------
{1=>"One of the most significant discussions regarding the digitization of mathematical and scientific content and its applications is about semantic enrichment of mathematical documents , that is , adding or associating semantic tags - usually concepts - with mathematical expressions .\n", 2=>"One of the most significant current discussions in the digitization of mathematical and scientific content and its applications is the semantic enrichment of mathematical documents , that is adding or associating semantic tags - usually concepts - to mathematical expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact \n"}nil_first --> [6, 18, 27, 37]
nil_second --> [5, 7, 19, 37]
--------------------------
{1=>"By encoding the underlying mathematical meaning of an expression explicitly , it is possible to interchange information more precisely between systems that semantically process mathematical objects .\n", 2=>"By encoding the underlying mathematical meaning of an expression explicitly , it is possible to interchange information more precisely between systems that semantically process mathematical objects .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13,14#11,12,13#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"The direct application of this idea enables semantic searches for mathematical expressions whereby the system 's �eunderstanding ' of the intent of the searcher and the contextual meaning of mathematical terms improves search accuracy .\n", 2=>"The direct application of this is enabling semantic searches for mathematical expressions by understanding the intent of the searcher and the contextual meaning of mathematical terms improve search accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#14#13#exact -1#16#18#exact -1#17#19#exact -1#15#20#exact -1#23#21#exact -1#20#22#exact -1#18#23#exact -1#19#24#exact -1#21#26#exact -1#22#27#exact -1#24#29#exact -1#25#30#exact -1#26#31#stem -1#27#32#exact -1#28#33#exact -1#29#34#exact \n"}nil_first --> [5, 12, 14, 15, 16, 17, 25, 28]
nil_second --> [5, 12, 13]
--------------------------
{1=>"However , as is the case with natural language , semantic enrichment of mathematical expressions is a non-trivial task .\n", 2=>"However , as is the case with natural language , the semantic enrichment of mathematical expressions is a non-trivial task .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"- The third problem is that new notations tend to be introduced and used when needed so a mechanism is required for referring to mathematical concepts outside of the base collection .\n", 2=>"- The third problem is that new notations tend to be introduced and used as and when needed so a mechanism is required for referring to mathematical concepts outside of the base collection , allowing them to be represented .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#39#31#exact \n"}nil_first --> []
nil_second --> [14, 15, 33, 34, 35, 36, 37, 38]
--------------------------
{1=>"The aim of this paper is to describe a method of automatic semantic enrichment for mathematics that is capable of analyzing and disambiguating mathematical terms .\n", 2=>"The aim of this paper is to introduce a method for automatic mathematics semantic enrichment that capable of analyze and disambiguate mathematical terms .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#17#10#exact -1#11#11#exact -1#13#12#exact -1#14#13#exact -1#10#14#exact -1#12#15#exact -1#15#16#exact -1#16#17,18#para -1#18#20#stem -1#19#21#exact -1#20#22#stem -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [7, 19]
nil_second --> [7]
--------------------------
{1=>"The semantic enrichment task then becomes one of generating Content MathML outputs from Presentation MathML expressions .\n", 2=>"The semantic enrichment task then becomes generating Content MathML outputs from Presentation MathML expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [6, 7]
nil_second --> []
--------------------------
{1=>"- Second , MathML can be used to encode both mathematical notations and mathematical content .\n", 2=>"- Second , MathML can be used to encode both mathematical notation and mathematical content .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#stem -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"- Last , large collections of formulas are available in MathML , and we can easily assess these collections .\n", 2=>"- Last , large collections of formulas are available in MathML and we can easily assess these collections .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"- In the scope of this paper , we only make use the information within a mathematical expression for disambiguation when translating it into content markup .\n", 2=>"- In the scope of this paper , we only make use the information within a mathematical expression for disambiguation when translating it to content markup .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [23]
nil_second --> [23]
--------------------------
{1=>"- Since it is a hand-written rule-based system , SnuggleTeX requires mathematical knowledge and human effort to develop .\n", 2=>"- Since it is a hand written rule-based system , SnuggleTeX requires mathematical knowledge and human effort to develop\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> [5, 18]
nil_second --> [5, 6]
--------------------------
{1=>"- Due to the diversity of mathematical expressions , SnuggleTeX is still considered experimental and has difficulty processing complicated mathematical symbols and expressions .\n", 2=>"- Due to the diversity of mathematical expressions , SnuggleTeX is still to be considered experimental and has difficulty processing complicated mathematical symbols and expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact \n"}nil_first --> []
nil_second --> [12, 13]
--------------------------
{1=>"In this paper , we propose an approach that automatically learns semantic inferences in a presentation from parallel markup data . // <The original has too many from to be logically clear . The rewrite is a guess . > .\n", 2=>"In this paper , we propose an approach that automatically learn the semantics inference from a presentation from parallel markup data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#12#11#stem -1#13#12#stem -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#14#27#exact -1#11#33#lc \n"}nil_first --> [13, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40]
nil_second --> []
--------------------------
{1=>"This approach is based on statistical machine translation .\n", 2=>"The idea of this approach is based on statistical machine translation .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact \n"}nil_first --> []
nil_second --> [0, 1, 2]
--------------------------
{1=>"The underlying mathematical meaning of an expression is inferred from the probability distribution $ p( c | p ) $ that a semantic expression $ c $ is the translation of a presentation expression $ p $ .\n", 2=>"The underlying mathematical meaning of an expression is inferred according to the probability distribution $ p( c | p ) $ that a semantic expression $ c $ is the translation of a presentation expression $ p $ .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact \n"}nil_first --> [9]
nil_second --> [9, 10]
--------------------------
{1=>"The probability distribution is automatically learned from both Presentation and Content MathML markup data , that is , parallel markup MathML data .\n", 2=>"The probability distribution will be automatically learned from data that have both Presentation and Content MathML markup , that is the parallel markup MathML data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#19#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#8#13#exact -1#17#14#exact -1#18#15#exact -1#3,4#16#para -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact \n"}nil_first --> [17]
nil_second --> [9, 10, 20]
--------------------------
{1=>"We also prepared other parallel markup MathML data by annotating mathematical expressions in 20 papers from The Archives of the Association for Computational Linguistics \\CITE ( ACL-ARC ) .\n", 2=>"We also prepare another parallel markup MathML data by annotating mathematical expressions on 20 papers from The Archives of the Association for Computational Linguistics \\CITE ( ACL-ARC ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [3, 12]
nil_second --> [3, 12]
--------------------------
{1=>"There are two main contributions in this paper :\n", 2=>"We have two main contributions in this paper\n", 3=>"-1#1,2#0,1,2#para -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact \n"}nil_first --> [8]
nil_second --> [0]
--------------------------
{1=>"- First , we successfully applied machine translation techniques to solving the problem of mathematic semantic enrichment .\n", 2=>"- First , successfully apply the machine translation techniques to the problem of mathematic semantic enrichment .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#stem -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#5#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [3, 10]
nil_second --> [10]
--------------------------
{1=>"Experimental results show that our system significantly outperforms the current rule-based system and it can handle a lot of practical cases in the semantic enrichment problem .\n", 2=>"Experimental results show that our system significantly outperforms the current rule-based system and it can handle a lot of practical cases in the mathematics semantic enrichment problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> []
nil_second --> [23]
--------------------------
{1=>"The quantity and quality of mathematical expressions are continuing to grow , and we believe that our system will be able to cover most mathematical expressions .\n", 2=>"Since both quantity and quality of mathematical expressions are continuing to grow and expand through time , we believe that our system will cover most of real life mathematical expressions .\n", 3=>"-1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#16#11#exact -1#12#12#exact -1#18,19,20#13,14,15,16#para -1#21#17#exact -1#22#18#exact -1#23#21,22#para -1#24#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact \n"}nil_first --> [0, 19, 20]
nil_second --> [0, 1, 13, 14, 15, 17, 25, 26, 27]
--------------------------
{1=>"- Second , mathematics knowledge such as a symbol 's meanings or structural relations is automatically learned while training ; therefore , the system requires no human effort or expertise , and it is easier to update with more data .\n", 2=>"- Second , mathematics knowledge such as symbol 's meanings or structural relations is automatically learned while training , therefor it is not required mathematics experts nor human effort and it is also easier to update the system given more data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#19#20#stem -1#18#21#exact -1#36#22#exact -1#37#23#exact -1#23#24#stem -1#27#25,26#para -1#28#27#exact -1#26#28#para -1#25#29#para -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact \n"}nil_first --> [19, 30, 37]
nil_second --> [20, 21, 22, 24, 32, 38]
--------------------------
{1=>"Since new notations keep cropping up , it is important to update the system as quickly as possible .\n", 2=>"Since new notations keep growing , it is important to update the system as quick as possible .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6,7,8,9#7,8,9#para -1#10#10,11#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#stem -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [4, 5]
nil_second --> [4]
--------------------------
{1=>"We performed a ten-fold cross validation on mathematical expressions from six categories of the Wolfram Functions Site to evaluate the effectiveness of our learning method .\n", 2=>"In our experiments , we performed a 10-folds cross validation on mathematical expressions from 6 categories of the Wolfram Functions Site to evaluate the effectiveness of our proposed learning method .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#syn -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#22,23,24,25#17,18,19,20,21#para -1#26#22#exact -1#28#23#exact -1#29#24#exact -1#30#25#exact \n"}nil_first --> [3]
nil_second --> [0, 1, 2, 3, 7, 21, 27]
--------------------------
{1=>"We performed another experiment to assess the correlation between the system 's performance and training set size and found that increasing the size of the training data boosted the system 's performance .\n", 2=>"We set up another experiment to confirm the correlation between system performance and training set size and saw that increasing the size of training data actually boost the system performance .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#20#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17,18#18,19#para -1#19#20#exact -1#27#21#exact -1#21#22#exact -1#22#23#exact -1#23#24,25#para -1#24#26#exact -1#26#27#stem -1#28#28,29#para -1#29#31#exact -1#30#32#exact \n"}nil_first --> [1, 5, 11, 30]
nil_second --> [1, 2, 6, 25]
--------------------------
{1=>"We also performed an extensive comparison with prior work \\CITE using a data set collected from ACL-ARC scientific papers .\n", 2=>"We also performed extensive side-by-side comparison with prior work \\CITE over a data set from ACL-ARC scientific papers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact \n"}nil_first --> [3, 10, 14]
nil_second --> [4, 10]
--------------------------
{1=>"Our experimental results show that our approach works well in dealing with the mathematics semantic enrichment problem and it outperforms the previous work by making significantly fewer errors .\n", 2=>"Our experimental results show that the proposed approach works well on the mathematics semantic enrichment problem and it excels the previous work by providing significantly less error rate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#5#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#11#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#24#25#exact -1#25#26#para -1#26#27#stem -1#28#28#exact \n"}nil_first --> [5, 9, 10, 11, 19, 24]
nil_second --> [6, 10, 18, 19, 23, 27]
--------------------------
{1=>"The remainder of this paper is organized as follows : In Section 2 , we give a brief overview of the background and related work on semantic enrichment of mathematical expressions .\n", 2=>"The remainder of this paper is organized as follows : In Section 2 , we give a brief overview of the background and related work for semantic enrichment of mathematical expressions , while in Section 3 we present our proposed method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#41#31#exact \n"}nil_first --> [25]
nil_second --> [25, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
--------------------------
{1=>"We present our method in Section 3 and describe the experimental setup and results in Section 4 .\n", 2=>"We then describe the experimental setup and results in Section 4 .\n", 3=>"-1#0#0#exact -1#8#4#exact -1#9#5#exact -1#6#7#exact -1#2#8#exact -1#3#9#exact -1#4#10#exact -1#5#11#exact -1#7#13#exact -1#10#16#exact -1#11#17#exact \n"}nil_first --> [1, 2, 3, 6, 12, 14, 15]
nil_second --> [1]
--------------------------
{1=>"Until recently , images have been used to represent mathematical formulas on the web .\n", 2=>"Until recently , images have been used to represent mathematical formulas on the web .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> []
nil_second --> [6, 7]
--------------------------
{1=>"This type of display does not need any markup language to decode the formulas , but it is hard to process them .\n", 2=>"This type of display does not need any markup language to decode the formulas , but it is hard to process them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15,16,17,18,19#15,16,17,18#para -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"A way of dealing with mathematical formulas in this format is to convert them into another text-based format , for example , InftyReader \\CITE .\n", 2=>"A way of dealing with mathematical formulas in this format is to convert them to another text-based format , as seen in InftyReader \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13#11,12,13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [19, 20, 21]
nil_second --> [14, 19, 20, 21]
--------------------------
{1=>"\\TeX{} has been used to encode mathematical formulas in scientific documents .\n", 2=>"For scientific documents , \\TeX{} has been used to encode mathematical formulas .\n", 3=>"-1#4#0#exact -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#1#8,9#para -1#2#10#exact -1#12#11#exact \n"}nil_first --> []
nil_second --> [0, 3]
--------------------------
{1=>"A formula is printed in a way a person would write by hand , or typeset the equation .\n", 2=>"The formula is printed in a way a person would write by hand , or typeset the equation .\n", 3=>"-1#7#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"In some web pages , such as on the Wikipedia site , formulas are displayed in both image and \\TeX{} formats .\n", 2=>"In some web pages , such as the Wikipedia site , a formula is displayed in both image and \\TeX{} formats .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#12#stem -1#13#13#syn -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [7]
nil_second --> [11]
--------------------------
{1=>"MathML has two types of encoding , content-based encoding , called Content MathML , dealing with the meaning of formulas , and presentation-based encoding , called Presentation MathML , dealing with the display of formulas .\n", 2=>"MathML has two types of encoding , content-based encoding which is called Content MathML , dealing with the meaning of formulas , and presentation-based encoding which is called Presentation MathML , dealing with the display of formulas .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#21#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#30#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact \n"}nil_first --> [24, 28]
nil_second --> [9, 10, 25, 26]
--------------------------
{1=>"The illustration trees of the Presentation and Content Markup of the expression $ C_{-\\frac{17}{2}}= \\tilde {\\infty} $ are depicted in Figure \\REF and Figure \\REF .\n", 2=>"The illustration tree display of Presentation and Content Markup of the expression $ C_{-\\frac{17}{2}}= \\tilde {\\infty} $ are depicted in Figure \\REF and Figure \\REF respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#9#3#exact -1#10#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#4#9#exact -1#11#10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#25#exact \n"}nil_first --> []
nil_second --> [3, 25]
--------------------------
{1=>"org Math \\CITE , ASCIIMathML \\CITE , and OpenMath \\CITE , but these markups can be converted into MathML by using freely available tools .\n", 2=>"org Math \\CITE , ASCIIMathML \\CITE and OpenMath \\CITE , but these markup can be converted to MathML using freely available tools .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#9#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#stem -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact \n"}nil_first --> [10, 17]
nil_second --> [16]
--------------------------
{1=>"There are not many studies on the semantic enrichment problem .\n", 2=>"There are not many studies on semantic enrichment problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact \n"}nil_first --> [6]
nil_second --> []
--------------------------
{1=>"In this section , we list some of the work on exploiting the meanings of mathematical expressions .\n", 2=>"In this section , we list some works that related to exploit the meaning of mathematical expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#14#7#exact -1#12#8#exact -1#7#9#stem -1#10,11#10,11#para -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [12, 13, 14]
nil_second --> [8, 9, 13]
--------------------------
{1=>"Grigole et al. \\CITE proposed an approach to understanding mathematical expressions based on the text surrounding the mathematical expressions .\n", 2=>"For understanding mathematical expressions , Grigole et al. \\CITE proposed an approach based on the surrounding text of mathematical expressions .\n", 3=>"-1#5#0#exact -1#6#1#exact -1#7#2#exact -1#8#3#exact -1#9#4#exact -1#10,11,12,13#5,6#para -1#0,1#7,8#para -1#2#9#exact -1#3#10#exact -1#14#13#exact -1#16#14#exact -1#15#15#exact -1#18#16,17#para -1#19#18#exact -1#20#19#exact \n"}nil_first --> [11, 12]
nil_second --> [4, 17]
--------------------------
{1=>"The similarity scores obtained are weighted , summed up , and normalized by the length of the considered context .\n", 2=>"The similarity scores obtained were weighted , summed up , and normalized by the length of the considered context .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#syn -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15,16#13,14,15#para -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"The Term Cluster with the highest similarity score is assigned as the interpretation .\n", 2=>"The assigned interpretation is the Term Cluster with the highest similarity score .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#3#8#exact -1#1#9#exact -1#0#11#lc -1#2#12#exact -1#12#13#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"The approach was evaluated on 451 manually annotated mathematical expressions , and the best result was an F_{0.5} $ score of 68.26 $ .\n", 2=>"The approach was evaluated on 451 manually annotated mathematical expressions and the best result was 68.26 $ F_{0.5} $ score .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#15#21#exact -1#16#22#exact -1#20#23#exact \n"}nil_first --> [10, 16, 20]
nil_second --> []
--------------------------
{1=>"To deal with the meanings of mathematical formulas , Nghiem et al. \\CITE proposed an approach for extracting names or descriptions of formulas by using the natural language text surrounding them .\n", 2=>"To deal with the meanings of mathematical formulas , Nghiem et al. \\CITE proposed an approach for extracting the names or descriptions of the formulas using natural language text surrounding them .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#24#22#exact -1#25#23,24#para -1#18#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> []
nil_second --> [23]
--------------------------
{1=>"The first is the SnuggleTeX project \\CITE , which provides a free and open-source Java library for converting fragments of LaTeX into XML including Content MathML .\n", 2=>"The first is the SnuggleTeX project \\CITE , which provides a free and open-source Java library for converting fragments of LaTeX to XML including Content MathML .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [21]
nil_second --> [21]
--------------------------
{1=>"This project investigates semantic enrichment , structural semantics , and ambiguity resolution in mathematical corpora .\n", 2=>"This project investigates semantic enrichment , structural semantics and ambiguity resolution in mathematical corpora .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"Unfortunately , there are no evaluations of these systems .\n", 2=>"Unfortunately , there are no evaluation report on these systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#stem -1#8#7#exact -1#9#8#exact -1#10#9#exact \n"}nil_first --> [6]
nil_second --> [6, 7]
--------------------------
{1=>"To translate mathematical expressions from the Presentation MathML into Content MathML format , a list of translation rules is required .\n", 2=>"To translate mathematical expressions from the Presentation MathML to Content MathML format , a list of rules for translation is required .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#18#16#exact -1#16#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> [8]
nil_second --> [8, 17]
--------------------------
{1=>"Our task is inherently domain-specific ; therefore , we devised an approach based on statistical machine learning for automatically extracting rules from a dataset .\n", 2=>"Our task is inherently domain specific therefore we propose an approach which is based on statistical machine learning methods that automatically extract these rules from a dataset .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#6#exact -1#7#8#exact -1#9,10#10,11,12,13#para -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#20#18#exact -1#21#19#stem -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact \n"}nil_first --> [4, 5, 7, 9, 17]
nil_second --> [4, 5, 8, 11, 12, 13, 14, 18, 19, 22]
--------------------------
{1=>"Statistical machine translation ( SMT ) is by far the most widely studied machine translation method .\n", 2=>"Nowadays , statistical machine translation ( SMT ) is by far the most widely-studied machine translation method .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8,9,10,11#6,7,8,9,10#para -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact \n"}nil_first --> [11, 12]
nil_second --> [0, 1, 12, 13]
--------------------------
{1=>"SMT uses a very large data set of good translations , that is , a corpus of texts which have already been translated into another language , and it uses those texts to automatically infer a statistical model of translation .\n", 2=>"SMT uses a very large data set of good translations , that is , a corpus of texts which have already been translated into other language , and then uses those texts to automatically infer a statistical model of translation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24,25#24,25#para -1#26#26#exact -1#27#27#exact -1#29#28,29#para -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact \n"}nil_first --> []
nil_second --> [28]
--------------------------
{1=>"The statistical model is then applied to new texts to make a translation of them .\n", 2=>"The statistical model is then applied to new texts to make a translation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#15#exact \n"}nil_first --> [13, 14]
nil_second --> []
--------------------------
{1=>"Tree-based or syntax-based SMT can be used for tree-to-tree translation but it has two drawbacks when it is applied to the problem of translating Presentation into Content MathML .\n", 2=>"Tree-based or syntax-based SMT can be used for tree-to-tree translation but it has two drawbacks when apply to the problem of translating from Presentation to Content MathML expression .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17,18#17,18,19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#23#24#exact -1#25#26#exact -1#26#27#exact -1#28#28#exact \n"}nil_first --> [16, 25]
nil_second --> [6, 7, 22, 24, 27]
--------------------------
{1=>"- The first drawback is that tree-based SMT focuses on generating surface texts rather than tree structures .\n", 2=>"- The first drawback is tree-based SMT focus on generating the surface texts rather than the tree structures .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7,8#8,9#para -1#9#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> [5]
nil_second --> [10, 15]
--------------------------
{1=>"Mathematical expressions have strict structures , and it fails to fulfill this requirement .\n", 2=>"While mathematical expressions have strict structures , it fails to fulfill this requirement .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [6]
nil_second --> [0]
--------------------------
{1=>"To overcome these limitations , we made two separate rule sets : fragment rules and translation rules .\n", 2=>"To overcome these limitations , we introduced two separated sets of rule : fragment rules and translation rules .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#para -1#7#7#exact -1#8#8#stem -1#11#9#exact -1#9#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"- Preprocessing : This module processes MathML expressions by removing error expressions or format tags with no semantic meaning .\n", 2=>"- Preprocessing : processes MathML expressions to remove error expressions or format tags with no semantic meaning .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#7#9#stem -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [3, 4, 8]
nil_second --> [6]
--------------------------
{1=>"- Rule Extraction : This module is given a dataset containing MathML parallel markup expressions , and it extracts translation rules from it . // <The original is ungrammatical and unclear . The rewrite is a guess .> .\n", 2=>"- Extracting Rules : given a dataset contains MathML parallel markup expressions , extract the rules for translation .\n", 3=>"-1#0#0#exact -1#3#3#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#stem -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#1#18#stem -1#17#19#exact -1#15#20#exact -1#18#23#exact -1#14#32#lc \n"}nil_first --> [1, 2, 4, 5, 6, 16, 17, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38]
nil_second --> [2, 13, 16]
--------------------------
{1=>"- Content MathML Generation : This module is given mathematical expressions in Presentation MathML markup and a set of rules , and it generates Content MathML expressions to enrich the Presentation MathML expressions .\n", 2=>"- Generating Content MathML : given a mathematical expressions in Presentation MathML markup , and a set of rules , generate Content MathML expressions to enrich the Presentation MathML expressions .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#1#3#stem -1#4#4#exact -1#5#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#14#15#exact -1#16,17,18#16,17,18,19#para -1#19#20#exact -1#20#23#stem -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact \n"}nil_first --> [5, 6, 7, 21, 22]
nil_second --> [6, 13, 15]
--------------------------
{1=>"Token elements represent the identifier 's names , function 's names , numbers , etc. // <the identifier 's names means there is one identifier with possibly many names . If this is what you want to say , it is okay . If not , maybe you mean simply \" identifier names \" . ><Likewise , maybe you mean \" function names \" .>\n", 2=>"Token elements represent identifier 's names , function 's names , numbers , etc.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [3, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]
nil_second --> []
--------------------------
{1=>"After investigating data on the Wolfram Function Site , we noticed that there are elements that have no specific meaning ; they are used for display purposes only and most of them are layout schemata .\n", 2=>"By investigating the data from the Wolfram Function Site , we noticed that there are elements that have no specific meaning , they are used for displaying purpose only and most of them are layout schemata .\n", 3=>"-1#1#1#exact -1#3#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#stem -1#27#26#stem -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact \n"}nil_first --> [0, 3, 20]
nil_second --> [0, 2, 4, 21]
--------------------------
{1=>"Another example is pairs of parentheses ; these are used to indicate that the expressions in the parentheses go together , despite that their structure already encodes that information . // <The original is unclear . The rewrite is a guess . > .\n", 2=>"Another example are the pairs of parentheses , it is used to indicate that the expressions in the parentheses go together , while its structure already encoded that information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#2#7,8#para -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#27#22#exact -1#23,24#23,24#para -1#25#25#exact -1#26#26#stem -1#28#27,28#para -1#29#29#exact -1#3#36#lc \n"}nil_first --> [6, 21, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43]
nil_second --> [7, 8, 22]
--------------------------
{1=>"This preprocessing step removes these elements .\n", 2=>"As a result , in this preprocessing step , these elements are removed .\n", 3=>"-1#5#0#lc -1#6#1#exact -1#7#2#exact -1#9#4#exact -1#10#5#exact -1#13#6#exact \n"}nil_first --> [3]
nil_second --> [0, 1, 2, 3, 4, 8, 11, 12]
--------------------------
{1=>"We also remove mathematical expressions with error markups such as expressions that have no Content markup .\n", 2=>"In this step , we also removed mathematical expressions with error markups such as expressions that have no Content markup .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#stem -1#7#3#exact -1#8#4#exact -1#9#5#exact -1#10#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3]
--------------------------
{1=>"In the training phase , we use GIZA++ \\CITE for aligning Presentation MathML terms and Content MathML terms .\n", 2=>"In the training phase , we use GIZA++ \\CITE for alignment between Presentation MathML terms and Content MathML terms .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#stem -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"Based on the aligned data , we use heuristics to extract rules that we call \" fragment rules \" .\n", 2=>"Based on the aligned data , we use some heuristics to extract rules which we called \" fragment rules \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#14#13#exact -1#15#14#stem -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [12]
nil_second --> [8, 13]
--------------------------
{1=>"These rules are used to break up a large Presentation MathML tree into smaller sub-trees while maintaining the structure of the output Content MathML trees .\n", 2=>"These rules are applied to break the large Presentation MathML tree into smaller sub-trees while maintaining the structure of output Content MathML trees .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#syn -1#4#4#exact -1#5#5#exact -1#7#7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16,17,18#17,18,19,20#para -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"Each rule in the fragment rule set is associated with a probability , that is , the frequency at which a rule occurs in the training data .\n", 2=>"Each rule in fragment rule set is associated with its probability , that is the frequent that rule happened in the training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#14#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#20#16#exact -1#15#17#para -1#17#20,21#para -1#18#22#syn -1#19#23#exact -1#21#24,25#para -1#22#26#exact -1#23#27#exact \n"}nil_first --> [10, 15, 18, 19]
nil_second --> [9, 16]
--------------------------
{1=>"Once the sub-trees cannot be broken down further , we start to extract other rules , which we call \" translation rules \" .\n", 2=>"If the sub-trees can not be broken any longer , we extract another rules , which we called \" translation rules \" , at that point .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3,4#3#para -1#5#4#exact -1#6#5#exact -1#7,8#7#para -1#9#8#exact -1#10#9#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#stem -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#26#23#exact \n"}nil_first --> [0, 6, 10, 11, 13]
nil_second --> [0, 12, 22, 23, 24, 25]
--------------------------
{1=>"We enhance the translation rule set with translation terms extracted by GIZA++ .\n", 2=>"We then enhances the translation rule set with the translation terms extracted by GIZA++ .\n", 3=>"-1#0#0#exact -1#2#1#stem -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact \n"}nil_first --> []
nil_second --> [1, 8]
--------------------------
{1=>"The pseudo code for extracting fragment rules is described in Algorithm \\REF .\n", 2=>"The pseudo code of the algorithm for extracting fragment rules is described in Algorithm \\REF .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact \n"}nil_first --> []
nil_second --> [3, 4, 5]
--------------------------
{1=>"In the previous steps , we get two sets of rules , a fragment rule set and a translation rule set .\n", 2=>"In the previous steps , we got two sets of rules , fragment rule set and translation rule set .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#syn -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact \n"}nil_first --> [12, 17]
nil_second --> []
--------------------------
{1=>"Given mathematical expressions in Presentation MathML markup , the system will generate Content MathML markup for each expression .\n", 2=>"Given a mathematical expressions in Presentation MathML markup , the system will generate Content MathML markup of that expression .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> [15, 16]
nil_second --> [1, 16, 17]
--------------------------
{1=>"- First , the expression is preprocessed to remove non-semantic elements .\n", 2=>"- First , the expression is preprocess to remove non semantic elements .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#11#10#exact -1#12#11#exact \n"}nil_first --> [9]
nil_second --> [9, 10]
--------------------------
{1=>"- Second , the fragment rule is applied to the expression until it cannot be divided any further .\n", 2=>"- Second , the fragment rule is applied to the expression until it could not be divided any further .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12,13,14,15#12,13,14#para -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"The reason for this is that there is an infinite number of rules . // <The rewrite is a guess .> .\n", 2=>"The reason for this is that there is infinite number and we could never present every number in the rule .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#19#12#stem -1#20#13#exact \n"}nil_first --> [8, 11, 14, 15, 16, 17, 18, 19, 20, 21]
nil_second --> [10, 11, 12, 13, 14, 15, 16, 17, 18]
--------------------------
{1=>"The experiments were carried out using datasets from the Wolfram Function site .\n", 2=>"The experiments were carried out using the datasets from the Wolfram Function site .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact \n"}nil_first --> []
nil_second --> [6]
--------------------------
{1=>"The datasets we used contain 205 , 653 mathematical expressions belonging to six categories .\n", 2=>"These datasets we used contain 205 , 653 mathematical expressions belong to 6 categories .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#syn -1#13#13#exact -1#14#14#exact \n"}nil_first --> [0]
nil_second --> [0]
--------------------------
{1=>"Training and testing were performed using ten-fold cross-validation ; for each category , the original corpus was partitioned into ten subsets .\n", 2=>"Training and testing were performed using 10-fold cross-validation ; for each category , the original corpus is partitioned into 10 subsets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#syn -1#17#17#exact -1#18#18#exact -1#19#19#syn -1#20#20#exact -1#21#21#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"The cross-validation process was repeated ten times , with each of the ten subsets used exactly once as the validation data .\n", 2=>"The cross-validation process is then repeated 10 times , with each of the 10 subsets used exactly once as the validation data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#syn -1#5#4#exact -1#6,7#5,6#para -1#8#7#exact -1#9,10#8,9,10,11#para -1#12,13#12#para -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> []
nil_second --> [4, 11]
--------------------------
{1=>"To prove the effectiveness of our models with real data , we conducted another experiment on the mathematical expressions in scientific papers .\n", 2=>"To prove the effectiveness of our models to real data , we conducted another experiment on the mathematical expressions in scientific papers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"Currently , we have 20 papers from the ACL archive , and we manually annotated all of the math expressions in these papers with both Presentation Markup and Content Markup . // The original is somewhat vague . The rewrite is a guess . Use it if it is correct . > .\n", 2=>"Currently we have 20 papers from ACL archive , all of the math expressions in these papers are annotated manually with both Presentation Markup and Content Markup .\n", 3=>"-1#0#0#exact -1#8#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#11#7#exact -1#6#8#exact -1#7#9#exact -1#24#11#exact -1#19#13#exact -1#18#14#exact -1#9#15#exact -1#10#16#exact -1#12#18#exact -1#13#19#exact -1#14#20#exact -1#15#21#exact -1#16#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#17#34#syn \n"}nil_first --> [10, 12, 17, 27, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
nil_second --> []
--------------------------
{1=>"In the first experiment , the data was not compatible with SnuggleTeX since SnuggleTeX uses ASCII MathML but the Wolfram Functions site does not .\n", 2=>"In the first experiment , the data is not compatible with SnuggleTeX since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#stem -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"In the second experiment with ACL-ARC data , we compared our model with SnuggleTeX .\n", 2=>"In the second experiment with ACL-ARC data , we compared our model side by side with SnuggleTeX .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact \n"}nil_first --> []
nil_second --> [12, 13, 14]
--------------------------
{1=>"Table \\REF lists the various data statistics .\n", 2=>"Table \\REF contains the various data statistics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"Given a Presentation MathML expression $ e $ , we assume that tree $ A $ is the correct Content MathML tree of expression $ e $ and tree $ B $ is the output of the automatic translation .\n", 2=>"Given a Presentation MathML expression $ e $ , we assume that tree $ A $ is the correct Content MathML tree of expression $ e $ and tree $ B $ is the output using the automatic translation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact \n"}nil_first --> [35]
nil_second --> [35]
--------------------------
{1=>"- the Translation Error Rate \\CITE : the translation error rate is an error metric for machine translation that measures the number of edits required to change a system output into one of the references .\n", 2=>"- Translation Error Rate \\CITE : translation error rate is an error metric for machine translation that measures the number of edits required to change a system output into one of the references .\n", 3=>"-1#0#0#exact -1#18#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#31#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#19,20#20,21,22#para -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28,29#30,31,32,33#para -1#32#34#exact -1#33#35#exact \n"}nil_first --> []
nil_second --> [30]
--------------------------
{1=>"TEDR is defined as the rate between ( 1 ) the minimal cost to transform a tree A into another tree B using edit operations and ( 2 ) the maximum number of nodes of A and B . // <The \" rate between \" is unclear to me . Do you mean , \" the ratio of \" >\n", 2=>"TEDR is defined as the rate between ( 1 ) the minimal cost to transform a tree A into another tree B using edit operations and ( 2 ) the maximum number of nodes of A and B .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact \n"}nil_first --> [39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
nil_second --> []
--------------------------
{1=>"Compared with the reference tree in Figure \\REF , we need to substitute X nodes , insert Y nodes , and delete Z nodes , so that $ TED( A , B ) = x $ , while the maximum number of nodes of the two trees is y .\n", 2=>"Compare to the reference tree in Figure \\REF , we need to substituting X node , inserting Y node , and deleting Z node , so that $ TED( A , B ) = x $ . While the maximum number of node of two trees is y .\n", 3=>"-1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#stem -1#13#13#exact -1#14#14#stem -1#15#15#exact -1#16#16#stem -1#17#17#exact -1#18#18#stem -1#19#19#exact -1#20#20#exact -1#21#21#stem -1#22#22#exact -1#23#23#stem -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#37#37#lc -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#stem -1#43#43#exact -1#44#44,45#para -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact \n"}nil_first --> [0, 1, 36]
nil_second --> [0, 1, 36]
--------------------------
{1=>"It appeared that SnuggleTeX was not applicable to the data from the Wolfram Function site since it uses ASCII MathML but the site does not .\n", 2=>"For the data in Wolfram Function site , it appeared that SnuggleTeX is not applicable to this data since SnuggleTeX use ASCII MathML while the Wolfram Functions site does not .\n", 3=>"-1#8#0#lc -1#9#1#exact -1#10#2#exact -1#11#3#exact -1#12,13#4,5#para -1#14#6#exact -1#15#7#exact -1#1#8#exact -1#2#9#exact -1#24#11#exact -1#25#12#exact -1#5#13#exact -1#6#14#exact -1#18#15#exact -1#20#16,17#para -1#21#18#exact -1#22#19#exact -1#27#21,22#para -1#28#23#exact -1#29#24#exact -1#30#25#exact \n"}nil_first --> [10, 20]
nil_second --> [0, 3, 4, 7, 16, 17, 19, 23, 26]
--------------------------
{1=>"Therefore , we could not do a comparison on this data .\n", 2=>"Therefore we could not do the side-by-side comparison on this data .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [1, 6]
nil_second --> [5, 6]
--------------------------
{1=>"Our experimental results show that our approach gives reasonable results , that is , a 20 percent TEDR with large training data .\n", 2=>"Our experimental results show that our approach can archive reasonable results , that is 20 percent TEDR with large training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact \n"}nil_first --> [7, 13]
nil_second --> [7, 8]
--------------------------
{1=>"For small data ( less than 3000 training samples ) , the results vary from 50 to 75 percent TEDR .\n", 2=>"For small data which has less than 3000 training samples , the results vary from 50 to 75 percent TEDR .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15,16,17#14,15,16,17#para -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [3, 9]
nil_second --> [3, 4, 14]
--------------------------
{1=>"For ACL-ARC data , the experimental results show that our system significantly outperforms SnuggleTeX in terms of the Tree Edit Distance Rate .\n", 2=>"For ACL-ARC data , the experimental results from our side-by-side comparison show that our system significantly outperforms SnuggleTeX in terms of Tree Edit Distance Rate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#11#7#exact -1#12#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18,19,20#14,15,16,17#para -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact \n"}nil_first --> []
nil_second --> [7, 8, 9, 10]
--------------------------
{1=>"Our system had a 24 percent lower TEDR in comparison with SnuggleTeX .\n", 2=>"Our system archived 24 percent TEDR less than the output using SnuggleTeX .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#4#exact -1#4#5#exact -1#6#6#para -1#5#7#exact -1#7#9,10#para -1#11#11#exact -1#12#12#exact \n"}nil_first --> [2, 3, 8]
nil_second --> [2, 8, 9, 10]
--------------------------
{1=>"To investigate the correlation between the TEDR score and training set size , we set up an experiment using mathematical expressions in the Elementary Functions category .\n", 2=>"To find out the correlation between TEDR score and training set size , we set up an experiment using mathematical expressions in Elementary Functions category .\n", 3=>"-1#0#0#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22,23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [1, 5]
nil_second --> [1, 2]
--------------------------
{1=>"We started with one fifth of the data and increased the data by one fifth in each run .\n", 2=>"We started with one fifth of the data , and then increase data one fifth each run .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#8#exact -1#11#9#stem -1#12#10,11#para -1#13#13#exact -1#14#14#exact -1#15#15,16#para -1#16#17#exact -1#17#18#exact \n"}nil_first --> [12]
nil_second --> [8, 10]
--------------------------
{1=>"Table \\REF and Table \\REF show the TEDR of our method on the Wolfram Functions Site data and in comparison with SnuggleTeX on ACL ARC data , respectively .\n", 2=>"Table \\REF and Table \\REF show the TEDR of our proposed method on the Wolfram Functions Site data and in comparison with SnuggleTeX on ACL ARC data , respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#19,20,21#17,18,19,20#para -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> []
nil_second --> [10, 18]
--------------------------
{1=>"We discussed the problem of semantic enrichment of mathematical expressions .\n", 2=>"In this paper , we discussed the problem of the semantic enrichment of mathematical expressions .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#6#2#exact -1#7#3#exact -1#8#4#exact -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 3, 9]
--------------------------
{1=>"Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expressions to Content MathML expressions is a significant improvement over prior systems .\n", 2=>"Our experimental results show that our approach based on the statistical machine translation method for translating a Presentation MathML expression to a Content MathML expression has the significant improvement over a prior system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9#6,7#para -1#26#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#stem -1#20#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#stem -1#25#24#para -1#21#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#31#29#exact -1#32#30#stem -1#33#31#exact \n"}nil_first --> [8]
nil_second --> [30]
--------------------------
{1=>"In the scope of this paper , we only considered the first sort of context information .\n", 2=>"In the scope of this paper , we only consider the first context information .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10#10#exact -1#11#11#exact -1#12#13,14#para -1#13#15#exact -1#14#16#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"Since this is a first attempt to translate from Presentation to Content MathML using a machine learning method , there is room for further improvement .\n", 2=>"Since this is a first attempt to translate Presentation to Content MathML using a machine learning method , there is room for further improvement .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"- Expanding the work by incorporating the surrounding information of mathematical expressions , for example , definitions or other mathematical expressions .\n", 2=>"- Expanding the work by incorporating the surrounding information of mathematical expressions , for example definitions or other mathematical expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"Our approach combining automatic extraction of fragment rules and translation rules has shown promising results .\n", 2=>"By combining the automatic extraction of fragment rules and translation rules , our approach has shown promising results .\n", 3=>"-1#12#0#lc -1#13#1#exact -1#1#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact \n"}nil_first --> []
nil_second --> [0, 2, 11]
--------------------------
{1=>"The experimental results confirm that it would be helpful for automatic understanding of mathematical expressions .\n", 2=>"The experimental results confirm that this approach is helpful to the understanding of mathematical expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#7#6,7#para -1#8,9#8,9#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact \n"}nil_first --> [5, 10]
nil_second --> [5, 6, 10]
--------------------------
{1=>"Currently , our system deals with a limited range of mathematical notations .\n", 2=>"Currently , our system deals only with a sub-part of mathematical notations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#5#exact -1#7#6#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact \n"}nil_first --> [7, 8]
nil_second --> [5, 8]
--------------------------
{1=>"In the future , we should consider expanding it to cover all mathematical notations .\n", 2=>"In future work , we should also consider expanding it to cover all mathematical notations .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact \n"}nil_first --> [1]
nil_second --> [2, 6]
--------------------------
{1=>"Current research has shown that major difficulties in event extraction cases for the biomedical domain are related to coreference .\n", 2=>"Recent research shows a major part of difficult cases in event extraction for the biomedical domain are related to coreference .\n", 3=>"-1#0,1#0,1#para -1#2#2,3#para -1#4#5#exact -1#6,7#6#para -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#8#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [4]
nil_second --> [3, 5]
--------------------------
{1=>"To address coreference resolution in molecular biology literature , the Protein Coreference ( COREF ) task was arranged in the BioNLP-ST 2011 , as a supporting task .\n", 2=>"To address the problem of coreference resolution in molecular biology literature , the Protein Coreference ( COREF ) task was arranged in the BioNLP-ST 2011 as a supporting task .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact \n"}nil_first --> [22]
nil_second --> [2, 3, 4]
--------------------------
{1=>"However , the shared task results indicated that transferring coreference resolution methods developed for other domains to the biological domain was not straightforward , due to the domain differences in the coreference phenomena .\n", 2=>"However , the shared task results showed that transferring coreference resolution methods developed for other domains to the biological domain was not straight forward , which is supposed to be caused by the domain differences in coreference phenomena .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#syn -1#24#23#exact -1#30,31#24,25#para -1#32#26#exact -1#33#27#exact -1#34#28#exact -1#35#29#exact -1#36#31#exact -1#37#32#exact -1#38#33#exact \n"}nil_first --> [30]
nil_second --> [23, 25, 26, 27, 28, 29]
--------------------------
{1=>"We studied the contribution of domain-specific information , including information that indicates the protein type , in a rule-based protein coreference resolution system .\n", 2=>"We studied the contribution of domain-specific information , i .e information indicating the protein type , in a rule-based protein coreference resolution system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#exact -1#11#11#stem -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [8, 10]
nil_second --> [8, 9]
--------------------------
{1=>"We compared our system with the top four systems in the BioNLP-ST 2011 ; surprisingly , we found that the minimal configuration had outperformed the best system in the BioNLP-ST 2011 .\n", 2=>"We compared our system with the top four systems in the BioNLP-ST 2011 , and surprisingly we found that the minimal configuration has outperformed the best system in the BioNLP-ST 2011 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#15#14#exact -1#13#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#syn -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [13]
nil_second --> [14]
--------------------------
{1=>"Analysis of the experimental results revealed that semantic classification , using protein information , had contributed to an increase in performance by 2.3 % on the test data , and 4 .0% on the development data , in F-score .\n", 2=>"Analysis of the experimental results showed that semantic classification using protein information has contributed to an increase in performance ( 2.3 % on the test data , and 4 .0% on the development data , in F-score ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#26#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#34#13#exact -1#12,13#14,15#para -1#15,16,17#16,17,18,19#para -1#18#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#35#37#exact -1#36#38#exact -1#38#39#exact \n"}nil_first --> [21, 28, 36]
nil_second --> [14, 19, 37]
--------------------------
{1=>"Since it is difficult to transfer domain-specific information across different domains , we need to continue to seek methods to exploit and use it in coreference resolution .\n", 2=>"Since such information is difficult to be transferred across different domains , we need to continue seeking for methods to exploit and use it in coreference resolution .\n", 3=>"-1#0#0#exact -1#23#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6,7#5#para -1#2#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15#13,14,15,16#para -1#16#17#stem -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [6, 23]
nil_second --> [1, 17]
--------------------------
{1=>"While named entity recognition ( NER ) and relation / event extraction are regarded as standard tasks for biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is being recognized more and more as an important component of IE to achieve a higher performance .\n", 2=>"While named entity recognition ( NER ) and relation or event extraction are regarded as standard tasks of biomedical information extraction ( IE ) , coreference resolution [ 2 , 16 , 30 ] is more and more recognized as an important component of IE for a higher performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#45#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#38#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact \n"}nil_first --> [9, 35, 46, 47]
nil_second --> [9, 17]
--------------------------
{1=>"Without coreference resolution , oftentimes , the IE performance issubstantially limited , due to the abundance of coreference relations in natural language text ; information pieces written in text with the involvement of a coreference relation are hard to be captured [ 9 , 14 ] .\n", 2=>"Without coreference resolution , the performance of IE is often substantially limited due to an abundance of coreference relations in natural language text , i.e. , information pieces written in text with involvement of a coreference relation are hard to be captured [ 9 , 14 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#23#5#exact -1#4#6#exact -1#7#7#exact -1#5#8#exact -1#11#10#exact -1#25#11#exact -1#12#12#exact -1#13#13#exact -1#15#14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32,33#30,31,32#para -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#46#45#exact -1#47#46#exact \n"}nil_first --> [4, 9, 23]
nil_second --> [6, 8, 9, 10, 14, 24]
--------------------------
{1=>"There have been several attempts for coreference resolution ; in particular , they have been for newswire texts [ 7 , 8 , 22 , 23 , 28 , 30 ] .\n", 2=>"There have been several attempts for coreference resolution , particularly for newswire texts [ 7 , 8 , 22 , 23 , 28 , 30 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10#9,10#para -1#8#11#exact -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact -1#19#24#exact -1#20#25#exact -1#21#26#exact -1#22#27#exact -1#23#28#exact -1#24#29#exact -1#25#30#exact -1#26#31#exact \n"}nil_first --> [8, 12, 13, 14, 15]
nil_second --> []
--------------------------
{1=>"Coreference resolution is also one of the lessons from the BioNLP Shared Task ( BioNLP-ST , hereafter ) 2009 , in which it was communicated that coreference relations in biomedical text substantially hinder the progress of fine-grained IE [ 10 ] .\n", 2=>"It is also one of the lessons from BioNLP Shared Task ( BioNLP-ST , hereafter ) 2009 that coreference relations in biomedical text substantially hinder the progress of fine-grained IE [ 10 ] .\n", 3=>"-1#18#0#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#25#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#20#20#exact -1#0#22#lc -1#17#25#exact -1#19#27#exact -1#21#29#exact -1#22#30#exact -1#23#31#exact -1#24#32#exact -1#26,27#33,34,35#para -1#28#36#exact -1#29#37#exact -1#30#38#exact -1#31#39#exact -1#32#40#exact -1#33#41#exact \n"}nil_first --> [1, 19, 21, 23, 24, 26, 28]
nil_second --> []
--------------------------
{1=>"This task definition focuses on protein , as a specific type of entity .\n", 2=>"This task definition focuses on a specific type of entities , i.e. Protein .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12#5#lc -1#10#6#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#stem -1#13#13#exact \n"}nil_first --> [7]
nil_second --> [11]
--------------------------
{1=>"In the figure , protein names P4 - P10 are highlighted in boldface ; the targeted anaphoric expressions of the shared task ( pronouns and definite noun phrases ) are T29 , and T32 , for which the antecedents are indicated by arrows , if found in the text .\n", 2=>"In the figure , protein names are highlighted in bold face , P4 - P10 , and targeted anaphoric expressions of the shared task , e.g. pronouns and definite noun phrases , are T29 , and T32 , of which the antecedents are indicated by arrows if found in the text .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#12#6#exact -1#13#7#exact -1#14#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#syn -1#21#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#40#19#exact -1#22#20#exact -1#23#21#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#32#29#exact -1#33#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#38,39#35,36#para -1#49#37#exact -1#41#38#exact -1#42#39#exact -1#43#40#exact -1#44#41#exact -1#45#42#exact -1#11#43#exact -1#46#44#exact -1#47#45#exact -1#48#46#exact -1#50#47,48#para -1#51#49#exact \n"}nil_first --> [13, 22, 28]
nil_second --> [10, 15, 16, 24, 25, 31]
--------------------------
{1=>"Without knowing this coreference relation , it becomes difficult to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is a localization of p65 ( out of nucleus ) , according to the framework of BioNLP-ST .\n", 2=>"Without knowing this coreference relation , it becomes hard to capture the information written in the phrase , nuclear exclusion of this transcription factor , which is localization of p65 ( out of nucleus ) according to the framework of BioNLP-ST .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact \n"}nil_first --> [27, 36]
nil_second --> []
--------------------------
{1=>"The terminologies used in this paper are similar to those in [ 25 ] .\n", 2=>"The terminologies used in this paper are similar to those in [ 25 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"A new term introduced in the BioNLP-ST is antecedent protein , which indicates the protein mention contained in the antecedent expression , e.g. , p65 in T28 .\n", 2=>"A new term is introduced in the BioNLP-ST is antecedent protein , which indicates the protein mention contained in the antecedent expression , e.g. p65 in T28 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [23]
nil_second --> [3]
--------------------------
{1=>"There are other coreferential expressions , which are ignored in the context of this COREF task , such as : this complex and the NF-kappa B transcription factor complex ( Figure 1 ) , since our focus is on the antecedent expressions that contain and point to protein mentions .\n", 2=>"There are other coreferential expressions which are ignored in the context of this COREF task such as this complex and the NF-kappa B transcription factor complex ( Figure 1 ) , since we only focus on the antecedent expressions that contain and point to protein mentions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#30#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#31#34#exact -1#34#35,36#para -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact -1#44#47#exact -1#45#48#exact -1#46#49#exact \n"}nil_first --> [16, 19, 33, 37]
nil_second --> [32, 33]
--------------------------
{1=>"The best system in the COREF shared task , according to the primary evaluation , found 22 .2% of the anaphoric protein references at the precision of 73 .3% ( 34 .1% F-score ) .\n", 2=>"The best system in the COREF shared task according to the primary evaluation found 22 .2% of anaphoric protein references at the precision of 73 .3% ( 34 .1% Fscore ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#21#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#30#33#exact -1#31#34#exact \n"}nil_first --> [8, 14, 24, 32]
nil_second --> [29]
--------------------------
{1=>"The results are promising , since the authors make use of an external coreference resolution tool originally built for the news domain , without much domain adaptation on the main coreference resolution algorithm .\n", 2=>"This is an encouraging result , since the authors make use of an external coreference resolution tool originally built for the news domain , without much domain adaptation on the main coreference resolution algorithm .\n", 3=>"-1#20#0#lc -1#3#3#para -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#29#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#30#28,29#para -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact \n"}nil_first --> [1, 2]
nil_second --> [0, 1, 2, 4]
--------------------------
{1=>"Modifications are mostly made to the markable detection component and post-processing for the output coreference links [ 11 ] .\n", 2=>"Modifications are mostly made to the markable detection component and post processing for the output coreference links [ 11 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact \n"}nil_first --> [10]
nil_second --> [10, 11]
--------------------------
{1=>"However , the external coreference tool \" s performance drops for biological texts than for news texts , from 66 .38% to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .\n", 2=>"However , the external coreference tool achieves much lower results on biological texts than that on news texts , from 66 .38% down to 49 .65% in MUC-score [ 11 , 27 ] , which is supposed to be caused by domain differences .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#9#7,8#para -1#22#9#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#16#14,15#para -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact -1#37#35#exact -1#38#36#exact -1#39#37#exact -1#40#38#exact -1#41#39#exact -1#42#40#exact -1#43#41#exact \n"}nil_first --> [6, 10]
nil_second --> [6, 7, 8, 10, 14, 15]
--------------------------
{1=>"A detailed analysis on the _nal submission of the COREF task participants was reported in the organizer 's papers [ 15 , 31 ] , and is summarized in table 2 .\n", 2=>"A detailed analysis on the _nal submissions of the COREF task participants was reported in the organizer 's papers [ 15 , 31 ] , which is summarized in table 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [25]
nil_second --> [25]
--------------------------
{1=>"Examples of the coreference types are outlined below :\n", 2=>"Below are examples of the coreference types .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#1#5#exact -1#0#7#lc \n"}nil_first --> [6, 8]
nil_second --> [7]
--------------------------
{1=>"- \" [ . . . ] the phosphorylation status of [ TRAF2 ] had significant effects on the ability of [ the protein ] to bind to CD40 , \" ( DNP )\n", 2=>"- \" . . . ,the phosphorylation status of [ TRAF2 ] had significant effects on the ability of [ the protein ] to bind to CD40 , \" ( DNP )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#9#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#11#6#exact -1#16#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#19#11#exact -1#10#12#exact -1#22#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#20#18#exact -1#17#19#exact -1#18#20#exact -1#21#22,23#para -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact \n"}nil_first --> [21, 24]
nil_second --> [5]
--------------------------
{1=>"- \" Subnuclear fractionation reveals that there are [ two ATF1 isoforms , which ] appear to differ with respect to DNA binding activity , \" ( RELAT )\n", 2=>"- \" Subnuclear fractionation reveals that there are [ two ATF1 isoforms ] [ which ] appear to differ with respect to DNA binding activity , \" ( RELAT )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#25#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact \n"}nil_first --> [24]
nil_second --> [12, 13]
--------------------------
{1=>"An analysis of the results indicated that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type was 27 .5% F-score and 10 .1 F-score , respectively ; the scores were much lower than the F-score for relative pronouns ( the RELAT type ) , which yielded a 66 .2 % F-score .\n", 2=>"The analysis results in also showed that the best resolution results for definite noun phrases ( the DNP type ) , and several pronouns of the PRON type are 27 .5% F-score and 10 .1 F-score respectively , which are far less than that for relative pronoun ( the RELAT type ) 66 .2 % F-score .\n", 3=>"-1#0,1#0,1#para -1#24#2#exact -1#25#3#exact -1#2#4#exact -1#5,6#5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#48#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#syn -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#37#36#exact -1#36#37#exact -1#39,40#41,42#para -1#41,42#43,44#para -1#55#46#exact -1#44#47#exact -1#45#48#exact -1#46#49#stem -1#47#50#exact -1#49#52#exact -1#50#53#exact -1#51#54#exact -1#38#56#exact -1#52#59#exact -1#53#60#exact -1#54#61#exact -1#56#63#exact \n"}nil_first --> [24, 38, 39, 40, 45, 51, 55, 57, 58, 62]
nil_second --> [3, 4, 43]
--------------------------
{1=>"Thus , it can be inferred that it is more difficult to resolve definite noun phrases and pronouns than relative pronouns .\n", 2=>"Thus , it can be inferred that definite noun phrases and pronouns are more difficult to be resolved than relative pronouns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#12,13#8,9#para -1#14#10#exact -1#15#11#exact -1#16,17#12#para -1#7#13#exact -1#8#14#exact -1#9#15#exact -1#10#16#exact -1#11#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"The top four official results of the COREF shared task are presented in the top four rows of Table 2 .\n", 2=>"The top four official results of the COREF shared task are shown again in the top four rows of Table 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#syn -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact \n"}nil_first --> []
nil_second --> [12]
--------------------------
{1=>"In this paper , we compare the contributions of different features in coreference resolution ; two simple types of domain-portable information : discourse preference and number-agreement , is compared , as well as domain-specific information , which is considered to be more difficult to be transferred across different domains .\n", 2=>"In this paper , we compare the contributions of different features in coreference resolution , two simple types of domain-portable information : discourse preference and number-agreement , and domain-specific information which can be considered as more difficult to be transferred across different domains .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#31,32#27#para -1#14#29#exact -1#34#30#exact -1#28#33#exact -1#29#34#exact -1#30#36#exact -1#38#37#syn -1#33#38#exact -1#37#39#exact -1#35#41#exact -1#36#42#exact -1#39#43,44,45#para -1#40#46#exact -1#41#47#exact -1#42#48#exact -1#43#49#exact \n"}nil_first --> [14, 28, 31, 32, 35, 40]
nil_second --> [27]
--------------------------
{1=>"We implemented a protein coreference system that makes use of syntactic information from the parser output , and protein-indicated information encoded in rule-based semantic classification .\n", 2=>"We implemented a protein coreference system that makes use of syntactic information from parser output , and protein-indicated information encoded in rule-based semantic classification .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [13]
nil_second --> []
--------------------------
{1=>"Experimental results showed that domain-specific semantic information is important for coreference resolution , and that simple semantic classification using semantic features helped our system to outperform the best-reported system results in the shared task .\n", 2=>"Experimental results showed that domain specific semantic information is important for coreference resolution , and that simple semantic classification using semantic features helped our system to outperform the best reported result in the shared task .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#30#29#stem -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> [4, 27, 28]
nil_second --> [4, 5, 28, 29]
--------------------------
{1=>"In order to acquire insight into the problem , we took a rule-based approach , analyzing the training data of BioNLP-ST 2011 Coref task .\n", 2=>"As we needed to get an insight into the problem , we took a rule-based approach , analyzing the training data of BioNLP-ST 2011 Coref task .\n", 3=>"-1#3#2#exact -1#4#3#syn -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact \n"}nil_first --> [0, 1]
nil_second --> [0, 1, 2, 5]
--------------------------
{1=>"The performance of the system evaluated on the official test dataset of the COREF task shows a significant improvement over the official winning system of the task .\n", 2=>"The performance of the system evaluated on the official test data set of the COREF task shows a significant improvement over the official winning system of the task .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact \n"}nil_first --> [10]
nil_second --> [10, 11]
--------------------------
{1=>"Processing of each component is briefly described below .\n", 2=>"Processing of each component is briefly described as below .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#7#exact -1#9#8#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"More details of implementation can be found in the method section .\n", 2=>"More details of implementation can be found in the method section .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6,7#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> []
nil_second --> [8]
--------------------------
{1=>"We used the Genia Sentence Splitter and Enju Parser [ 15 ] for sentence segmentation and syntactic parsing , respectively .\n", 2=>"We used Genia Sentence Splitter and Enju Parser [ 15 ] for the purposes , respectively .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#12#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact \n"}nil_first --> [13, 14, 15, 16, 17]
nil_second --> [13]
--------------------------
{1=>"( Enju parser comes with a default tokenizer and part-of-speech tagger for biological text . ) Row 1 in the example of Table 1 shows three sentences as the output from the Genia Sentence Splitter , and noun phrases as the output from the Enju Parser for the sentence , S3 .\n", 2=>"( Enju parser comes with a default tokenizer and part-of-speech tagger for biological text . ) Row 1 in the example Table 1 shows three sentences outputted from Genia Sentence Splitter , and noun phrases outputted from Enju Parser for the sentence S3 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#40#28#exact -1#26#29#stem -1#27#30#exact -1#28#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#41#stem -1#36#42#exact -1#37#44#exact -1#38#45#exact -1#39#46#exact -1#41#48#exact -1#42#50#exact -1#43#51#exact \n"}nil_first --> [21, 27, 31, 39, 40, 43, 47, 49]
nil_second --> []
--------------------------
{1=>"Due to the limited space , only a part of the phrases are shown in the table .\n", 2=>"Due to the limit of space , only a part of the phrases are shown in the table .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#stem -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#9,10#7,8,9,10#para -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> []
nil_second --> [4, 8, 11]
--------------------------
{1=>"The full parse tree for this sentence is separately shown in Figure 3 .\n", 2=>"The full parse tree of this sentence is separately shown in Figure 3 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"Step 1 - Markable detection : Text chunks that are candidate coreferential expressions , which are also called markables following the jargon of MUC-7 , are collected .\n", 2=>"Step 1 - Markable detection : collects text chunks that are candidate coreferential expressions , which are also called markables following the jargon of MUC-7 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#lc -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#6#26#stem -1#25#27#exact \n"}nil_first --> [24, 25]
nil_second --> []
--------------------------
{1=>"For the set of markables , noun phrases , which do not include a subordinate clause , are collected as they are analyzed by a syntactic parser ( in our case , Enju ) .\n", 2=>"For the set of markables , noun phrases , which do not include subordinate clause , are collected as analyzed by a syntactic parser , Enju in our case .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#21#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#22#exact -1#20#23#exact -1#22#25#exact -1#23#26#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#24#31#exact -1#25#32#exact -1#29#34#exact \n"}nil_first --> [20, 21, 24, 27, 33]
nil_second --> []
--------------------------
{1=>"Then , for chunks that share the same head word , which is normally the main noun of a noun phrase , only the longest chunk is taken .\n", 2=>"Then , for chunks that share the same head word , which is normally the main noun of a noun phrase , only the longest is taken .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [25]
nil_second --> []
--------------------------
{1=>"Since the Enju parser outputs head-word information for every noun phrase , we make use of this information for our processing , without any modification .\n", 2=>"Since the Enju parser output such head-word information for every noun phrase , we make use of this information for our processing without any modification .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#stem -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13,14,15#12,13,14,15#para -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [21]
nil_second --> [5, 16]
--------------------------
{1=>"In the sentence S3 , three noun phrases recognized by the NX and NP tags of the Enju output , role , role for c-Myc in apoptosis , and this role for c-Myc in apoptosis ( Step 0 results ) share the same head-word role ; thus , only the longest noun phrase , this role for c-Myc in apoptosis , is selected .\n", 2=>"In the sentence S3 , three noun phrases recognized by the NX and NP tags of Enju output , role , role for c-Myc in apoptosis , and this role for c-Myc in apoptosis ( Step 0 results ) share the same head word role , thus only the longest one this role for c-Myc in apoptosis is selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#40#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#48#41#exact -1#41#42#exact -1#44#44#exact -1#46#46#exact -1#45#47#exact -1#47#48#exact -1#49#49,50#para -1#43#52#syn -1#51#54#exact -1#52#55#exact -1#53#56#exact -1#54#57#exact -1#55#58#exact -1#56#59#exact -1#57#61#exact -1#58#62#exact -1#59#63#exact \n"}nil_first --> [43, 45, 51, 53, 60]
nil_second --> [42, 50]
--------------------------
{1=>"Step 2 - Anaphor selection : Candidate anaphoric expressions , which are basically pronouns and definite noun phrases , are determined . A minority of anaphors are indefinite noun phrases or entity names , which act as appositions .\n", 2=>"Step 2 - Anaphor selection : determines candidate anaphoric expressions , which are basically pronouns and definite noun phrases ( a minority of anaphors are indefinite noun phrases or entity names , which act as appositions . )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#lc -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#31#18#exact -1#24#19#exact -1#6#20#stem -1#36#21#exact -1#20#22#lc -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact \n"}nil_first --> [26, 33, 38]
nil_second --> [19, 37]
--------------------------
{1=>"We implemented two types of filters : syntactic and semantic .\n", 2=>"We implemented two types of filters : syntactic and semantic filters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"Syntactic filters are used to filter out pleonastic its , or pronouns , like : he , she , which are not expected to refer to proteins .\n", 2=>"Syntactic filters are used to filter out pleonastic its , or pronouns such as he , she , which are not expected to refer to proteins .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#15#12#exact -1#14#15#exact -1#17#16#exact -1#16#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [13, 14, 18]
nil_second --> [12, 13]
--------------------------
{1=>"Moreover , because our task focuses on protein references , semantic filters can be used to filter out non-protein anaphors at this stage .\n", 2=>"Moreover , because the focus of our task is protein references , semantic filters can be used to filter out non-protein anaphors at this stage .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#para -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14,15,16#12,13,14,15#para -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact \n"}nil_first --> [6]
nil_second --> [3, 4, 5, 17]
--------------------------
{1=>"In practice , for definite noun phrase type of anaphors , this is accomplished , by using a list of possible head-words of protein references ; for pronouns , their context words are used .\n", 2=>"In practice , for definite noun phrase type of anaphors , this is done using a list of possible head words of protein references , and for pronouns , their context words are used .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#para -1#24#14#exact -1#14,15#15,16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> [21, 25]
nil_second --> [19, 20, 25]
--------------------------
{1=>"One of the candidates will become the response antecedent , as a result of the antecedent prediction step .\n", 2=>"One of the candidates will become the response antecedent as a result of the antecedent prediction step .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10,11,12,13#10,11,12,13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [9]
nil_second --> [9]
--------------------------
{1=>"In theory , all expressions in the set of markables can become antecedent candidates ; however , too many candidates makes it difficult to achieve correct antecedent prediction .\n", 2=>"In theory , all expressions in the set of markables can become antecedent candidates , however too much candidates makes it difficult to achieve correct antecedent prediction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#14#16#exact -1#16#17#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [14, 18]
nil_second --> [17]
--------------------------
{1=>"In our system , this is done by using a particular window size in sentences , together with several syntactic filters .\n", 2=>"In our system , this is done by using a window size in sentences , together with several syntactic filters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"The idea behind this filter is that some types of syntactic relations imply the impossibility of coreference relations between its argument noun phrases and the inclusive expressions of these noun phrases .\n", 2=>"The idea behind this is that some types of syntactic relations imply the impossibility of coreference relations between its argument noun phrases and the inclusive expressions of these noun phrases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"For example , the two expressions : dominant negative form and its in our example in Table 1 , cannot be coreferential with each other , since they are connected via the preposition of .\n", 2=>"For example , the two expressions dominant negative form and its in our example in Table 1 , can not be coreferential with each other , since they are connected via the preposition of .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18,19#19,20#para -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact \n"}nil_first --> [6]
nil_second --> [20]
--------------------------
{1=>"Step 4 - Antecedent prediction : The best candidate in the antecedent candidate set is selected , and a response coreference link is formed .\n", 2=>"Step 4 - Antecedent predicion : selects the best candidate in the antecedent candidate set , and forms a response coreference link .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#7#6#lc -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#6#15#stem -1#15#16#exact -1#16#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#17#23#stem -1#22#24#exact \n"}nil_first --> [4, 14, 22]
nil_second --> [4]
--------------------------
{1=>"-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate , which does not conflict in number with the anaphor , is selected .\n", 2=>"-Rule 1 ( Number agreement - NUM-AGREE ) : The candidate which is not number conflict with anaphor is selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#13#13,14#para -1#15#15#exact -1#14#17#exact -1#16#18#exact -1#17#20#exact -1#12#22#exact -1#19#23#exact -1#20#24#exact \n"}nil_first --> [11, 16, 19, 21]
nil_second --> [18]
--------------------------
{1=>"-Rule 2 ( Semantic constraint - SEM-CONS ) : If the anaphor is a protein reference , then a protein candidate is selected .\n", 2=>"-Rule 2 ( Semantic constraint - SEM-CONS ) : If anaphor is a protein reference , then protein candidate is selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [10, 18]
nil_second --> []
--------------------------
{1=>"The rules are implemented using different features of expressions , such as syntactic types of expressions , head noun , semantic types , etc. , in a similar way to [ 22 ] .\n", 2=>"The rules are implemented using different features of expressions such as syntactic types of expression , head noun , semantic types , etc. , in a similar way to [ 22 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#15#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#stem -1#18#16#exact -1#16#17#exact -1#17#18#exact -1#21#19#exact -1#19#20#exact -1#20#21#exact -1#23#22#exact -1#22#23#exact -1#24,25,26,27,28#25,26,27,28#para -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact \n"}nil_first --> [24, 29]
nil_second --> []
--------------------------
{1=>"Each rule in the decision list compares two candidates , and returns the preferable candidate in concern with the anaphor .\n", 2=>"Each rule in the decision list compares two candidates , and returns the preferrable candidate in concern with the anaphor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [13]
nil_second --> [13]
--------------------------
{1=>"Because of this particular rule , the decision list never results in the equility result .\n", 2=>"Thanks to this rule , the decision list never results in the equility result .\n", 3=>"-1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [0, 1, 3]
nil_second --> [0, 1]
--------------------------
{1=>"In this way , candidates can be sorted , and the best candidate is selected as the antecedent .\n", 2=>"By this way , candidates can be sorted , and the best candidate is selected as antecedent .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [0, 16]
nil_second --> [0]
--------------------------
{1=>"Figure 4 illustrates how the decision list works when comparing two candidates : and .\n", 2=>"Figure 4 illustrates how the decision list works when comparing two candidates and .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"In this step , we want to filter out those pronouns and definite noun phrases that are not a target of this task . The expressions are comprised of two types : non-anaphoric expressions , and anaphoric expressions , which do not point to proteins .\n", 2=>"In this step , we want to filter out those pronouns and definite noun phrases that are not target of this task , comprised of two types : non-anaphoric expressions , and anaphoric expressions which do not point to proteins .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18,19#para -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#40#23#exact -1#29#24,25#para -1#35,36#26#para -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact -1#28#32#exact -1#33#33#exact -1#22#34#exact -1#31#35#exact -1#32#36#exact -1#30#38#exact -1#34#39#exact -1#37#41,42#para -1#38#43#exact -1#39#44#exact \n"}nil_first --> [37, 40, 45]
nil_second --> []
--------------------------
{1=>"The term anaphoric is used with the common sense in the NLP community .\n", 2=>"The term anaphoric is used with the common sense in NLP community .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"Anaphoric expression refers to an expression that has a noun phrase as an antecedent .\n", 2=>"Anaphoric expression means an expression that has a noun phrase as antecedent .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact \n"}nil_first --> [2, 3, 12]
nil_second --> [2]
--------------------------
{1=>"Thus , expressions with a sentence or phrase antecedents , or nominal but successive antecedents , are not our target and should be filtered out .\n", 2=>"This means expressions with a sentence or phrase antecedents , or nominal but successive antecedents , are not our target and should be filtered out .\n", 3=>"-1#9#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#15#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [0, 15]
nil_second --> [0, 1]
--------------------------
{1=>"Non-anaphoric expressions include first and second-person pronouns such as I , we , you , and pleonastic it .\n", 2=>"Non-anaphoric expressions includes first and second person pronouns such as I , we , you , . . . , and pleonastic it .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#stem -1#3#3#exact -1#4#4#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#20#15#exact -1#21#16#exact -1#22#17#exact -1#23#18#exact \n"}nil_first --> [5]
nil_second --> [5, 6, 16, 17, 18, 19]
--------------------------
{1=>"First and second-person pronouns are easily recognized by the part-of-speech tags ; thus , we use part-of-speech information for the filtering .\n", 2=>"First and second person pronouns are easily to be recognized by the part-of-speech tags , thus we use part-of-speech information for the filtering .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#15#12#exact -1#14#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact \n"}nil_first --> [2, 11]
nil_second --> [2, 3, 7, 8]
--------------------------
{1=>"For pleonastic it , we make use of the following four patterns , which are similar to [ 13 ] :\n", 2=>"For pleonastic it , we make use of the following four patterns , which are similar to [ 13 ]\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6#4,5,6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14,15,16#13,14,15#para -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [16, 20]
nil_second --> [7]
--------------------------
{1=>"Basically , all of the expressions detected in the initial expression set are an antecedent candidate , with the exception of anaphoric pronouns .\n", 2=>"Basically all expressions detected in the initial expression set are antecedent candidate , except for anaphoric pronouns .\n", 3=>"-1#0#0#exact -1#12#1#exact -1#1#2#exact -1#5#4#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#6#8,9#para -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#14#exact -1#11#15#exact -1#13,14#17,18,19,20#para -1#15#21#exact -1#16#22#exact -1#17#23#exact \n"}nil_first --> [3, 13, 16]
nil_second --> []
--------------------------
{1=>"Therefore , candidates that are not probable to be an antecedent of the anaphor should be filtered out .\n", 2=>"Therefore , candidates that are not probable to be antecedent of the anaphor should be filtered out .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"Window size Borders are set to include or exclude antecedent candidates .\n", 2=>"Window size sets a border to include or exclude antecedent candidates .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#2,3#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [4]
nil_second --> [2, 3]
--------------------------
{1=>"This is a common method for antecedent candidate filtering , as seen in the previous work [ 3 , 5 , 26 ] .\n", 2=>"This is a common method for antecedent candidate filtering having been used in the previous work [ 3 , 5 , 26 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#18#9#exact -1#10#11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#20#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact \n"}nil_first --> [10, 20]
nil_second --> [9, 11]
--------------------------
{1=>"Since our task focuses on anaphoric coreference , antecedent expressions normally appear not too far ( in sentence distance ) from the anaphors . Thus , using window sizes is a proper technique .\n", 2=>"Since our task focuses on anaphoric coreference , antecedent expressions normally appear not too far ( in sentence distance ) from the anaphors , using window sizes is a proper technique .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#31#23#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}nil_first --> [24, 33]
nil_second --> []
--------------------------
{1=>"Syntactic dependency relations Since arguments of some dependency relations ( such as poss-arg12 and prep-arg12 ) do not corefer with each other , they can be used to correctly eliminate the number of antecedent candidates .\n", 2=>"Syntactic dependency relations The fact that arguments of some dependency relations such as poss-arg12 and prep-arg12 do not corefer with each other enables us to use them to correctly eliminate the number of antecedent candidates .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#25#23,24,25,26#para -1#24#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> [3, 9, 15, 22]
nil_second --> [3, 4, 5, 22, 23, 26, 27]
--------------------------
{1=>"For instance , two such truncated forms definitely cannot be an antecedent of the protein in this context : two such truncated forms of the protein .\n", 2=>"For instance , two such truncated forms definitely cannot be antecedent of the protein in this context two such truncated forms of the protein\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [10, 18, 26]
nil_second --> []
--------------------------
{1=>"The relative pronoun can be said to be the easiest type of coreference resolution , because its antecedent expression is very close to the anaphor , and in many cases , it is right before the anaphor .\n", 2=>"The relative pronoun can be said to be the easiest type of coreference resolution , because its antecedent expression is very close to the anaphor , and in many cases , it is right before the anaphor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5,6#3,4,5,6#para -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact \n"}nil_first --> []
nil_second --> [3, 4]
--------------------------
{1=>"Our system accomplishes this task .\n", 2=>"This is exactly what our system does .\n", 3=>"-1#4#0#lc -1#5#1#exact -1#1#2#para -1#0#3#lc -1#7#5#exact \n"}nil_first --> [4]
nil_second --> [2, 3, 6]
--------------------------
{1=>"However , a disadvantage to using this method is that when the parser makes a mistake on finding the correct arguments , the coreference also fails . This is exemplified in the following : \" . . .of transcription factor NF-kappa B also encodes a p70 I kappa B protein , I kappa B gamma , which is identical to the C-terminal 607 amino acids of . . . \"\n", 2=>"However , a disadvantage of this method is when the parser makes mistake on finding the correct arguments , coreference also fails , as in the example \" . . .of transcription factor NF-kappa B also encodes a p70 I kappa B protein , I kappa B gamma , which is identical to the C-terminal 607 amino acids of . . . \"\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#52#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#37#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#25#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#28#26#exact -1#50#28#exact -1#24#30#exact -1#53#31#exact -1#27#34#exact -1#59#35#exact -1#60#36#exact -1#30#37#exact -1#31#38#exact -1#32#39#exact -1#33#40#exact -1#34#41#exact -1#35#42#exact -1#36#43#exact -1#38#45#exact -1#39#46#exact -1#40#47#exact -1#41#48#exact -1#42#49#exact -1#43#50#exact -1#44#51#exact -1#45#52#exact -1#46#53#exact -1#47#54#exact -1#48#55#exact -1#49#56#exact -1#51#58#exact -1#54#61#exact -1#55#62#exact -1#56#63#exact -1#57#64#exact -1#58#65#exact -1#29#66#exact -1#61#67#exact -1#62#69#exact \n"}nil_first --> [5, 9, 27, 29, 32, 33, 44, 57, 59, 60, 68]
nil_second --> [4, 22, 23, 26]
--------------------------
{1=>"If one candidate satisfies and the other does not , the procedure ends with the result that the former will be preferable over the latter .\n", 2=>"If one candidate satisfies and the other does not , the procedure ends with the result that the former will be preferable to the latter .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> [22]
nil_second --> [22]
--------------------------
{1=>"The rules are applied in a successive order , one after another , until the inequality occurs , or until the end-of-the-rule list is reached .\n", 2=>"The rules are applied in a succession order one after another until the inequality occurs , or end of the rule list is reached .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#15#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#16#18#exact -1#19#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [12, 17, 19, 21]
nil_second --> [17, 18, 20]
--------------------------
{1=>"The default rule of the procedure , is in the preference of the closer antecedent candidate .\n", 2=>"The default rule of the procedure prefers the closer antecedent candidate .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#9#exact -1#6#10#stem -1#8#12,13#para -1#9#14#exact -1#10#15#exact -1#11#16#exact \n"}nil_first --> [6, 7, 8, 11]
nil_second --> []
--------------------------
{1=>"By definition , two coreferential expressions are identical , which implies a semantic-constraint on coreference relationship .\n", 2=>"By definition , two coreferential expressions refer to the same thing , which implies a semantic-constraint on coreference relationship .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8,9#6,7#para -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact \n"}nil_first --> []
nil_second --> [6, 7, 10]
--------------------------
{1=>"In practice , this compatibility is checked based on a given taxonomy of semantic classes in the following manner : two semantic classes are considered compatible or agreed with each other , when they have a synonym relation , or hypernym-hyponym relation .\n", 2=>"In practice , this compatibility is checked based on a given taxonomy of semantic classes in the following manner : two semantic classes are considered compatible or agreed with each other , when they have synonym relation , e.g. , or hypernym-hyponym relation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32,33,34#32,33,34,35#para -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#40#39#exact -1#41#40#exact -1#42#41#exact -1#43#42#exact \n"}nil_first --> []
nil_second --> [38, 39]
--------------------------
{1=>"In this work , we only focus on the Protein type , ignoring other possible semantic types , so the structure of the taxonomy is not taken into account .\n", 2=>"In this work , we only focus on the Protein type , ignoring other possible semantic types , so we do not take the structure of taxonomy into account .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#24,25#19,20,21,22#para -1#26#23#exact -1#21#24,25#para -1#22#26,27#para -1#28#28#exact -1#29#29#exact \n"}nil_first --> []
nil_second --> [19, 20, 23, 27]
--------------------------
{1=>"Therefore , the likelihood that two expressions are semantically compatible , is definitely beneficial for antecedent prediction , besides syntactic information .\n", 2=>"Thus , the likelihood that two expressions are semantically compatible is definitely beneficial for antecedent prediction , besides syntactic information .\n", 3=>"-1#0#0#syn -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#16#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Focusing on specific entity types , i.e. , Protein type , enables us to find a proper method for determining the likelihood , and method for encoding the likelihood in coreference resolution .\n", 2=>"Focusing on specific entity types , i.e. Protein type , helps us to find a proper method for determining the likelihood , and how to encode the likelihood in coreference resolution .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#7#exact -1#7#8#exact -1#8#9#exact -1#21#10#exact -1#11,12#11,12#para -1#13,14#13,14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#25#26#stem -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [22, 24, 25]
nil_second --> [10, 23, 24]
--------------------------
{1=>"Since gold protein annotations are given , we can use them in combination with syntactic information to judge whether an expression is a protein-referential expression or not .\n", 2=>"Since gold protein annotations are given , we can use them in combination with syntactic information to judge whether an expression is protein-referential expession or not .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [22, 24]
nil_second --> [23]
--------------------------
{1=>"If an expression is a noun phrase with a single head word , and it contains a protein mention that completely overlaps with the head word , then the expression is classified as Protein .\n", 2=>"In details , if an expression is a noun phrase with a single head word , and it contains a protein mention that completely overlaps with the head word , then the expression is classied as Protein .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#20#17#exact -1#21#18#exact -1#22#19#exact -1#23#20#exact -1#24#21#exact -1#25#22#exact -1#26#23#exact -1#27#24#exact -1#28#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact -1#32#29#exact -1#33#30#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact \n"}nil_first --> [31]
nil_second --> [0, 1, 2, 34]
--------------------------
{1=>"In another case , when the head noun is either protein or gene , and has a protein mention as its premodifier , such as the Tax protein .\n", 2=>"Another case is when the head noun is either protein or gene , and has a protein mention as its premodifier , such as the Tax protein .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#12#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#21#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact \n"}nil_first --> [0, 22]
nil_second --> [2]
--------------------------
{1=>"For a coordinated noun phrase , if one of its constituents is classified as a Protein , then that noun phrase is also classified as a Protein .\n", 2=>"For a coordinated noun phrase , if one of its constituents is classified as Protein , then that noun phrase is also classified as Protein .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [14, 25]
nil_second --> []
--------------------------
{1=>"Pronouns , in particular , possessive pronouns , occupy the majority of anaphoric pronouns in biological texts ( Table 5 ) .\n", 2=>"Pronouns , in particular , possessive pronouns occupy the majority of anaphoric pronouns in biological texts ( Table 5 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"However , they do not contain very much useful information for the resolution ; thus , we need to exploit more information from its context [ 17 ] .\n", 2=>"However , they do not contain in themselves much useful information for the resolution , thus we need to exploit more information from its context [ 17 ] .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#8#6,7#para -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#15#14#exact -1#14#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [13]
nil_second --> [6, 7]
--------------------------
{1=>"Fortunately , the key to this problem lies in the context of pronouns .\n", 2=>"Fortunately , the key to this problem lies in the context of pronouns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"We implemented a simple function to classify the semantic type of a possessive pronoun , based on its context word .\n", 2=>"We implemented a simple function to classify the semantic type of a possessive pronoun based on its context word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"In particular , we check the noun phrase in which the determiner is its or their ; if the noun phrase contains a protein key word , then the inclusive pronoun is classified into the Protein semantic type .\n", 2=>"In particular , we check the noun phrase whose determiner is its or their ; if the noun phrase contains a protein key word then the inclusive pronoun is classified into the Protein semantic type .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#para -1#16#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#25#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#27#exact -1#31#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#32#34,35#para -1#33#36#exact -1#34#37#exact -1#35#38#exact \n"}nil_first --> [8, 26]
nil_second --> []
--------------------------
{1=>"Protein key words can be a verb , a noun or an adjective that co-occurred with protein mentions , and can be used as a clue to distinguish the protein type from other semantic types .\n", 2=>"protein key words can be a verb , a noun or an adjective that coocurred with protein mentions and can be used as a clue to distinguish the protein type from other semantic types .\n", 3=>"-1#0#0#lc -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19,20,21,22,23#20,21,22,23#para -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [14, 18, 24]
nil_second --> [14]
--------------------------
{1=>"For example , the word binding in the following noun phrases : its heterodimeric binding partner , or its binding site , is a clue to infer that it must be a protein reference .\n", 2=>"For example , the word binding in the following noun phrases its heterodimeric binding partner , or its binding site is a good clue to infer that it must be a protein reference .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [11, 21]
nil_second --> [22]
--------------------------
{1=>"For our preliminary experiment , we collect these keywords manually by checking the noun phrases containing its and their in the training data .\n", 2=>"For our preliminary experiment , we collect these key words manually by checking the noun phrases containing its and their in training data .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20,21#para -1#22#22#exact -1#23#23#exact \n"}nil_first --> [8]
nil_second --> [8, 9]
--------------------------
{1=>"Our final protein keyword set includes 12 words : binding , expression , interaction , regulation , phosphatase activity , localization , gene , sequence , region , phosphorylation , transactivation , and transcription .\n", 2=>"Our final protein key word set includes 12 words : binding , expression , interaction , regulation , phosphatase activity , localization , gene , sequence , region , phosphorylation , transactivation , and transcription .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact \n"}nil_first --> [3]
nil_second --> [3, 4]
--------------------------
{1=>"Coreferential definite noun phrases in text are used to include a broader definition of coreference .\n", 2=>"Coreferential definite noun phrases in text are used in broader meaning of coreference .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#10,11#para -1#11#13#exact -1#12#14#exact -1#13#15#exact \n"}nil_first --> [9, 12]
nil_second --> [10]
--------------------------
{1=>"In other words , their antecedents do not necessarily exist in the textual context ; in particular , in biomedical scientific papers , many definite noun phrases do not have antecedents , since the referenced concepts can include any concept that is understood by subject matter experts in the domain .\n", 2=>"In other words , their antecedents do not necessarily exist in the textual context ; in particular in biomedical scientific papers , many definite noun phrases do not have antecedents since the referred concepts can be anything understood by experts in the domain .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#21#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#32#exact -1#31#33#exact -1#33#35#exact -1#34#36#exact -1#35#41#syn -1#37#42#exact -1#38#43#exact -1#39#46#exact -1#40#47#exact -1#41#48#exact -1#42#49#exact -1#43#50#exact \n"}nil_first --> [22, 31, 34, 37, 38, 39, 40, 44, 45]
nil_second --> [32, 36]
--------------------------
{1=>"Knowing their semantic type helps to filter out irrelevant candidate antecedents , thereby increasing the chance of picking up the right antecedent , and increasing the precision of antecedent prediction .\n", 2=>"Knowing their semantic type helps to filter out irrelevant candidate antecedents , increasing chance to pick up the right antecedent or the precision of antecedent prediction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12,13#para -1#17#14#exact -1#13#15#exact -1#23#16#exact -1#15#17#stem -1#16#18#exact -1#21#19#exact -1#18#20#exact -1#19#21#exact -1#22#26#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact \n"}nil_first --> [22, 23, 24, 25, 27]
nil_second --> [14, 20]
--------------------------
{1=>"We tested two different head word lists : one is built automatically from the gold anaphoric nominals in gold data ; the other word list contains the top seven common head words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .\n", 2=>"We tested two different head word lists : one is built automatically from the gold anaphoric nominals in gold data , the other word list contains top seven common head words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact \n"}nil_first --> [20, 26]
nil_second --> [20]
--------------------------
{1=>"First , in anaphor selection , semantic information can be used to filter out non-protein anaphoric expressions .\n", 2=>"First , in anaphor selection , semantic information can be used to filter out non-protein anaphoric expressions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"For example , if an anaphor is classified as a protein referent , then the non-protein antecedent candidates are removed from the candidate set of the anaphor .\n", 2=>"For example , if anaphor is classified as protein referent , then non-protein antecedent candidates are removed from the candidate set of the anaphor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#18#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#22#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#23#26#exact -1#24#27#exact \n"}nil_first --> [4, 9, 25]
nil_second --> []
--------------------------
{1=>"Our minimal system configuration includes all of the processing and filters from step 0 to step 3 , as explained in the section above ( RB-MIN ) .\n", 2=>"Our minimal system configuration includes all the processing and filters from step 0 to step 3 as explained in the above section ( RB-MIN ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5,6#4,5,6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#21#22#exact -1#20#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"For antecedent candidate selection , the window size used in step 4 is set to 2 , which means that antecedent candidates are collected in the two nearest sentences from the anaphor , and the sentence embedding the anaphor .\n", 2=>"For antecedent candidate selection , the window size used in step 4 is set to 2 , which means antecedent candidates are collected in the two nearest sentences from the anaphor , and the sentence embedding the anaphor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"The statistics measured on the training set of the corpus shows that 97 .0% percent of protein coreference links have antecedents appearing in within 2 sentences .\n", 2=>"As the statistics measured on the training set of the corpus shows that 97 .0% percent of protein coreference links have antecedents appearing in within 2 sentences .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"The word list used to filter out anaphoric definite noun phrases in step 2 contains the following words : protein , gene , factor , molecule , element , family , inhibitor , and receptor .\n", 2=>"The word list used to filter out anaphoric definite noun phrases in step 2 contains the following words : protein , gene , factor , molecule , element ,family , inhibitor , and receptor .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#28#exact -1#31#30#exact -1#30#31#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [29, 32]
nil_second --> [28]
--------------------------
{1=>"Premodifiers of definite noun phrases are also limited to numbers and popular premodifiers of proteins , such as nuclear , and transcription .\n", 2=>"Besides , premodifiers of definite noun phrases are also limited to numbers and popular premodifiers of proteins such as nuclear , transcription .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#1#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> [20]
nil_second --> [0]
--------------------------
{1=>"To keep the minimal configuration simple , step 4 - antecedent selection of the baseline only uses the default comparison rule , which assures that the closest antecedent candidate is selected .\n", 2=>"To keep the minimal configuration simple , step 4 - antecedent selection of the baseline only uses the default comparison rule , which assures the closest antecedent candidate is selected .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact \n"}nil_first --> [24]
nil_second --> []
--------------------------
{1=>"This primary evaluation method , which was particularly designed for the shared task , is based on protein coreference links that have been automatically generated from manually annotated coreference links .\n", 2=>"This primary evaluation method , which was particularly designed for the shared task , is based on protein coreference links automatically generated from manually annotated coreference links .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact \n"}nil_first --> [20, 21, 22]
nil_second --> []
--------------------------
{1=>"Note that the results from RB-MIN with minimal configuration , already surpasses the best results obtained by the UU team , with up to 7 .1% higher performance in F-score .\n", 2=>"Note that RB-MIN with minimal configuration already outperforms the best result by the UU team , with up to 7 .1% higher in Fscore .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#12#2#exact -1#10#3#stem -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#15#9#exact -1#6#10#exact -1#7#11#syn -1#8#12#exact -1#9#13#exact -1#11#16#exact -1#13#18#exact -1#14#19#exact -1#16#21#exact -1#17#22#exact -1#18#23#exact -1#19#24#exact -1#20#25#exact -1#21#26#exact -1#22#28#exact -1#24#30#exact \n"}nil_first --> [4, 14, 15, 17, 20, 27, 29]
nil_second --> [23]
--------------------------
{1=>"Since RB-MIN uses similar preprocessing tools as UU [ 11 ] , but less information in antecedent prediction , this gap in performance is likely caused by the different markable detection methods .\n", 2=>"Since RB-MIN uses similar preprocessing tools as UU [ 11 ] , but less information in antecedent prediction , this gap in performance is supposed to be caused by the different markable detection methods .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#26,27,28,29#25,26,27#para -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact \n"}nil_first --> [24]
nil_second --> [24, 25]
--------------------------
{1=>"Breaking down the system performance by the different types of anaphors provides us with insight into what has been accomplished / solved by our methods , and also provides us with improvement opportunities .\n", 2=>"Breaking down the system performance by types of anaphors gives us an insight into what have been solved by our methods , and what needs more improvement effort .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7,8,9#para -1#8#10#exact -1#9,10#11,12#para -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15,16#17,18#para -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#26#31#exact -1#28#33#exact \n"}nil_first --> [13, 19, 20, 27, 28, 29, 30, 32]
nil_second --> [11, 23, 24, 25, 27]
--------------------------
{1=>"Concerning the RELAT type of coreference , we can see that RB-MIN and RB-FULL both achieve comparable results with the best team in BioNLP-ST 2011 .\n", 2=>"Concerning the RELAT type of coreference , we can see that RB-MIN and RB-FULL both achieve comparable results with the best team in BioNLP-ST 2011 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8,9#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"However , it should be noted that our antecedent prediction for the RELAT type is based solely on the output of the Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See Discussions section ) .\n", 2=>"However , it should be noted that our antecedent prediction for the RELAT type is based completely on the output of Enju parser for the RELAT type , so in order to improve this type of coreference , we have to find ways to overcome the parse errors on noun phrase boundary detection and relative clause attachment ( See section Discussions ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3,4,5#2,3,4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#24#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#45#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#31,32#30,31,32,33#para -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38,39#39,40,41,42#para -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#55#56#exact -1#56#57#exact -1#57#58#exact -1#58#59#exact -1#60#60#exact -1#59#61#exact -1#61#62#exact -1#62#63#exact \n"}nil_first --> [16, 46]
nil_second --> [2, 16, 29, 30, 40, 41]
--------------------------
{1=>"On the other hand , the major contribution to the improvement of DNP resolution is from rule 2 .\n", 2=>"On the other hand , the major contribution to the improvement of DNP resolution is from rule 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"The analyses of the results are provided in the section entitled Discussions .\n", 2=>"The analysis results are given in section Discussions .\n", 3=>"-1#0#0#exact -1#1#1,2,3#para -1#2#4#exact -1#3#5#exact -1#5#7#exact -1#6#8,9#para -1#7#11#exact -1#8#12#exact \n"}nil_first --> [6, 10]
nil_second --> [4]
--------------------------
{1=>"The combination of rule 1 , 2 and 3 resulted in a 62 .4% F-score ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contributes to a 4-point F-score increase in the development set , and 2 .3-point F-score increase on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .\n", 2=>"The combination of rule 1 , 2 and 3 resulted in 62 .4% fscore ( RB-MIN+1 , 2 , 3 ) ( Table 3 ) In this configuration , rule 2 contribute to the increasement of 4 points Fscore on the development set , and 2 .3 points Fscore on the test set , when comparing RB-MIN+1 , 3 and RB-MIN+1 , 2 , 3 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31,32#32,33#para -1#34#37#stem -1#39,40,41#38,39,40#para -1#42#41#exact -1#43#42#exact -1#44#43#exact -1#45#44#exact -1#49#48#exact -1#50#49#exact -1#51#50#exact -1#52#51#exact -1#53#52#exact -1#54#53#exact -1#55#54#exact -1#56#55#exact -1#57#56#exact -1#58#57#exact -1#59#58#exact -1#60#59#exact -1#61#60#exact -1#62#61#exact -1#63#62#exact -1#64#63#exact -1#65#64#exact \n"}nil_first --> [11, 14, 34, 35, 36, 45, 46, 47]
nil_second --> [13, 33, 35, 36, 37, 38, 46, 47, 48]
--------------------------
{1=>"However , the result of RB-MIN is still more than 7 points higher than in state-of-the-art performance .\n", 2=>"However , the result of RB-MIN is more than still 7 points higher than the state-of-the-art performance .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#9#7#exact -1#7#8#exact -1#8#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [14]
nil_second --> [14]
--------------------------
{1=>"This gain is due to the fact that the rule ensures that the semantic type of antecedents is the same as for their anaphors , thus enabling the correct detection of antecedents .\n", 2=>"This gain is due to the fact that the rule ensures the semantic type of antecedents is the same as their anaphors , enabling the correct detection of antecedents .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4#2,3,4,5#para -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#5#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16,17,18,19#17,18,19#para -1#20#21,22#para -1#21#23#exact -1#22#24#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact \n"}nil_first --> [11, 20, 25]
nil_second --> [11]
--------------------------
{1=>"In other words , if an anaphor is classified as a protein reference , then the antecedent must also be a protein reference .\n", 2=>"In other words , if anaphor is classified as a protein reference , then antecedent must also be a protein reference .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact \n"}nil_first --> [5, 15]
nil_second --> []
--------------------------
{1=>"( Coreference examples in this paper are represented in the following manner : gold anaphoric and antecedent expressions are bracketed , antecedents before anaphors ; gold protein mentions are underlined ; and incorrect response antecedents are in italics . )\n", 2=>"( Coreference examples in this paper are represented as below : gold anaphoric and antecedent expressions are bracketed , antecedents before anaphors ; gold protein mentions are underlined ; and incorrect response antecedents are in italics . )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#34#8#exact -1#16#10#syn -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#26#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#33#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact \n"}nil_first --> [9, 11, 35, 36]
nil_second --> [8, 9]
--------------------------
{1=>"- \" Therefore , [ IRF-1 ] may be an important contributor to IL-12 signaling , and we speculate that the defective IL-12 responses seen in IRF-1- / - mice might be attributable , in part , to the absence of [ this transcription factor ] . \" ( PMID-10358173 )\n", 2=>"- \" Therefore , [ IRF-1 ] may be an important contributor to IL-12 signaling , and we speculate that the defective IL-12 responses seen in IRF-1- / - mice might be attributable , in part , to the absence of [ this transcription factor ] . \" ( PMID-10358173 )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10,11,12#9,10,11,12#para -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"Meanwhile , since this transcription factor is recognized as a protein reference , its closest protein antecedent IRF-1 was successfully detected by RB-FULL .\n", 2=>"Meanwhile since this transcription factor is recognized as a protein reference , its closest protein antecedent IRF-1 was successfully detected by RB-FULL .\n", 3=>"-1#0#0#exact -1#11#1#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact \n"}nil_first --> [12]
nil_second --> []
--------------------------
{1=>"Another example is :\n", 2=>"Another interesting example is\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact \n"}nil_first --> [3]
nil_second --> [1]
--------------------------
{1=>"Although studies and a dominant negative form of its heterodimeric binding partner have been crossed out because of disagreement in numbers , and violation of abandoned syntactic constraints , correspondingly , the system would return the incorrect antecedent apoptosis instead of c-Myc .\n", 2=>"Although studies and a dominant negative form of its heterodimeric binding partner have been crossed out because of disagreement in numbers , and violation of abandoned syntactic constraints correspondingly , the system would return the incorrect antecedent apoptosis instead of c-Myc .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#28#exact -1#28#29#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact \n"}nil_first --> [30]
nil_second --> []
--------------------------
{1=>"In our system , domain-specific semantic information is utilized in two places : anaphor selection and antecedent prediction .\n", 2=>"In our system , domain-specific semantic information is ultilized at two places : anaphor selection and antecedent prediction .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9,10#9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"To classify anaphors into protein or non-protein reference , our system employs a head-word based classifier for definite noun phrases , DEFNP-ANA-SEM , and a context-based classifier for pronouns , PRO-ANA-SEM ( Section Methods ) .\n", 2=>"To classify anaphors into protein or non-protein reference , our system employs a head-word based classfier for definite noun phrases , DEFNP-ANA-SEM , and a context-based classifier for pronouns , PRO-ANA-SEM ( Section Methods ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#26#15#exact -1#27#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#16#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> [26]
nil_second --> [15]
--------------------------
{1=>"Without limiting the number of anaphors by using semantic information-based filtering , the precision significantly drops , causing a big decrease in the F-score ( Table 4 , RB-FULL without DEFNP-ANA-SEM ) .\n", 2=>"Without limiting the number of anaphors by using semantic information-based filtering , the precision significantly drops , causing a big decrease in Fscore ( Table 4 , RB-FULL w / o DEFNP-ANA-SEM ) . .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact \n"}nil_first --> [22, 23, 29]
nil_second --> [22, 28, 29, 30, 34]
--------------------------
{1=>"This decrease is due to the fact that the semantic filter is the only way to filter out definite noun phrase anaphors .\n", 2=>"This is because the semantic filter is the only way to filter out definite noun phrase anaphors .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3,4,5,6,7#para -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6,7,8,9,10#11,12,13,14#para -1#11#16#exact -1#12#17#exact -1#13#18#exact -1#14#19#exact -1#15#20#exact -1#16#21#exact -1#17#22#exact \n"}nil_first --> [1, 15]
nil_second --> []
--------------------------
{1=>"Without the filter , all definite expressions , which include a huge amount of non-anaphoric expressions , are considered as anaphors .\n", 2=>"Without the filter , all definite expressions , which include a huge amount of non-anaphoric expressions , are considered as anaphors .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11,12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> []
nil_second --> [12, 13]
--------------------------
{1=>"In our system , contextual information of possessive pronouns is utilized through the protein key words ( Section Methods ) , and this contributed to a 1 .8% gain in F-score ( Table 4 , RB-FULL without PRO-ANA-SEM ) .\n", 2=>"In our system , contextual information of possessive pronouns is utilized through the protein key words ( Section Methods ) , and this contributed to 1 .8% gain in f-score ( Table 4 , RB-FULL w / o PRO-ANA-SEM ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25,26#para -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#lc -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact \n"}nil_first --> [36]
nil_second --> [35, 36, 37]
--------------------------
{1=>"This gain is a good indication for seeking a systematic method to develop and include such contextual information in coreference resolution .\n", 2=>"This is an encouraging sign to seek for a systematic method to exploit and include such contextual information in coreference resolution .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#8#3#exact -1#7#6#exact -1#9#8,9#para -1#10#10#exact -1#11#11#exact -1#12,13#12,13#para -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [1, 4, 5, 7]
nil_second --> [2, 3, 4, 5, 6]
--------------------------
{1=>"Examples showing the effectiveness of semantic information from the context of pronouns is provided below :\n", 2=>"Below are the examples showing the effectiveness of semantic information from the context of pronouns .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#1#12#syn -1#0#14#lc \n"}nil_first --> [13, 15]
nil_second --> [2, 15]
--------------------------
{1=>"In all the examples above , the appearance of words such as binding , transactivation , DNA target sequence in the noun phrases for which the anaphor plays a role as a determiner , is a contextual indicator for the protein type .\n", 2=>"In all the above examples , the appearance of words such as binding , transactivation , DNA target sequence in the noun phrases of which the anaphor plays a role as a determiner , is contextual indicator for the protein type .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#3#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23,24#23,24#para -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact \n"}nil_first --> [35]
nil_second --> []
--------------------------
{1=>"However , in the data , we found several coreferential expressions that violate this constraint .\n", 2=>"However , we found in the data several coreferential expressions violating this constraint .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#4#2#exact -1#5#3#exact -1#6#4#exact -1#2#6#exact -1#3#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"The anaphor and antecedent in the following is an instance of this violation :\n", 2=>"For instance , the anaphor and antecedent in the following :\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#1#9#exact -1#10#13#exact \n"}nil_first --> [7, 8, 10, 11, 12]
nil_second --> [0, 2]
--------------------------
{1=>"Therefore , when the proteins appear in premodifiers or postmodifers of noun phrases as [ cDNAs encoding EBF or a covalent homodimer of E47 ] in this example :\n", 2=>"Therefore , when the proteins appear in premodifiers or postmodifers of noun phrases as [ cDNAs encoding EBF or a covalent homodimer of E47 ] in this example\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact \n"}nil_first --> [28]
nil_second --> []
--------------------------
{1=>"- \" With the aim of identifying genetic targets for these transcription factors , we stably transfected [ cDNAs encoding EBF or a covalent homodimer of E47 ] , individually or together , into immature hematopoietic Ba / F3 cells , which lack [ both factors ] . \" ( PMID-9252117 )\n", 2=>"- \" With the aim of identifying genetic targets for these transcription factors , we stably transfected [ cDNAs encoding EBF or a covalent homodimer of E47 ] , individually or together , into immature hematopoietic Ba / F3 cells , which lack [ both factors ] . \" ( PMID-9252117 )\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5#2,3,4#para -1#25#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#40#exact -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact -1#46#46#exact -1#47#47#exact -1#48#48#exact -1#49#49#exact -1#50#50#exact -1#51#51#exact \n"}nil_first --> [25]
nil_second --> []
--------------------------
{1=>"In future , revision of corpus annotation and evaluation schemes would benefit the ease of automation of coreference resolution .\n", 2=>"In furture , corpus annotation and evaluation scheme should be revised for the ease of automation of coreference resolution .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#9,10#3#para -1#14#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#stem -1#8#10#para -1#12#12#exact -1#13#13#exact -1#16#14#exact -1#15#15#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [1, 11, 16]
nil_second --> [1, 11]
--------------------------
{1=>"Parse error Coreference expression boundary is determined mostly based on noun phrase boundary output from the parser .\n", 2=>"Parse error Coreference expression boundary is determined mostly based on noun phrase boundary output from parser .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"The following example shows a coordination-structured antecedent AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 that failed to be detected by the parser .\n", 2=>"The following example shows a coordination-structured antecedent AML1 / CBF beta , C / EBP , Ets , c-Myb , HOX , and MZF-1 that was failed to be detected by the parser .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact \n"}nil_first --> []
nil_second --> [25]
--------------------------
{1=>"Our current work has reconfirmed that domain knowledge is indispensable for coreference resolution .\n", 2=>"Our work has confirmed again that domain knowledge is indispensable for coreference resolution .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [1, 4]
nil_second --> [3, 4]
--------------------------
{1=>"Since the biological domain has richer knowledge resources than any other domain , it would be interesting to continue studying how to exploit and employ domain-specific semantic information in coreference resolution for this domain .\n", 2=>"Since the biologicaldomain has richer knowledge resources than any other domain , it would be interesting to continue studying how to exploit and employ domain-specific semantic information in coreference resolution for this domain .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#10#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#32#11#exact -1#11#12#exact -1#12,13,14,15,16#13,14,15,16#para -1#20#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#21,22#para -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#33#34#exact \n"}nil_first --> [2, 33]
nil_second --> [2]
--------------------------
{1=>"Another conclusion concerns markable detection .\n", 2=>"Another conclusion concerns with markable detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact \n"}nil_first --> []
nil_second --> [3]
--------------------------
{1=>"This sub-problem is often regarded as an easy task in coreference resolution systems ; however , in actuality , it is an important subtask , which strongly affects the performance of coreference system .\n", 2=>"This subproblem is often thought as an easy task in coreference resolution systems , however , indeed it is an important subtask which strongly affects the performance of coreference system .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#13#18#exact -1#18,19,20#19,20,21,22#para -1#21#23#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact \n"}nil_first --> [1, 4, 13, 16, 17, 24]
nil_second --> [1, 4, 16, 17]
--------------------------
{1=>"Sticking to the gold data in designing the markable detection method , as done in this paper , is one employed strategy .\n", 2=>"Sticking to the gold data in the designing markable detection method as we did in this paper is one of the strategies .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#6#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#13,14#13,14#para -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#21#21#stem -1#22#22#exact \n"}nil_first --> [11, 17, 20]
nil_second --> [12, 19, 20]
--------------------------
{1=>"However , from the perspective of coreference data creation , revision of the markable annotations would aid in automatic and robust markable detection .\n", 2=>"However , from another perspective , the perspective of coreference data creation , we should revise the markable annotations , for the sake of automatic and robust markable detection .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#15,16#10,11,12#para -1#17#13#exact -1#18#14#exact -1#14#15#para -1#24#18#exact -1#25#19#exact -1#26#20#exact -1#27#21#exact -1#28#22#exact -1#29#23#exact \n"}nil_first --> [16, 17]
nil_second --> [3, 4, 5, 13, 19, 20, 21, 22, 23]
--------------------------
{1=>"For future opportunities , more effort should be spent on automating the semantic classification for coreference expressions , using context .\n", 2=>"As for the future , more effort should be spent on automating the semantic classification for coreference expressions using context .\n", 3=>"-1#1#0#lc -1#3#1#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact \n"}nil_first --> [2, 17]
nil_second --> [0, 2]
--------------------------
{1=>"Furthermore , it would be interesting to test the results in this study in a machine-learning framework .\n", 2=>"Furthermore , it would be interesting to test the results in this study in a machine learning framework .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3,4,5,6#2,3,4,5#para -1#7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> [15]
nil_second --> [15, 16]
--------------------------
{1=>"Syntactically annotated corpora have become important resources for natural language processing , due in part to the success of corpus-based methods .\n", 2=>"The success of corpus-based methods has made syntactically annotated corpora important resources for natural language processing .\n", 3=>"-1#7#0#lc -1#8#1#exact -1#9#2#exact -1#5#3,4#para -1#10#5#exact -1#11#6#exact -1#12#7#exact -1#13#8#exact -1#14#9#exact -1#15#10#exact -1#0,1,2#15,16,17,18#para -1#3#19#exact -1#4#20#exact -1#16#21#exact \n"}nil_first --> [11, 12, 13, 14]
nil_second --> [6]
--------------------------
{1=>"Since words are often considered as primitive units of language structures , the annotation of word segmentation forms the basis of these corpora .\n", 2=>"Since words are often considered as the primitive units of language structures , the annotation of word segmentation forms the basis of these corpora .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19,20#18,19,20,21#para -1#23#22#exact -1#24#23#exact \n"}nil_first --> []
nil_second --> [6, 21, 22]
--------------------------
{1=>"This is also a concern for the Vietnamese Treebank ( VTB ) , which is the first and only publicly available syntactically annotated corpus thus far for the Vietnamese language .\n", 2=>"This is also a concern of Vietnamese Treebank ( VTB ) , the first and the only publicly available syntactically annotated corpus so far for the Vietnamese language .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#24#5#exact -1#25#6#exact -1#26#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22,23#24,25#para -1#15#27#exact -1#6#28#exact -1#27#29#exact -1#28#30#exact \n"}nil_first --> [13, 14, 26]
nil_second --> [5]
--------------------------
{1=>"Although word segmentation is straight-forward for space-delimited languages like English , this is not the case for languages like Vietnamese for which a standard criterion for word segmentation does not exist .\n", 2=>"Although word segmentation is straight-forward for space-delimited languages like English , this is not true for languages like Vietnamese of which no standard criterion for word segmentation exists .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13,14#11,12,13,14,15#para -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19,20#20,21#para -1#22#22,23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#30#stem -1#28#31#exact \n"}nil_first --> [28, 29]
nil_second --> [21]
--------------------------
{1=>"Then , by combining and splitting the inconsistent annotations that were detected , we are able to observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation .\n", 2=>"Then , by combining and splitting the inconsistent annotations detected , we could observe the influence of different word segmentation criteria on automatic word segmentation , and the applications of word segmentation , including text classification and English-Vietnamese statistical machine translation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14,15#para -1#13,14#16,17,18#para -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact -1#28#32#exact -1#29#33#exact -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#42#exact -1#39#43#exact -1#40#44#exact -1#41#45#exact \n"}nil_first --> [9, 10]
nil_second --> []
--------------------------
{1=>"Treebanks , which are corpora annotated with syntactic structures , have become more and more important for language processing .\n", 2=>"Treebanks , corpora annotated with syntatic structures , have become more and more impor-tant for language processing .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#8,9#3#para -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#6#8#exact -1#7#9#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact \n"}nil_first --> [2, 7, 10, 11, 15]
nil_second --> [5, 13]
--------------------------
{1=>"In order to strengthen the automatic processing of the Vietnamese language , the Vietnamese Treebank ( VTB ) has been built as a part of the national project , `` Vietnamese language and speech processing ( VLSP ) '' ( Nguyen et al ., 2009b ) .\n", 2=>"To strengthen the automatic processing of the Vietnamese language , the Vietnamese treebank ( VTB ) has been built as a part of the national project `` Vietnamese language and speech processing ( VLSP ) '' ( Nguyen et al ., 2009b ) .\n", 3=>"-1#0#2#lc -1#1#3#exact -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#lc -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#21,22,23#22,23,24,25#para -1#24#26#exact -1#25#27#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact -1#35#38#exact -1#36#39#exact -1#37#40#exact -1#38#41#exact -1#39#42#exact -1#40#43#exact -1#41#44#exact -1#42#45#exact -1#43#46#exact \n"}nil_first --> [0, 1, 28]
nil_second --> [20]
--------------------------
{1=>"This score is far lower than the state-of-the-art performance reported for the Berkeley Parser on the English Penn Treebank , which reported 90 .3% in F-score ( Petrov et al ., 2006 ) .\n", 2=>"This performance is far lower than the state-of-the-art performance reported for Berkeley Parser on English Penn Treebank , 90 .3% in F-score ( Petrov et al ., 2006 ) .\n", 3=>"-1#0#0#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15,16#para -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact -1#28#32#exact -1#29#33#exact \n"}nil_first --> [1, 11, 20, 21]
nil_second --> [1]
--------------------------
{1=>"There are two possible reasons to explain this outcome .\n", 2=>"There are two possible reasons for this .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4,5#4,5#para -1#6#7#exact -1#7#9#exact \n"}nil_first --> [6, 8]
nil_second --> []
--------------------------
{1=>"One reason for this outcome is the quality of VTB , including the quality of the annotation scheme , the annotation guidelines , and the annotation process .\n", 2=>"First , the quality of VTB is not good enough to build a good a parser , including the quality of the annotation scheme , the annotation guidelines , and the annotation process .\n", 3=>"-1#6#5#exact -1#2#6#exact -1#3#7#exact -1#4#8#exact -1#5#9#exact -1#16#10#exact -1#17,18,19,20,21#11,12,13,14#para -1#30#15#exact -1#31#16#exact -1#23#17#exact -1#24#18#exact -1#25#19#exact -1#26#20#exact -1#27#21#exact -1#28#22#exact -1#29#23#exact -1#22#25#exact -1#32#26#exact -1#33#27#exact \n"}nil_first --> [0, 1, 2, 3, 4, 24]
nil_second --> [0, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15]
--------------------------
{1=>"The second reason is the difficulty of parsing Vietnamese ; we need to seek new solutions to address this problem .\n", 2=>"Second , parsing Vietnamese is a diffcult problem by its own , and we need to seek new solutions to the problem .\n", 3=>"-1#0#1#lc -1#4#3#exact -1#20#4#exact -1#21#5,6#para -1#2#7#exact -1#3#8#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact -1#19#16#exact -1#7#19#exact -1#22#20#exact \n"}nil_first --> [0, 2, 9, 17, 18]
nil_second --> [1, 5, 6, 8, 9, 10, 11, 12]
--------------------------
{1=>"This paper focuses on the word segmentation , since the most basic unit of a treebank are words ( Di Sciullo and Edwin , 1987 ) , and defining `` words '' is the first step ( Xia , 2000b ,a ; Sornlertlamvanich et al ., 1997 , 1999 ) .\n", 2=>"This paper focuses on the word segmentation issues since the most basic unit of a treebank is word ( Di Sciullo and Edwin , 1987 ) , and defining `` What are words ? '' is the first problem that a treebank has to solve ( Xia , 2000b ,a ; Sornlertlamvanich et al ., 1997 , 1999 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#23#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#31#16#exact -1#32#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#26#23#exact -1#24#24#exact -1#25#25#exact -1#47#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#17#30#stem -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#37#34#exact -1#45#36#exact -1#46#37#exact -1#56#38#exact -1#48#39#exact -1#49#40#exact -1#50#41#exact -1#51#42#exact -1#52#43#exact -1#53#44#exact -1#54#45#exact -1#55#46#exact -1#57#48#exact -1#58#49#exact -1#59#50#exact \n"}nil_first --> [35, 47]
nil_second --> [7, 16, 30, 33, 38, 39, 40, 41, 42, 43, 44]
--------------------------
{1=>"For languages like English , defining `` words '' is almost trivial , because the blank spaces denote word delimiters .\n", 2=>"For languages like English , answering this question is almost trivial because the blank spaces denote word delimiters .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#16#7#stem -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#17#19#exact -1#18#20#exact \n"}nil_first --> [5, 6, 8, 12, 18]
nil_second --> [5, 6, 7]
--------------------------
{1=>"For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words , `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) '' . Word segmentation is expected to break down the sentence at the boundaries of these words , instead of splitting `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .\n", 2=>"For example , the sentence `` H❅c sinh h❅c sinh h❅c ( students learn biology )1 '' is composed of three words `` h❅c sinh ( student ) '' , `` h❅c ( learn ) , '' and `` sinh h❅c ( biology ) ; '' Word segmentation is expected to break down the sentence at the boundaries of these words , not to split `` h❅c sinh ( student ) '' and `` sinh h❅c ( biology ) '' .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#29#22#exact -1#30#23#exact -1#31#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#35#30#exact -1#22#31#exact -1#23#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#61#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#45#45#exact -1#80#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#55#56#exact -1#56#57#exact -1#57#58#exact -1#58#59#exact -1#59#60#exact -1#60#61#exact -1#62#63#para -1#63,64#64,65#para -1#65#66#exact -1#66#67#exact -1#67#68#exact -1#68#69#exact -1#69#70#exact -1#70#71#exact -1#71#72#exact -1#72#73#exact -1#73#74#exact -1#74#75#exact -1#75#76#exact -1#76#77#exact -1#77#78#exact -1#78#79#exact -1#79#80#exact \n"}nil_first --> [62, 81]
nil_second --> [44]
--------------------------
{1=>"Note that the terminology `` word segmentation '' also refers to the task of extracting words statistically without concerning a gold-standard for segmentation , as in ( Ha , 2003 ; Le et al ., 2010 ) .\n", 2=>"Note that the terminology `` word segmentation '' also refers to the task of extracting words statistically without concerning a gold-standard for segmentation , as in ( Ha , 2003 ; Le et al ., 2010 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9,10#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"In such a context , the extracted words are more appropriate for building a dictionary , rather than for corpus-based language processing , which are outside of the scope of this paper .\n", 2=>"In such context , the extracted words are more appropriate for building a dictionary than for corpus-based language processing , which are out of the focus of this paper .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#12#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8,9,10#8,9,10,11#para -1#11#12#exact -1#13#13,14#para -1#19#15#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#20#23#exact -1#21#24#exact -1#22,23#25,26#para -1#24#27#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact \n"}nil_first --> [16, 22, 28]
nil_second --> [7, 25]
--------------------------
{1=>"Because of the discussed characteristics of the language , there are challenges in establishing a gold standard for Vietnamese word segmentation .\n", 2=>"Establishing a gold standard for Vietnamese word segmentation faces some diffcuties coming from the characteristics of the language .\n", 3=>"-1#15#1#exact -1#16#2#exact -1#14#4#exact -1#13#6#exact -1#17#7#exact -1#0#13#lc -1#1#14#exact -1#2#15#exact -1#3#16#exact -1#4#17#exact -1#5#18#exact -1#6#19#exact -1#7#20#exact -1#18#21#exact \n"}nil_first --> [0, 3, 5, 8, 9, 10, 11, 12]
nil_second --> [8, 9, 10, 11, 12]
--------------------------
{1=>"The difficulties in Vietnamese word segmentation have been recognized by many researchers ( Ha , 2003 ; Nguyen et al ., 2004 , 2006 ; Le et al ., 2010 ) .\n", 2=>"The diffculties of Vietnamese word segmentation have been recognized by many researchers ( Ha , 2003 ; Nguyen et al ., 2004 , 2006 ; Le et al ., 2010 ) .\n", 3=>"-1#0#0#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [1, 2]
nil_second --> [1, 2]
--------------------------
{1=>"Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus as to the methodology for segmenting a sentence into words .\n", 2=>"Although most people agree that the Vietnamese language has two types of words : single and compound , there is little consensus on how to segment a sentence into words .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#24#23#exact -1#25#27#stem -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact \n"}nil_first --> [22, 24, 25, 26]
nil_second --> [22, 23]
--------------------------
{1=>"The disagreement occurs not only because of the different functions of blank spaces ( as mentioned above ) , but also because Vietnamese is not an inflectional language , as is the case for English or Japanese , for which morphological forms can provide useful clues for word segmentation .\n", 2=>"The disagreement is not only because of the different functions of blank spaces as mentioned above , but also because Vietnamese is not an inflectional language like English or Japanese , where morphological forms can be useful clues for word segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#para -1#3,4,5,6#3,4,5#para -1#10#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#30#28#exact -1#26#29,30,31,32#para -1#38#33#exact -1#27#34#exact -1#28#35#exact -1#29#36#exact -1#31#38,39#para -1#32#40#exact -1#33#41#exact -1#34#42#exact -1#36#44#exact -1#37#45#exact -1#39#47#exact -1#40#48#exact -1#41#49#exact \n"}nil_first --> [10, 13, 17, 37, 43, 46]
nil_second --> [35]
--------------------------
{1=>"While similar problems also occur with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more difficult , because the modern Vietnamese writing system is based on Latin characters , which represent the pronunciation , but not the meaning of words .\n", 2=>"While the similar problems also happen with Chinese word segmentation ( Xia , 2000b ) , Vietnamese word segmentation may be more diffcult because the modern Vietnamese writing system is based on Latin characters , which represents the pronunciation but not the meaning of words .\n", 3=>"-1#0#0#exact -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#syn -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#34#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#35#35#exact -1#36,37#36,37#para -1#38#38#exact -1#39#40#exact -1#40#41#exact -1#42,43,44#42,43,44,45#para -1#45#46#exact \n"}nil_first --> [21, 34, 39]
nil_second --> [1, 22, 41]
--------------------------
{1=>"All of these characteristics make it diffcult to perform word segmentation for Vietnamese , both manually and automatically , and have thus resulted in different criteria for word segmentation .\n", 2=>"All these characteristics make it diffcult to perform word segmentation for Vietnamese both manually and automatically , and have resulted in different criteria for word segmenation .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#16#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#17#19#exact -1#18#20#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#23#26#exact -1#24#27#exact -1#26#29#exact \n"}nil_first --> [1, 18, 21, 28]
nil_second --> [25]
--------------------------
{1=>"However , so far , there have been few studies on the challenges in word segmentation , and the comparison of different word segmentation criteria .\n", 2=>"However , so far there have been few studies on the challenges in word segmentation , and the comparison of different word segmentation criteria .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#15#4#exact -1#6,7#5,6,7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [16]
nil_second --> [4, 5]
--------------------------
{1=>"In this paper , a brief introduction of the Vietnamese Treebank VTB and its annotation scheme are provided in Section 2 .\n", 2=>"In this paper , a brief introduction of the Vietnamese treebank VTB and its annotation scheme are given in Section 2 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#lc -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"We classified the problematic annotations into several patterns of inconsistency , part of which were manually fixed to improve the quality of the corpus .\n", 2=>"We classified the problematic annotations into several patterns of inconsistency , part of which were manually fixed to improve the quality of the corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18,19,20,21#17,18,19,20,21#para -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> []
nil_second --> [17]
--------------------------
{1=>"The rest , which can be considered as the most difficult and controversial instances of word segmentation , were used to create different versions of the VTB corpus , representing different word segmentation criteria .\n", 2=>"The rest , which can be considered as the most diffcult and controversial cases of word segmentation , were used to create different versions of the VTB corpus representing different word segmentation criteria .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13,14#13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact \n"}nil_first --> [10, 28]
nil_second --> [10]
--------------------------
{1=>"Finally , we evaluated these criteria in automatic word segmentation , and its application in text classification and English-Vietnamese statistical machine translation , in Section 4 .\n", 2=>"Finally , we evaluated these criteria in automatic word segmentation , and its application in text classification and English-Vietnamese statistical machine translation in Section 4 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"This study is not only beneficial for the development of computational processing technologies for Vietnamese , a language spoken by over 90 million people , but also for similar languages such as Thai , Laos , and so on .\n", 2=>"This study is not only beneficial for the development of computational processing technologies for Vietnamese , a language spoken by over 90 million people , but also for the similar languages such as Thai , Laos , and so on .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8#6,7,8,9#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25,26,27,28#25,26,27#para -1#29#28#exact -1#30#29#exact -1#31#30#exact -1#32#31#exact -1#33#32#exact -1#34#33#exact -1#35#34#exact -1#36#35#exact -1#37#36#exact -1#38#37#exact -1#39#38#exact -1#40#39#exact \n"}nil_first --> []
nil_second --> [9]
--------------------------
{1=>"This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language , like English , to a language that has not yet been intensively studied .\n", 2=>"This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language like English to a language that has not yet intensively studied .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24,25#26,27,28,29#para -1#27#30#exact -1#28#31#exact -1#29#32#exact \n"}nil_first --> [18, 21]
nil_second --> [26]
--------------------------
{1=>"VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al ., a ) , which can be divided up into three groups : single , compound , and special `` words '' .\n", 2=>"VTB specifies 12 types of units that should be identified as words ( Table 1 ) ( Nguyen et al ., a ) , which can be divided into three groups : single , compound , and special `` words '' .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact \n"}nil_first --> [28]
nil_second --> []
--------------------------
{1=>"The terminology tokens refers to text spans that are separated from each other by blank spaces .\n", 2=>"The terminology tokens refers to text spans separated with each other by blank spaces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8,9#para -1#8,9,10#10,11,12#para -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact \n"}nil_first --> [7]
nil_second --> []
--------------------------
{1=>"Compound words have two or more tokens , and are divided into four types : compound words composed by semantic coordination ( semantic-coordinated compound ) , compound words composed by semantic subordination ( semantic-subordinated compound ) , compound words with an affx , and reduplicated words .\n", 2=>"Compound words have two or more tokens , and are divided into four types : compound words composed by semantic coordination ( semantic-coordinated compound ) , compound words composed by semantic subordination ( semantic-subordinated compound ) , compound words with affx , and reduplicated words .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact -1#38#38#exact -1#39#39#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact \n"}nil_first --> [40]
nil_second --> []
--------------------------
{1=>"Special `` words '' include idioms , locutions , proper names , date times , numbers , symbols , sentence marks , foreign words , or abbreviations .\n", 2=>"Special `` words '' can be idioms , locutions , proper names , date times , numbers , symbols , sentence marks , foreign words , or abbreviations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#25#exact -1#27#26#exact -1#28#27#exact \n"}nil_first --> [4]
nil_second --> [4, 5]
--------------------------
{1=>"The segmentation of these types of words forms a basis for the POS tagging , with 18 different POS tags , as shown in Table 2 ( Nguyen et al ., c ) .\n", 2=>"The segmentation of these types of words forms a basis for the POS tagging , with 18 different POS tags shown in Table 2 ( Nguyen et al ., c ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9,10,11#para -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact \n"}nil_first --> [20, 21]
nil_second --> [10, 11]
--------------------------
{1=>"Each unit in Table 1 goes with several example words ; English translations are provided in parentheses .\n", 2=>"Each unit in Table 1 goes with several example words of which English translations are given in parentheses .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact \n"}nil_first --> [10, 14]
nil_second --> [10, 11, 15]
--------------------------
{1=>"Furthermore , we added a translation for each token , where possible , so that readers who are unfamiliar with Vietnamese can have an intuitive idea as to how the compound words are formed .\n", 2=>"Besides , we added a translation for each token when possible , so that the readers unfamiliar with Vietnamese can have an intuitive idea of how the compound words are formed .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#9#exact -1#9,10#10,11#para -1#12#13#exact -1#13#14#exact -1#15#15#exact -1#29#16,17#para -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24,25,26#26,27,28,29#para -1#27#30#exact -1#28#31#exact -1#30#32#para -1#31#34#exact \n"}nil_first --> [0, 12, 33]
nil_second --> [0, 14]
--------------------------
{1=>"However , for some tokens , we could not find any appropriate English translation , so we gave it an empty translation , marked with an asterisk .\n", 2=>"However , for some tokens , we could not find any appropriate English translation , so we give it an empty translation marked with an asterisk .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#syn -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"Note that a Vietnamese word or a token in context can have other meanings , in addition to the given translations .\n", 2=>"Note that a Vietnamese word or a token in context can have other meanings in addition to the given translations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16,17#15,16,17#para -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [14, 18]
nil_second --> []
--------------------------
{1=>"A classifier noun , denoted by the part-of-speech Nc in Table 2 , is a special type of word in Vietnamese .\n", 2=>"A special type of words in Vietnamese is classifer noun , denoted by the part-of-speech Nc in Table 2 .\n", 3=>"-1#0#0#exact -1#9#2#exact -1#10#3#exact -1#11#4#exact -1#12#5#exact -1#13#6#exact -1#14#7#exact -1#15#8#exact -1#16#9#exact -1#17#10#exact -1#18#11#exact -1#7#13#exact -1#1#14,15#para -1#2#16#exact -1#3#17#exact -1#4#18#stem -1#5#19#exact -1#6#20#exact -1#19#21#exact \n"}nil_first --> [1, 12]
nil_second --> [8]
--------------------------
{1=>"Classifier nouns are specific to several Southeast Asian languages , like Vietnamese and Thai .\n", 2=>"Classifier nouns are specific to several Southeast Asian languages like Vietnamese and Thai .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"For example , the common noun `` bàn '' generally means tables , while `` cái bàn '' means a specific table , similar to the table in English .\n", 2=>"For example , the common noun `` bàn '' means tables in general , while `` cái bàn '' means a specific table similar to the table in English .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11,12#9#para -1#9#10#exact -1#10#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"In this section , we analyzed the VTB corpus to determine whether the difficulties in Vietnamese word segmentation affected the quality of VTB annotations .\n", 2=>"In this section , we analyzed the VTB corpus to know whether the diffculties in Vietnamese word segmentation affected the quality of VTB annotations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10,11#para -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact \n"}nil_first --> [13]
nil_second --> [10, 13]
--------------------------
{1=>"The analysis revealed several types of inconsistent annotations , which are also\n", 2=>"The analysis results revealed several types of inconsistent annotations , which are also\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact \n"}nil_first --> []
nil_second --> [2]
--------------------------
{1=>"Our analysis is based on two types of inconsistencies : variation and structural inconsistency , which are defined below .\n", 2=>"Our analysis is based on two types of inconsistency : variation and structural inconsistency , whose definitions and details are given below .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#19#15,16#para -1#16#17#para -1#21#18#exact -1#22#19#exact \n"}nil_first --> []
nil_second --> [15, 17, 18, 20]
--------------------------
{1=>"Variation inconsistency : is a sequence of tokens , which has more than one way of segmentation in the corpus .\n", 2=>"Variation inconsistency : is a sequence of tokens which have more than one way of seg-mentation in the corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9,10,11#10,11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [8, 16]
nil_second --> [15]
--------------------------
{1=>"For example , `` con gái/girl '' can remain as one word , or be segmented into two words , `` con '' and `` gái '' .\n", 2=>"For example , `` con gái/girl '' can remain as one word , or be segmented into two words `` con '' and `` gái '' .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [19]
nil_second --> []
--------------------------
{1=>"A variation can be an annotation inconsistency , or an ambiguity in Vietnamese .\n", 2=>"A variation can be an annotation inconsistency , or an ambiguity inVietnamese .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#13#exact \n"}nil_first --> [11, 12]
nil_second --> [11]
--------------------------
{1=>"While ambiguity cases reflect the difficulty of the language , annotation inconsistencies are usually caused by the confusion in the decision of annotators , which should be eliminated in annotation .\n", 2=>"While ambiguity cases reflect the diffculty of the language , annotation inconsistencies are usually caused by the confusion in the decision of annotators , which should be eliminated in annotation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact \n"}nil_first --> [5]
nil_second --> [5]
--------------------------
{1=>"We use the term variation instance to refer to a single occurrence of a variation .\n", 2=>"We use the term variation instance to refer a single occurence of a variation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8,9#8,9,10#para -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [11]
nil_second --> [10]
--------------------------
{1=>"Structural inconsistency : happens when different sequences have similar structures , and thus should be split in the same way , but are segmented in different ways in the corpus .\n", 2=>"Structural inconsistency : happens when different sequences have similar structures , thus should be splitted in the same way , but are segmented in different ways in the corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#stem -1#16,17,18#16,17,18,19#para -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23,24,25,26#24,25,26#para -1#15#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact \n"}nil_first --> [11]
nil_second --> []
--------------------------
{1=>"For example , `` con gái/girl '' and `` con trai/boy '' have similar structures : a combination of a classifier noun and a common noun , Nc + N , so when `` con gái/girl '' is split , and `` con trai/boy '' is not , it is considered as a structural inconsistency of Nc .\n", 2=>"For example , `` con gái/girl '' and `` con trai/boy '' have similar structures , a combination of a classifier noun and a common noun Nc + N , so when `` con gái/girl '' is splitted and `` con trai/boy '' is not , it is considered as a structural inconsistency of Nc .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#15#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#stem -1#45#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact -1#42#44#exact -1#43#45#exact -1#44#46#exact -1#46#48#exact -1#47#49#exact -1#48#50#exact -1#49#51#exact -1#50#52#exact -1#51#53#exact -1#52#54#exact -1#53#55#exact -1#54#56#exact -1#55#57#exact \n"}nil_first --> [15, 47]
nil_second --> []
--------------------------
{1=>"It is likely that structural inconsistency at the word segmentation level complicates the higher levels of processing , including POS tagging and bracketing .\n", 2=>"It is likely that structural inconsistency in word segmentation level makes the higher levels of processing , POS tagging and bracketing , become more complicated .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#11#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#23,24#11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#25#23#exact \n"}nil_first --> [6, 18]
nil_second --> [6, 10, 21, 22]
--------------------------
{1=>"The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in the VTB , following the definition for variation inconsistency , above .\n", 2=>"The detection method for variation inconsistency is based on N-gram sequences and the phrase structures in VTB treebank , following the definition of variation inconsistency above .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#20#16#exact -1#16#17#exact -1#18#18#exact -1#19#19#exact -1#21#20,21#para -1#23#23#exact -1#24#24#exact -1#25#26#exact -1#26#27#exact \n"}nil_first --> [22, 25]
nil_second --> [17, 22]
--------------------------
{1=>"Table 3 shows the overall statistics of the variation inconsistency detected by the method described above .\n", 2=>"Table 3 shows the overall statistics of the variation inconsistency detected by the above method .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14#13#exact -1#13#15#exact -1#15#16#exact \n"}nil_first --> [14]
nil_second --> []
--------------------------
{1=>"Most of the diffcult cases of word segmentation occur in two-token variations , occupying the majority of variations ( 92 .9% ) .\n", 2=>"Most of the diffcult cases of word segmentation lie in two-token variations , occupying the majority of variations ( 92 .9% ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"This ratio of 2-gram variations is much higher than the average ratio of two-token words in Vietnamese , as reported in ( Nguyen et al., 2009a ) , which is 80% .\n", 2=>"This ratio of 2-gram variations is much higher than the evarage ratio of two-token words in Vietnamese reported in ( Nguyen et al., 2009a ) , which is 80% percent .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#25#17#exact -1#17#18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#30#31#exact \n"}nil_first --> [10, 27]
nil_second --> [10, 29]
--------------------------
{1=>"Variations that have lengths of three and four tokens occupy 6 .1% and 1 .0% , respectively .\n", 2=>"Variations have lengths of three and four tokens occupy 6 .1% and 1 .0% , respectively .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"We estimated the precision of our method by randomly selecting 130 2-gram variation instances , extracted from the method described above , and manually checked whether the inconsistencies are true .\n", 2=>"We estimated the precision of our method by randomly selected 130 2-gram variation instances extracted from the above method , and manually checked whether they are true inconsistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#stem -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#19#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#18#18#exact -1#17#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#25#28#exact -1#26#29#exact -1#28#30#exact \n"}nil_first --> [19, 21, 26, 27]
nil_second --> [24, 27]
--------------------------
{1=>"Only one instance of inconsistency was an ambiguous sequence giá c , which is one word when it means price , and two words giá / price c / all in đàu có giá c / all have ( their own ) price .\n", 2=>"Only one instance is an ambiguous sequence giá c , which is one word when it means price , and two words giá / price c / all in đàu có giá c / all have ( their own ) price .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#5,6#para -1#5#7#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#40#42#exact -1#41#43#exact \n"}nil_first --> [3, 4]
nil_second --> []
--------------------------
{1=>"The precision for our method is high , so we can use the extracted variations to provide insights on the word segmentation problem .\n", 2=>"The precision of our method is high enough so that so we can use the extracted variations to study the insights of word segmentation problem .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#20#17#exact -1#19#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact \n"}nil_first --> [2, 7, 16, 18]
nil_second --> [2, 7, 8, 9, 18, 21]
--------------------------
{1=>"We further analyzed the 2-gram variations to understand what types of 2-grams were most confusing for annotators .\n", 2=>"We further analyzed the 2-gram variations to know what types of 2-grams were most confusing to annotators .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> [7, 15]
nil_second --> [7, 15]
--------------------------
{1=>"The analysis revealed that compound nouns , compound verbs , and compound adjectives are the most difficult cases of word segmentation .\n", 2=>"The analysis results showed that compound nouns , compound verbs , and compound adjectives are the top diffcult cases of word segmentation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#para -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> [15, 16]
nil_second --> [2, 16, 17]
--------------------------
{1=>"There are a total of 54 patterns of POS sequences . The top 10 confusing patterns , their counts of 2-gram variations , and examples are depicted in Table 4 .\n", 2=>"There are totally 54 patterns of POS sequence , of which top 10 confusing patterns , a long with their counts of 2-gram variations , and examples are shown in Table 4 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#16#2#exact -1#9#4#exact -1#3#5#exact -1#4#6#exact -1#5#7#exact -1#6#8#exact -1#7#9#stem -1#32#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#syn -1#29#27#exact -1#30#28#exact -1#31#29#exact \n"}nil_first --> [3, 11, 30]
nil_second --> [2, 8, 10, 17, 18]
--------------------------
{1=>"Table 5 and Table 6 show the POS patterns that are a specific POS tag , appearing at the beginning or ending of the sequence .\n", 2=>"Table 5 and Table 6 show the POS patterns which a specific POS tag appearing at the beginning or ending of the sequence .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact \n"}nil_first --> [9, 10, 15]
nil_second --> [9]
--------------------------
{1=>"Investigating the inconsistent 2-grams extracted , we found that most of them are compound words , according to the VTB guidelines ( Section 2 ) .\n", 2=>"Investigating the inconsistent 2-grams extracted , we found that most of them are compound words according to the VTB guidelines ( Section 2 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [15]
nil_second --> []
--------------------------
{1=>"This can be seen through the examples provided in Table 4 , where the meanings of tokens are given with a subscript .\n", 2=>"This can be seen through the examples given in Table 4 , where the meanings of tokens are given with a subscript .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6,7,8#5,6,7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact \n"}nil_first --> []
nil_second --> [5]
--------------------------
{1=>"This scenario has proven to be problematic for the annotators of VTB .\n", 2=>"This problem seems to have caused a lot of trouble for the annotators of VTB .\n", 3=>"-1#0#0#exact -1#5,6#2#para -1#3#4#exact -1#9,10,11#6#para -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact \n"}nil_first --> [1, 3, 5, 7, 8]
nil_second --> [1, 2, 4, 7, 8]
--------------------------
{1=>"Furthermore , by observing the POS patterns in Table 5 and Table 6 , we can see the potential for structural inconsistency , particularly for closed-set POS tags .\n", 2=>"Furthermore , observing the POS patterns in Table 5 and Table 6 , we can see the potential of structural inconsistency , in particular for closed-set POS tags .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#14,15,16#14,15,16,17#para -1#17#18#exact -1#18,19#19,20#para -1#20#21#exact -1#21#22#exact -1#22,23,24#23,24#para -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> [2]
nil_second --> [13]
--------------------------
{1=>"Among them , classifier nouns ( Nc ) and affixes ( S ) are two typical cases of structural inconsistency , which will be used in several settings for our experiments .\n", 2=>"Among them , classifier nouns ( Nc ) and affxes ( S ) are two typical cases of structural inconsistency , which will be used in several settings of our experiments .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22,23,24,25#21,22,23,24,25#para -1#26#26#exact -1#27#27#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [9, 28]
nil_second --> [9, 21, 28]
--------------------------
{1=>"The same affx or classifier noun can modify different nouns , so when they are sometimes split and combined in the variations , we can conclude that classifier nouns and affixes involve in-structural inconsistencies .\n", 2=>"The same affx or classifier noun can modify different nouns , so when they are sometimes splitted , and sometimes combined in the variations , we can conclude that classifier nouns and affxes involve in structural inconsistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#stem -1#18#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#33#31#exact -1#36#33#stem -1#37#34#exact \n"}nil_first --> [30, 32]
nil_second --> [17, 19, 32, 34, 35]
--------------------------
{1=>"In the following section , we present our detection method for structural inconsistency for classifier nouns and affixes .\n", 2=>"In the following section , we presents our detection method for structural inconsistency for classifier nouns and affxes .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#stem -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"The detection method for structural inconsistency of classifier nouns and affixes is simple .\n", 2=>"The detection method for structural inconsistency of classifier nouns and affxes is simple .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [10]
nil_second --> [10]
--------------------------
{1=>"We collected all affixes and classifier nouns in the VTB corpus , and then extracted 2-grams containing these affixes or classifier nouns , which are also structural inconsistencies .\n", 2=>"First , we collected all affxes and classifier nouns in the VTB corpus . Then , extracted 2-grams containing these affxes or classifier nouns , which also are the structural inconsistencies .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#6#4#exact -1#7#5#exact -1#8#6#exact -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#1#11#exact -1#14#12,13#para -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#27#24#exact -1#26#25#exact -1#29#26#exact -1#30#27#exact -1#31#28#exact \n"}nil_first --> [3, 18]
nil_second --> [0, 5, 13, 15, 20, 28]
--------------------------
{1=>"Even though the sequence , \" con trai \" is always split into two words throughout the corpus , it can still be an inconsistency , if we consider similar structures such as \" con gái \" .\n", 2=>"Note that even though the sequence \" con trai \" is always splitted into two words throughout the corpus , it can still be an inconsistency if we consider similar structures such as \" con gái \" .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#4#2#exact -1#5#3#exact -1#19#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#stem -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact -1#36#36#exact -1#37#37#exact \n"}nil_first --> [18, 25]
nil_second --> [0, 1]
--------------------------
{1=>"In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent , if we consider the higher analysis levels , POS tagging .\n", 2=>"In other words , by this method , we extract sequences that may be consistent at the surface level , but are not consistent if we consider the higher analysis levels , POS tagging .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#31#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact \n"}nil_first --> [32]
nil_second --> []
--------------------------
{1=>"According to the VTB POS-tagging annotation guidelines ( Nguyen et al., c ) , classifier nouns should be separated from the words that they modify .\n", 2=>"According to the VTB POS-tagging annotation guidelines ( Nguyen et al., c ) , classifier nouns should be separated from the words they modify .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"However , in practice , it is confusing when the classifier noun can be standalone as a single word .\n", 2=>"However , in practice it is confusing when the classifier noun can be stand alone as a single word .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [4, 14]
nil_second --> [13, 14]
--------------------------
{1=>"For example a classifier noun , e.g. , \" con \" in \" con trai ( boy ) \" , or \" con gái ( girl ) \" , can also be a simple word , which means \" I ( first person pronoun used by a child when talking to his / her parents ) \" , or part of a complex noun \" con cái ( children ) \" .\n", 2=>"For example a classifier noun , e.g. , \" con \" in \" con trai ( boy ) \" , or \" con gái ( girl ) \" , can also be a simple word which means \" I ( first person pronoun used by a child when talking to his / her parents ) \" , or part of a complex noun \" con cái ( children ) \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#56#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact -1#39#40#exact -1#40#41#exact -1#41#42#exact -1#42#43#exact -1#43#44#exact -1#44#45#exact -1#45#46#exact -1#46#47#exact -1#47#48#exact -1#48#49#exact -1#49#50#exact -1#50#51#exact -1#51#52#exact -1#52#53#exact -1#53#54#exact -1#54#55#exact -1#55#56#exact -1#57#58#exact -1#58#59#exact -1#59#60#exact -1#60#61#exact -1#61#62#exact -1#62#63#exact -1#63#64#exact -1#64#65#exact -1#65#66#exact -1#66#67#exact -1#67#68#exact -1#68#69#exact -1#69#70#exact -1#70#71#exact \n"}nil_first --> [57]
nil_second --> []
--------------------------
{1=>"Therefore , in our experiments , we want to evaluate the \" splitting \" and \" combining \" of these cases , in order to see whether the solution is successful for applications of the corpus .\n", 2=>"Therefore , in our experiments , we want to evaluate the \" splitting \" and \" combining \" of these diffcult cases , to see whether the solution is fruitful for applications of the corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#20#exact -1#22#21#exact -1#23,24#22,23,24,25#para -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29,30#30,31#para -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact \n"}nil_first --> []
nil_second --> [20]
--------------------------
{1=>"By examining the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like a percentage ( % ) in \" 30% \" , is split or combined with \" 30 \" .\n", 2=>"Examing the variations extracted by the variation inconsistency detection , we found that there are cases when a special character like percentage % in \" 30% \" , is splitted or combined with \" 30 \" .\n", 3=>"-1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22,23#para -1#22#25#exact -1#23#27#exact -1#24#28#exact -1#25#29#exact -1#26#30#exact -1#27#31#exact -1#28#32#exact -1#29#33#stem -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact \n"}nil_first --> [0, 1, 24, 26]
nil_second --> [0]
--------------------------
{1=>"By checking structural inconsistencies of these special characters , including percentages ( % ) , hyphens ( - ) , and other symbols , we found quite a significant number of inconsistent annotations .\n", 2=>"Checking structural inconsistency of these special characters including percentage% , hyphen - , and so on , we found quite a significant amount of inconsistent annotations .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#stem -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#12#8#exact -1#7#9#exact -1#9#14#exact -1#10#15#stem -1#11#17#exact -1#16#19#exact -1#13#20#exact -1#17#24#exact -1#18#25#exact -1#19#26#exact -1#20,21,22,23#27,28,29,30#para -1#24#31#exact -1#25#32#exact -1#26#33#exact \n"}nil_first --> [0, 10, 11, 12, 13, 16, 18, 21, 22, 23]
nil_second --> [8, 14, 15]
--------------------------
{1=>"For example , the character , % , in \" 30% \" is split , but is combined with a number in \" 50 % \" , which is considered to be a structural inconsistency .\n", 2=>"For example , the character % in \" 30% \" is splitted but is combined with the number in \" 50 % \" , which is considered as a structural inconsistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#23#5#exact -1#5#6#exact -1#6#8#exact -1#7#9#exact -1#8#10#exact -1#9#11#exact -1#10#12#exact -1#11#13#stem -1#12#15#exact -1#13#16#exact -1#14#17#exact -1#15#18#exact -1#28#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact -1#24,25,26#27,28,29,30#para -1#29#32,33#para -1#30#34#exact -1#31#35#exact \n"}nil_first --> [7, 14, 26, 31]
nil_second --> [16, 27]
--------------------------
{1=>"Note that it can be argued that splitting \" N% \" into two words or combined in one word is dependent on the blank space in-between N and \" % \" .\n", 2=>"Note that although it can be argued that whether \" N% \" can be splitted into two words or combined in one word is dependent on the blank space in between N and \" % \" .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#14#7#stem -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact -1#26#22#exact -1#27#23#exact -1#28#24#exact -1#31#26#exact -1#32#27#exact -1#33#28#exact -1#34#29#exact -1#35#30#exact -1#36#31#exact \n"}nil_first --> [25]
nil_second --> [2, 8, 12, 13, 29, 30]
--------------------------
{1=>"Higher-levels of annotation such as POS tagging is significant , because we may need one or two different POS tags for the different methods of annotation .\n", 2=>"It does matter higher-levels of annotation such as POS tagging because we may need one or two different POS tags for different ways of annotation .\n", 3=>"-1#3#0#lc -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#0,1#7#para -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21,22#para -1#22#23#para -1#23#24#exact -1#24#25#exact -1#25#26#exact \n"}nil_first --> [8, 9]
nil_second --> [2]
--------------------------
{1=>"Therefore , we think that it is better to carefully preprocess text and segment these special characters in a consistent way .\n", 2=>"Therefore , we think it is better to carefully preprocess text and segment these special characters in a consistent way .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5,6,7#5,6,7,8#para -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [4]
nil_second --> [4]
--------------------------
{1=>"To improve the quality of the VTB corpus , we extracted the problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency .\n", 2=>"To improve the quality of VTB corpus , we extracted the probably problematic sequences using patterns of the special characters , and manually fixed this type of inconsistency .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#10#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#17#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#17,18#para -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact \n"}nil_first --> []
nil_second --> [11]
--------------------------
{1=>"Automatically modification is diffcult , since we must check the semantics of the special characters in their contexts .\n", 2=>"Automatically modification is diffcult since we must check the semantics of the special characters in their contexts .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact \n"}nil_first --> [4]
nil_second --> []
--------------------------
{1=>"For example , hyphens in date expressions like \" 5-4-1975 \" , which refers to the date , \" the fifth of April , 1975 , \" are combined with the numbers .\n", 2=>"For example , hyphens in date expressions like \" 5-4-1975 \" , which means the date \" April the fifth , 1975 , \" are combined with the numbers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13,14#13,14,15#para -1#15#16#exact -1#20#17#exact -1#16#18#exact -1#18#19#exact -1#19#20#exact -1#17#21,22#para -1#22#23#exact -1#21#24#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact \n"}nil_first --> [25]
nil_second --> []
--------------------------
{1=>"However , when the hyphen indicates \" ( from ) to \" or \" around .\n", 2=>"However , when the hypen has a meaning of \" ( from ) to \" or \" around .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact -1#12#9#exact -1#13#10#exact -1#14#11#exact -1#15#12#exact -1#16#13#exact -1#17#14#exact -1#18#15#exact \n"}nil_first --> [4, 5]
nil_second --> [4, 5, 6, 7, 8]
--------------------------
{1=>"or \" , as in \" 2-3 gi░ sáng \" , meaning \" around 2 or 3 o’clock in the morning \" , we decided to separate it from the surrounding numbers .\n", 2=>"or \" , as in \" 2-3 gi░ sáng \" meaning \" around 2 or 3 o’clock in the morning \" , we decided to separate it from the surrounding numbers .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#21#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact \n"}nil_first --> [22]
nil_second --> []
--------------------------
{1=>"The variation inconsistency and structural inconsistency found in Section 3 can also be seen as representatives of different word segmentation criteria for Vietnamese .\n", 2=>"The variation inconsistency and structural inconsistency found in Section 3 above can also be seen as representatives of different word segmentation criteria for Vietnamese .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> []
nil_second --> [10]
--------------------------
{1=>"Then , by using these data sets , we could observe the influence of the different word segmentation criteria on three tasks : automatic word segmentation , text classification , and English-Vietnamese statistical machine translation .\n", 2=>"Then , by using these data sets , we could observe the influence of the different word segmentation criteria on three tasks : automatic word segmenation , text classification , and English-Vietnamese statistical machine translation .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11,12,13,14#11,12,13#para -1#15#14,15#para -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> [25]
nil_second --> [25]
--------------------------
{1=>"These data sets are used in our experiments , as illustrated in Figure 1 .\n", 2=>"These data sets are used in our experiments as illustrated in Figure 1 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [8]
nil_second --> []
--------------------------
{1=>"In this section , we briefly describe the task settings and the methods used for word segmentation ( WS ) , text classification ( TC ) , and English-Vietnamese statistical machine translation ( SMT ) .\n", 2=>"In this section , we briefly describe the task settings and the methods used for word segmentation ( WS ) , text classification ( TC ) , and English-Vietnamese statistical machine translation ( SMT ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10,11,12,13#10,11,12,13,14#para -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#34#34#exact -1#35#35#exact \n"}nil_first --> []
nil_second --> [14]
--------------------------
{1=>"The core of YamCha is the Support Vector Machine ( SVM ) machine learning method , which has been proven to be effective for NLP tasks .\n", 2=>"The core of YamCha is the Support Vector Machine ( SVM ) machine learning method , which has been proved to be effective in NLP tasks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19,20,21,22#19,20,21,22#para -1#24#24#exact -1#25#25#exact -1#26#26#exact \n"}nil_first --> [23]
nil_second --> [23]
--------------------------
{1=>"The label of each token is determined based on the lexical features of two preceding words , and the two following words of that token .\n", 2=>"Label of each token is determined based on the lexical features of two preceding words and two following words of that token .\n", 3=>"-1#8#0#lc -1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#17#exact -1#16#18,19#para -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#21#24#exact -1#22#25#exact \n"}nil_first --> [9, 16]
nil_second --> []
--------------------------
{1=>"Since the Vietnamese language is not inflectional , we cannot utilize inflection features for word segmentation .\n", 2=>"Since Vietnamese language is not inflectional , we cannot utilize inflection features for word segmentation .\n", 3=>"-1#0#0#exact -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [1]
nil_second --> []
--------------------------
{1=>"Text classification is defined as a task of determining the most suitable topic from the predefined topics , for an input document .\n", 2=>"Text classification is defined as a task of determining for an input document the most suitable topic from the predefined topics .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#13#9#exact -1#14#10#exact -1#15#11#exact -1#16#12#exact -1#17#13#exact -1#18#14#exact -1#19#15#exact -1#20#16#exact -1#9#18#exact -1#10#19#exact -1#11#20#exact -1#12#21#exact -1#21#22#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"The difference is that we performed the task at the document level , instead of at the sentence level .\n", 2=>"The difference is that we performed for document level , not for sentence level .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#9,10#para -1#8#11#exact -1#9#12#exact -1#10#13#para -1#12#17#exact -1#13#18#exact -1#14#19#exact \n"}nil_first --> [6, 7, 8, 14, 15, 16]
nil_second --> [6, 11]
--------------------------
{1=>"The processing of the system is summarized as follows .\n", 2=>"Processing of the system is summarized as follows .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [0]
nil_second --> []
--------------------------
{1=>"Then , the document is represented in the form of a vector of weighted words appearing in the document .\n", 2=>"Then , the document is represented in the form of a vector of weighted words appearing in the document .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7,8,9,10#6,7,8,9#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact \n"}nil_first --> [10]
nil_second --> []
--------------------------
{1=>"An SVM-based classifier predicts the most probable topic for the vector , which also is the topic for the input document .\n", 2=>"An SVM-based classifier predicts the most probable topic for the vector , which also is the topic of the input document .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [17]
nil_second --> [17]
--------------------------
{1=>"In our experiment , for comparison of different word segmentation criteria in topic classification , we only vary the word segmentation model used for this task , while fixing other configurations .\n", 2=>"In our experiment for comparison of different word segmentation criteria in topic classification , we only vary the word segmentation model used for this task , while fixing other configurations .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#13#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#25#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact \n"}nil_first --> [26]
nil_second --> []
--------------------------
{1=>"Translation results are evaluated using the BLUE score ( Papineni et al., 2002 ) .\n", 2=>"Translation results are evaluated using BLUE score ( Papineni et al., 2002 ) .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>"The experimental results with text classification and English-Vietnamese statistical machine translation are shown in Table 10 and Table 11 , respectively .\n", 2=>"And the experimental results with text classification and English-Vietnamese statistical machine translation are shown in Table 10 and Table 11 , respectively .\n", 3=>"-1#1#0#lc -1#2#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#9#8#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#13#exact -1#15#14#exact -1#16#15#exact -1#17#16#exact -1#18#17#exact -1#19#18#exact -1#20#19#exact -1#21#20#exact -1#22#21#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"There are two important conclusions that can be drawn from these tables : ( 1 ) The quality of the treebank strongly affects the applications , since our BASE model and most of the other enhanced models improved the performance of TC and SMT systems ; ( 2 ) \" Splitting \" seems to be a good solution for word segmentation for controversial cases , including the split of variations , affxes , and classifier nouns .\n", 2=>"There are two important conclusions can be drawn from these tables : ( 1 ) Quality of the treebank strongly affects the applications since our BASE model and most of other enhanced models improved the performance of TC and SMT systems ; ( 2 ) \" Splitting \" seems to be a good solution for word segmentation of controversial cases , including the split of variations , affxes , and classifier nouns .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5,6#para -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15,16,17#16,17,18,19#para -1#18#20#exact -1#19#21#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#60#25#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#27,28#30,31,32,33#para -1#30#34#exact -1#31#35#exact -1#32#36#exact -1#33#37#exact -1#34#38#exact -1#35#39#exact -1#36#40#exact -1#37#41#exact -1#38#42#exact -1#39#43#exact -1#40#44#exact -1#41#45#exact -1#42#46#exact -1#43#47#exact -1#44#48#exact -1#45#49#exact -1#46#50#exact -1#47#51#exact -1#48#52#exact -1#50,51,52#53,54,55,56#para -1#53#57#exact -1#54#58#exact -1#55#59#exact -1#56#60#exact -1#58#62#exact -1#59#63#exact -1#68#64#exact -1#61#65#exact -1#62#66#exact -1#63#67#exact -1#64#68#exact -1#65#69#exact -1#66#70#exact -1#67#71#exact -1#69#73#exact -1#70#74#exact -1#71#75#exact -1#72#76#exact \n"}nil_first --> [61, 72]
nil_second --> [29, 49, 57]
--------------------------
{1=>"With the exception of STRUCT_NC , all of the modifications to the original VTB corpus increase the performance of WS .\n", 2=>"Except for STRUCT_NC , all the modifications to the original VTB corpus increase the performance of WS .\n", 3=>"-1#5#1#exact -1#0,1#2,3#para -1#2#4#exact -1#3#5#exact -1#4#6#exact -1#15#7#exact -1#13#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#14#16,17#para -1#16#19#exact -1#17#20#exact \n"}nil_first --> [0, 18]
nil_second --> []
--------------------------
{1=>"In both SMT and TC experiments , the BASE model , which is based on the manually-modified inconsistency of special characters , achieved better results than the ORG model .\n", 2=>"In both SMT and TC experiments , the BASE model which is based on the manually-modified inconsistency of special characters , achieved better results than the ORG model .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#20#10#exact -1#12,13#11,12,13,14#para -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [21]
nil_second --> [10, 11]
--------------------------
{1=>"The SMT results show that three out of six augmented models , VAR_SPLIT , VAR_FREQ and BASE , performed better than the ORG configuration .\n", 2=>"The SMT results show that three out of six augmented models , VAR_SPLIT , VAR_FREQ and BASE , gave higher performance than the ORG configuration .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#20#18#stem -1#21#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact -1#25#24#exact \n"}nil_first --> [19]
nil_second --> [18, 19]
--------------------------
{1=>"Among them , the best-performing model , VAR_SPLIT achieved 36 .91 BLEU score , which is 0 .55 higher than ORG .\n", 2=>"Among them , the best model VAR_SPLIT achieved 36 .91 BLEU score , which is 0 .55 higher than ORG .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#12#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact \n"}nil_first --> [4, 13]
nil_second --> [4]
--------------------------
{1=>"In TC results , all six augmented models achieved higher results than ORG .\n", 2=>"In TC results , all six augmented models have higher results than ORG .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact \n"}nil_first --> [8]
nil_second --> [8]
--------------------------
{1=>"In general , the augmented models performed better than the ORG .\n", 2=>"In general , the augmented models are better than the ORG .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> [6]
nil_second --> [6]
--------------------------
{1=>"Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining affxes with their head nouns resulted in slightly better results for WS and TC , and did not change the performance of SMT .\n", 2=>"Comparing the results of STRUCT_AFFIX and STRUCT_NC with BASE in WS , TC , and SMT , we can observe that combining affxes with their head nouns resulted in slightly better results for WS , TC , and does not change the performance of SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact -1#32#32#exact -1#33#33#exact -1#37#34#exact -1#35#35#exact -1#36#36#exact -1#39,40#38,39,40#para -1#41#41#exact -1#42#42#exact -1#43#43#exact -1#44#44#exact -1#45#45#exact \n"}nil_first --> [37]
nil_second --> [34, 38]
--------------------------
{1=>"However , the combination of classifier nouns with their head nouns had negative effects on WS and SMT .\n", 2=>"However , the combination of clasifier nouns with their head nouns had negative effects on WS and SMT .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact \n"}nil_first --> [5]
nil_second --> [5]
--------------------------
{1=>"Another part of the scope of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining .\n", 2=>"Another intention of our experiment is to compare two solutions for controversial cases of word segmentation , splitting and combining .\n", 3=>"-1#0#0#exact -1#13#2#exact -1#2#5#exact -1#3#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact \n"}nil_first --> [1, 3, 4, 16]
nil_second --> [1]
--------------------------
{1=>"Splitting and combining variations are reflected by VAR_COMB and VAR_SPLIT , while STRUCT_AFFIX and STRUCT_NC represent the combination of affixes or classifier nouns with the words that they modify .\n", 2=>"Splitting and combining variations are reflected by VAR_COMB and VAR_SPLIT , while STRUCT_AFFIX and STRUCT_NC represent the combination of affxes or classifier nouns with the words they modify .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [19, 26]
nil_second --> [19]
--------------------------
{1=>"Comparing VAR_COMB and VAR_SPLIT in both the TC experiment and SMT experiment , we see that the VAR_SPLIT results are better in both cases .\n", 2=>"Comparing VAR_COMB and VAR_SPLIT in both TC experiment and SMT experiment , we see that the VAR_SPLIT results are better in both cases .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#15#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [16]
nil_second --> []
--------------------------
{1=>"In this paper , we have provided a quantitative analysis of the difficulties in word segmentation , through the detection of problematic cases in the Vietnamese Treebank .\n", 2=>"In this paper , we have shown a quantitative analysis of the diffculties in word segmentation , through the detection of problematic cases in the Vietnamese treebank .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#lc -1#27#27#exact \n"}nil_first --> [6, 12]
nil_second --> [6, 12]
--------------------------
{1=>"Our experimental results showed that manual modification , done for annotation of special characters , and most other word segmentation criteria , significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , in comparison with the use of the original VTB corpus .\n", 2=>"Our experimental results showed that manual modification done for annotation of spe-cial characters and most of other word segmentation criteria significantly improved the performances of automatic word segmentation , text classification and statistical machine translation , comparing with the use of the original VTB corpus .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#28#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#12#13#exact -1#35#14#exact -1#13#15#exact -1#14#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#36#39#syn -1#37,38,39,40#40,41,42#para -1#41,42#43,44,45#para -1#43#46#exact -1#44#47#exact -1#45#48#exact \n"}nil_first --> [12, 21, 30, 37, 38]
nil_second --> [11, 15]
--------------------------
{1=>"Since the VTB corpus is the first effort in building a treebank for Vietnamese , and is the only corpus that is publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building efficient NLP systems .\n", 2=>"Since the VTB corpus is the first effort in building a treebank for Vietnamese , and is the only corpus publicly available for NLP research , this study contributes to further improvement of the corpus quality , which is essential for building effcient NLP systems .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#37,38#20,21#para -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#29#31#exact -1#30#32#exact -1#31#33#exact -1#32#34#exact -1#33#35#exact -1#34#36#exact -1#35#37#exact -1#36#38#exact -1#39,40#39,40,41,42#para -1#41#43#exact -1#43#45#exact -1#44#46#exact -1#45#47#exact \n"}nil_first --> [44]
nil_second --> [42]
--------------------------
{1=>"Face retrieval in large-scale news video datasets\n", 2=>"Face retrieval on large-scale news video datasets\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact \n"}nil_first --> [2]
nil_second --> [2]
--------------------------
{1=>"Face retrieval in news video has been identified as a challenging task due to huge variations in the visual appearance of the human face .\n", 2=>"Face retrieval in news video has been identified as a challenging task due to huge variations in visual appearance of human face .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21,22#para -1#21#23#exact -1#22#24#exact \n"}nil_first --> [17]
nil_second --> []
--------------------------
{1=>"Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .\n", 2=>"Although there are several approaches proposed to cope with this problem , their extremely high computational cost limits their scalability on largescale video datasets that may contain millions faces of hundreds characters .\n", 3=>"-1#0#0#exact -1#3#1#exact -1#4#2#exact -1#2#3,4#para -1#5#5#exact -1#6#6#exact -1#7,8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#29#28#exact -1#28#29#exact -1#30#30,31#para -1#31#33#exact -1#32#34#exact \n"}nil_first --> [20, 21, 32]
nil_second --> [1, 20, 21]
--------------------------
{1=>"In this paper , we introduce approaches to face retrieval that are scalable to such datasets while maintaining competitive performances with state-of-the-art approaches .\n", 2=>"In this paper , we introduce approaches for face retrieval which are scalable on such datasets while maintaining competitive performances with the state-of-the-art approaches .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#8#8#exact -1#9#9#exact -1#10,11#10,11#para -1#12#12#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#22#21#exact -1#23#22#exact -1#24#23#exact \n"}nil_first --> [7, 13]
nil_second --> [7, 13, 21]
--------------------------
{1=>"To utilize the variability of face appearances in video , we use a set of face images called face track to represent the appearance of a character in a video shot .\n", 2=>"To utilize the variability of face appearances in video , we use a set of face images called face-track to represent for the appearance of a character in a video shot .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#19#20#exact -1#20#21#exact -1#22#22#exact -1#23#23#exact -1#24#24#exact -1#25#25#exact -1#26#26#exact -1#27#27#exact -1#28#28#exact -1#29#29#exact -1#30#30#exact -1#31#31#exact \n"}nil_first --> [18, 19]
nil_second --> [18, 21]
--------------------------
{1=>"Our first proposal is an approach to extracting face tracks .\n", 2=>"Our first proposal is an approach for extracting face-tracks .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#7#7#exact -1#9#10#exact \n"}nil_first --> [6, 8, 9]
nil_second --> [6, 8]
--------------------------
{1=>"We use a point tracker to explore the connections between detected faces belonging to the same character and , then group them into one face track .\n", 2=>"We use a point tracker for exploring the connections between detected faces belonging to the same character , then grouping them into one face-track .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#13#5#exact -1#6,7#6,7#para -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#14,15#13,14,15#para -1#16#16#exact -1#17#18#exact -1#18#19#exact -1#19#20#stem -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#24#26#exact \n"}nil_first --> [17, 24, 25]
nil_second --> [5, 23]
--------------------------
{1=>"In the second proposal , we introduce an efficient approach to matching face tracks for retrieval .\n", 2=>"In the second proposal , we introduce an efficient approach to match face-tracks for retrieval .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#stem -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [12, 13]
nil_second --> [12]
--------------------------
{1=>"Instead of using all the faces in the face tracks to compute their similarity , our approach selects representative faces for each face track .\n", 2=>"Instead of using all faces in face-tracks to compute their similarity , our approach select representative faces for each face-track .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#16#8#stem -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#13#16#exact -1#14#17#stem -1#15#18#exact -1#17#20#exact -1#18#21#exact -1#20#24#exact \n"}nil_first --> [4, 7, 9, 19, 22, 23]
nil_second --> [6, 19]
--------------------------
{1=>"The representative faces are sampled from the original face track .\n", 2=>"The representative faces are sampled from the original face-track .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#10#exact \n"}nil_first --> [8, 9]
nil_second --> [8]
--------------------------
{1=>"As a result , we significantly reduce the computational cost of face-track matching while taking into account the variability of faces in face tracks to achieve high matching accuracy .\n", 2=>"As a result , we significantly reduce the computational cost for face-track matching while taking into account variability of faces in face-tracks for high matching accuracy .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#18#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14,15,16#14,15,16,17#para -1#17#18#exact -1#19#20#exact -1#20#21#exact -1#23#26#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact \n"}nil_first --> [19, 22, 23, 24, 25]
nil_second --> [10, 21, 22]
--------------------------
{1=>"Experiments are conducted on two face-track datasets extracted from real-world news videos , of such .\n", 2=>"Experiments are conducted on two face-track datasets extracted from real-world news videos , .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#15#exact \n"}nil_first --> [13, 14]
nil_second --> []
--------------------------
{1=>"scales that have never been considered in the literature .\n", 2=>"Their scales have not been considered in literature ever .\n", 3=>"-1#1#0#exact -1#2#2#exact -1#3#3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#9#9#exact \n"}nil_first --> [1, 7]
nil_second --> [0, 8]
--------------------------
{1=>"One dataset contains 1,497 face tracks of 41 characters extracted from 370 hours of TRECVID videos .\n", 2=>"One dataset contains 1,497 face-tracks of 41 characters extracted from 370 hours of TRECVID videos .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact \n"}nil_first --> [4, 5]
nil_second --> [4]
--------------------------
{1=>"The other dataset provides 5,567 face tracks of 111 characters observed from a television news program ( NHK News 7 ) over 11 years .\n", 2=>"The other dataset provides 5,567 face-tracks of 111 characters observed from television news program ( NHK News 7 ) channel in 11 years .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#13#exact -1#12#14#exact -1#13#15#exact -1#14#16#exact -1#15#17#exact -1#16#18#exact -1#17#19#exact -1#18#20#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact \n"}nil_first --> [5, 6, 12, 21]
nil_second --> [5, 19, 20]
--------------------------
{1=>"News videos play an important role as a source of information nowadays because of their rich and relevant contents .\n", 2=>"News videos play an important role in our sources of information nowadays because of their rich and important contents .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5,6#5,6#para -1#8#7,8#para -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16,17#16,17#para -1#18#18#exact -1#19#19#exact \n"}nil_first --> []
nil_second --> [7]
--------------------------
{1=>"With the advances in modern technology , a huge amount of news videos can be obtained easily .\n", 2=>"With the advances of modern technology , a huge amount of news videos can be obtained easily .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2,3#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7,8#7,8,9,10#para -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact \n"}nil_first --> []
nil_second --> [9, 10]
--------------------------
{1=>"Accordingly , this creates an urgent demand to retrieve useful information from such news video datasets .\n", 2=>"Accordingly , it creates an urgent demand for retrieving useful information in such news video datasets .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6,7#6,7#para -1#8#8#stem -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact \n"}nil_first --> [2, 11]
nil_second --> [2, 11]
--------------------------
{1=>"Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .\n", 2=>"Since most of the news is related to human , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .\n", 3=>"-1#0,1#0,1#para -1#4#2#exact -1#5#3#syn -1#6#4#exact -1#7#5#exact -1#8#6#para -1#9#7#exact -1#10#8#exact -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#16#14#exact -1#17#15#exact -1#18#16#exact -1#19#17#exact -1#20#18#exact -1#21#19#exact -1#22#20#exact -1#23#21#exact -1#24#22#exact -1#25#23#exact -1#26#24#exact -1#27#25#exact -1#28#26#exact -1#29#27#exact -1#30#28#exact -1#31#29#exact -1#32#30#exact -1#33#31#exact -1#34#32#exact -1#35#33#exact -1#36#34#exact \n"}nil_first --> []
nil_second --> [2, 3]
--------------------------
{1=>"A robust face retrieval system for large-scale news video datasets is indeed of much benefit in a wide range of applications .\n", 2=>"A robust face retrieval system on large-scale news video datasets is indeed of much benefit to a wide range of applications .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact \n"}nil_first --> [5, 15]
nil_second --> [5, 15]
--------------------------
{1=>"However , developing an accurate face retrieval system is not a trivial task because of the fact that the imaged appearance of a face changes dramatically under large variations in poses , facial expressions , and complex capturing conditions .\n", 2=>"However , developing an accurate face retrieval system is not a trivial task because of the fact that imaged appearance of a face changes dramatically under large variations in poses , facial expressions , and complex capturing conditions .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#15,16,17#14,15,16,17#para -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact -1#29#30#exact -1#30#31#exact -1#31#32#exact -1#32#33#exact -1#33#34#exact -1#34#35#exact -1#35#36#exact -1#36#37#exact -1#37#38#exact -1#38#39#exact \n"}nil_first --> [18]
nil_second --> [14]
--------------------------
{1=>"Besides accuracy , efficiency is also an issue in such a face retrieval system because the scales of available datasets are rapidly getting larger , for instance , exceeding thousands of hours of videos with millions of faces of hundreds of characters .\n", 2=>"On the other hand , efficiency is also an issue of such a face retrieval system beside its accuracy since scales of available datasets are getting larger rapidly , for instance , exceeding thousands hours of videos with millions faces of hundreds character .\n", 3=>"-1#18#1#exact -1#4#2#exact -1#5#3#exact -1#6,7#4,5,6,7#para -1#11#9#exact -1#12#10#exact -1#13#11#exact -1#14#12#exact -1#15#13#exact -1#1#15#exact -1#20#16#exact -1#21#17#exact -1#22#18#exact -1#23#19#exact -1#24#20#exact -1#27#21#exact -1#25#22#exact -1#26#23#exact -1#28#24#exact -1#29#25#exact -1#30#26#exact -1#31#27#exact -1#32#28#exact -1#33#29#exact -1#34,35#30,31#para -1#10#32#exact -1#36#33#exact -1#37#34#exact -1#38#35#exact -1#40#36#exact -1#39#37#exact -1#41#38,39#para -1#42#41#stem -1#43#42#exact \n"}nil_first --> [0, 8, 14, 40]
nil_second --> [0, 2, 3, 8, 9, 16, 17, 19]
--------------------------
{1=>"Generally , a face retrieval system consists of two principal steps .\n", 2=>"Generally , there are two principle steps in a face retrieval system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#8#2#exact -1#9#3#exact -1#10#4#exact -1#11#5#exact -1#3#6#para -1#4#7,8#para -1#6#10#exact -1#12#11#exact \n"}nil_first --> [9]
nil_second --> [2, 5, 7]
--------------------------
{1=>"The first step is extracting the appearance of faces in videos .\n", 2=>"The first step is extracting appearance of faces in video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#stem -1#10#11#exact \n"}nil_first --> [5]
nil_second --> []
--------------------------
{1=>", The second step is matching the extracted appearances with a given query so as to return a rank list .\n", 2=>"And , the second step is matching the extracted ones with a given query to return a rank list .\n", 3=>"-1#1#0#exact -1#2#1#lc -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#7#6#exact -1#8#7#exact -1#10#9#exact -1#11#10#exact -1#12#11#exact -1#13#12#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [8, 13, 14]
nil_second --> [0, 9]
--------------------------
{1=>"Whereas conventional approaches consider single face images as the basic units in extracting and matching \\CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .\n", 2=>"While conventional approaches consider single face images as the basic units for extracting and matching \\CITE , recently proposed approaches sifted towards sets of face images called face-tracks .\n", 3=>"-1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#21#21,22#para -1#23#24#exact -1#22#25#exact -1#24#27#exact -1#25#28#exact -1#26#29#exact -1#28#32#exact \n"}nil_first --> [0, 11, 20, 23, 26, 30, 31]
nil_second --> [0, 11, 20, 27]
--------------------------
{1=>"A face track contains multiple face images belonging to the same individual character within a video shot .\n", 2=>"A face-track contains multiple face images belonging to the same individual character within a video shot .\n", 3=>"-1#0#0#exact -1#2#3#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact \n"}nil_first --> [1, 2]
nil_second --> [1]
--------------------------
{1=>"The face images in a face track may present the corresponding character from different viewpoints and with different facial expressions ( as shown in Figure 1 ) .\n", 2=>"Face images in a face-track may present the corresponding character under different viewpoints and facial expressions ( as shown in Figure 1 ) .\n", 3=>"-1#7#0#lc -1#0#1#lc -1#1#2#exact -1#2#3#exact -1#3#4#exact -1#5#7#exact -1#6#8#exact -1#8#9,10#para -1#9#11#exact -1#11#12,13#para -1#12#14#exact -1#13#15#exact -1#14#18#exact -1#15#19#exact -1#16#20#exact -1#17#21#exact -1#18#22#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact \n"}nil_first --> [5, 6, 16, 17]
nil_second --> [4, 10]
--------------------------
{1=>"By exploiting the plenteous information from the multiple exemplar faces in the face tracks , face track-based approaches are expected to achieve a more robust and stable performance.\n", 2=>"By exploiting the plenteous information from multiple exemplar faces in face-tracks , face-track based approaches are expected to achieve more robust and stable performance.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6,7#para -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#11#14#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#23#exact -1#20#24#exact -1#21#25#exact -1#22#26#exact -1#23#27#exact \n"}nil_first --> [11, 12, 13, 15, 16, 22]
nil_second --> [10, 12, 13]
--------------------------
{1=>"Once all the face tracks in the video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .\n", 2=>"Once all face-tracks in video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#24#2#exact -1#25#3#exact -1#3#5#exact -1#13#6#exact -1#4#7#exact -1#5#8#exact -1#6#9#exact -1#7#10#exact -1#8#11#exact -1#9#12#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#21#16#exact -1#14#17#exact -1#15#18#exact -1#16#19#exact -1#17#20#exact -1#18#21#exact -1#19#22#exact -1#20#23#exact -1#22#24,25#para -1#23#26#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact \n"}nil_first --> [4, 27, 28]
nil_second --> [2]
--------------------------
{1=>"Because each face track is a set of face images , matching face tracks can essentially be thought of as a problem of matching image sets .\n", 2=>"Since each face-track is a set of face images , matching face-tracks essentially can be thought of as a problem of matching image sets .\n", 3=>"-1#0,1#0,1#para -1#7#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#13#14#exact -1#12#15#exact -1#14#16#exact -1#15,16,17,18#17,18,19#para -1#19#20,21#para -1#20#22#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact \n"}nil_first --> [3, 8, 12, 13]
nil_second --> [2, 11]
--------------------------
{1=>"Several approaches have been introduced to deal with this problem \\CITE .\n", 2=>"There are several approaches introduced to deal with this problem \\CITE .\n", 3=>"-1#2#0#lc -1#3#1#exact -1#1#2,3#para -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact \n"}nil_first --> []
nil_second --> [0]
--------------------------
{1=>"Using these approaches , the image set has been modeled in different ways , including as distributions \\CITE , subspaces \\CITE , a convex geometric region in a feature space \\CITE , or more general manifolds \\CITE .\n", 2=>"In these works , image set has been modeled in different way , such as distributions \\CITE , subspaces \\CITE , convex geometric region in feature space \\CITE , or more general manifolds \\CITE .\n", 3=>"-1#1#1#exact -1#3#3#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#stem -1#12#13#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#28#exact -1#26#29#exact -1#27#30#exact -1#28#31#exact -1#29#32#exact -1#30#33#exact -1#31#34#exact -1#32#35#exact -1#33#36#exact -1#34#37#exact \n"}nil_first --> [0, 2, 4, 14, 22, 27]
nil_second --> [0, 2, 13]
--------------------------
{1=>"Although these approaches have shown promising results in benchmark datasets , they require high computational costs to characterize the representation of face tracks , such as computing the convex geometric region in \\CITE , the probability in \\CITE , and the eigenvectors in \\CITE .\n", 2=>"Although these approaches shown promising results on benchmark datasets , they require high computational costs to characterize the representation of face-tracks , such as computing the convex geometric region in \\CITE , the probability in \\CITE , and the eigenvectors in \\CITE .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#29#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#21#23#exact -1#22#24#exact -1#23#25#exact -1#24#26#exact -1#25#27#exact -1#26#28#exact -1#27#29#exact -1#28#30#exact -1#34#31#exact -1#35#32#exact -1#36#33#exact -1#32#34#exact -1#33#35#exact -1#40#36#exact -1#41#37#exact -1#31#38#exact -1#37#39#exact -1#38#40#exact -1#39#41#exact -1#30#43#exact -1#42#44#exact \n"}nil_first --> [3, 21, 22, 42]
nil_second --> [6, 20]
--------------------------
{1=>"Their complexity in modeling face tracks and estimating the similarity between face tracks limits their practicability in large-scale datasets.\n", 2=>"Their complexity in modeling facetracks and estimating similarity between face-tracks limits their practicability on large-scale datasets.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6#7#exact -1#7#9#exact -1#8#10#exact -1#10#13#exact -1#11#14#exact -1#12#15#exact -1#14#17#exact -1#15#18#exact \n"}nil_first --> [4, 5, 8, 11, 12, 16]
nil_second --> [4, 9, 13]
--------------------------
{1=>"This paper provides a threefold contribution toward solving the above problems , .\n", 2=>"Working toward solving the above problems , our contributions in this paper is three-fold.\n", 3=>"-1#10#0#lc -1#11#1#exact -1#12#2#para -1#1#6#exact -1#2#7#exact -1#3#8#exact -1#4#9#exact -1#5#10#exact -1#6#11#exact \n"}nil_first --> [3, 4, 5, 12]
nil_second --> [0, 7, 8, 9, 13]
--------------------------
{1=>"Robust face-track extraction from news video .\n", 2=>"Robust face-track extraction on news video .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact \n"}nil_first --> [3]
nil_second --> [3]
--------------------------
{1=>"To enhance the performance of face-track matching , face tracks should first be extracted accurately .\n", 2=>"To enhance the performance of face-track matching , face-tracks should be first extracted accurately .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#9#10#exact -1#10,11#11,12#para -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [8, 9]
nil_second --> [8]
--------------------------
{1=>"motivated by a study of Everingham et al .\n", 2=>"Our approach is motivated by a study of Everingham et al .\n", 3=>"-1#3#0#exact -1#4#1#exact -1#5#2#exact -1#6#3#exact -1#7#4#exact -1#8#5#exact -1#9#6#exact -1#10#7#exact -1#11#8#exact \n"}nil_first --> []
nil_second --> [0, 1, 2]
--------------------------
{1=>"Our approach incorporates techniques to overcome specific problems with news video caused by sudden illumination change and partial occlusion , in contrast to the approach in \\CITE , which failed to deal with , these problems .\n", 2=>", In constrast to the approach in \\CITE , which is failed to deal with specific problems of news video caused by sudden illumination change and partial occlusion , our approach is incorporated techniques to overcome the problems .\n", 3=>"-1#29#0#lc -1#30#1#exact -1#31,32#2#para -1#33#3#exact -1#34#4#exact -1#35#5#exact -1#15#6#exact -1#16#7#exact -1#14#8#exact -1#18#9#exact -1#19#10#exact -1#20#11#exact -1#21#12#exact -1#22#13#exact -1#23#14#exact -1#24#15#exact -1#25#16#exact -1#26#17#exact -1#27#18#exact -1#28#19#exact -1#1#20#lc -1#3#22#exact -1#4#23#exact -1#5#24#exact -1#6#25#exact -1#7#26#exact -1#8#27#exact -1#9#28#exact -1#11#29#exact -1#12#30#exact -1#13#31#exact -1#0#33#exact -1#36,37#34,35#para -1#38#36#exact \n"}nil_first --> [21, 32]
nil_second --> [2, 10, 17]
--------------------------
{1=>"Evaluations of a collection of real-world news videos showed that our proposed face-track extraction approach achieved approximately 95% accuracy , a significant improvement compared to the approach in \\CITE .\n", 2=>"Evaluations on a collection of real-world news videos showed that our proposed face-track extraction approach achieved approximately 95% accuracy , a significant improvement compare the approach in \\CITE .\n", 3=>"-1#0#0#exact -1#4#1#exact -1#2#2#exact -1#3#3#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#18#18#exact -1#19#19#exact -1#20#20#exact -1#21#21#exact -1#22#22#exact -1#23#23#stem -1#24#25#exact -1#25#26#exact -1#26#27#exact -1#27#28#exact -1#28#29#exact \n"}nil_first --> [4, 24]
nil_second --> [1]
--------------------------
{1=>"We introduce an approach that significantly reduces the computational cost for face-track matching while maintaining a competitive performance with state-of-the-art approaches .\n", 2=>"We introduce an approach which significantly reduces the computational cost for face-track matching while maintaining a competitive performance compare to those of the state-of-the-art approaches .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3,4#3,4#para -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#11#11#exact -1#12#12#exact -1#13#13#exact -1#14#14#exact -1#15#15#exact -1#16#16#exact -1#17#17#exact -1#23#19#exact -1#24#20#exact -1#25#21#exact \n"}nil_first --> [18]
nil_second --> [18, 19, 20, 21, 22]
--------------------------
{1=>"Based on the observation that face tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all the faces in a face track for learning the variation of faces .\n", 2=>"Based on the observation that face-tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all faces in a face-track for learning the variation of faces .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#12#5#stem -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#26#13#exact -1#27#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact -1#20#21#exact -1#21#22#exact -1#22#23#exact -1#23#24#exact -1#24#25#exact -1#25#26#exact -1#32#27#exact -1#35#28#exact -1#13#29#exact -1#28#30#exact -1#30#33#exact -1#31#34#exact -1#33#35,36#para -1#34#37#exact -1#36#39#exact \n"}nil_first --> [6, 31, 32, 38]
nil_second --> [5, 29]
--------------------------
{1=>"Thus , a set of faces is sampled from the original face track for matching .\n", 2=>"Thus , a set of faces is sampled from the original face-track for matching .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#9#9#exact -1#10#10#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact \n"}nil_first --> [11, 12]
nil_second --> [11]
--------------------------
{1=>"The size of the set is much smaller than that of the original face track .\n", 2=>"The size of the set is much smaller than the size of original face-track .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#11#10#exact -1#9#11#exact -1#12#12#exact -1#14#15#exact \n"}nil_first --> [9, 13, 14]
nil_second --> [10, 13]
--------------------------
{1=>"The , mean face of the sampled faces in the set is then computed .\n", 2=>"Then , the mean face of sampled faces in the set is computed .\n", 3=>"-1#2#0#lc -1#1#1#exact -1#3#2#exact -1#4#3#exact -1#5#4#exact -1#9#5#exact -1#6#6#exact -1#7#7#exact -1#8#8#exact -1#10#10#exact -1#11#11#exact -1#0#12#lc -1#12#13#exact -1#13#14#exact \n"}nil_first --> [9]
nil_second --> []
--------------------------
{1=>"The similarity between two face tracks is the distance between their mean faces.\n", 2=>"The similarity between two face-tracks is the distance between their mean faces.\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact \n"}nil_first --> [4, 5]
nil_second --> [4]
--------------------------
{1=>"We investigated the problem of face retrieval in news video datasets whose scales have never been considered in the literature .\n", 2=>"We investigated the problem of face-retrieval on news video datasets whose scales have not been considered in literature ever .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#16#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact -1#13,14#14,15#para -1#15#16#exact -1#17#19#exact -1#19#20#exact \n"}nil_first --> [5, 6, 17, 18]
nil_second --> [5, 6, 18]
--------------------------
{1=>"Our first dataset is from 370 hours of TRECVID news videos and contains 405,887 detected faces belonging to 41 individuals .\n", 2=>"Our first dataset is from 370 hours TRECVID news videos which contains 405,887 detected faces belonging to 41 individuals .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#11#12#exact -1#12#13#exact -1#13#14#exact -1#14#15#exact -1#15#16#exact -1#16#17#exact -1#17#18#exact -1#18#19#exact -1#19#20#exact \n"}nil_first --> [7, 11]
nil_second --> [10]
--------------------------
{1=>"The second dataset includes 1.2 million faces of 111 individuals observed in the NHK News 7 program over 11 years .\n", 2=>"The second dataset is observed from NHK News7 channel in 11 years .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#4#10#exact -1#9#11#exact -1#6#13#exact -1#10#18#exact -1#11#19#exact -1#12#20#exact \n"}nil_first --> [3, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17]
nil_second --> [3, 5, 7, 8]
--------------------------
{1=>", .\n", 2=>"In this dataset , 1.2 millions faces of 111 individuals are provided .\n", 3=>"-1#3#0#exact -1#12#1#exact \n"}nil_first --> []
nil_second --> [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11]
--------------------------
{1=>"The total number of available face tracks is 5,567 .\n", 2=>"The total number of available face-track is 5,567 .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact \n"}nil_first --> [5, 6]
nil_second --> [5]
--------------------------
{1=>"The number of occurrences of each individual character varies from 4 to 550 .\n", 2=>"Number of occurrence of each individual character varies from 4 to 550 .\n", 3=>"-1#0#1#lc -1#1#2#exact -1#2#3#stem -1#3#4#exact -1#4#5#exact -1#5#6#exact -1#6#7#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#12#13#exact \n"}nil_first --> [0]
nil_second --> []
--------------------------
{1=>"Sections 3 and 4 describe our approaches to face-track extraction and matching , respectively .\n", 2=>"Section 3 and Section 4 describe our face-track extraction and matching , approaches respectively .\n", 3=>"-1#0#0#stem -1#1#1#exact -1#2#2#exact -1#4#3#exact -1#5#4#exact -1#6#5#exact -1#12#6#exact -1#7#8#exact -1#8#9#exact -1#9#10#exact -1#10#11#exact -1#11#12#exact -1#13#13#exact -1#14#14#exact \n"}nil_first --> [7]
nil_second --> [3]
--------------------------
{1=>"Section 5 presents our experimental settings , and Section 6 provides our .\n", 2=>"Section 5 presents our experimental settings , .\n", 3=>"-1#0#0#exact -1#1#1#exact -1#2#2#exact -1#3#3#exact -1#4#4#exact -1#5#5#exact -1#6#6#exact -1#7#12#exact \n"}nil_first --> [7, 8, 9, 10, 11]
nil_second --> []
