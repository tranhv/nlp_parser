Face retrieval on large-scale news video datasets
Face retrieval in large-scale news video datasets
2 0:0:preserved 1:1:preserved 2:2:bigrammar-prep 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Face retrieval in news video has been identified as a challenging task due to huge variations in visual appearance of human face .
Face retrieval in news video has been identified as a challenging task due to huge variations in the visual appearance of the human face .
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:22:preserved 21:23:preserved 22:24:preserved :17:mogrammar-det :21:mogrammar-det

Although there are several approaches proposed to cope with this problem , their extremely high computational cost limits their scalability on largescale video datasets that may contain millions faces of hundreds characters .
Although several approaches have been proposed to deal with this problem , their extremely high computational cost limits their scalability to large-scale video datasets that may contain millions of faces of hundreds of characters .
7 0:0:preserved 3:1:preserved 4:2:preserved 5:3,4,5:paraphrase 6:6:preserved 7:7:paraphrase 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-prep 21:21:spelling 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:33:preserved 32:34:preserved :28:mogrammar-prep :32:mogrammar-prep

In this paper , we introduce approaches for face retrieval which are scalable on such datasets while maintaining competitive performances with the state-of-the-art approaches .
In this paper , we introduce approaches to face retrieval that are scalable to such datasets while maintaining competitive performances with state-of-the-art approaches .
8 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-prep 8:8:preserved 9:9:preserved 10:10:bigrammar-others 11:11:preserved 12:12:preserved 13:13:bigrammar-prep 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved

To utilize the variability of face appearances in video , we use a set of face images called face-track to represent for the appearance of a character in a video shot .
To utilize the variability of face appearances in video , we use a set of face images called face track to represent the appearance of a character in a video shot .
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18,19:spelling 19:20:preserved 20:21:preserved 21::mogrammar-prep 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

Our first proposal is an approach for extracting face-tracks .
Our first proposal is an approach to extracting face tracks .
10 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-prep 7:7:preserved 8:9,8:spelling 9:10:preserved

We use a point tracker for exploring the connections between detected faces belonging to the same character , then grouping them into one face-track .
We use a point tracker to explore the connections between detected faces belonging to the same character and , then group them into one face track .
11 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5,6:paraphrase 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:bigrammar-wform 20:21:preserved 21:22:preserved 22:23:preserved 23:24,25:spelling 24:26:preserved

We present techniques to make the approach robust to common problems caused by sudden illumination changes , partial occlusions , and scattered appearances of characters in news videos .
We present techniques to make the approach robust to common problems caused by sudden illumination changes , partial occlusions , and scattered appearances of characters in news videos .
12 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

In the second proposal , we introduce an efficient approach to match face-tracks for retrieval .
In the second proposal , we introduce an efficient approach to matching face tracks for retrieval .
13 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-wform 12:12,13:spelling 13:14:preserved 14:15:preserved 15:16:preserved

Instead of using all faces in face-tracks to compute their similarity , our approach select representative faces for each face-track .
Instead of using all the faces in the face tracks to compute their similarity , our approach selects representative faces for each face track .
14 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:8,9:spelling 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:bigrammar-nnum 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22,23:spelling :4:mogrammar-det :7:mogrammar-det

The representative faces are sampled from the original face-track .
The representative faces are sampled from the original face track .
15 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9,8:spelling 9:10:preserved

As a result , we significantly reduce the computational cost for face-track matching while taking into account variability of faces in face-tracks for high matching accuracy .
As a result , we significantly reduce the computational cost of face-track matching while taking into account the variability of faces in face tracks to achieve high matching accuracy .
16 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-prep 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22,23:spelling 22:24,25:paraphrase 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved :17:mogrammar-det

Experiments are conducted on two face-track datasets extracted from real-world news videos , .
Experiments are conducted on two face-track datasets extracted from real-world news videos , of such .
17 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12,13,14:paraphrase 13:15:preserved

Their scales have not been considered in literature ever .
scales that have never been considered in the literature .
18 0,1:0:paraphrase 2,3,4,5,8:2,3,4,5,1:paraphrase 6:6:preserved 7:8:preserved 9:9:preserved :7:mogrammar-det

One dataset contains 1,497 face-tracks of 41 characters extracted from 370 hours of TRECVID videos .
One dataset contains 1,497 face tracks of 41 characters extracted from 370 hours of TRECVID videos .
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5,4:spelling 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved

The other dataset provides 5,567 face-tracks of 111 characters observed from television news program ( NHK News 7 ) channel in 11 years .
The other dataset provides 5,567 face tracks of 111 characters observed from a television news program ( NHK News 7 ) over 11 years .
20 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:spelling 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 20:21:bigrammar-prep 21:22:preserved 22:23:preserved 23:24:preserved :12:mogrammar-det

We make both datasets public for research community .
We make both datasets public for the research community .
21 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved :6:mogrammar-det

The experimental results demonstrate that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
The experimental results show that our proposed approaches achieved a remarkable balance between accuracy and efficiency.
22 0:0:preserved 1:1:preserved 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

News videos play an important role in our sources of information nowadays because of their rich and important contents .
News videos play an important role as a source of information nowadays because of their rich and relevant contents .
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6,7,8:6,7,8:paraphrase 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:paraphrase 18:18:preserved 19:19:preserved

With the advances of modern technology , a huge amount of news videos can be obtained easily .
With the advances in modern technology , a huge amount of news videos can be obtained easily .
28 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-prep 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Accordingly , it creates an urgent demand for retrieving useful information in such news video datasets .
Accordingly , this creates an urgent demand to retrieve useful information from such news video datasets .
29 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8:7,8:paraphrase 9:9:preserved 10:10:preserved 11:11:bigrammar-prep 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Since most of the news is related to human , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
Because most news are related to people , human face retrieval , which is defined as the task of extracting and returning faces relevant to a given query , obviously becomes an important task .
30 0:0:paraphrase 1:1:preserved 4:2:preserved 5:3:bigrammar-inter 6:4:preserved 7:5:preserved 8:6:paraphrase 9:11:preserved 10:8:preserved 11:9:preserved 12:10:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved 27:25:preserved 28:26:preserved 29:27:preserved 30:28:preserved 31:29:preserved 32:30:preserved 33:31:preserved 34:32:preserved 35:33:preserved 36:34:preserved

A robust face retrieval system on large-scale news video datasets is indeed of much benefit to a wide range of applications .
A robust face retrieval system for large-scale news video datasets is indeed of much benefit in a wide range of applications .
31 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:bigrammar-prep 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

For example , by applying face retrieval to a news video dataset , we are returned a list of relevant shots or scenes containing appearance of a selected well-known character .
For example , by applying face retrieval to a news video dataset , we are returned a list of relevant shots or scenes containing the appearance of a selected well-known character .
32 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved :24:mogrammar-det

With the list , important events related to the character can be detected or summarized.
With such a list , important events related to the character can be found or summarized.
33 0:0:preserved 1:1,2:paraphrase 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:paraphrase 13:14:preserved 14:15:preserved

However , developing an accurate face retrieval system is not a trivial task because of the fact that imaged appearance of a face changes dramatically under large variations in poses , facial expressions , and complex capturing conditions .
However , developing an accurate face retrieval system is not a trivial task because of the fact that the imaged appearance of a face changes dramatically under large variations in poses , facial expressions , and complex capturing conditions .
36 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved :18:mogrammar-det

On the other hand , efficiency is also an issue of such a face retrieval system beside its accuracy since scales of available datasets are getting larger rapidly , for instance , exceeding thousands hours of videos with millions faces of hundreds character .
Besides accuracy , efficiency is also an issue in such a face retrieval system because the scales of available datasets are rapidly getting larger , for instance , exceeding thousands of hours of videos with millions of faces of hundreds of characters .
37 0,1,2,3::unaligned 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:bigrammar-prep 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16,17,18:0,1:paraphrase 19:14:paraphrase 24,23,22,21,20:20,19,18,17,16:preserved 25:22:preserved 26:23:preserved 27:21:preserved 28:27:preserved 29,30:25,26:preserved 32,33:28,29:preserved 34:31:preserved 35:32:preserved 36:33:preserved 37:34:preserved 38:35:preserved 39:37:preserved 40:38:preserved 41:39:preserved 42:41,40:paraphrase 43:42:preserved :15:mogrammar-det :30:mogrammar-prep :36:mogrammar-prep

Thus , accurate and efficient approaches for face retrieval are always required.
Thus , accurate and efficient approaches to face retrieval are always required.
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-prep 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Generally , there are two principle steps in a face retrieval system .
Generally , a face retrieval system consists of two principal steps .
41 0:0:preserved 1:1:preserved 2,3:6:paraphrase 4,5,6:9,8,10:preserved 7:7:bigrammar-prep 11,10,9,8:5,4,3,2:preserved 12:11:preserved

The first step is extracting appearance of faces in video .
The first step is extracting the appearance of faces in videos .
42 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-nnum 10:11:preserved :5:mogrammar-det

And , the second step is matching the extracted ones with a given query to return a rank list .
, The second step is matching the extracted appearances with a given query so as to return a rank list .
43 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:paraphrase 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved

While conventional approaches consider single face images as the basic units for extracting and matching \CITE , recently proposed approaches sifted towards sets of face images called face-tracks .
Whereas conventional approaches consider single face images as the basic units in extracting and matching \CITE , recently proposed approaches shifted toward the use of sets of face images called face tracks .
44 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-det 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:typo 21:21:bigrammar-prep 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30,31:spelling

A face-track contains multiple face images belonging to the same individual character within a video shot .
A face track contains multiple face images belonging to the same individual character within a video shot .
45 0:0:preserved 1:1,2:spelling 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved

Face images in a face-track may present the corresponding character under different viewpoints and facial expressions ( as shown in Figure 1 ) .
The face images in a face track may present the corresponding character from different viewpoints and with different facial expressions ( as shown in Figure 1 ) .
46 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5,6:spelling 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:bigrammar-prep 11:13:preserved 12:14:preserved 13:15:preserved 14,15:18,19,17:paraphrase 22,21,20,19,18,17,16:20,21,22,24,23,25,26:preserved :0:mogrammar-det :16:mogrammar-prep

By exploiting the plenteous information from multiple exemplar faces in face-tracks , face-track based approaches are expected to achieve more robust and stable performance.
By exploiting the plenteous information from the multiple exemplar faces in the face tracks , face track-based approaches are expected to achieve a more robust and stable performance.
47 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12,13:spelling 11:14:preserved 12,13:16,15:spelling 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 23,22,21,20,19:27,26,25,24,23:preserved :6:mogrammar-det :11:mogrammar-det :22:mogrammar-det

Once all face-tracks in video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .
Once all the face tracks in the video shots are extracted , they are matched with the query to return a ranked list as the output of the face retrieval system .
50 0:0:preserved 1:1:preserved 2:4,3:spelling 3:5:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved :2:mogrammar-det :6:mogrammar-det

Since each face-track is a set of face images , matching face-tracks essentially can be thought of as a problem of matching image sets .
Because each face track is a set of face images , matching face tracks can essentially be thought of as a problem of matching image sets .
51 0:0:paraphrase 1:1:preserved 2:3,2:spelling 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13,12:spelling 12:15:preserved 13:14:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved

There are several approaches introduced to deal with this problem \CITE .
Several approaches have been introduced to deal with this problem \CITE .
52 0,1,4:2,3,4:para-freeword 2:0:preserved 3:1:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

They differ in the ways in which the sets are modeled and the similarity between sets is computed .
They differ in the ways in which the sets are modeled and the similarity between sets is computed .
53 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

In these works , image set has been modeled in different way , such as distributions \CITE , subspaces \CITE , convex geometric region in feature space \CITE , or more general manifolds \CITE .
Using these approaches , the image set has been modeled in different ways , including as distributions \CITE , subspaces \CITE , a convex geometric region in a feature space \CITE , or more general manifolds \CITE .
54 0:0:paraphrase 1:1:preserved 2:2:paraphrase 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:bigrammar-others 12:13:preserved 13,14:14,15:paraphrase 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37:preserved :4:mogrammar-det :22:mogrammar-det :27:mogrammar-det

Although these approaches shown promising results on benchmark datasets , they require high computational costs to characterize the representation of face-tracks , such as computing the convex geometric region in \CITE , the probability in \CITE , and the eigenvectors in \CITE .
Although these approaches have shown promising results in benchmark datasets , they require high computational costs to characterize the representation of face tracks , such as computing the convex geometric region in \CITE , the probability in \CITE , and the eigenvectors in \CITE .
55 0:0:preserved 1:1:preserved 2:2:preserved 3:3,4:bigrammar-vtense 4:5:preserved 5:6:preserved 6:7:bigrammar-prep 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21,22:spelling 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:40:preserved 39:41:preserved 40:42:preserved 41:43:preserved 42:44:preserved

Their complexity in modeling facetracks and estimating similarity between face-tracks limits their practicability on large-scale datasets.
Their complexity in modeling face tracks and estimating the similarity between face tracks limits their practicability in large-scale datasets.
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4,5:spelling 5:6:preserved 6:7:preserved 7:9:preserved 8:10:preserved 9:11,12:spelling 10:13:preserved 11:14:preserved 12:15:preserved 13:16:bigrammar-prep 14:17:preserved 15:18:preserved :8:mogrammar-det

Working toward solving the above problems , our contributions in this paper is three-fold.
This paper provides a threefold contribution toward solving the above problems , .
59 7,8,9,12,0:2,3,5:paraphrase 5,4,3,2,1:10,9,8,7,6:preserved 10,11:0,1:preserved 13:4:spelling

Robust face-track extraction on news video .
Robust face-track extraction from news video .
62 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-prep 4:4:preserved 5:5:preserved 6:6:preserved

To enhance the performance of face-track matching , face-tracks should be first extracted accurately .
To enhance the performance of face-track matching , face tracks should first be extracted accurately .
63 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:spelling 9:10:preserved 10:12:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved

, We introduce an approach for this purpose .
For this purpose , we introduce an approach .
64 0:3:preserved 1:4:preserved 2:5:preserved 3:6:preserved 4:7:preserved 5,6,7:0,1,2:preserved 8:8:preserved

Our approach is motivated by a study of Everingham et al .
motivated by a study of Everingham et al .
65 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved

The basic idea is to employ a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
The basic idea is to use a point tracker ( Kanade-Lucas-Tomasi tracker \CITE ) to establish the connections between faces belonging to the same character in consecutive frames of a shot .
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved

, In constrast to the approach in \CITE , which is failed to deal with specific problems of news video caused by sudden illumination change and partial occlusion , our approach is incorporated techniques to overcome the problems .
Our approach incorporates techniques to overcome specific problems with news video caused by sudden illumination change and partial occlusion , in contrast to the approach in \CITE , which failed to deal with , these problems .
67 4,5,6,7,1,2,3:23,24,25,26,20,21,22:preserved 9,12,13,14:30,31,32,28:preserved 11,10:29:para-passact 16:35:preserved 17:8:bigrammar-prep 18,19,20,21,22,23,24,25,26,27:9,10,11,12,13,14,15,16,17,18:preserved 28:27:preserved 29,30:0,1:preserved 32,33,34,35:5,4,3,2:preserved 36::mogrammar-det 37:7:preserved 38:36:preserved :34:mogrammar-det

Evaluations on a collection of real-world news videos showed that our proposed face-track extraction approach achieved approximately 95% accuracy , a significant improvement compare the approach in \CITE .
Evaluations of a collection of real-world news videos showed that our proposed face-track extraction approach achieved approximately 95% accuracy , a significant improvement compared to the approach in \CITE .
68 0:0:preserved 1:1:bigrammar-prep 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:bigrammar-wform 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :24:mogrammar-prep

Efficient face-track matching .
Efficient face-track matching .
71 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

We introduce an approach which significantly reduces the computational cost for face-track matching while maintaining a competitive performance compare to those of the state-of-the-art approaches .
We introduce an approach that significantly reduces the computational cost for face-track matching while maintaining a competitive performance with state-of-the-art approaches .
72 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-others 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18,19,20,21:18:paraphrase 22::mogrammar-det 23,24:19,20:preserved

Based on the observation that face-tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all faces in a face-track for learning the variation of faces .
Based on the observation that face tracks obtained by tracking provide highly similar faces in consecutive frames , we argue that it is redundant to use all the faces in a face track for learning the variation of faces .
73 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:spelling 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31,32:spelling 30:33:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37:preserved 35:38:preserved 36:39:preserved :27:mogrammar-det

Thus , a set of faces is sampled from the original face-track for matching .
Thus , a set of faces is sampled from the original face track for matching .
74 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12,11:spelling 12:13:preserved 13:14:preserved 14:15:preserved

The size of the set is much smaller than the size of original face-track .
The size of the set is much smaller than that of the original face track .
75 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10:9:paraphrase 11:10:preserved 12:12:preserved 13:14,13:spelling 14:15:preserved :11:mogrammar-det

Then , the mean face of sampled faces in the set is computed .
The , mean face of the sampled faces in the set is then computed .
76 0:12:preserved 1::unaligned 2:5:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved

The similarity between two face-tracks is the distance between their mean faces.
The similarity between two face tracks is the distance between their mean faces.
77 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4,5:spelling 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

Large-scale face-track datasets from real-world news videos .
Large-scale face-track datasets from real-world news videos .
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

We investigated the problem of face-retrieval on news video datasets whose scales have not been considered in literature ever .
We investigated the problem of face retrieval in news video datasets whose scales have never been considered in the literature .
81 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:spelling 6:7:bigrammar-prep 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13,18:14:paraphrase 14:15:preserved 15:16:preserved 16:17:preserved 17:19:preserved 19:20:preserved :18:mogrammar-det

Our first dataset is from 370 hours TRECVID news videos which contains 405,887 detected faces belonging to 41 individuals .
Our first dataset is from 370 hours of TRECVID news videos and contains 405,887 detected faces belonging to 41 individuals .
82 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:paraphrase 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :7:mogrammar-prep

The second dataset is observed from NHK News7 channel in 11 years .
The second dataset includes 1.2 million faces of 111 individuals observed in the NHK News 7 program over 11 years .
83 0:0:preserved 1:1:preserved 2:2:preserved 4,5:10,3,4,5,6,7,8,9:paraphrase 6:13:preserved 7:14,15:spelling 8:16:paraphrase 9:11:preserved 10:18:preserved 11:19:preserved 12:20:preserved :12:mogrammar-det

In this dataset , 1.2 millions faces of 111 individuals are provided .
, .
84 3:0:preserved

The total number of available face-track is 5,567 .
The total number of available face tracks is 5,567 .
85 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:spelling 6:7:preserved 7:8:preserved 8:9:preserved

Number of occurrence of each individual character varies from 4 to 550 .
The number of occurrences of each individual character varies from 4 to 550 .
86 0:1:preserved 1:2:preserved 2:3:bigrammar-nnum 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :0:mogrammar-det

Both datasets are published for the research community.
Both datasets are published for the research community.
87 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

The remaining of this paper is organized as follows .
The remainder of this paper is organized as follows .
90 0:0:preserved 1:1:bigrammar-wform 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

In Section 2 , we introduce related works in details .
In Section 2 , we introduce related works in detail .
91 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-nnum 10:10:preserved

Section 3 and Section 4 describe our face-track extraction and matching , approaches respectively .
Sections 3 and 4 describe our approaches to face-track extraction and matching , respectively .
92 3,0:0:bigrammar-others 1:1:preserved 2:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:6:preserved 13:13:preserved 14:14:preserved :7:mogrammar-prep

Section 5 presents our experimental settings , .
Section 5 presents our experimental settings , and Section 6 provides our .
93 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7,8,9,10,11:paraphrase

Conclusion is given in the final Section 6.
conclusions.
94 0:0:paraphrase

Face-track extraction .
Face-track extraction .
99 0:0:preserved 1:1:preserved 2:2:preserved

Face-track extraction is a key step in a video-based face retrieval system .
Face-track extraction is a key step in a video-based face retrieval system .
100 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Existing studies on automatic face-track extraction follow a standard paradigm that consists of two basic steps , detecting faces in frames and grouping faces of the same character into face-tracks .
The existing studies on automatic face-track extraction follow a standard paradigm that consists of two basic steps , detecting faces in frames and grouping faces of the same character into face tracks .
101 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:31,30:spelling 30:32:preserved :0:mogrammar-det

In the first step , Viola-Jones detector is usually employed to detect near frontal faces in frames of videos .
In the first step , the Viola-Jones detector is usually used to detect near frontal faces in frames of videos .
102 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:paraphrase 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved :5:mogrammar-det

Then , in the second step , detected faces of the same character will be grouped by using either clustering approaches \CITE or tracking approaches \CITE .
, In the second step , the detected faces of the same character are grouped by using either clustering \CITE or tracking approaches \CITE .
103 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13,14,15:13,14:bigrammar-vtense 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20,21:19:paraphrase 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved 26:24:preserved :6:mogrammar-det

In \CITE , Ramanan et al .
In \CITE , Ramanan et al .
104 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

builds a color histogram for the hair , face , and torso associated with each detected face in a frame .
built a color histogram for the hair , face , and torso associated with each detected face in a frame .
105 0:0:bigrammar-vtense 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

A concatenated vector of the normalized color histograms represents the face .
A concatenated vector of the normalized color histogram represented the face .
106 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-nnum 8:8:bigrammar-vtense 9:9:preserved 10:10:preserved 11:11:preserved

They then cluster all vectors to obtain groups of similar faces , using agglomerative clustering .
They then clustered all vectors to obtain groups of similar faces , using agglomerative clustering .
107 0:0:preserved 1:1:preserved 2:2:bigrammar-vtense 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Limitations of this approach includes the expensive computational cost for constructing and clustering high dimensional representation feature vectors; and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure no group contains faces of multiple characters and groups are not over-fragmented.
The limitations of this approach include its high computational cost for constructing and clustering high-dimensional representation feature vectors and , its dependence on determining a reasonable threshold for the clustering algorithm to ensure that no group contains faces of multiple characters and that groups are not over-fragmented.
108 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:bigrammar-inter 5:6:bigrammar-det 6:7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13,14:14:spelling 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:41:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved :0:mogrammar-det

On the other hand , Everingham etl al .
On the other hand , Everingham et al .
111 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:typo 7:7:preserved 8:8:preserved

in \CITE and Sivic et al .
\CITE and Sivic et al .
112 0::mogrammar-prep 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved

In \CITE , an affine covariance tracker of \CITE is used .
In \CITE , an affine covariance tracker of \CITE is used .
113 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

This tracker can develop tracks on deforming objects , where the between frame region deformation can be modelled by an affine geometric transformation plus perturbations .
This tracker can develop tracks on deforming objects , where the between-frame region deformation can be modeled by an affine geometric transformation plus perturbations .
114 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12:11:spelling 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:spelling 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved

The outcome is that a face can be tracked ( by the collection of regions on it ) through significant pose variations and expression changes , allowing association of possibly distant face detections .
The outcome is that a face can be tracked ( by the collection of regions on it ) through significant pose variations and expression changes , allowing the association of possibly distant face detections .
115 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved :27:mogrammar-det

The disadvantage of this tracker is the computational cost for locating and tracking affine covariance regions .
The disadvantage of this tracker is its high computational cost for locating and tracking affine covariance regions .
116 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-det 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved

Another way of using tracker is introduced by Everingham et al .
Another way of using a tracker was introduced by Everingham et al .
117 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:bigrammar-vtense 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :4:mogrammar-det

in \CITE , .
in \CITE , in which .
118 0:0:preserved 1:1:preserved 2:2:preserved 3:5,3,4:paraphrase

The authors employ Kanade-Lucas-Tomasi ( KLT ) tracker to create a set of point tracks starting at some frame in a shot and continuing until some later frame .
they used a Kanade-Lucas-Tomasi ( KLT ) tracker to create a set of point tracks starting at some frame in a shot and continuing until some later frame .
119 0,1,2:0,1,2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

Grouping faces in different frames of one character is based on enumerating track points shared between faces .
Grouping faces in different frames for one character is based on enumerating the track points shared between faces .
120 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved :12:mogrammar-det

Although using tracking is an efficient solution , it may return poor tracking results since trackers are very sensitive to illumination changes and partial occlusions .
Although using tracking is an efficient solution , it may return poor tracking results because trackers are very sensitive to illumination changes and partial occlusions .
121 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:paraphrase 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Face-track matching .
Face-track matching .
124 0:0:preserved 1:1:preserved 2:2:preserved

There are two major categories of approaches target to employ multiple-exemplar of faces in face-tracks ( i.e. , sets of face images ) for robust face matching and recognition .
There are two major categories of approaches to using multiple exemplars of faces in face tracks ( i.e. , sets of face images ) for robust face matching and recognition .
125 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 8:7:preserved 9:8:paraphrase 10:9,10:spelling 11:11:preserved 12:12:preserved 13:13:preserved 14:14,15:spelling 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved

Approaches in the first category \CITE make use of both face images and temporal order of their appearances .
The approaches in the first category \CITE make use of both face images and the temporal order of their appearances .
126 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved :0:mogrammar-det :14:mogrammar-det

Face dynamics within the video sequence are modeled and exploited to improve recognition accuracy .
The face dynamics within the video sequence are modeled and exploited to improve recognition accuracy .
127 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :0:mogrammar-det

For instance , Li et al .
For instance , Li et al .
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Edwards et al .
Edwards et al .
129 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

They than use the trained statistical face model to incorporate identity evidence over a sequence .
They then used the trained statistical face model to incorporate identity evidence over a sequence .
130 0:0:preserved 1:1:typo 2:2:bigrammar-vtense 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

In \CITE , Liu and Chen use an adaptive Hidden Markov Model ( HMM ) for this face recognition problem .
In \CITE , Liu and Chen used an adaptive hidden Markov model ( HMM ) for this face recognition problem .
131 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

In the training face , they create a HMM model for each character to learn the statistics and temporal dynamics using the eigen-face image sequence .
In the training face , they created a HMM for each character to learn the statistics and temporal dynamics using the eigen-face image sequence .
132 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-vtense 7:7:preserved 8:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved

The implicit constraint of these approaches is that dynamics of faces should be temporally consecutive .
The implicit constraint of these approaches is that the dynamics of faces should be temporally consecutive .
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved :8:mogrammar-det

In general , this constraint is not always satisfied.
In general , this constraint is not always satisfied.
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Without relying on temporal coherence between consecutive images , approaches in the second category uses multiple face images only .
Without relying on temporal coherence between consecutive images , the approaches in the second category use multiple face images only and .
137 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:bigrammar-others 15:16:preserved 16:17:preserved 17:18:preserved 18:19,20:paraphrase 19:21:preserved :9:mogrammar-det

They treat the problem as a set matching problem .
treat the problem as a set-matching problem .
138 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 7,6:5:spelling 8:6:preserved 9:7:preserved

These approaches are differentiated based on the ways in which the sets are modeled and the similarity between sets is computed .
These approaches are differentiated based on the ways in which the sets are modeled and the similarity between sets is computed .
139 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Shakhnarovich et al .
Shakhnarovich et al .
140 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

However , to make the computation tractable , they made a assumption that faces are normally distributed , which may not be true \CITE .
However , to make the computation tractable , they made the assumption that faces are normally distributed , which may not be true \CITE .
141 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-det 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

Cevikalp and Triggs \CITE claimed a face sequence was a set of points and discovered a convex geometric region expanded by these points .
Cevikalp and Triggs \CITE claimed that a face sequence is a set of points and discovered a convex geometric region expanded by these points .
142 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:bigrammar-vtense 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

The min-min approach \CITE considered a face sequence as a cluster of points and measured the distance between these clusters .
The min-min approach \CITE considered a face sequence as a cluster of points and measured the distance between these clusters .
143 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Subspace methods \CITE viewed a face sequence as points spread over a subspace .
Subspace methods \CITE viewed a face sequence as points spread over a subspace .
144 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Although these methods can be highly accurate , a lot of computation is needed to represent the distribution of the face sequence , such as computing the convex hulls in \CITE , the probability models in \CITE , and the eigenvectors in \CITE .
Although these methods can be highly accurate , a lot of computation is needed to represent the distribution of the face sequence , such as computing the convex hulls in \CITE , the probability models in \CITE , and the eigenvectors in \CITE .
145 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved

For this reason , they are not scalable for large-scale video datasets .
For this reason , they are not scalable to large-scale video datasets .
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:bigrammar-prep 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Face Datasets .
Face datasets .
149 0:0:preserved 1:1:preserved 2:2:preserved

To evaluate performance of face matching approaches , most of recent works on face retrieval in video uses two benchmark datasets Mobo ( Motion of Body ) \CITE and Honda / UCSD \CITE .
In evaluating the performance of face-matching approaches , most of the recent works on face retrieval in video use two benchmark datasets: Mobo ( Motion of Body ) \CITE and Honda / UCSD \CITE .
150 1,0:0,1:paraphrase 2:3:preserved 3:4:preserved 4,5:5:spelling 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved :2:mogrammar-det :10:mogrammar-det

Scales of these datasets are limited , they are varying from hundreds to thousands face images of tens individual characters .
The scales of these datasets are limited , varying from hundreds to thousands of face images of tens of individual characters .
151 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19:preserved 19:20:preserved 20:21:preserved :0:mogrammar-det :13:mogrammar-prep :18:mogrammar-prep

Particularly , Honda / UCSD consists of 75 videos involving 20 individual .
Particularly , Honda / UCSD consists of 75 videos involving 20 individuals .
152 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-nnum 12:12:preserved

Each video contains approximately 300-500 frames .
Each video contains approximately 300-500 frames .
153 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

Meanwhile , Mobo provides 96 image sets of 24 individuals .
Meanwhile , Mobo provides 96 image sets of 24 individuals .
154 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Hence , there are only 4 image sets for each individual .
Hence , there are only 4 image sets for each individual .
155 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

One of the largest available face dataset recently is the Youtube Faces dataset \CITE , .
One of the largest face datasets recently available is the YouTube Faces dataset \CITE , which .
156 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:7:preserved 5:4:preserved 6:5:bigrammar-nnum 7:6:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14,15:paraphrase 15:16:preserved

It provides 3,425 videos of 1,595 individual characters .
provides 3,425 videos of 1,595 individual characters .
157 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved

However , one character has only around 2.15 videos .
However , each character has only around 2.15 videos .
158 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Such a small number of samples for each character is not sufficient for stably evaluating a face matching or recognition approach , which is an important part of a face retrieval system .
Such a small number of samples for each character is not sufficient to stably evaluate a face-matching or recognition approach , which is an important part of a face retrieval system .
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:paraphrase 15:15:preserved 16,17:16:spelling 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved 31:30:preserved 32:31:preserved

In addition , there is no face dataset related to real-world news videos , which is our targeted domain .
In addition , there is no face dataset related to real-world news videos , which is our targeted domain .
160 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Because of all above mentioned reasons , we prepare new datasets for evaluating the approaches.
In view of all the above-mentioned considerations , we prepare new datasets for evaluating the approaches.
161 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3,4:5:spelling 5:6:paraphrase 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :4:mogrammar-det

Figure 2 illustrates the overview of our framework .
Figure 2 illustrates the overview of our framework .
166 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

In the offline stage , face-tracks in all shots of videos are extracted using our face-track extraction approach ( described in Section 4 ) .
In the off-line stage , the face tracks in all video shots are extracted using our face-track extraction approach ( described in Section 4 ) .
167 0:0:preserved 1:1:preserved 2:2:spelling 3:3:preserved 4:4:preserved 5:6,7:spelling 6:8:preserved 7:9:preserved 8:11:preserved 9::mogrammar-prep 10:10:bigrammar-nnum 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :5:mogrammar-det

One extracted face-track contains multiple face images of one individual character , varied under different viewpoints , illumination conditions , and expressions within a shot .
Each extracted face track contains multiple face images of one individual character , varied under different viewpoints , illumination conditions , and expressions within a shot .
168 0:0:paraphrase 1:1:preserved 2:2,3:spelling 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved

A single face image in a face-track is represented by a feature vector .
Each single face image in a face track is represented by a feature vector .
169 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:spelling 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved

The process consisting of face-track extraction and face image representation is performed once for the entire video dataset .
The process consisting of face-track extraction and face image representation is performed once for the entire video dataset .
170 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Our contribution here is to make the face-track extraction approach robust to sudden illumination changes , scattered appearance of characters , and occlusions.
Our contribution here is making the face-track extraction approach robust to sudden illumination changes , scattered appearances of characters , and occlusions.
171 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:4:bigrammar-wform 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:bigrammar-nnum 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved

Given a face-track as an input retrieval query , the online stage of our system starts by using our proposed face-track matching algorithm ( described in Section 5 ) to estimate the similarity between a query face-track and each face-track in the retrieved set containing all face-tracks extracted from the dataset in the offline stage .
Given a face track as an input retrieval query , the online stage of our system starts by using our proposed face-track matching algorithm ( described in Section 5 ) to estimate the similarity between a query face track and each face track in the retrieved set containing all face tracks extracted from the dataset in the offline stage .
174 0:0:preserved 1:1:preserved 2:2,3:spelling 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37,38:spelling 37:39:preserved 38:40:preserved 39:41,42:spelling 40:43:preserved 41:44:preserved 42:45:preserved 43:46:preserved 44:47:preserved 45:48:preserved 46:50,49:spelling 53,54,52,51,50,49,48,47:57,58,56,55,54,53,52,51:preserved

A ranked list of the evaluated face-tracks is returned as retrieval results of the online stage .
A ranked list of the evaluated face tracks is returned as the retrieval result of the online stage .
175 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:spelling 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:bigrammar-nnum 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved :11:mogrammar-det

Since the retrieved set is huge , our approach targets an extremely efficient face-track matching strategy while maintaining competitive performance with state-ofthe-art approaches.
Because the retrieved set is huge , our approach targets an extremely efficient face-track matching strategy while maintaining a competitive performance with state-of-the-art approaches.
176 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:typo 22:23:preserved :18:mogrammar-det

Given a video shot with occurrences of multiple characters , face-track extraction is the process of extracting sets of face images .
Given a video shot with occurrences of multiple characters , face-track extraction is the process of extracting sets of face images .
181 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

A set is supposed to contain face images of only one character who appears in the shot .
A set is supposed to contain the face images of only one character who appears in the shot .
182 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved :6:mogrammar-det

Such sets of face images are called face-tracks ( sometimes called face sequences ) .
Such sets of face images are called face tracks ( sometimes called face sequences ) .
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:spelling 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

A common strategy of existing approaches for face-track extraction consists of detecting faces in frames and grouping detected faces of the same character .
A common strategy in the existing approaches to face-track extraction consists in detecting faces in frames and grouping detected faces of the same character .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-prep 4:5:preserved 5:6:preserved 6:7:bigrammar-prep 7:8:preserved 8:9:preserved 9:10:preserved 10:11:bigrammar-prep 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved :4:mogrammar-det

While detecting faces is done by using a standard face detector ( e.g. , Viola-Jones face detector ) \CITE , grouping detected faces requires comprehensive techniques to identify faces of the same character.
Whereas detecting faces is done by using a standard face detector ( e.g. , Viola-Jones face detector ) \CITE , grouping detected faces requires comprehensive techniques to identify faces of the same character.
185 0:0:bigrammar-others 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved

In this section , we first briefly introduce an approach for face-track extraction proposed by Everingham et al .
In this section , we first briefly introduce an approach to face-track extraction proposed by Everingham et al .
188 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-prep 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Its problems as it is applied to news video and our proposed solutions to overcome the problems is then presented.
We then present the problems with this approach as applied to news video and our proposed solutions.
189 0:3:bigrammar-det 1:4:preserved 2,3,4:5,6,7,8:paraphrase 7,8,5,6,9,10,11,12:9,10,11,12,13,14,15,16:preserved 13::unaligned 17,19:0,2:paraphrase 18:1:preserved

To group detected faces into face-tracks , connections between faces belonging to the same character in different frames should be established .
To group detected faces into face tracks , connections should be established between faces belonging to the same character in different frames .
193 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6,5:spelling 6:7:preserved 7:8:preserved 11,10,9,8:15,14,13,12:preserved 17,16,15,14,13,12:21,20,19,18,17,16:preserved 20,19,18:11,10,9:preserved 21:22:preserved

Motion analysis can be used to investigate such connections .
Motion analysis can be used to investigate such connections .
194 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

If two faces in different frames are defined that they are translated faces of each other according to a motion , they are likely faces of the same character .
If two faces in different frames are defined that they are translated faces of each other according to a motion , they are likely faces of the same character .
195 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved

Everingham et al .
Everingham et al .
196 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

in \CITE propose to use KLT tracker for this purpose .
in \CITE proposed the use of a KLT tracker for this purpose .
197 0:0:preserved 1:1:preserved 2:2:bigrammar-vtense 3,4:3,4,5:paraphrase 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved :6:mogrammar-det

Their algorithm starts by detecting interest points in the first frame of the shot and propagating them to the next frames based on local appearance matching .
Their algorithm starts by detecting interest points in the first frame of the shot and propagating them to the next frames based on local appearance matching .
198 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Points which can not be propagated from one frame to the next are eliminated and replaced with new points .
Points that cannot be propagated from one frame to the next are eliminated and replaced with new points .
199 0:0:preserved 1:1:bigrammar-others 2,3:2:spelling 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved

Given two faces in different frames , if the number of point tracks passing through both faces is larger than half of the total number of point tracks which are not in common to both faces , they are grouped into one face-track.
Given two faces in different frames , if the number of point tracks passing through both faces is larger than half of the total number of point tracks that are not common to both faces , the faces are grouped into one face track.
200 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:bigrammar-others 29:29:preserved 30:30:preserved 32:31:preserved 33:32:preserved 34:33:preserved 35:34:preserved 36:35:preserved 37:36,37:paraphrase 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42,43:spelling

Although the approach by Everingham et al .
Although the approach by Everingham et al .
205 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

has demonstrated its efficiency and robustness on drama videos \CITE , directly applying the approach to news videos results poor performances due to following issues.
has shown its efficiency and robustness with drama videos \CITE , directly applying the approach to news videos results in poor performance due to the following issues.
206 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-prep 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:20:preserved 20:21:bigrammar-nnum 21:22:preserved 22:23:preserved 23:25:preserved 24:26:preserved :19:mogrammar-prep :24:mogrammar-det

Tracking errors due to sudden illumination change .
Tracking errors due to sudden illumination change .
209 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Since the KLT tracker uses intensity variance for computing the image motion to find the correspondence between points in different frames , it is unreliable when there is a sudden and significant change in illumination .
Because the KLT tracker uses intensity variance for computing the image motion to find the correspondence between points in different frames , it is unreliable when there is a sudden and significant change in illumination .
210 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved

As shown in Figure 3 ( top ) , points are distracted when flash occurs .
As shown in Figure 3 ( top ) , points are distracted when a flash occurs .
211 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved :13:mogrammar-det

As a result , the points are badly tracked .
As a result , the points are badly tracked .
212 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

The flash breaks all connections between faces in frames before and after its occurrence.
The flash breaks all connections between faces in the frames before and after its occurrence.
213 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :8:mogrammar-det

Unadaptive track point generation .
Unadaptive track point generation .
216 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved

In \CITE , track point generation is totally independent with face appearances .
In \CITE , the track point generation is totally independent from face appearances .
217 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-prep 10:11:preserved 11:12:preserved 12:13:preserved :3:mogrammar-det

New points are generated at the first frame of the shot or at a frame in which some existing points can not be propagated .
New points are generated at the first frame of the shot or at a frame in which some existing points cannot be propagated .
218 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20,21:20:spelling 22:21:preserved 23:22:preserved 24:23:preserved

As a result , a face , which does not appear in the aforementioned frames , may not contain any point .
As a result , a face that , does not appear in the aforementioned frames , may not contain any point .
219 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:6:bigrammar-others 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Its connections with other faces in the shot cannot be established for grouping.
Its connections with other faces in the shot cannot be established for grouping.
220 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Tracking errors due to occlusion .
Tracking errors due to occlusion .
223 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

To successfully connect actual faces of the same character in different frames , track points generated for the first face should be tracked and retained inside the latter faces for a sufficient number of shared points between faces .
To successfully connect actual faces of the same character in different frames , the track points generated for the first face should be tracked and retained inside the latter faces for a sufficient number of shared points between faces .
224 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved :13:mogrammar-det

However , when occlusion occurs , points are distracted by occluded regions .
However , when occlusion occurs , the points are distracted by occluded regions .
225 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved :6:mogrammar-det

Thus , the number of shared points drops , .
Thus , the number of shared points drops , .
226 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

It results in face connection failure .
resulting in face connection failure .
227 0,1:0:para-freeword 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved

As shown in Figure 3 ( bottom ) , when the woman moves the paper , which partially occludes her face in several frames , some points in her facial region are drifted with the paper .
As shown in Figure 3 ( bottom ) , when the woman moves the paper , which partially occludes her face in several frames , some points in her facial region are drifted with the paper .
228 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved

These points are not lost so they are not replaced by new points .
These points are not lost so they are not replaced by new points .
229 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

But , they become meaningless to determine the connection between faces.
However , they become meaningless in determining the connection between faces.
230 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:5,6:paraphrase 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Based on above observed limitations of the approach in \CITE on news videos , we integrate techniques to bypass these liminations in our proposed approach for face-track extraction on news videos.
Based on the observed limitations of the approach in \CITE when applied to news videos , we integrate techniques to bypass these restrictions in our proposed approach to face-track extraction in news videos.
233 0:0:preserved 1:1:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9,10,11:paraphrase 10:12:bigrammar-prep 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:paraphrase 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:bigrammar-prep 26:28:preserved 27:29:preserved 28:30:bigrammar-prep 29:31:preserved 30:32:preserved :2:mogrammar-det

Firstly , \CITE , our approach does not compare all possible pairs of faces in a shot for face grouping as in \CITE .
First , unlike in \CITE , our approach does not compare all possible pairs of faces in a shot for face grouping\CITE; .
236 0:0:bigrammar-wform 1:1,2,3:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19,20,21,22:21:para-colocation 23:22:preserved

Such pair-wise comparison rapidly becomes intractable as the number of faces in a shot increases .
such pairwise comparison rapidly becomes intractable as the number of faces in a shot increases .
237 0:0:preserved 1:1:spelling 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Instead of that , we group faces into face-track following temporal order of their appearances .
Instead , we group faces into face tracks according to the temporal order of their appearances .
238 0:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6,7:spelling 9:8,9:paraphrase 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved :10:mogrammar-det

A detected face in the current frame is considered to group into existing face-tracks formed by previously detected faces only .
A detected face in the current frame is considered for grouping into existing face tracks formed by previously detected faces only .
239 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10:10,9:paraphrase 11:11:preserved 12:12:preserved 13:13,14:spelling 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved

By doing this , we avoid greedy pairwise comparison.
By doing this , we avoid greedy pairwise comparison.
240 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Secondly , as our first observation , a sudden illumination change in any frame make the KLT tracker failed to track points properly .
Second , as described in our first observation , a sudden illumination change in any frame causes the KLT tracker to fail to track points properly .
243 0:0:bigrammar-wform 1:1:preserved 2:2,3,4:paraphrase 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:paraphrase 15:17:preserved 16:18:preserved 17:19:preserved 18:21,20:paraphrase 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved

Because such illumination changes are very common and they mostly appear together with important character in a news , a solution to this problem is vital .
Because such illumination changes are very common and mostly occur simultaneously with important characters in a news video , finding a solution to this problem is vital .
244 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 9:8:preserved 10:9:paraphrase 11:10:paraphrase 12:11:preserved 13:12:preserved 14:13:bigrammar-nnum 15:14:preserved 16:15:preserved 17:16:preserved 18:18:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved

We learn that the occurences of such illumination changes are usually very short ( less than 3 frames ) .
We learn that the occurrences of such illumination changes are usually very short ( less than 3 frames ) .
245 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-nnum 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

And , faces appeared in those frames are less informative for recognition since most of the facial identity characteristics are loss due to overlighting .
and that the , faces that appear in those frames are less informative for recognition because most of the facial identity characteristics are lost due to over-lighting .
246 0:0,1,2:paraphrase 1:3:preserved 2:4:preserved 3:5,6:paraphrase 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:paraphrase 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:bigrammar-wform 21:24:preserved 22:25:preserved 23:26:spelling 24:27:preserved

, They can not enrich information of its corresponding face-track , but may add noise .
Thus , the faces cannot enrich the information on its corresponding face track , but may only add noise .
247 0:0,1:paraphrase 1:2,3:paraphrase 2,3:4:spelling 4:5:preserved 5:7:preserved 6:8:bigrammar-prep 7:9:preserved 8:10:preserved 9:11,12:spelling 10:13:preserved 11:14:preserved 12:15,16:paraphrase 13,14:17,18:preserved :6:mogrammar-det

Therefore , our solution is to detect and skip all frames contain sudden illumination changes , .
Therefore , our solution is to detect and skip all frames containing sudden illumination changes , which .
248 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-wform 12:12:preserved 13:13:preserved 14:14:preserved 15:15,16:paraphrase 16:17:preserved

We call such frames as flashframes.
we call flash frames.
249 0:0:preserved 1:1:preserved 5:2,3:spelling

To indetify flash-frames , we measures the brightness of frames in the video shot .
To identify flash frames , we measure the brightness of the frames in the video shot .
252 0:0:preserved 1:1:typo 2:3,2:spelling 3:4:preserved 4:5:preserved 5:6:bigrammar-inter 6:7:preserved 7:8:preserved 8:9:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved :10:mogrammar-det

If the brightness of a frame significantly increases compared with those of its neighbors , the frame is declared as a flash-frame and is skipped for processing .
If the brightness of a frame is significantly increased compared with its neighbors , the frame is declared a flash frame and skipped in processing .
253 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8,6:paraphrase 8:9:preserved 9:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 20:18:preserved 21:19,20:spelling 22:21:preserved 24:22:preserved 25:23:bigrammar-prep 26:24:preserved 27:25:preserved

Particularly , given a frame \SYM with t indicates its frame index , we compute the average luminosity L of the frame \SYM and its consicutive frames \SYM , where i = \SYM; t +W+ 1 , and W is the potential length of a sudden illumination change .
Particularly , given a frame \SYM with t indicating its frame index , we compute the average luminosity L of the frame \SYM and its consecutive frames \SYM , where i = \SYM; t +W+ 1 , and W is the potential length of a sudden illumination change .
254 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:bigrammar-wform 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:spelling 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34:preserved 35:35:preserved 36:36:preserved 37:37:preserved 38:38:preserved 39:39:preserved 40:40:preserved 41:41:preserved 42:42:preserved 43:43:preserved 44:44:preserved 45:45:preserved 46:46:preserved 47:47:preserved 48:48:preserved

Then , we compare the average luminosity L of each frame \SYM in the set S = \SYM with s = t; t +W to those of \SYM and \SYM .
Then , we compare the average luminosity L of each frame \SYM in the set S = \SYM with s = t; t +W to those of \SYM and \SYM .
255 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved

If L( \SYM ) > L( \SYM ) and L( \SYM ) > L( \SYM ) , \SYM is defined as flash-frames regarding a predefined brightness sensitive threshold \SYM .
If L( \SYM ) > L( \SYM ) and L( \SYM ) > L( \SYM ) , \SYM is defined as flash frames according to a predefined brightness sensitive threshold \SYM .
256 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21,22:spelling 22:23,24:paraphrase 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved

In our experiments , we found that \SYM = 1:25 and W = {1; 2; 3} are optimal for detecting all flash-frames with a low false alarm rate.
In our experiments , we found that \SYM = 1:25 and W = {1; 2; 3} are optimal for detecting all flash frames with a low false alarm rate.
257 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21,22:spelling 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved

Given a video shot , our approach starts by finding the first frame in which faces are detected .
Given a video shot , our approach starts by finding the first frame in which faces are detected .
260 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

All point tracking and face grouping processes are initialized from this frame , not at the first frame of the shot as in \CITE .
All point-tracking and face-grouping processes are initialized from this frame , not at the first frame of the shot as in \CITE .
261 0:0:preserved 1,2:1:spelling 3:2:preserved 4,5:3:spelling 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved

This helps us to save computational cost as well as to avoid tracking errors caused by transition effects between shots .
This helps us to save on computational cost and avoid tracking errors caused by transition effects between shots .
262 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7,8,9:8:paraphrase 10,11:9:bigrammar-wform 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved :5:mogrammar-prep

Initial track points will be generated for all detected faces in the frame .
Initial track points will be generated for all detected faces in the frame .
263 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Each face now becomes the first face of a corresponding newly formed face-track.
Each face now becomes the first face of a corresponding newly formed face track.
264 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12,13:spelling

After the initialization , we sequentially process each frame afterwards , knowing all flash-frames will be skipped .
After the initialization , we sequentially process each frame , knowing all flash frames will be skipped .
267 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12,13:spelling 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

At a given frame , points from the previous frame are tracked by the KLT tracker to update their locations .
In a given frame , the points from the previous frame are tracked by the KLT tracker to update their locations .
268 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved :5:mogrammar-det

If there are faces detected , each face is checked against all existing facetracks formed in the previous frames to find out which facetrack it belongs to .
If there are faces detected , each face is checked against all the existing face tracks formed in the previous frames to find out to which face track the face belongs .
269 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14,15:spelling 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:25:preserved 23:26,27:spelling 24:28,29:paraphrase 25:30:preserved 26:24:preserved

Checking between a face and a facetrack is based on enumerating points shared by both the face and the last appeared face of the face-track .
The checking between a face and a face track is based on enumerating the points shared by both the face and the last face that appeared on the face track .
270 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7,8:spelling 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:25,24:paraphrase 21:23:preserved 22:26:bigrammar-prep 23:27:preserved 24:28,29:spelling :0:mogrammar-det :13:mogrammar-det

If the enumerated number is larger than half of the total number of points which are not in common to both faces , the faces is grouped into the face-track .
If the enumerated number is larger than half of the total number of points that are not common to both faces , the face is grouped into the face track .
271 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:bigrammar-others 15:15:preserved 16:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:bigrammar-others 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28,29:spelling 30:30:preserved

Our grouping criterion here is similar to \CITE .
Our grouping criterion here is similar to that in \CITE .
272 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7,8:paraphrase 7:9:preserved 8:10:preserved

A face which can not be grouped into any face-track is treated as an initial face of a new face-track .
A face that cannot be grouped into any face track is treated as the initial face of a new face track .
275 0:0:preserved 1:1:preserved 2:2:bigrammar-others 3,4:3:spelling 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8,9:spelling 10:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-det 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19,20:spelling 20:21:preserved

We then generate new track points inside such faces for tracking an grouping its corresponding faces in latter frames .
We then generate new track points within such faces for tracking and grouping its corresponding faces in latter frames .
276 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-prep 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:typo 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

In our approach , track points are generated in conjunction with face appearances .
In our approach , track points are generated in conjunction with face appearances .
277 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

We can ensure that there are always track points for all faces appear in the shot .
We can ensure that there are track points for all faces that appear in the shot .
278 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11,12:paraphrase 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Consequently , our approach overcomes the second observed limitation of \CITE.
Consequently , our approach overcomes the second observed limitation in \CITE.
279 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-prep 10:10:preserved

In other case , when a face in the current frame is grouped to an existing face-track , we prepare points for further tracking .
In other cases , when a face in the current frame is grouped into an existing face track , we prepare points for further tracking .
282 0:0:preserved 1:1:preserved 2:2:bigrammar-nnum 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:bigrammar-prep 14:14:preserved 15:15:preserved 16:16,17:spelling 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved

We remove all points which are inside the last appeared face of the face-track but not inside the current face , and vice versa .
We remove all points that are inside the last face that appeared on the face track but are not inside the current face , and vice versa .
283 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-others 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10,11:paraphrase 10:9:preserved 11:12:bigrammar-prep 12:13:preserved 13:14,15:spelling 14:16:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved

Since such points are likely tracked incorrectly , eliminating them prevent us from transferring tracking errors to latter frames .
Because such points are likely tracked incorrectly , eliminating them prevents us from transferring tracking errors to latter frames .
284 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-inter 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Points which are shared by both faces are kept .
Points that are shared by both faces are kept .
285 0:0:preserved 1:1:bigrammar-others 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

Besides , we generate additional points to replace the removed ones and to provide updated points .
Besides , we generate additional points to replace the removed ones and to provide updated points .
286 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

By doing that , our tracking results through a long sequence of frames become more accurate and reliable .
By doing so , our tracking results over a long sequence of frames become more accurate and reliable .
287 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-prep 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

As a result , we can partly bypass the third observed limitation of \CITE .
As a result , we can partly bypass the third observed limitation of \CITE .
288 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

When a face is partly and slowly occluded , our approach can discard incorrectly tracked points as well as reproduce points for the face after being occluded .
When a face is partly and slowly occluded , our approach can discard incorrectly tracked points and reproduce points for the face after it has been occluded .
289 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16,18,17:16:paraphrase 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23,24,25:bigrammar-vtense 26:26:preserved 27:27:preserved

Thus , the connection between faces before and after the occlusion are retained.
Thus , the connection between faces before and after the occlusion is retained.
290 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:bigrammar-inter 12:12:preserved

Our approach continuously process the next frame until reaching the end of the shot .
Our approach continuously processes the next frame until the end of the shot is reached .
293 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-inter 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:13,14:para-passact 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:15:preserved

The pseudo-code is presented in the Algorithm 1 as follows.
The pseudo-code is presented in Algorithm 1 as follows.
294 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5::mogrammar-det 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved

There are several approaches have been proposed for matching face-tracks ( as presented in Section 2 ) .
Several approaches to matching face tracks have been proposed ( as presented in Section 2 ) .
299 2:0:preserved 3:1:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:2:bigrammar-prep 8:3:preserved 9:5,4:spelling 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved

, Although these existing approaches achive high accuracy on benchmark datasets , their expensive computational costs limits their practical applications on large-scale datasets .
However , although these approaches have shown high accuracy in benchmark datasets , their high computational costs limit their practical applications in large-scale datasets .
300 0:1:preserved 1:2:preserved 2:3:preserved 4:4:preserved 5:5,6:paraphrase 6:7:preserved 7:8:preserved 8:9:bigrammar-prep 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:paraphrase 14:15:preserved 15:16:preserved 16:17:bigrammar-nnum 17:18:preserved 18:19:preserved 19:20:preserved 20:21:bigrammar-prep 21:22:preserved 22:23:preserved 23:24:preserved

This motivate us to target an matching approach which is balanced between accuracy and computational cost .
This motivates us to target a matching approach that provides a good balance between accuracy and computational cost .
301 0:0:preserved 1:1:bigrammar-nnum 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-det 6:6:preserved 7:7:preserved 8:8:bigrammar-others 9,10:9,10,11,12:paraphrase 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved

The approach should be extremely efficient while archiving competitive performance compare to state-of-the-art approachesf.
The approach should be extremely efficient while achieving a competitive performance with state-of-the-art approaches.
302 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:spelling 8:9:preserved 9:10:preserved 10,11:11:paraphrase 12:12:preserved 13:13:preserved :8:mogrammar-det

In order to maintain a competitive accuracy , we still employ plenteous information from multiple faces of a facetrack to enrich its representation .
To maintain competitive accuracy , we still use the plenteous information from the multiple faces of a face track to enrich the representation .
305 2:0:preserved 3:1:preserved 4::mogrammar-det 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:paraphrase 11:9:preserved 12:10:preserved 13:11:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17,18:spelling 19:19:preserved 20:20:preserved 21:21:bigrammar-det 22:22:preserved 23:23:preserved :8:mogrammar-det :12:mogrammar-det

However , instead of using all faces in a face-track , we propose to subsample the faces .
However , instead of using all the faces in a face track , we propose taking a subsample of faces .
306 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10,11:spelling 10:12:preserved 11:13:preserved 12:14:preserved 14,13:16,17,15:paraphrase 15::mogrammar-det 16:19:preserved 17:20:preserved :6:mogrammar-det :18:mogrammar-prep

By doing that , the require computational cost can be reduced while a sufficient amount of information is kept for improving accuracy .
In doing so , the required computational cost can be reduced while keeping the amount of information sufficient to improve accuracy .
307 0:0:bigrammar-prep 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:bigrammar-vtense 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12,13,17,18:17,12,13:paraphrase 14:14:preserved 15:15:preserved 16:16:preserved 19,20:18,19:paraphrase 21:20:preserved 22:21:preserved

We called our approach as k-Faces.
We call our approach k-Faces.
308 0:0:preserved 1:1:bigrammar-vtense 2:2:preserved 3:3:preserved 5:4:preserved

Given a specific value of k , which indicates the expected size of the sub-sampled set of a face-track , the approach starts by dividing each face-track into k parts following its temporal order .
Given a specific value of k , which indicates the expected size of the subsampled set of a face track , the approach starts by dividing each face track into k parts according to the temporal order of appearances .
311 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:spelling 15:15:preserved 16:16:preserved 17:17:preserved 18:18,19:spelling 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27,28:spelling 27:29:preserved 28:30:preserved 29:31:preserved 30:32,33:paraphrase 31:34:bigrammar-det 33,32:36,37,38,35:paraphrase

For each part , one face is selected to represent for all faces within the part .
For each part , one face is selected to represent all faces within the part .
312 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10::mogrammar-prep 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved

The mean face of k selected faces is then computed .
The mean face of k selected faces is then computed .
313 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

The similarity between two face-tracks is now the distance between their mean faces.
The similarity between two face tracks is now the distance between their mean faces.
314 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4,5:spelling 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved

Let denote mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} are two mean faces of two face-track A and B , respectively , with N imposes the number of dimension of the feature space .
Let mA = {\SYM; \SYM; : :; \SYM} and mB = {\SYM ; \SYM; : :; \SYM} denote the mean faces of face tracks A and B , respectively , with N representing the number of dimensions of the feature space .
317 0:0:preserved 1:17:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 23,19::unaligned 20:19:preserved 21:20:preserved 22:21:preserved 24:22,23:spelling 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved 31:30:preserved 32:31:preserved 33:32:paraphrase 34:33:preserved 35:34:preserved 36:35:preserved 37:36:bigrammar-nnum 38:37:preserved 39:38:preserved 40:39:preserved 41:40:preserved 42:41:preserved :18:mogrammar-det

We employ following standard distance types to compute the distance between mA and mB.
We use following standard distance types to compute the distance between mA and mB.
318 0:0:preserved 1:1:paraphrase 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

An illustration of our k-Faces , is shown in Figure 4 .
Figure 4 illustrates our k-Faces , with the following .
321 0,1,2,6,7,8:2,6,7,8:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 9,10:0,1:preserved 11:9:preserved

Its pseudo-code is presented as follows .
pseudo-code: .
322 1:0:paraphrase

Clearly , the higher value of k is selected , the more faces in each face-track are selected to compute the representative face of the face track .
Clearly , the higher the value of k selected , the more faces in each face track selected to compute the representative face and the .
325 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7,8:8:para-passact 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15,16:spelling 16,17:17:para-passact 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:paraphrase 24:24:preserved 27:25:preserved :4:mogrammar-det

And , better approximations , may result in higher accuracies .
, better the approximations , which may result in higher accuracies .
326 1:0:preserved 2:1:preserved 3:3:preserved 4:4,5:paraphrase 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved :2:mogrammar-det

However , the computational cost can overly increases .
However , the computational cost can overly increases .
327 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

By using k as a predefined parameter , k-Faces provides flexibility for users in balancing their expected accuracy and the cost which they can afford ( or time they can wait for the result ).
By using k as a predefined parameter , k-Faces provides users with flexibility in balancing the accuracy they expect and the cost they can afford ( or the time they can spend waiting for the result ).
328 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:12:preserved 11::mogrammar-prep 12:10:preserved 13:13:preserved 14:14:preserved 15,16:17,18:paraphrase 17:16:preserved 18:19:preserved 19:20:preserved 20:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31,32:paraphrase 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved :11:mogrammar-prep :15:mogrammar-det :27:mogrammar-det

Besides that , since k-Faces averages multiple faces for a representative face of a face-track , the effects of noisy or outliers faces on estimating the similarity of face-tracks will be substantially reduced.
Besides , because k-Faces averages multiple faces for the representative face of a face track , the effects of noisy or outlier faces on estimating the similarity of face tracks will be substantially reduced.
331 0:0:preserved 2:1:preserved 3:2:paraphrase 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:bigrammar-det 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13,14:spelling 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:bigrammar-nnum 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28,29:spelling 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved

In this section , we present our experiments to evaluate the proposed approaches .
In this section , we present our experiments to evaluate the proposed approaches .
336 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

The experiments are divided into two parts .
The experiments are divided into two parts; .
337 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

In the first part , we evaluate the performance of the proposed approach for face-track extraction , .
the first , evaluates the performance of the proposed approach in face-track extraction , and the second in .
338 1:0:preserved 2:1:preserved 4:2:preserved 5,6:3:paraphrase 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:bigrammar-prep 14:11:preserved 15:12:preserved 16:13:preserved 17:18:preserved

Evaluation of the proposed approach for face-track matching is given in the second part.
face-track matching.
339 6,7:0,1:paraphrase

We tested our proposed approach for face-track extraction on 8 video sequences from different video broadcasting stations , including NHK News 7 , ABC News , and CNN News.
We tested our proposed approach to face-track extraction on 8 video sequences from different video broadcasting stations , including NHK News 7 , ABC News , and CNN News.
343 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

All shot boundaries are provided in advance .
All shot boundaries are provided in advance .
346 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

A face detector based on Viola-Jones approach \CITE was used for detecting near frontal faces in every frame of these video sequences .
A face detector based on the Viola-Jones approach \CITE is used to detect near frontal faces in every frame of the video sequences .
347 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8,9:9,10:bigrammar-vtense 10,11:11,12:paraphrase 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:bigrammar-det 20:21:preserved 21:22:preserved 22:23:preserved :5:mogrammar-det

A conservative threshold is used to reduce the number of false positives ( i.e. , a non-face classified as a face ).
A conservative threshold is used to reduce the number of false positives ( i.e. , a non-face classified as a face ).
348 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved

Ground-truth information on face-tracks in videos is manually prepared .
Ground-truth information on the face tracks in videos is manually prepared .
351 0:0:preserved 1:1:preserved 2:2:preserved 3:4,5:spelling 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved :3:mogrammar-det

A face-track of one character appearing in a video shot is annotated by indexes of the frames which the first face and the last face of that character occur .
Each face track of a character appearing in a video shot is annotated by indexes of the frames in which the first face and the last face of that character occur .
352 0:0:paraphrase 1:1,2:spelling 2:3:preserved 3:4:paraphrase 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved :18:mogrammar-prep

An approach is called exactly extracting a face-track if it provides precise starting and ending frame indexes of the face-track , compared to ground-truth annotation .
An approach is considered as exactly extracting a face track if it provides precise starting and ending frame indexes of the face track , compared to ground-truth annotation .
353 0:0:preserved 1:1:preserved 2:2:preserved 3:3,4:paraphrase 4:5:preserved 5:6:preserved 6:7:preserved 7:8,9:spelling 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21,22:spelling 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved

Note that if a character moves out of the frame then moves in again , annotators will divide the appearance of that character into two independent face-tracks in our ground-truth .
Note that if a character moves out of the frame and then moves back into it again , annotators will divide the appearance of that character into two independent face tracks in ground-truth annotation .
354 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11,10:paraphrase 11:12:preserved 12:13,14,15:paraphrase 13:16:preserved 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29,30:spelling 27:31:preserved 28::mogrammar-det 29:32:preserved

The number of frames , faces , and face tracks are shown in Table 1 .
Table 1 shows the number of frames , faces , and face tracks .
355 0:3:preserved 1:4:preserved 2:5:preserved 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10,11,12:2:paraphrase 13,14:0,1:preserved 15:13:preserved

In this experiment , we directly compare our approach with one proposed by Everingham et al .
In this experiment , we directly compare our approach with that proposed by Everingham et al .
356 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:paraphrase 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

in \CITE.
in \CITE.
357 0:0:preserved 1:1:preserved

As shown in Table 2 , by detecting flash-frames , our approach successfully overcomes the problem of face-track fragmentation due to illumination changes .
As shown in Table 2 , by detecting flash frames , our approach successfully overcomes the problem of face-track fragmentation due to illumination changes .
360 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:spelling 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

Meanwhile , the approach by Everingham et al .
Meanwhile , the approach by Everingham et al .
361 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

is almost failed to do that .
almost completely fails to do that .
362 0:1,2:paraphrase 1:0:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved

In addition , the results also shows that our approach is superior to the approach by Everingham et al .
In addition , the results also show that our approach is superior to that of Everingham et al .
363 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-inter 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13,14:13:paraphrase 15:14:bigrammar-prep 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved

in handling problem caused by partial occlusion and appearance of character in the middle of a shot .
in handling problems caused by partial occlusion and the appearance of a character in the middle of a shot .
364 0:0:preserved 1:1:preserved 2:2:bigrammar-nnum 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved :8:mogrammar-det :11:mogrammar-det

All face-tracks which we could not extract exactly are those fully occluded at some frames during their occurences .
The only face tracks that we could not extract exactly are those fully occluded in some frames during their occurrences .
365 0:0,1:paraphrase 1:2,3:spelling 2:4:bigrammar-others 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:bigrammar-prep 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:spelling 18:20:preserved

In those cases , all points in face regions are drifted to background region .
In those cases , all points in the face regions are drifted to the background region .
366 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:14:preserved 13:15:preserved 14:16:preserved :7:mogrammar-det :13:mogrammar-det

Thus , there is no clue to re-group face of that person after such full occlusions .
After such full occlusions , there is no clue to regrouping the face of that person .
367 0::unaligned 1:4:preserved 2:5:preserved 3:6:preserved 4:7:preserved 5:8:preserved 6:9:preserved 7:10:bigrammar-wform 8,9,10,11:12,13,14,15:preserved 12,13,14,15:0,1,2,3:preserved 16:16:preserved :11:mogrammar-det

To handle this problem , using only tracker is not enough .
, Using only a tracker is not enough to handle this problem .
368 0,1,2,3:8,9,10,11:preserved 5,6:1,2:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:12:preserved :3:mogrammar-det

One can apply visual information based clustering to group the fragmented face-track , as in \CITE , .
One can apply visual information-based clustering to group the fragmented face track , as in \CITE , but this .
369 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:4:spelling 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10,11:spelling 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16,17,18:paraphrase 17:19:preserved

Obviously , extra cost is required .
obviously , requires extra cost .
370 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4,5:2:para-freeword 6:5:preserved

However , we observe that fully occlusion is rarely happened in news video since characters reported in the news are recorded with care , especially with important and well-known character .
Nevertheless , we observe that full occlusion rarely happens in news video because the characters featured in the news are recorded with care , especially the important and well-known ones .
371 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-wform 6:6:preserved 7,9:8:paraphrase 8:7:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:paraphrase 14:14:preserved 15:15:paraphrase 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25::mogrammar-prep 26:26:preserved 27:27:preserved 28:28:preserved 29:29:paraphrase 30:30:preserved :13:mogrammar-det :25:mogrammar-det

This is a special property of news videos .
This is a special characteristic of news videos .
372 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:paraphrase 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

The last column of the table shows the overall extraction performance of both approaches .
The last column of the table shows the overall extraction performance of both approaches .
373 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

These facts clearly indicate that our approach is robust and outperforms the approach of Everingham et al .
These facts clearly indicate that our approach is robust and outperforms that of Everingham et al .
374 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12:11:paraphrase 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved

in \CITE.
in \CITE.
375 0:0:preserved 1:1:preserved

In terms of speed , our approach is approximately 2 times slower than the approach of Everingham .
In terms of speed , our approach is approximately 2 times slower than that of Everingham et al .
378 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13,14:13:paraphrase 15:14:preserved 16:15:preserved 17:18,16,17:paraphrase

However , our complexity is somehow linear to total number of face , because we consequently enlarge face-tracks following temporal order by checking new faces with only one last appeared face of each face-track .
However , our complexity is somehow linear to the total number of faces , because we consequently enlarge face tracks according to the temporal order by checking new faces with only the last face that appeared on each face track .
379 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:bigrammar-nnum 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18,19:typo 18:20,21:paraphrase 19,20,21,22,23,24,25,26:23,24,25,26,27,28,29,30:preserved 29,27:35,34,31:paraphrase 28:32:preserved 30:33:preserved 31:36:bigrammar-prep 32:37:preserved 33:39,38:spelling :8:mogrammar-det :22:mogrammar-det

Meanwhile , Everingham et al .
Meanwhile , Everingham et al .
380 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved

compare all pairs of faces in the shot .
compared all pairs of faces in the shot .
381 0:0:bigrammar-vtense 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

Their complexity is polynomial to the total number of faces .
Their complexity is polynomial to the total number of faces .
382 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

If this number is getting larger , the gap in speed between our approach and the approach by Everingham et al .
If the number of faces increases , the gap in speed between our approach and that by Everingham et al .
383 0:0:preserved 1:1:bigrammar-det 2:2:preserved 3,4,5:5:paraphrase 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15,16:15:paraphrase 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved

will be narrowed rapidly.
will narrow rapidly.
384 0:0:preserved 2,1:1:para-passact 3:2:preserved

Because all presented problems here , such as those due to flash , occlusion , and in-the-middle face appearance , are practically observed , overcoming them is vital for practical application .
Because all the problems presented here , such as those due to flash , occlusion , and in-the-middle face appearance , are practically observed , overcoming them is vital for the practical application of our approach .
387 0:0:preserved 1:1:preserved 2:4:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:31:preserved 30:32,33,34,35:paraphrase :2:mogrammar-det :30:mogrammar-det

In this experiment , we show that our proposed techniques and solutions for the problems are robust and efficient enough for extracting face-tracks in real-world news videos by successfully extracting 94% of all face-tracks .
In this experiment , we show that our proposed techniques and solutions to the problems are robust and efficient enough for extracting face tracks in real-world news videos by successfully extracting 94% of all face tracks .
388 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-prep 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:23,22:spelling 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved 33:34,35:spelling 34:36:preserved

From our observations , one can use other complex techniques to handle the problems .
Based on our observations , other complex techniques can be applied to handle the problems .
389 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4,5,6:8,9,10:paraphrase 7:5:preserved 8:6:preserved 9:7:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

However , a trade-o_ between completely obtaining 6% remaining face-tracks and an overly expensive computational cost should be considered with care.
However , the trade-off between obtaining the 6% remaining face tracks and incurring an overly high computational cost should be considered with care.
390 0:0:preserved 1:1:preserved 2:2:bigrammar-det 3:3:typo 4:4:preserved 6:5:preserved 7:7:preserved 8:8:preserved 9:10,9:spelling 10:11,12:paraphrase 11:13:preserved 12:14:preserved 13:15:paraphrase 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved :6:mogrammar-det

Due to the limitations of existing public datasets , we prepare new datasets for experiments .
Due to the limitations of existing public datasets , we prepared new datasets for the experiments .
396 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-vtense 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved :14:mogrammar-det

Face-tracks in videos of the datasets are extracted by using our proposed approach for face-track extraction ( see section 4.2 ) .
Face tracks are extracted from videos of the datasets by using our proposed approach to face-track extraction ( see section 4.2 ) .
397 0:1,0:spelling 6,7,1:2,3,4:paraphrase 2:5:preserved 3:6:preserved 4:7:preserved 5:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:bigrammar-prep 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved

Identity of the character associated with each extracted face-track is given by annotators .
The identity of the character associated with each extracted face track is given by annotators .
398 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9,10:spelling 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved :0:mogrammar-det

Since our approach extract face-tracks in each video shot , shot boundaries for videos are required .
Because our approach extracts face tracks in each video shot , the shot boundaries of videos are required .
399 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:bigrammar-inter 4:4,5:spelling 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:bigrammar-prep 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved :11:mogrammar-det

A simple shot boundary detector based on color histogram of frames is used .
A simple shot boundary detector based on a color histogram of frames is used .
400 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved :7:mogrammar-det

The whole process , including detecting shot boundaries and face-track extraction , is fully automatic.
The whole process , including shot boundary detection and face-track extraction , is fully automated.
401 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,7:6,7:paraphrase 6:5:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:bigrammar-wform

TRECVID Dataset .
TRECVID dataset .
404 0:0:preserved 1:1:preserved 2:2:preserved

We used the TRECVID news videos from 2004 to 2006 .
We used TRECVID news videos from 2004 to 2006 .
405 0:0:preserved 1:1:preserved 2::mogrammar-det 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved

This dataset contains 370 hours of videos in different languages , such as English , Chinese , and Arabic .
This dataset contains 370 hours of videos in different languages , such as English , Chinese , and Arabic .
406 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

The total number of frames that we processed was approximately 35 millions frames .
The total number of frames that we processed was approximately 35 million .
407 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11,12:11:paraphrase 13:12:preserved

Among those , 20 millions faces were grouped into 157,524 face tracks .
Among those , 20 million faces were grouped into 157,524 face tracks .
408 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:bigrammar-nnum 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

We filtered out short face tracks that had less than ten faces , .
We filtered out short face tracks that had less than 10 faces , which .
409 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:spelling 11:11:preserved 12:12,13:paraphrase 13:14:preserved

This resulted in 35,836 face tracks .
resulted in 35,836 face tracks .
410 1:0:preserved 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved

Finally , we annotated 1,497 face tracks containing 405,887 faces of 41 well known individual characters .
Finally , we annotated 1,497 face tracks containing 405,887 faces of 41 well-known individual characters .
411 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12,13:12:spelling 14:13:preserved 15:14:preserved 16:15:preserved

NHKNews7 Dataset .
NHKNews7 dataset .
414 0:0:preserved 1:1:preserved 2:2:preserved

This dataset is observed from NHKNews7 channel in 11 years .
This dataset consists of observations from the NHK News 7 program over 11 years .
415 0:0:preserved 1:1:preserved 2,3:2,3,4:paraphrase 4:5:preserved 5:7,8,9:spelling 6:10:paraphrase 7:11:bigrammar-prep 8,9:12,13:preserved :6:mogrammar-det

After the annotation process , 1,259,320 faces of 111 individuals are provided .
After the annotation process , 1,259,320 faces of 111 individuals are provided .
416 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

The total number of face-tracks is 5,567 .
The total number of face tracks is 5,567 .
417 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4,5:spelling 5:6:preserved 6:7:preserved 7:8:preserved

Each character has from 4 to 550 face-tracks .
Each character has from 4 to 550 face tracks .
418 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:spelling 8:9:preserved

In this dataset , we discard facetracks with fewer than 100 faces and more than 500 faces .
In this dataset , we discard face tracks with fewer than 100 faces and more than 500 faces .
419 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:spelling 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved

Compared to the TRECVID dataset , NHKNews7 dataset is much more challenging.
Compared to the TRECVID dataset , the NHKNews7 dataset is much more challenging.
420 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :6:mogrammar-det

In the Table 4 , we compare our datasets with some public benchmark datasets .
Table 4 shows a , comparison between our datasets and some public benchmark datasets .
423 0,1,5,6:2,3,5,6:paraphrase 2:0:preserved 3:1:preserved 4:4:preserved 7:7:preserved 8:8:preserved 9:9:paraphrase 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

, It is obvious that our datasets are extremely higher than datasets , such as MoBo and Honda / UCSD , on all statistical terms , including the number of videos , characters , and average length of face-track .
Based on the results , it is obvious that our datasets are superior over the other datasets , such as MoBo and Honda / UCSD , on all statistical terms , including number of videos , number of characters , and average face-track length .
424 6,5,4,3,2,1:10,9,8,7,6,5:paraphrase 7:11:preserved 8,9,10:12,13:paraphrase 24,23,22,21,20,19,18,17,16,15,14,13,12,11:29,28,27,26,24,23,22,21,20,25,19,18,17,16:preserved 25::unaligned 26:31:preserved 27::mogrammar-det 34,32,33,30,29,28:40,38,39,34,33,32:preserved 31:36,35,37:paraphrase 35,36,37,38:41,42,43:para-colocation :14:mogrammar-det

Compared to Youtube Faces dataset , although ours have less number of character ( or subjects ) , we provide much more face-tracks ( or video shots ) per character , .
Compared to the YouTube Faces dataset , , we provide much more face tracks ( or video shots ) per character , although our datasets have smaller numbers of characters ( or subjects ) .
425 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:22:preserved 7:23,24:paraphrase 8:25:preserved 9:26:paraphrase 10:27:bigrammar-nnum 11:28:preserved 12:29:bigrammar-nnum 14,15,16,13:32,31,30,33:preserved 18,19,20,21:8,9,10,11:preserved 22:12,13:spelling 23,24,25,26,27:14,15,16,17,18:preserved 28,29:19,20:preserved 31:34:preserved :2:mogrammar-det

Thus , ours are more relevant for evaluating retrieval system.
Thus , our datasets are more relevant in evaluating a face retrieval system.
426 0:0:preserved 1:1:preserved 2:2,3:paraphrase 3:4:preserved 4:5:preserved 5:6:preserved 6:7:bigrammar-prep 7:8:preserved 8:11,10:paraphrase 9:12:preserved :9:mogrammar-det

Statistical information of our datasets is given in the Figure 5 .
Figure 5 presents statistical information on our datasets .
429 0:3:preserved 1:4:preserved 2:5:bigrammar-prep 3:6:preserved 4:7:preserved 5,6,7:2:paraphrase 8::mogrammar-det 9,10:0,1:preserved 11:8:preserved

The datasets can be downloaded at http: / / satohlab .
The datasets can be downloaded from http: / / satohlab .
430 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

ex.nii.ac.jp / users / ndthanh / NIIFacetrackDatasets .
ex.nii.ac.jp / users / ndthanh / NIIFacetrackDatasets .
431 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

However , due to copyright issues , face images in face-tracks can not be published .
However , due to copyright issues , the face images in the face tracks cannot be published .
432 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:12,13:spelling 11,12:14:spelling 13:15:preserved 14:16:preserved 15:17:preserved :7:mogrammar-det :11:mogrammar-det

Instead , we provide a feature vector , used in \CITE , for each face image .
Instead , we provide a feature vector , used in \CITE , for each face image .
433 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

A feature vector of a face is extracted by computing descriptors of the local appearance of the face around each of the located facial features .
The feature vector of a face is extracted by computing the descriptors of the local appearance of the face around each of the located facial features .
434 0:0:bigrammar-det 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved :10:mogrammar-det

Before extracting descriptors , the face is geometrically normalized to reduce the effect of pose variation .
Before extracting the descriptors , the face is geometrically normalized to reduce the effect of pose variation .
435 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :2:mogrammar-det

They estimate an affine transformation , which transform the located facial feature points to a canonical set of feature positions .
An affine transformation is estimated , which transforms the located facial feature points to a canonical set of feature positions .
436 0,1:3,4:paraphrase 2:0:preserved 3:1:preserved 4:2:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-inter 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Then , appearance descriptors are computed around each facial feature .
Then , the appearance descriptors around each facial feature are computed .
437 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4,5:9,10:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:11:preserved :2:mogrammar-det

The final feature representation of the face is formed by concatenating all descriptors of its facial features.
The final feature representation of the face is formed by concatenating all the descriptors of its facial features.
438 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :12:mogrammar-det

We compare k-Faces with several approaches , including approaches based on pair-wise distances , MSM \CITE and CMSM \CITE.
We compared k-Faces with several approaches , including those based on pair-wise distances , MSM \CITE and CMSM \CITE.
443 0:0:preserved 1:1:bigrammar-vtense 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:paraphrase 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Given two face-tracks having multiple face images represented as feature vectors , pair-wise based approaches compute distances between each possible pair of feature vectors in two face-tracks .
Given two face tracks having multiple face images represented as feature vectors , pair-wise-based approaches compute the distances between each possible pair of feature vectors in two face tracks .
446 0:0:preserved 1:1:preserved 2:2,3:spelling 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12,13:13:spelling 14:14:preserved 15:15:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27,28:spelling 27:29:preserved :16:mogrammar-det

They then use the maximum distance , the minimum distance , or the mean distance of the computed pair-wise distances as the similarity measurement between two face-tracks .
The maximum distance , the minimum distance , or the mean distance of the computed pair-wise distances is the used as the similarity measurement between two face tracks .
447 0,1,2:17,19:paraphrase 3:0:preserved 4:1:preserved 5:2:preserved 6:3:preserved 7:4:preserved 8:5:preserved 9:6:preserved 10:7:preserved 11:8:preserved 12:9:preserved 13:10:preserved 14:11:preserved 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26,27:spelling 27:28:preserved :18:mogrammar-det

We denote the approaches as pair:max , pair:min , and pair:mean , respectively ( see Figure 6 for illustration ) .
We refer to the approaches as pair:max , pair:min , and pair:mean , respectively ( see Figure 6 for the illustration ) .
448 0:0:preserved 1:1,2:paraphrase 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:20:preserved 19:21:preserved 20:22:preserved :19:mogrammar-det

The pair:min ( sometimes called min-min ) is a state-of-the-art approach widely used in other studies \CITE.
The pair:min ( sometimes called min-min ) is a state-of-the-art approach widely used in other studies \CITE.
449 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

Regarding to \CITE , if the pair-wise based approaches are representative for non-parametric sampled based approaches , MSM and CMSM are representative for approaches based on parametric model .
Regarding \CITE , if the pair-wise-based approaches are representative of nonparametric sample-based approaches , MSM and CMSM are representative of approaches based on a parametric model .
452 0:0:preserved 1::mogrammar-prep 2:1:preserved 3:2:preserved 4:3:preserved 5:4:preserved 6,7:5:spelling 8:6:preserved 9:7:preserved 10:8:preserved 11:9:bigrammar-prep 12:10:spelling 13,14:11:spelling 15:12:preserved 16:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20:17:preserved 21:18:preserved 22:19:bigrammar-prep 23:20:preserved 24:21:preserved 25:22:preserved 26:24:preserved 27:25:preserved 28:26:preserved :23:mogrammar-det

MSM , introduced by Yamaguchi et al .
MSM , introduced by Yamaguchi et al .
453 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

The similarity between the sets is computed using the angle between subspaces .
The similarity between the sets is computed using the angle between subspaces .
454 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

CMSM is an extension of MSM , in which subspaces of the sets are projected on a constraint subspace .
CMSM is an extension of MSM , in which subspaces of the sets are projected onto a constraint subspace .
455 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:bigrammar-prep 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

By doing that , the subspaces are expected to be better separatable .
In doing so , the subspaces are expected to be more separable .
456 0:0:bigrammar-prep 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:paraphrase 11:11:spelling 12:12:preserved

All of these approaches had been shown their robustness on benchmark datasets , such as MoBo , HondaUCSD , and Youtube Faces .
All of these approaches have shown their robustness in benchmark datasets , such as MoBo , HondaUCSD , and YouTube Faces .
457 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5,6:4,5:bigrammar-vtense 7:6:preserved 8:7:preserved 9:8:bigrammar-prep 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved

Therefore , it is appealing to compare our k-Faces with them for a comprehensive evaluation.
Therefore , it is appealing to compare our k-Faces with them for a comprehensive evaluation.
458 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Besides evaluating k-Faces with different values of k as well as different types of distance ( e.g. , Euclidean , L1 , cosine ) , we try another criterion to select k representative faces in a face-track .
Besides evaluating k-Faces with different values of k and different types of distance ( e.g. , Euclidean , L1 , and cosine ) , we try another criterion for selecting k representative faces in a face track .
461 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8,9,10:8:paraphrase 11:9:preserved 12:10:preserved 13:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:20:paraphrase 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29,30:28,29:paraphrase 31:30:preserved 32:31:preserved 33:32:preserved 34:33:preserved 35:34:preserved 36:36,35:spelling 37:37:preserved :19:unaligned

In the original way , we proposed to select these faces by partitioning the face-track following temporal order and selecting the middle face of each partition .
In the original way , we proposed selecting these faces by partitioning the face track according to the temporal order and choosing the middle face of each partition .
462 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7,8:7:bigrammar-wform 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13,14:spelling 15:15,16:paraphrase 16:18:preserved 17:19:preserved 18:20:preserved 19:21:paraphrase 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved :17:mogrammar-det

However , an yet another criterion can be applied to select these representative faces is based on clustering .
However , another criterion that is based on clustering can be applied in selecting these representative faces .
463 0:0:preserved 1:1:preserved 4:2:preserved 5:3:preserved 6:9:preserved 7:10:preserved 8:11:preserved 9,10:12,13:paraphrase 11:14:preserved 12:15:preserved 13:16:preserved 14,15,16,17:5,6,7,8:preserved 18:17:preserved

In this new way , all faces in a face-track will be clustered in to k groups by a clustering algorithm .
In this new way , all the faces in a face track will be clustered to k groups by using a clustering algorithm .
464 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10,11:spelling 10:12:preserved 11:13:preserved 12:14:preserved 13::mogrammar-prep 14:15:preserved 15:16:preserved 16:17:preserved 17:18,19:paraphrase 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved :6:mogrammar-det

The centroid of each group is selected .
The centroid of each group is selected .
465 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved

Then , the mean of k centroids is used as the representative face for the face-track .
Then , the mean of k centroids is used as the representative face for the face track .
466 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15,16:spelling 16:17:preserved

In this experiment , we use the standard K-Means for clustering .
In this experiment , we use the standard K-Means for clustering .
467 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

We denote the former k-Faces as k-Faces.Temporal and the latter k-Faces as k-Faces.KMeans.
We refer to the former k-Faces as k-Faces.Temporal and to the latter k-Faces as k-Faces.KMeans.
468 0:0:preserved 1:1,2,9:paraphrase 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved

We evaluate performance of a face-track matching approach by computing the average precision on the rank list returned by the approach .
We evaluate the performance of a face-track matching approach by computing the average precision of the rank list that it returned .
471 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:bigrammar-prep 14:15:preserved 15:16:preserved 16:17:preserved 17,18,19,20:20,18,19:paraphrase 21:21:preserved :2:mogrammar-det

In particular , for each dataset , each face-track is alternatively picked out as a query facetrack , while the remaining face-tracks are used as the retrieved database .
In particular , in each dataset , a face track is alternatively picked out as a query face track , while the remaining face tracks are used as the retrieved database .
472 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-prep 4:4:preserved 5:5:preserved 6:6:preserved 7:7:paraphrase 8:8,9:spelling 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:18,17:spelling 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23,24:spelling 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved

, Average precision of the returned ranked list is computed , given a query .
Given a query , the average precision of the returned ranked list is computed , .
473 0:3:preserved 3,2,1:7,6,5:preserved 9,8,7,6,5,4:13,12,11,10,9,8:preserved 11,12,13:0,1,2:preserved 14:15:preserved :4:mogrammar-det

Finally , the mean of all average precision ( MAP ) from all query is reported as the overall evaluation metric for the approach on the database.
Finally , the mean of all average precision ( MAP ) values for all queries is reported as the overall evaluation metric for the approach with the given database.
474 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9,11:paraphrase 10:10:preserved 11:12:bigrammar-prep 12:13:preserved 13:14:bigrammar-others 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:bigrammar-prep 25:26:preserved 26:28,27:paraphrase

Let denote r as a rank in the returned face-track list , Pre( r ) as is the precision at the rank r of the list , Nl as the length of the list , Nhit as the total number of face-tracks matched with the query face-track q , and I sMatched( k ) as a binary function returning 1 if the face-track at rank r is matched with q ( based on ground-truth annotations ) , zero otherwise .
Let r denote a rank in the returned face-track list , Pre( r ) the precision at rank r of the list , Nl the length of the list , Nhit the total number of face tracks matched with the query face track q , and I sMatched( k ) a binary function returning 1 if the face track at rank r is matched with q ( based on ground-truth annotations ) and , zero otherwise .
477 0:0:preserved 1:2:preserved 2:1:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 17:14:preserved 18:15:preserved 19:16:preserved 20::mogrammar-det 21,22,23,24,25:20,17,18,19,21:preserved 26:29:preserved 27,30,29,31,32,33,35,36:23,24,25,26,27,28,30,31:preserved 37::mogrammar-det 38,39,40:32,33,34:preserved 41:35,36:spelling 43,42:38,37:preserved 45,44:40,39:preserved 46:42,41:spelling 47,49,50,51,52,53,48,54,55,56,57,58,59,60,61:43,45,46,47,44,48,49,50,51,52,53,54,55,56:preserved 62:57,58:spelling 66,67,68,69,65,64,63:62,63,64,65,61,60,59:preserved 70,71,72,73,74,75:71,70,69,68,67,66:preserved 76:73:preserved 77:74:preserved 78:75:preserved 79:76:preserved

Then , the MAP of the evaluated approach can be computed as following: \MATH
Then , the MAP of the evaluated approach can be computed as follows: \MATH
478 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:paraphrase 13:13:preserved

MAP is a standard metric to evaluate retrieval and matching systems .
The MAP is a standard metric for evaluating retrieval and matching systems .
481 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5,6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved :0:mogrammar-det

Besides MAP , we record processing times of the approaches on each dataset for efficiency comparison.
Besides the MAP , we record the processing times of the approaches in each dataset to compare their efficiency.
482 0:0:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:bigrammar-prep 11:13:preserved 12:14:preserved 13,15,14:15,16,17,18:paraphrase :1:mogrammar-det

Figure 7 presents Mean Average Precision ( MAP ) of all evaluated approaches on our two datasets , Trecvid and NHKNews7 .
Figure 7 presents the mean average precision ( MAP ) of all the evaluated approaches in our two datasets , Trecvid and NHKNews7 .
487 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:13:preserved 12:14:preserved 13:15:bigrammar-prep 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved :3:mogrammar-det :12:mogrammar-det

Generally , all MAPs vary from 64.61% to 76.54% on Trecvid dataset .
Generally , all the MAPs vary from 64.61% to 76.54% in the Trecvid dataset .
488 0:0:preserved 1:1:preserved 2:2:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-prep 10:12:preserved 11:13:preserved 12:14:preserved :3:mogrammar-det :11:mogrammar-det

Meanwhile , , the best MAP is 60.99% , and the worst MAP is 42.75% on NHKNews7 dataset .
Meanwhile , in the NHKNews7 dataset , the best MAP is 60.99% , and the worst is 42.75% .
489 0:0:preserved 1:1:preserved 4,5,3,6,7:7,8,9,10,11:preserved 8:6:preserved 9,10,11:13,14,15:preserved 12::unaligned 13:16:preserved 14:17:preserved 16,17:4,5:preserved 18:18:preserved :2:mogrammar-prep :3:mogrammar-det

The gap of MAPs between two datasets can be explained by following reasons .
The difference in the MAPs between the two datasets can be explained by following reasons .
490 0:0:preserved 1:1:paraphrase 2:2:bigrammar-prep 3:4:preserved 4:5:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved :3:mogrammar-det :6:mogrammar-det

Firstly , the number of characters in NHKNews7 is more larger than those in Trecvid , 111 characters in NHKNews7 compared to 41 characters in Trecvid .
First , the number of characters in NHKNews7 is larger than that in Trecvid , 111 characters in NHKNews7 compared to 41 characters in Trecvid .
491 0:0:bigrammar-wform 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 10:9:preserved 11:10:preserved 12:11:bigrammar-others 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 24:23:preserved 25:24:preserved 26:25:preserved

This clearly increases the probability of mismatching face-tracks .
This clearly increases the probability of mismatching face tracks .
492 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:spelling 8:9:preserved

Secondly , videos in NHKNews7 are recorded during a long time ( i.e. , 11 years ) .
Second , the videos in NHKNews7 were recorded over a long time ( i.e. , 11 years ) .
493 0:0:bigrammar-wform 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 6,5:7,6:bigrammar-vtense 7:8:bigrammar-prep 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved :2:mogrammar-det

Thus , besides facial variations caused by enviromental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) in each face-track , face-tracks of a character themself also contain biological variation of the character during time .
Thus , besides facial variations in each face track caused by the environmental conditions at the time of recording ( e.g. , illumination , pose , viewpoint ) , the face tracks of the character themselves also reflect the biological variations of the character over time; .
494 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6:10,9:preserved 7,8:12,13:preserved 9,10,11,12,13:14,15,16,17,18:preserved 14,15,16,17,18,19,20,22,21:19,20,21,22,23,24,26,25,27:preserved 23,24:5,6:preserved 25:7,8:spelling 26::unaligned 27:30,31:spelling 28:32:preserved 29:33:bigrammar-det 30:34:preserved 31:35:bigrammar-nnum 32:36:preserved 33:37:paraphrase 34:39:preserved 35:40:bigrammar-nnum 36,37,38:41,42,43:preserved 39:44:bigrammar-prep 40:45:preserved :11:moproblematic :29:mogrammar-det :38:mogrammar-det

For instance , a character may look older after several years ( see Figure 8 , for example ) .
for instance , a character may look older after several years ( see Figure 8 , for example ) .
495 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

Due to those reasons , matching faces in NHKNews7 becomes more challenging , .
For these reasons , matching faces in NHKNews7 becomes more challenging , which .
496 1,2,0,3:0,1,2:paraphrase 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11,12:paraphrase 13:13:preserved

It results in drops of MAP( s ) of all evaluated approaches.
resulted in decreased MAP( s ) for all the evaluated approaches.
497 0,1:0:bigrammar-vtense 2:1:preserved 3,4,5:2,3:para-colocation 6:4:preserved 7:5:preserved 8:6:bigrammar-prep 9:7:preserved 10:9:preserved 11:10:preserved :8:mogrammar-det

A clear and consistent observation from both datasets is that pair:min ( i.e. , min-min ) always achieves the best MAPs , which are 76.54% and 60.99% on two dataset , respectively .
A clear and consistent observation from both datasets is that pair:min ( i.e. , min-min ) always achieves the best MAPs , which are 76.54% and 60.99% in the two datasets , respectively .
500 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:bigrammar-prep 28:29:preserved 29:30:bigrammar-nnum 30:31:preserved 31:32:preserved 32:33:preserved :28:mogrammar-det

Among several distance types , L1 is the optimal one to be used with pair:min .
Among the distance types , L1 is the optimal for use with pair:min .
501 0:0:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 10,11,12:10,9:paraphrase 13:11:preserved 14:12:preserved 15:13:preserved :1:mogrammar-det

A reasonable replacement can be Euclidean distance .
A reasonable replacement is the Euclidean distance .
502 0:0:preserved 1:1:preserved 2:2:preserved 3,4:3:bigrammar-vtense 5:5:preserved 6:6:preserved 7:7:preserved :4:mogrammar-det

However , there is a minor accuracy gap between pair:min using L1 and pair:min using Euclidean .
However , there is a minor accuracy gap between pair:min using L1 and pair:min using the Euclidean distance .
503 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16,17:paraphrase 16:18:preserved :15:mogrammar-det

And , computing Euclidean distance between two feature vectors is more expensive than computing their L1 distance .
In addition , computing the Euclidean distance between two feature vectors is more expensive than computing their L1 distance .
504 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved :4:mogrammar-det

The results also show that pair:min is better than pair:mean .
The results also show that pair:min is better than pair:mean .
507 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

This is because pair:mean uses the mean of all pair-wise distances between two face-tracks as their similarity score .
This is because pair:mean uses the mean of all pair-wise distances between two face tracks as the similarity score .
508 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14,13:spelling 14:15:preserved 15:16:bigrammar-det 16:17:preserved 17:18:preserved 18:19:preserved

By computing the mean , pair:mean reduces the effect of noisy pairs .
By computing the mean , pair:mean reduces the effect of noisy pairs .
509 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

At the same time , it eliminates the influence of pairs containing identical faces , which can help to instantly determine they are belong to the same character .
At the same time , it eliminates the influence of pairs containing identical faces , which can help to instantly determine that the faces belong to the same character .
510 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:22,23:paraphrase 22,23:24:paraphrase 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved

Thus , discriminative power of the computed similarity score is reduced , compared to one computed by pair:min .
Thus , the discriminative power of the computed similarity score is reduced , compared to that computed by pair:min .
511 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:paraphrase 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :2:mogrammar-det

It causes the gap of MAPs between pair:min and pair:min .
This causes the difference in MAPs between pair:min and pair:min .
512 0:0:paraphrase 1:1:preserved 2:2:preserved 4,3:4,3:paraphrase 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

More generally , this explains why such a gap between pair:min and pair:mean on NHKNews7 is larger than on Trecvid .
More generally , this explains why such a gap between pair:min and pair:mean is larger in NHKNews7 than in Trecvid .
513 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:15:bigrammar-prep 14:16:preserved 15:13:preserved 16:14:preserved 17:17:preserved 18:18:bigrammar-prep 19:19:preserved 20:20:preserved

Since the average length of face-tracks on NHKNews7 is longer ( i.e. , each face-track contains more sample faces of a character ) , there is more chance that two face-tracks of the same character contain identical faces.
Because the average length of face tracks on NHKNews7 is longer ( i.e. , each face track contains more sample faces of a character ) , there is a greater chance that two face tracks of the same character contain identical faces.
514 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5,6:spelling 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15,16:spelling 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:29,28:paraphrase 27:30:preserved 28:31:preserved 29:32:preserved 30:34,33:spelling 37,36,35,34,33,32,31:41,40,39,38,37,36,35:preserved

About our k-Faces , its MAP increases when k increases .
Regarding our k-Faces , its MAP increases when k increases .
517 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

Between k-Faces.Temporal and k-Faces.KMeans , the impact of k on MAP of k-Faces.KMeans is less significant .
Between k-Faces.Temporal and k-Faces.KMeans , the impact of k on the MAP of k-Faces.KMeans is less significant .
518 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved :10:mogrammar-det

Since k-Faces.KMeans always use all faces in a facetrack for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
Because k-Faces.KMeans always uses all the faces in a face track for clustering and selecting centroids for representative faces , the final mean face is less sensitive to k .
519 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:bigrammar-inter 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9,10:spelling 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved :5:mogrammar-det

On the contrary , k plays an important role in k-Faces.Temporal .
In contrast , k plays an important role in k-Faces.Temporal .
520 0,1,2:0,1:paraphrase 3:2:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved

The higher k is set , the more representative faces of each facetrack are selected .
The higher the k set , the more representative faces of each face track selected .
521 0:0:preserved 1:1:preserved 2:3:preserved 3,4:4:paraphrase 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12,13:spelling 13,14:14:paraphrase 15:15:preserved :2:mogrammar-det

Thus , the final mean face of each facetrack becomes more reliable and accurate .
Thus , the final mean face of each face track becomes more reliable and accurate .
522 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:spelling 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved

The advantages of k-Faces.KMeans is that it can achieve high accuracy even when k is very small .
The advantages of k-Faces.KMeans is that it can achieve high accuracy even when k is very small .
523 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Meanwhile , its disadvantage is the expensive computational cost to perform clustering faces on a high dimensional feature space ( i.e. , 1937 dimensions ) .
However , its disadvantage is the high computational cost of clustering faces on a high-dimensional feature space ( i.e. , 1,937 dimensions ) .
524 0:0:paraphrase 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:paraphrase 7:7:preserved 8:8:preserved 9,10:9:paraphrase 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15,16:14:spelling 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:22:preserved 25:23:preserved

When k is large enough , there is no substantial difference in MAP between k-Faces.KMeans and k-Faces.Temporal.
When k is large enough , there is no substantial difference in MAP between k-Faces.KMeans and k-Faces.Temporal.
525 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved

On both datasets , when k increases from 2 to 20 , MAPs of k-Faces approaches grow rapidly .
In both datasets , when k increases from 2 to 20 , the MAPs of k-Faces approaches grow rapidly .
528 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :12:mogrammar-det

However , theirs MAPs become stable from 20 afterwards .
However , the MAPs become stable from k = 20 upward .
529 0:0:preserved 1:1:preserved 2:2:bigrammar-det 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:9:preserved 8:10:bigrammar-prep 9:11:preserved

Since keep increasing k does not help to obtain imporant accuracy improvement but expensive computational cost , we select k = 20 to investigate the trade-off between accuracy and computational costs of k-Faces approaches compared to others .
Because further increasing k does not help improve accuracy but increases the computational cost , we select k = 20 for investigating the trade-off between the accuracy and computational cost of k-Faces approaches in comparison to others .
530 0:0:paraphrase 1,2:1,2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 8,9,10,7,11:7,8:paraphrase 12:9:preserved 13:10,11:paraphrase 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved 20:18:preserved 21:19:preserved 22,23:20,21:paraphrase 24:22:preserved 25:23:preserved 26:24:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:bigrammar-inter 31:30:preserved 32:31:preserved 33:32:preserved 34:33,34:paraphrase 35:35:preserved 36:36:preserved 37:37:preserved

We report MAP and processing time of each approach in the Table 5 .
Table 5 shows the MAP and processing time of each approach .
531 0,1,9:2:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9:preserved 8:10:preserved 10::mogrammar-det 11,12:0,1:preserved 13:11:preserved :3:mogrammar-det

Processing time is separated into two parts , corresponding to preprocessing time and matching time .
Processing time is divided into two parts , preprocessing and matching .
532 0:0:preserved 1:1:preserved 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 10:8:preserved 12:9:preserved 13:10:preserved

Preprocessing time presents time required for preprocessing face-tracks before matching .
The preprocessing time refers to the time required to preprocess face tracks before matching .
533 0:1:preserved 1:2:preserved 2:3,4:paraphrase 3:6:preserved 4:7:preserved 5,6:8,9:paraphrase 7:10,11:spelling 8,9:12,13:preserved :0:mogrammar-det :5:mogrammar-det

With k-Faces approaches , preprocessing facetracks includes selecting representative faces and computing their mean face .
In k-Faces approaches , the preprocessing of face tracks includes selecting representative faces and computing their mean face .
534 0:0:bigrammar-prep 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:7,8:spelling 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved :4:mogrammar-det :6:mogrammar-prep

In MSM and CMSM , it indicates time for computing subspaces for face-tracks .
In MSM and CMSM , preprocessing includes computing subspaces for face tracks .
535 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8:5,6:paraphrase 9:7:preserved 10:8:preserved 11:9:preserved 12:10,11:spelling 13:12:preserved

Matching time is averaged for one query run .
The matching time is averaged over one query run .
536 0:1:preserved 1:2:preserved 2:3:preserved 3:4:preserved 4:5:bigrammar-prep 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved :0:mogrammar-det

Time unit is second.
The time unit used is seconds.
537 0:1:preserved 1:2:preserved 2:4:preserved 3:5:bigrammar-inter :0:mogrammar-det

According to Table 5 , k-Faces.KMeans and k- Faces.Temporal achieve almost equal accuracy and consume the same amount of time for one query on both datasets .
As shown in Table 5 , k-Faces.KMeans and k- Faces.Temporal achieve almost equal accuracy and consume the same amount of time for one query in both datasets .
540 0,1:0,1,2:paraphrase 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:bigrammar-prep 24:25:preserved 25:26:preserved 26:27:preserved

However , k-Faces.Temporal is hundreds times ( 240 times on Trecvid and 360 times on NHKNews7 ) faster than k-Faces.Temporal in the preprocessing phase .
However , k-Faces.Temporal is hundreds of times ( 240 times in Trecvid and 360 times in NHKNews7 ) faster than k-Faces.Temporal in the preprocessing phase .
541 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:bigrammar-prep 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:bigrammar-prep 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :5:mogrammar-prep

This suggest that , selecting presentative faces based on tempo .
This suggests that in terms of both accuracy and efficiency , selecting representative faces based on temporal sampling is better than that based on clustering .
542 0:0:preserved 1:1:bigrammar-others 2:2:preserved 4:11:preserved 5:12:spelling 6:13:preserved 7:14:preserved 8:15:preserved 9:17,18,19,20,21,22,23,24,3,4,5,6,7,8,9:paraphrase

ral sampling is better than one based on clustering , in both terms of accuracy and efficiency.
,
543

Compared to state-of-the-art approaches , our k- Faces.Temporal is thousands times faster than the best approach , which is pair:min , and hundred times faster than MSM and CMSM on both datasets .
Compared to state-of-the-art approaches , our k- Faces.Temporal is thousands of times faster than the best approach , which is pair:min , and hundreds of times faster than MSM and CMSM in both datasets .
546 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:bigrammar-nnum 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:bigrammar-prep 30:32:preserved 31:33:preserved 32:34:preserved :10:mogrammar-prep :24:mogrammar-prep

In terms of accuracy , k-Faces take second place , with 73.65% on Trevid dataset , after pair:min .
In terms of accuracy , k-Faces takes second place , with 73.65% in the Trevid dataset , after pair:min .
547 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:bigrammar-inter 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-prep 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved :13:mogrammar-det

The gap with pair:min is 2.89% difference in MAP .
The difference in MAP between our approach and pair:min is 2.89% .
548 0,1,2:4,5,6,7:paraphrase 3,4:8,9:preserved 5:10:preserved 6:1:preserved 7,8:2,3:preserved 9:11:preserved :0:mogrammar-det

Meanwhile , it is significantly better than MSM and CMSM , which respectively achieve 69.20% and 64.62% .
Meanwhile , k- Faces.Temporal is significantly better than MSM and CMSM , which respectively achieved 69.20% and 64.62% accuracy .
549 0:0:preserved 1:1:preserved 2:3,2:paraphrase 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:bigrammar-vtense 14:15:preserved 15:16:preserved 16:17:preserved 17:19:preserved

On NHKNews7 dataset , our k-Faces.Temporal is still better than CMSM , but is worse than pair:min and MSM .
In the NHKNews7 dataset , k-Faces.Temporal is better than CMSM , but worse than pair:min and MSM .
550 0:0:bigrammar-prep 1:2:preserved 2:3:preserved 3:4:preserved 4::mogrammar-det 5:5:preserved 6:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 14:12:preserved 15:13:preserved 16:14:preserved 17:15:preserved 18:16:preserved 19:17:preserved :1:mogrammar-det

One may concern that why MSM perform poorly on Trecvid dataset , but it is superior to our k-Faces.Temporal on NHKNews7 .
One may question why MSM performed poorly in the Trecvid dataset , but was superior to k-Faces.Temporal in NHKNews7 .
551 0:0:preserved 1:1:preserved 2:2:paraphrase 4:3:preserved 5:4:preserved 6:5:bigrammar-vtense 7:6:preserved 8:7:bigrammar-prep 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 14:13:bigrammar-vtense 15:14:preserved 16:15:preserved 17::mogrammar-det 18:16:preserved 19:17:bigrammar-prep 20:18:preserved 21:19:preserved :8:mogrammar-det

This is due to the fact that face-tracks on NHKNews7 dataset is larger than those on Trecvid dataset .
The reason for this is the fact that the face tracks in the NHKNews7 dataset are larger than those in the Trecvid dataset .
552 0,1,2,3:0,1,2,3,4:paraphrase 4:5:preserved 5:6:preserved 6:7:preserved 7:9,10:spelling 8:11:bigrammar-det 9:13:preserved 10:14:preserved 11:15:bigrammar-others 12,13,14:16,17,18:preserved 15:19:bigrammar-prep 16,17:21,22:preserved :8:mogrammar-det :12:mogrammar-det :20:mogrammar-det

Therefore , more sample faces in each face-track can be used to obtain a reliable subspace .
Therefore , more sample faces in each face track can be used to obtain a reliable subspace .
553 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:spelling 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved

As expected , the results in this experiment demonstrate that our proposed approach is extremely efficient while archiving comparable performance with state-of-the-art approachesf.
As expected , the results of this experiment show that our proposed approach is extremely efficient while achieving comparable performance with state-of-the-art approaches .
556 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:bigrammar-prep 6:6:preserved 7:7:preserved 8:8:paraphrase 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:spelling 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

In this paper , we investigate face retrieval on large-scale news video datasets .
In this paper , we investigate face retrieval in large-scale news video datasets .
563 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:bigrammar-prep 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

Our contributions is 3-fold .
Our contribution is threefold .
564 0:0:preserved 1:1:bigrammar-wform 2:2:preserved 3:3:spelling 4:4:preserved

Firstly , we presented practical problems when a tracker is used to extract face-tracks in news videos .
First , we present the practical problems encountered when a tracker is used to extract face tracks in news videos .
565 0:0:bigrammar-wform 1:1:preserved 2:2:preserved 3:3:bigrammar-vtense 4:5:preserved 5:6:preserved 6:8:preserved 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15,16:spelling 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved :4:mogrammar-det

Based on that , we introduce techniques and solutions to bypass the problems for robust face-track extraction .
Based on these , we introduce techniques and solutions to overcome these problems to achieve robust face-track extraction .
566 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:paraphrase 11:11:bigrammar-det 12:12:preserved 13:13,14:paraphrase 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved

Secondly , we present an approach for face-track matching which significantly reduces the computational cost and achive competitive performance compared to state-of-the-art approaches .
Second , we present an approach for face-track matching that significantly reduces the computational cost while achieving competitive performance compared with state-of-the-art approaches .
567 0:0:bigrammar-wform 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:bigrammar-others 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15,16:15,16:paraphrase 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-prep 21:21:preserved 22:22:preserved 23:23:preserved

Thirdly , we prepare , evaluate state-of-the-art face retreival approaches , and publish real-world face-track datasets whose scale have not been considered in literature ever.
Third , we prepare datasets , evaluate state-of-the-art face retrieval approaches , and publish real-world face-track datasets of such scales that have never been considered in the literature.
568 0:0:bigrammar-wform 1:1:preserved 2:2:preserved 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:typo 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16,17:17,18,19,20:paraphrase 18,20,21:21,23,24:preserved 19,24:22:paraphrase 22:25:preserved 23:27:preserved :26:mogrammar-det

