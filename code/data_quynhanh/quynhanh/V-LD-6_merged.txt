ROBUST OBJECT DETECTION USING FAST FEATURE SELECTION FROM HUGE FEATURE SETS
ROBUST OBJECT DETECTION USING FAST FEATURE SELECTION FROM HUGE FEATURE SETS
2 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

This paper describes an efficient feature selection method which quickly selects a small subset out of a given huge feature set for building robust object detection systems .
This paper describes an efficient feature selection method which that quickly selects a small subset out of a given huge feature set ; the proposed method for will be useful for building robust object detection systems .
6 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8,9:bigrammar-others 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21,23,24,25,26,28,27,29:paraphrase 21,22,23,24,25,26:31,30,32,33,34,35:preserved

In this filter-based method , features are selected so that not only maximizing their relevance with the target class but also minimizing their mutual dependency .
In this filter-based method , features are selected so that not only to maximizeing their relevance with the target class but also to minimizeing their mutual dependency .
7 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:spelling 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:23:spelling 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved :12:mogrammar-prep :22:mogrammar-prep

As a result , the selected feature set only contains highly informative and non-redundant features which when combined together , significantly improve classification performance .
As a result , the selected feature set only contains only highly informative and non-redundant features , which significantly improve classification performance when combined together , significantly improve classification performance .
8 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11,10:paraphrase 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16,17,18:22,23,24:preserved 19:16:preserved 20:18:preserved 21:19:preserved 22:20:preserved 23:21:preserved 24:26,27,28,29,30,25:paraphrase

The relevance and mutual dependency of features are measured by using conditional mutual information ( CMI ) in which features and classes are treated as discrete random variables .
The relevance and mutual dependency of features are measured by using conditional mutual information ( CMI ), in which features and classes are treated as discrete random variables . //[ ,?<--A comma can be used here if the following describes CMI in general .]
9 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

Experiments on different huge feature sets have shown that the proposed CMI-based feature selection can both reduce significantly the training time and achieve high accuracy .
Experiments on different huge feature sets have shown that the proposed CMI-based feature selection can both reduce significantly the training time significantly and achieve high accuracy .
10 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:22,21:paraphrase 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved

One of the fundamental research issues in pattern recognition is feature selection which is the task of finding a small subset out of a given large set of features .
One of the fundamental research issues in pattern recognition is feature selection , which is the task of finding a small subset out of a given large set of features .
15 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved

It is significant due to the following three reasons .
Improving the method of accomplishing this task is important due to the following three reasons .
16 0,1:0,1,2,3,4,5,6,7:paraphrase 2:8:paraphrase 3,4:9,10:preserved 6,5,7,8:12,11,13,14:preserved

First , there are many ways to represent a target object , leading to a huge feature set .
First , there are many ways can be used to represent a target object , and this variety leadsleading to a huge feature set .
19 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:9,6,7,8:paraphrase 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14,15,16,17:paraphrase 12:18:bigrammar-wform 13,14,15,16,17:19,20,21,22,23:preserved

For example , the number of Haar wavelet features used in [1] for face detection is hundreds of thousands .
For example , the number of Haar wavelet features used in [1] for face detection is hundreds of thousands .
20 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved

However , only small and incomplete training sets are available .
However , only small and incomplete training sets are available .
21 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved

As a result , these systems suffer from the curse of dimensionality and over-fitting .
As a result , these systems suffer from the curse of dimensionality and over-fitting .
22 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

Second , a huge feature set usually includes many irrelevant and redundant features that can degrade the generalization performance of classifiers , waste storage space and increase training time [2 , 3] .
Second , a huge feature set usually includes many irrelevant and redundant features that can degrade the generalization performance of classifiers , waste storage space , and increase training time [2 , 3] .
25 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved 29:30:preserved 30:31:preserved 31:32:preserved 32:33:preserved

Third , selecting an optimal feature subset from a huge feature set can improve the performance and speed of classifiers .
Third , selecting an optimal feature subset from a huge feature set can improve the performance and speed of classifiers .
27 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Furthermore , less complex model is easier to understand and verify .
Furthermore , less complex models is are easier to understand and verify .
30 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:6,4:bigrammar-inter 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved

In face detection , the success of systems such as those in [1 , 4] comes mainly from efficient feature selection methods .
In face detection , the success of systems such as those in [1 , 4] comes mainly from efficient feature selection methods .
31 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

Generally , feature selection methods can be categorized into two kinds : filter-based approach and wrapper-based approach [5] .
Generally , feature selection methods can be categorized into two kinds : the filter-based approach and the wrapper-based approach [5] .
34 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved :12:mogrammar-det :16:mogrammar-det

The filter-based approach is independent of any induction algorithm while the wrapper-based approach is associated with a specific induction algorithm to evaluate the goodness of the selected feature subset .
The filter-based approach is independent of any induction algorithm , while but the wrapper-based approach is associated with a specific induction algorithm to evaluate the quality of the selected feature subset . //[goodness / quality / appropriateness?<--If " goodness " is the word you would usually use in your field for this , it is fine , but I would suggest a different word choice otherwise . " Goodness " seems vague , so in what sense do you mean " good " ?]
35 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:12,11:bigrammar-others 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:paraphrase 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved

In the filter-based approach , features are normally selected based on their individual predictive power which is measured by Fisher scores , Pearson correlation [6] or mutual information [7] .
In the filter-based approach , features are normally selected based on their individual predictive power . This power is measured by Fisher scores , Pearson correlation [6] , or mutual information [7] .
38 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16,17:paraphrase 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved

The major advantage of these methods is their speed and ability to scale to huge feature sets .
The major advantage of these measurement methods is their speed and ability to scale to huge feature sets .
39 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6,5:paraphrase 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved

However , the mutual relationships between features is often not taken into account , leading selected features might be highly redundant and less informative because two features with high individual predict power when combined together might not bring significant performance improvement compared with two features which one of them has low predictive power but is useful when combined with others .
However , because the mutual relationships between features is are often not taken into account , leading the selected features might be highly redundant and less informative because two features with high individual predictive power , when combined together , might not bring significant performance improvement . Combining compared with two features of which one of them has low predictive power but is useful when combined with others would thus be more effective for improving performance .
40 0:0:preserved 1:1:preserved 2:2,3:bigrammar-others 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved 27:30:preserved 28:31:preserved 29:32:preserved 30:33:bigrammar-wform 31:34:preserved 32,33,34,35,36,37,38:36,37,38,40,41,42,43:preserved 39,40:44,45:preserved 41:48,47,46:paraphrase 42:49:preserved 43:50:preserved 44:51:preserved 52,45,46,47,48,49,50,51:53,54,55,56,57,58,59,60:preserved 53:61:preserved 54,55,56,57,58:62,63,64,65,66:preserved 59,60:67,68,69,70,71,72,73,74,75,76:paraphrase :17:mogrammar-det :52:mogrammar-prep

Since wrapper-based feature selection methods use machine learning algorithms as a black box in selection process , they can suffer from over-fitting in situations of small training sets .
Since wrapper-based feature selection methods use machine learning algorithms as a black box in the selection process , they can suffer from over-fitting in situations of when applied to small training sets . //[when used with / when applied to?]
42 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:27,28,26,25:paraphrase 25,26,27:29,30,31:preserved :14:mogrammar-det

Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands features , using wrapper-based methods is obviously inefficient because of very high computation cost .
Furthermore , in practical object detection systems as in [1 , 8] , the feature sets usually have hundreds of thousands of features , so using wrapper-based methods is obviously inefficient because of the very high computation costs they incur .
43 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:22:preserved 22:23:preserved 23:25,24:bigrammar-others 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37,38,39:paraphrase :21:mogrammar-prep :33:mogrammar-det

For example , in the state of the art face detection system [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by AdaBoost has taken several weeks .
For example , in the state- of- the- art face detection system in [1] , choosing a 6 ,061- feature set out of a 180 ,000-feature set by using AdaBoost has takentook several weeks . //[by using / generated by?]
44 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5,6,7,8:5,6,7,8:spelling 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:28,27:paraphrase 27:29:preserved 28,29:30,31,12:bigrammar-vtense 30:32:preserved 31:33:preserved 32:34:preserved

Consequently , conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of above approaches for handling large scale feature sets .
Consequently , feature selection methods based on conditional mutual information ( CMI ) based feature selection methods have been proposed [9 , 8 , 7 , 10] to take full advantage of the above approaches for handling large scale feature sets .
47 0:0:preserved 1:1:preserved 2:7,6,5,4,3,2:paraphrase 3,4:8,9:preserved 6,5,7:11,12,10:preserved 9,10,8,11,12,13,14:13,14,15,16,17,18,19:preserved 15,17,16,18,19,20,21:20,21,22,23,24,25,26:preserved 22,23,24,25,26:27,28,29,30,31:preserved 27,28,29,30,31,32,33,34:33,34,35,36,37,38,39,40:preserved :32:mogrammar-det

The main idea of CMI-based methods is to select features which maximize their relevance with the target class and simultaneously minimize mutual dependency between selected ones .
The main goal of these CMI-based methods is to select features which that maximize their relevance with the target class and to simultaneously minimize mutual dependency between selected ones . //[idea / goal?]
50 0:0:preserved 1:1:preserved 2:2:paraphrase 3:3:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11,12:bigrammar-others 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved 23:26:preserved 24:27:preserved 25:28:preserved 26:29:preserved :4:mogrammar-det

It does not select a feature similar to already selected ones , even if it is individual powerful , as selecting it might not increase much information about the target class [7] .
It doesThese methods do not select a feature similar to ones already selected ones , even if itthe feature is individually powerful , as because selecting it might not do much to increase much information about the target class [7] .
51 0:0:preserved 1:1,2,3:paraphrase 2:4:preserved 3:5:preserved 4:6:preserved 5:7:preserved 6:8:preserved 7:9,10:paraphrase 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17,18:paraphrase 15:19:preserved 16:20:preserved 17:21:preserved 18:22:preserved 19:23,24:bigrammar-others 20:25:preserved 21:26:preserved 22:27:preserved 23:28,29,30,31:paraphrase 24:32:preserved 25:33:preserved 26:34:preserved 27:35:preserved 28:36:preserved 29:37:preserved 30:38:preserved 31:39:preserved

One of the important tasks in using CMI-based methods is mutual information estimation which involves to compute probability densities of continuous random variables .
One of the important tasks in using CMI-based methods is mutual information estimation , which involves to computecomputing the probability densities of continuous random variables .
54 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:paraphrase 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved :18:mogrammar-det

In [9] , Kwak and Choi used Parzen windows based density estimation method in which many parameters such as kernel function and window width are complicated to determine .
In [9] , Kwak and Choi used a Parzen windows -based density estimation method in which many parameters such as kernel function and window width are complicated to determine .
55 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 9,8:9,10:spelling 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved :7:mogrammar-det

For simplification , discretizing features is often used .
For simplification , discretizing features is often used on the features . //[discretizing features is often used on the features / the features are often discretized?]
56 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,10,8,9:paraphrase 8:11:preserved

So far , in object detection systems like [8 , 7] , features are treated as binary random variables by choosing appropriate thresholds .
So far , in object detection systems like [8 , 7] treat , features are treated as binary random variables by choosing appropriate thresholds .
57 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:12,11:paraphrase 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved

However , binarizing features is not a suitable way to handle highly complex data for which it is hard to find the best threshold .
However , binarizing features is not a suitable way to handle highly complex data for which it is hard to finding the best threshold is difficult .
58 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:bigrammar-wform 21:21:preserved 22:22:preserved 23:23,24,25:paraphrase 24:26:preserved

It is better if multiple thresholds are used to discretize data .
Using multiple thresholds to discretize data is better than using a binary approach .
59 0,1,2,3,6,7:0,6,7:paraphrase 4:1:preserved 5:2:preserved 9,10,8:4,5,3:preserved 11:13:preserved

Such a simple method is equal-width binning which divides the range of feature values into m equal sized bins , where m must be known in advance .
Such a simple method is equal-width binning , which divides the range of feature values into m equally sized bins , where m must be known in advance .
60 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:bigrammar-wform 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved

Our method is also a CMI-based feature selection method .
Our method is also a CMI-based feature selection method .
63 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

However , the main distinguished point is that it employs the entropy-based discretization method [11] to discretize features .
However , the method�fs main distinguishing point is that it employs the entropy-based discretization method [11] to discretize features . //[distinguishing / unique?]
64 0:0:preserved 1:1:preserved 2:2:preserved 3:4,3:paraphrase 4:5:bigrammar-wform 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved

This discretization method is simpler than Parzen windows based density estimation method and more efficient than binary discretization .
This discretization method is simpler than the Parzen window-s based density estimation method and is more efficient than binary discretization .
65 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:spelling 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:15,14:paraphrase 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved :6:mogrammar-det

Furthermore , contrary to equal-width binning , it can automatically evaluate the optimal number of bins based on data distribution .
Furthermore , contrary to equal-width binning , it can automatically evaluate the optimal number of bins based on data distribution . //[evaluate / determine?]
66 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved

Experiments show that the proposed method can well handle huge feature sets for face detection such as Haar wavelets [1] and Gabor wavelets [12] , significantly reduce the training time while maintaining high classification performance .
Experiments show that the proposed method can well capably handle huge feature sets of data such as Haar wavelets [1] and Gabor wavelets [12] for face detection , significantly reducinge the training time while maintaining high classification performance .
67 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9,8:paraphrase 9:10:preserved 10:11:preserved 11:12,13,14:paraphrase 12,13,14:24,25,26:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:27:preserved 25:28:preserved 26:29:bigrammar-wform 27:30:preserved 28:31:preserved 29:32:preserved 30:33:preserved 31:34:preserved 32:35:preserved 33:36:preserved 34:37:preserved 35:38:preserved

FEATURE SELECTION " >
FEATURE SELECTION " >
71 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved

Huge feature sets usually contain four kinds of features : ( i ) irrelevant features , ( ii ) weakly relevant and redundant features , ( iii ) weakly relevant but non-redundant features and ( iv ) strongly relevant features in which ( iii ) and ( iv ) are the objective of feature selection methods [13] .
Huge feature sets usually contain four kinds of features : ( i ) irrelevant features , ( ii ) weakly relevant and redundant features , ( iii ) weakly relevant but non-redundant features , and ( iv ) strongly relevant features ; in which ( iii ) and ( iv ) are the objectives of feature selection methods [13] .
73 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:34:preserved 34:35:preserved 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:preserved 40:42:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved 45:47:preserved 46:48:preserved 47:49:preserved 48:50:preserved 49:51:preserved 50:52:preserved 51:53:bigrammar-nnum 52:54:preserved 53:55:preserved 54:56:preserved 55:57:preserved 56:58:preserved 57:59:preserved

To measure relevance of a feature , the entropy-based measure which quantifies the uncertainty of random variables is normally used .
To measure the relevance of a feature , an entropy-based measure , which quantifies the uncertainty of random variables , is normally used .
74 0:0:preserved 1:1:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:8:bigrammar-det 8:9:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved :2:mogrammar-det

The entropy of a discrete random variable X is defined as : \MATH and the conditional entropy of X after another variable Y is known is defined as \MATH
The entropy of a discrete random variable X is defined as : \MATH and the conditional entropy of X after another variable Y is known is defined as \MATH
77 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved

The mutual dependence between two random variables is measured by mutual information \MATH .
The mutual dependence between two random variables is measured by mutual information : \MATH .
80 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:13:preserved 13:14:preserved

The conditional mutual information is defined as : \MATH
The conditional mutual information is defined as : \MATH .
83 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved

In the first step , the most relevant feature F1 which has the highest mutual information is selected .
In the first step , the most relevant feature F1 , which has the highest largest amount of mutual information , is selected .
86 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14,15:18,19,15,16,17:paraphrase 16,17:21,22:preserved

However , in the second step , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .
However , iIn the second step , however , the condition to select feature F2 is not its mutual information alone , but how much information of F2 can add with respect to the already existing F1 .
87 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7,8,6:bigrammar-others 7:9:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved

Therefore , F2 is selected so that maximizing :\MATH .
Therefore , F2 is selected so that maximizingas to maximize the information it can add :\MATH .
88 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8,9,10,11,12,13,14:paraphrase 8:15:preserved 9:16:preserved

Following the same scheme , we iteratively add the feature that brings the highest increase of information content contained in current selected feature set .
Following the same scheme, we iteratively add the feature that brings the highest increase of the information content contained in the current selected feature set . //[the / an?<-- " An " is correct if there is more than one such measure .]
91 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved :15:mogrammar-det :20:mogrammar-det

The next feature Ft to be added at iteration t is defined by :\MATH .
The next feature Ft to be added at iteration t is defined by :\MATH .
92 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved

In order to simply estimate mutual information , the easiest way is features are discretized in binary values by specifying thresholds [8 , 7] .
To simply estimate mutual information , the easiest way is to discretize features are discretized in binary values by specifying thresholds [8 , 7] .
95 2:0:preserved 3:1:preserved 4:2:preserved 5:3:preserved 6:4:preserved 7:5:preserved 8:6:preserved 9:7:preserved 10:8:preserved 11:9,10,11:paraphrase 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved

However , for complex data , it is not efficient ; therefore , we use entropy-based method proposed by Fayyad and Irani [11] for discretization .
However , for complex data , doing thisit is not efficient ; therefore , we use the entropy-based method proposed by Fayyad and Irani [11] for discretization .
96 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved :16:mogrammar-det

This method is a supervised method , thus it is generic and can adapt very well to any kind of data distributions .
This method is a supervised method , thus so it is generic and can adapt very well to any kind of data distributions .
97 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:bigrammar-others 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved

Basically , discretization is a quantizing process that converts continuous values into discrete values .
Discretization is essentially a quantizing process that converts continuous values into discrete values .
101 0:2:paraphrase 2:0:preserved 3:1:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved

Suppose that we are given a set of instances S , a feature A and a cut-point T ( a cutpoint is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold ) .
Suppose that we are given a set of instances S , a feature A , and a cut-point T . ( A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold . ) .
102 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:20:preserved 19:21:preserved 20:22:spelling 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:40:preserved 39:41:preserved 40:42:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved 45:47:preserved 46:48:preserved 47:49:preserved 48:50:preserved 49:51:preserved 50:52:preserved 51:53:preserved 52:54:preserved 53:55:preserved 54:56:preserved 55:57:preserved 56:59:preserved 57:60:preserved

The class-information entropy of the partition induced by T is defined as :
The class-information entropy of the partition induced by T is defined as :
105 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Among candidate cut-points , the best candidate cut-point Tmin which minimizes the entropy function \MATH is selected to split \MATH into two partitions \MATH and \MATH .
Among candidate cut-points , the best candidate cut-point Tmin , which minimizes the entropy function \MATH , is selected to split \MATH into two partitions \MATH and \MATH .
108 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:17:preserved 16:18:preserved 17:19:preserved 18:20:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved

This process can then be repeated recursively to \MATH and \MATH until some stopping condition is satisfied , thus creating multiple intervals on the feature \MATH .
This process can then be repeated recursively forto \MATH and \MATH until some stopping condition is satisfied , thus creating multiple intervals on the feature \MATH .
109 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:bigrammar-prep 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved

Using MDLP , the stopping criteria is proposed by Fayyad and Irani [11] as follows :
Using MDLP , the stopping criteria is was proposed by Fayyad and Irani [11] as follows :
112 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6,7:paraphrase 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved

MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \MATH Where \MATH ,where \MATH , \MATH , \MATH is the number of classes in \MATH , \MATH , \MATH .
MDLP Criteria : A partition induced by cut-point T for a set S of N examples is accepted if : \MATH wWhere \MATH ,where \MATH , \MATH , and \MATH is are the numbers of classes in \MATH , \MATH , and \MATH , respectively .
115 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:typo 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27,28:bigrammar-others 28:29:preserved 29:30,31:paraphrase 30:32:preserved 31:33:bigrammar-nnum 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:41,40:bigrammar-others 39:42:preserved 40:44,45,43:bigrammar-others

Extensive experiments [11 , 14] have shown that this method is one of the best variable discretization one because it gives small number of cut-points while maintaining consistency .
Extensive experiments [11 , 14] have shown that this method is one of the best in variable discretization one because it gives a small number of cut-points while maintaining consistency .
116 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved :15:mogrammar-prep :22:mogrammar-det

The outline of the proposed feature selection method is shown in Algorithm 1 .
The outline of the proposed feature selection method is shown in Algorithm 1 .
119 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved

For experiments , a set face and non-face patterns of size 24x24 was used .
For experiments , a set of face and non-face patterns of size 24x24 was used .
126 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:6:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved :5:mogrammar-prep

A set of 10 ,000 face patterns were collected from the Internet .
A set of 10 ,000 face patterns were collected from the Internet .
127 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved

Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Another set of 10 ,000 complex non-face patterns were false positives collected by running a face detector based on a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images that contained no faces ; the images with included various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
128 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved 26:26:preserved 27:27:preserved 28:28:preserved 29:29:preserved 30:30:preserved 31:31:preserved 32:32:preserved 33:33:preserved 34:34,35,36,37,38,39,40,41,42:paraphrase 36,35:44,43:preserved 38,39,40:46,47,48:preserved 42,44,46,48,49,51,52,53:50,52,54,56,57,59,60,61:preserved 43:45:preserved 47:49:preserved 50:51:preserved

The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 patterns .
The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 patterns .
129 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved 23:23:preserved 24:24:preserved 25:25:preserved

Some examples of the collected 24x24 face and non-face patterns are shown in Figure 1 .
Some examples of the collected 24x24 face and non-face patterns are shown in Figure 1 .
130 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

Two types of features that are Haar wavelet feature and Gabor wavelet feature were used in experiments .
Two types of features ?that are Haar wavelet features and Gabor wavelet features ? were used in our experiments .
133 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:bigrammar-nnum 9:9:preserved 10:10:preserved 11:11:preserved 12:12:bigrammar-nnum 13:14:preserved 14:15:preserved 15:16:preserved 16:18:preserved 17:19:preserved :17:mogrammar-det

Haar wavelet features have been widely used in many face detection systems [1 , 15] .
Haar wavelet features have been widely used in many face detection systems [1 , 15] .
134 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved

It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
They consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape . //I�fm not 100 percent clear on what " they " points to here . " These Haar wavelet features , " perhaps? But can features consist of other kinds of features? You may want to clarify here .]
135 0:0:bigrammar-nnum 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

The feature value is defined as the difference of sum of the pixels within rectangles .
The feature value is defined as the difference of the sum of the pixels within the rectangles .
138 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:16:preserved 15:17:preserved :9:mogrammar-det :15:mogrammar-det

In total , 134 ,736 features were used for training classifiers .
In total , 134 ,736 features were used for training classifiers .
139 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved

Gabor wavelet features have also often been used in face recognition systems [12] and are defined as : \MATH where \MATH and \MATH define the orientation and scale of the Gabor kernels respectively , \MATH , and the wave vector \MATH , is defined as : \MATH where \MATH , \MATH \MATH .
Gabor wavelet features have also often been used often in face recognition systems [12] and are defined as : \MATH , where \MATH and \MATH define the orientation and scale of the Gabor kernels respectively , \MATH , and the wave vector \MATH , is defined as : \MATH where \MATH , \MATH and \MATH .
142 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7,8:bigrammar-others 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:21:preserved 20:22:preserved 21:23:preserved 22:24:preserved 23:25:preserved 24:26:preserved 25:27:preserved 26:28:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:preserved 31:33:preserved 32:34:preserved 33:35:preserved 34:36:preserved 35:37:preserved 36:38:preserved 37:39:preserved 38:40:preserved 39:41:preserved 40:42:preserved 41:43:preserved 42:44:preserved 43:45:preserved 44:46:preserved 45:47:preserved 46:48:preserved 47:49:preserved 48:50:preserved 49:51:preserved 50:52:preserved 51:54,53:bigrammar-others 52:55:preserved

The Gabor representation of a face image is computed by convolving the face image with the Gabor filters .
The Gabor representation of a face image is computed by convolving the face image with the Gabor filters .
145 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved

Let \MATH be the face image , its convolution with a Gabor filter �� ,_( z ) is defined as : \MATH where \MATH denotes the convolution operator .
Let \MATH be the face image ; , its convolution with a Gabor filter �� ,_( z ) is defined as : \MATH where \MATH denotes the convolution operator .
146 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:7:preserved 7:8:preserved 8:9:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:28:preserved 28:29:preserved

Similar to [12] , Gabor kernels at five scales \MATH and eight orientations \MATH were used .
Similar to [12] , Gabor kernels at five scales , \MATH , and eight orientations , \MATH , were used .
149 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:12:preserved 11:13:preserved 12:14:preserved 13:16:preserved 14,15:18,19:preserved

At each pixel position , 40 Gabor features are computed by convolving the input image with the real part of Gabor filters .
At each pixel position , 40 Gabor features are computed by convolving the input image with the real part of Gabor filters .
150 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:18:preserved 19:19:preserved 20:20:preserved 21:21:preserved 22:22:preserved

As a result , \MATH there are \MATH Gabor features for one 24x24 training sample .
As a result , one \MATH training sample hasthere are \MATH Gabor features for one 24x24 training sample .
151 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4,5:5,4,8,7,6:paraphrase 6:9:preserved 7:10:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12:15:preserved 13:16:preserved 14:17:preserved 15:18:preserved

In order to show effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods that are forward feature selection ( FFS ) [16] and CMI-basedmethod using binary features ( CMIBinary ) [8 , 7] on the data set and feature setsmentioned above .
To prove the effectiveness of the proposed feature selection method ( CMI-Multi ) , we compared it with two other feature selection methods ?that are forward feature selection ( FFS ) [16] and a CMI-based methods using binary features ( CMI-Binary ) [8 , 7] ? on the data set and feature sets mentioned described above .
156 0,1,3:1,2:paraphrase 2:0:preserved 4:3:preserved 5:4:preserved 6:5:preserved 7:6:preserved 8:7:preserved 9:8:preserved 10:9:preserved 11:10:preserved 12:11:preserved 13:12:preserved 14:13:preserved 15:14:preserved 16:15:preserved 17:16:preserved 18:17:preserved 19:18:preserved 20:19:preserved 21:20:preserved 22:21:preserved 23:22:preserved 25,24:24,23:preserved 26:25:preserved 27:26:preserved 28:27:preserved 29:28:preserved 30:29:preserved 31:30:preserved 32:31:preserved 33:32:preserved 34:34,35:typo 35:36:preserved 36:37:preserved 37:38:preserved 38:39:preserved 39:40:spelling 40:41:preserved 41:42:preserved 42:43:preserved 43:44:preserved 44:46:preserved 45:47:preserved 46:48:preserved 47:49:preserved 48:50:preserved 49:51:preserved 50:52,53:typo 51:55,54:paraphrase :33:mogrammar-det

All classifiers were trained using AdaBoost similar to [1] .
All classifiers were trained using AdaBoost similar to [1] .
159 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved

We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results when not only reducing significantly the training time of AdaBoost-based face detection system [1] ( about 100 times ) but also maintaining comparable performance .
We chose the forward feature selection proposed by Wu et. al. [16] because it has very impressive results , when not only reducing significantly the training time of the AdaBoost-based face detection systems [1] by ( about 100 times , ) but also maintaining comparable performance .
160 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved 25:26:preserved 26:27:preserved 27:29:preserved 28:30:preserved 29:31:preserved 30:32:bigrammar-nnum 31:33:preserved 32:35:preserved 33:36:preserved 34:37:preserved 35:38:preserved 36:39,40:preserved 37:41:preserved 38:42:preserved 39:43:preserved 40:44:preserved 41:45:preserved 42:46:preserved :34:mogrammar-prep

Figure 2 shows performance of classifiers trained by Haar feature subsets selected by three feature selection methods .
Figure 2 shows performance of classifiers trained by Haar feature subsets selected by three feature selection methods .
163 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

It indicates that , the proposed method CMI-Multi outperforms the others while FFS and CMI-Binary have comparable performance .
The figureIt indicates that , the proposed method , CMI-Multi , outperforms the others while the performances of FFS and CMI-Binary have were comparable performanceto one another .
164 0:0,1:paraphrase 1:2:preserved 2:3:preserved 3:4:preserved 4:5:preserved 5:6:preserved 6:7:preserved 7:9:preserved 8:11:preserved 9:12:preserved 10:13:preserved 11:14:preserved 12,13,14:18,19,20,15,16,17:paraphrase 15:21,22:para-passact 16,17:23,24:preserved 18:25,26,27:paraphrase

The similar result is also shown when tested on Gabor wavelet features .
The A similar result is was also shown when the three feature selection methods were tested on Gabor wavelet features .
167 0:0,1:bigrammar-det 1:2:preserved 2:3:preserved 3:4,5:paraphrase 4:6:preserved 5:7:preserved 6:8:preserved 7:15,14,13,12,9,10,11:paraphrase 8:16:preserved 9,10,11:17,18,19:preserved

In this case , CMI-based feature selection methods obviously outperform FFS and CMI-Multi is confirmed to be more efficient than CMI-Binary .
In this case , CMI-based feature selection methods obviously clearly outperformed FFS , and CMI-Multi is was confirmed to be more efficient than CMI-Binary .
168 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:9,8:paraphrase 9:10:bigrammar-vtense 10:11:preserved 11:13:preserved 12:14:preserved 13:15,16:paraphrase 14:17:preserved 15:18:preserved 16:19:preserved 17:20:preserved 18:21:preserved 19:22:preserved 20:23:preserved 21:24:preserved

Because our proposed method uses same principle as FFS which only trains weak classifiers once , it is extremely fast compared with AdaBoost [1] .
Because our proposed method uses same principle as FFS , which only trains weak classifiers once , it is extremely fast compared with AdaBoost [1] .
171 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:10:preserved 10:11:preserved 11:12:preserved 12:13:preserved 13:14:preserved 14:15:preserved 15:16:preserved 16:17:preserved 17:18:preserved 18:19:preserved 19:20:preserved 20:21:preserved 21:22:preserved 22:23:preserved 23:24:preserved 24:25:preserved

We built two cascade of AdaBoost classifiers that use CMI-Multi and AdaBoost [1] as feature selection methods .
We built two cascades of AdaBoost classifiers that use CMI-Multi and AdaBoost [1] as feature selection methods .
172 0:0:preserved 1:1:preserved 2:2:preserved 3:3:bigrammar-nnum 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

Testing on the standard benchmark MIT+CMU test set , they have comparable performance .
Testing on the standard benchmark MIT+CMU test set , they hadve comparable performance .
173 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:bigrammar-vtense 11:11:preserved 12:12:preserved 13:13:preserved

However , CMI-Multi is trained faster than AdaBoost approximately 70 times .
However , CMI-Multi wasis trained faster than was AdaBoost by approximately 70 times .
176 0:0:preserved 1:1:preserved 2:2:preserved 3:3:paraphrase 4:4:preserved 5:5:preserved 6:6,7:paraphrase 7:8:preserved 8:10:preserved 9:11:preserved 10:12:preserved 11:13:preserved :9:mogrammar-prep

We have presented a fast feature selection method using conditional mutual information to handle huge feature sets .
We have presented a fast feature selection method using conditional mutual information to handle huge feature sets .
182 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9:9:preserved 10:10:preserved 11:11:preserved 12:12:preserved 13:13:preserved 14:14:preserved 15:15:preserved 16:16:preserved 17:17:preserved

The estimation of mutual information is simplified by using MDLP based discretization method .
The estimation of mutual information is simplified by using an MDLP- based discretization method .
183 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:7:preserved 8:8:preserved 9,10:10,11:spelling 11:12:preserved 12:13:preserved 13:14:preserved :9:mogrammar-det

Integrated into AdaBoost-based object detection systems , it can not only reduce the training time significantly but also achieve high classification performance .
Integrated into AdaBoost-based object detection systems , our proposed methodit can not only reduces the training time significantly , but also achieves high classification performance .
184 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 6:6:preserved 7:8,9,7:paraphrase 8:10:preserved 9:11:preserved 10:12:preserved 11:13:bigrammar-inter 12:14:preserved 13:15:preserved 14:16:preserved 15:17:preserved 16:19:preserved 17:20:preserved 18:21:bigrammar-inter 19:22:preserved 20:23:preserved 21:24:preserved 22:25:preserved

Experiments on two popular feature sets such as Haar wavelets and Gabor wavelets have demonstrated the effectiveness of the proposed method .
Experiments on two popular feature sets have demonstrated the effectiveness of the proposed method . //[Please note : I am not sure which of the following you mean .--> one composed of such as Haar wavelets and the other composed of Gabor wavelets / ? Haar wavelets and Gabor wavelets ?]
187 0:0:preserved 1:1:preserved 2:2:preserved 3:3:preserved 4:4:preserved 5:5:preserved 13,14:6,7:preserved 15,16,17,18,19,20:8,9,10,11,12,13:preserved

