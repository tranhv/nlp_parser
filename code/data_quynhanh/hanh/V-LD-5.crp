0
<document>
<document>
1
<title>
<title>
2
Ent-Boost : Boosting Using Entropy Measure
Ent-Boost : Boosting Using Entropy Measures
3
for Robust Object Detection
for Robust Object Detection
4
</title>
</title>
5
<abstract>
<abstract>
6
<p>
<p>
7
Recently , boosting is used widely in object detection applications because of its impressive performance in both speed and accuracy .
Recently , boosting has come to be used widely in object detection applications because of its impressive performance in both speed and accuracy .
8
However , learning weak classifiers which is one of the most significant tasks in using boosting is left for users .
However , learning weak classifiers , which is one of the most significant tasks in using boosting , is left to users . //learning / training / identifying / finding?<--Here and throughout , I am not sure that " learning " is the best word choice . If you change it here , it should be changed throughout .
9
In Discrete AdaBoost , weak classifiers with binary output are too weak to boost when the training data is complex .
In Discrete AdaBoost , weak classifiers with binary output are too weak to boost when the training data is complex .
10
Meanwhile , determining the appropriate number of bins for weak classifiers learned by Real AdaBoost is a challenging task because small one might not well approximate the real distribution while large one might cause over-fitting , increase computation time and waste storage space .
Meanwhile , determining the appropriate number of bins for weak classifiers learned by Real AdaBoost is a challenging task because small ones might not accurately approximate the real distribution while large ones might cause over-fitting , increase computation time , and waste storage space .
11
</p>
</p>
12
<p>
<p>
13
This paper describes a novel method for efficiently learning weak classifiers using entropy measures , called Ent-Boost .
We have developed Ent-Boost , a novel method for efficiently learning weak classifiers using entropy measures . //method / boosting scheme?
14
The class entropy information is used to estimate the optimal number of bins automatically through discretization process .
Class entropy information is used to automatically estimate the optimal number of bins through discretization .
15
Then Kullback-Leibler divergence which is the relative entropy between probability distributions of positive and negative samples is employed to select the best weak classifier in the weak classifier set .
Then Kullback-Leibler divergence , which is the relative entropy between probability distributions of positive and negative samples , is used to select the best weak classifier in the weak classifier set .
16
Experiments have shown that strong classifiers learned by Ent-Boost can achieve good performance , and have compact storage space .
Experiments showed that strong classifiers learned by Ent-Boost can achieve good performance and be stored compactly . //[be stored compactly / achieve compact storage?]
17
Results on building a robust face detector are also reported .
The results of building a robust face detector using Ent-Boost showed the boosting scheme to be effective .
18
</p>
</p>
19
</abstract>
</abstract>
20
<section label " Introduction " >
<section label " Introduction " >
21
<p>
<p>
22
Building a robust and reliable classifier is always a fundamental problem of pattern recognition .
Building a robust and reliable classifier is always a fundamental problem of pattern recognition .
23
Several kinds of classifiers , such as Neural Network [1] and Support Vector Machines [2] , have been proposed and applied successfully in many object-detection systems .
Several kinds of classifiers , such as neural networks [1] and support vector machines [2] , have been proposed and applied successfully in many object-detection systems .
24
Boosting [3] and its variants [4] ,[5] ,[6] ,[7] ,[8] ,[9] ,[10] have recently gained a lot of attentions from researchers because of its excellent performance .
Boosting [3] and its variants [4] ? [10] have recently gained much attention from researchers because of their excellent performance .
25
In regards to face detection , for example , the methods described in works [4] ,[5] ,[10] represent the state of the art in terms of both high accuracy and running speed .
In regard to face detection , for example , the methods described in [4] , [5] , and [10] are state-of-the-art in terms of both accuracy and running speed .
26
</p>
</p>
27
<p>
<p>
28
The main idea of boosting is to combine the performance of weak classifiers to form a strong classifier .
The main idea of boosting is to combine the performance of weak classifiers to form a strong classifier .
29
Typically , each weak classifier is any classifier whose performance is better than random guessing ( i.e. , error rate is less than 0 .5 ) .
Typically , a weak classifier is any classifier whose performance is better than random guessing ( i.e. , its error rate is less than 0 .5 ) .
30
Performances of weak classifiers are integrated into the final form of the strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .
The performances of these weak classifiers are integrated into the final form of a strong classifier through a learning process in which more accurate weak classifiers have larger weights in final voting .
31
In practical problems , designing and learning weak classifiers are left for practitioners with two main challenges : computational evaluation and discriminant power .
In practical problems , designing and learning weak classifiers leave practitioners with two main challenges : computational evaluation and discriminant power .
32
</p>
</p>
33
<p>
<p>
34
Generally , for efficient computation , the dimension of the input space of weak classifiers is reduced to much lower than that of the strong classifier .
Generally , for efficient computation , the dimensions of the input space of weak classifiers are reduced be to much smaller than those of the strong classifier[s?] .
35
In object-detection frameworks [4] ,[5] ,[11] ,[12] ,[13] weak classifiers are usually constructed from one or several features .
In object-detection frameworks [4] , [5] , [11] ? [13] , weak classifiers are usually constructed from one or several features .
36
For example , a weak classifier can be constructed from one Haar wavelet feature that is evaluated very rapidly through an integral image [4] .
For example , a weak classifier can be constructed from one Haar wavelet feature that is evaluated very rapidly through an integral image [4] .
37
Given a feature type , choosing the suitable way to form a weak classifier that balance efficiency and computation is still a open problem [14] .
Given a feature type , choosing the suitable way to form a weak classifier that balances efficiency and computation is still an open problem [14] .
38
</p>
</p>
39
<p>
<p>
40
There are two key trends for seeking the most discriminant weak classifier .
Two key trends exist for seeking the most discriminant weak classifier .
41
The first trend is dealing with the problem of how to design features for best representation of the target object .
The first trend is dealing with the problem of how to design features for best representing the target object .
42
Besides Haar wavelet features [4] , Gabor wavelets [5] , edge orientation histogram ( EOH ) [11] , orientation dominants [12] , scale invariant feature transform ( SIFT )-based-high-level features [13] and local binary pattern ( LBP ) [15] have also been used .
Besides Haar wavelet features [4] , Gabor wavelets [5] , edge orientation histograms ( EOH ) [11] , orientation dominants [12] , scale invariant feature transform ( SIFT )-based high-level features [13] , and local binary patterns ( LBP ) [15] have also been used .
43
The second trend is studying how to optimally select the best weak classifier from a weak classifier set .
The second trend is studying how to optimally select the best weak classifier from a weak classifier set .
44
</p>
</p>
45
<p>
<p>
46
In Discrete AdaBoost [16] , weak classifiers are threshold-functions whose the output is restricted to binary .
In Discrete AdaBoost [16] , weak classifiers are threshold-functions whose output is restricted to binary data. //[data / values??I think you need a noun here?binary what?]
47
This leads weak classifiers are too weak to boost when handling complex data sets .
This leads weak classifiers to be too weak to boost when handling complex data sets .
48
For example , in later layers of the cascaded face classifiers [4] , the error rate of weak classifiers is between 0 .4 and 0 .5 .
For example , in later layers of the cascaded face classifiers [4] , the error rate of weak classifiers is between 0 .4 and 0 .5 .
49
Meanwhile , in Real AdaBoost [3] , a generalized version of Discrete AdaBoost , weak classifiers are piece-wise functions whose the output is a real value representing the confidence-rated prediction .
Meanwhile , in Real AdaBoost [3] , a generalized version of Discrete AdaBoost , weak classifiers are piece-wise functions whose output is a real value representing the confidence-rated prediction .
50
Normally , to construct such weak classifiers , one splits the input space \MATH into non-overlapping blocks ( or subspaces ) \MATH , \MATH , . . . , \MATH so that the predictions of the weak classifier are the same for all instances falling into the same block .
Normally , to construct such weak classifiers , one splits the input space \MATH into non-overlapping blocks ( or subspaces ) \MATH , \MATH , . . . , \MATH so that the predictions of the weak classifier are the same for all instances falling into the same block .
51
In the case of one-feature-based weak classifiers , this is equivalent to dividing the real line into intervals .
In the case of one-feature-based weak classifiers , this is equivalent to dividing the real line into intervals .
52
Typically , most current works [5] ,[17] ,[6] ,[8] ,[10] split the data into \MATH bins that are equal width which suffers from following limitations :
Typically , most current works [5] , [6] , [8] , [10] , [17] split the data into \MATH bins that are equal in width . This method suffers from the following limitations : //[works / systems?]
53
</p>
</p>
54
<p>
<p>
55
-Choosing the appropriate number of bins is undetermined .
-The way to choose the appropriate number of bins is undetermined .
56
Normally , it has been done by trials and errors [6] ,[17] - a tedious task .
Normally , it has been done by trial and error [6] , [17] ? a tedious task .
57
In the training cascade of classifiers [6] ,[17] , when the complexity of the training data changes over time , using the same number of bins for training every layers is not optimal .
In the training cascade of classifiers [6] , [17] , when the complexity of the training data changes over time , using the same number of bins for training every layer is not optimal .
58
</p>
</p>
59
<p>
<p>
60
-Choosing a large number of bins might cause over-fitting because of outliers in the case of noisy data [18] .
-Choosing a large number of bins might cause over-fitting because of outliers in the case of noisy data [18] .
61
Furthermore it might increase computation and training time , waste storage space which is critical in applications with limited resources , for example , face detection on mobile phones .
Furthermore , it might lengthen computation and training time and waste storage space , which is critical in applications with limited resources , for example , face detection on mobile phones .
62
Meanwhile choosing a small number of bins might not well approximate the real densities of the data distribution and thus influence selection of the best weak classifier .
Choosing a small number of bins , however , might not accurately approximate the real densities of the data distribution and could influence the selection of the best weak classifier .
63
</p>
</p>
64
<p>
<p>
65
It is therefore necessary to have a deterministic method to choose this number of bins automatically and optimally .
A deterministic method is therefore needed to automatically and optimally choose the number of bins .
66
This problem can be formulated as a discretization problem in which subspace boundaries are found by some criteria .
This problem can be formulated as a discretization problem in which subspace boundaries are found by some criteria . //[some criteria?This sounds a bit vague . Could you be more specific?]
67
Among discretization methods , the entropy based method [19] has been proved most efficiently ; hence , we propose using it to solve the problem .
Among discretization methods , the entropy-based method [19] has been proved most efficient . Hence , we propose using it to solve the problem .
68
</p>
</p>
69
<p>
<p>
70
The entropy based discretization method is an algorithm that automatically selects appropriate thresholds to split feature values into optimal bins by using entropy measurement .
The entropy-based discretization method is an algorithm that automatically selects appropriate thresholds to split feature values into optimal bins by using entropy measurement .
71
It is a supervised discretization method which takes into account class information and data distribution , so it is generic and can be applied for any kinds of input data .
It is a supervised discretization method that takes into account class information and data distribution , so it is generic and can be applied to any kind of input data .
72
Furthermore , many studies have been shown that discretization process might help to improve performance in induction tasks [18] , it can also work with a weighted data distribution ; therefore , it is most appropriate for boosting-based methods .
Furthermore , many studies have shown that the discretization process might help to improve performance in induction tasks [18] and it can also work with a weighted data distribution . Therefore , it is most appropriate for boosting-based methods .
73
</p>
</p>
74
<p>
<p>
75
Besides learning weak classifiers , selecting the best weak classifier in the large weak classifier set in each round of boosting is also important .
Besides learning weak classifiers , selecting the best weak classifier in the large set of weak classifiers in each round of boosting is also important .
76
Adopting [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples .
Following the method used in [5] , it is done by choosing the weak classifier that maximizes Kullback-Leibler ( KL ) divergence between two distributions of positive and negative samples . // [used / proposed?]
77
</p>
</p>
78
<p>
<p>
79
The integration of entropy-based discretization process and optimal weak classifier selection into the current boosting framework forms a new variant of AdaBoost , called Ent-Boost .
The integration of the entropy-based discretization process and optimal weak classifier selection into the current boosting framework formed a new variant of AdaBoost , called Ent-Boost .
80
Experiments on building a robust face detector have shown effectiveness of this new boosting scheme .
Experiments on building a robust face detector have shown the effectiveness of this new boosting scheme .
81
</p>
</p>
82
</section>
</section>
83
<section label " Review of AdaBoost Algorithm " >
<section label " Review of AdaBoost Algorithm " >
84
<p>
<p>
85
Originally , Discrete AdaBoost proposed by Freund and Schapire [16] is a learning method of combining weak classifiers to a strong classier .
Originally , Discrete AdaBoost , proposed by Freund and Schapire [16] , was a learning method of combining weak classifiers to form a strong classier .
86
</p>
</p>
87
<p>
<p>
88
Given a training set \MATH where \MATH and \MATH , a weak classifier \MATH has the form \MATH .
Given a training set \MATH , where \MATH and \MATH , a weak classifier \MATH has the form \MATH .
89
Normally , a weak classifier is any classifier whose performance measured by error rate is less than 0 .5 .
Normally , a weak classifier is any classifier whose performance measured by error rate is less than 0 .5 .
90
Therefore , in many applications [4] ,[5] ,[7] , it is simplified by associating to one feature \MATH .
Therefore , in many applications [4] , [5] , [7] , it is simplified by associating with one feature \MATH .
91
Through boosting processing , weak classifiers are combined into a strong classifier \MATH where \MATH are values that measure performance of the selected weak classifier .
Through boosting processing , weak classifiers are combined into a strong classifier \MATH where \MATH are values that measure the performance of the selected weak classifier .
92
</p>
</p>
93
<p>
<p>
94
In boosting process , a distribution \MATH or set of weights over the training samples are maintained and updated so that subsequent weak classifiers focus on the hard classified samples .
In the boosting process , a distribution \MATH or set of weights over the training samples are maintained and updated so that subsequent weak classifiers focus on the strong-classified samples . //[hard / strong?]
95
</p>
</p>
96
<p>
<p>
97
Real AdaBoost [3] is a generalized version of Discrete AdaBoost in that weak classifiers are real-valued functions instead of binary ones and \MATH is found numerically in general instead of predescription .
Real AdaBoost [3] is a generalized version of Discrete AdaBoost in that weak classifiers are real-valued functions instead of binary ones and \MATH is found numerically instead of by predescription . //[This method also involves?NOTE : A method cannot propose something .
98
This method also proposes designing weak classifiers that partition the input space into subspaces so that its predictions are unique in each subspace .
Do you mean that the creators of this system proposed this?] designing weak classifiers that partition the input space into subspaces so that the predictions are unique in each subspace .
99
Such weak classifiers are used widely in current state of the art object detection systems [5] ,[17] ,[8] .
Such weak classifiers are used widely in current state-of-the-art object detection systems [5] , [8] , [17] .
100
</p>
</p>
101
<p>
<p>
102
Suppose that \MATH , \MATH , . . . , \MATH is a partition of the domain \MATH on which such weak classifiers $h$ are defined .
Suppose that \MATH , \MATH , . . . , \MATH is a partition of the domain \MATH on which such weak classifiers $h$ are defined .
103
The prediction of \MATH depends only on which block \MATH a given instance falls into .
The prediction of \MATH depends only on which block \MATH a given instance falls into .
104
On the other hand , \MATH for all \MATH .
On the other hand , \MATH for all \MATH .
105
In the case of one-feature-based weak classifiers , the histograms of positive and negative samples are computed as follows \MATH where \MATH .
In the case of one-feature-based weak classifier , the histograms of positive and negative samples are computed as follows \MATH where \MATH .
106
</p>
</p>
107
<p>
<p>
108
It is proved in [3] that the most appropriate choice for the prediction of the weak classifier on block \MATH to maximize the margin is \MATH where \MATH is a smoothed value in order to handle cases that \MATH is very small or even zero .
It is proven in [3] that the most appropriate choice for the prediction of the weak classifier on block \MATH to maximize the margin is \MATH where \MATH is a smoothed value in order to handle cases in which \MATH is very small or even zero .
109
A summary of the Real AdaBoost algorithm is given in Algorithm 1 .
A summary of the Real AdaBoost algorithm is given in Algorithm 1 .
110
</p>
</p>
111
<p>
<p>
112
Real AdaBoost is easy to implement ; however , in practical applications , designing and learning weak classifiers depend on specific applications .
Real AdaBoost is easy to implement , but in practical applications , designing and learning weak classifiers depend on specific applications .
113
In such face detection systems as [5] ,[6] ,[17] ,[8] , weak classifiers are usually associated with one feature .
In such face detection systems as [those described in?] [5] , [6] , [8] , and [17] , weak classifiers are usually associated with one feature .
114
With a very large number of available features , hundreds of thousands , there are a lot of choices to choose one weak classifier for each round of boosting .
With a very large number of available features ? hundreds of thousands ? [there are many candidates from which to / many choices must be made to?] select one weak classifier for each round of boosting .
115
Generally , optimally selecting the suitable weak classifier will make the final strong classifier more robust and efficient .
Optimally selecting the suitable weak classifier makes the final strong classifier more robust and efficient .
116
Furthermore , it can reduce the number of boosting rounds that directly shorten training time .
Furthermore , optimal selection can reduce the number of boosting rounds , thus directly shortening training time .
117
</p>
</p>
118
<p>
<p>
119
So far , most current studies have been focused on how to measure the discriminant power of weak classifiers in order to select the best weak classifier .
Most studies so far have been focused on how to measure the discriminant power of weak classifiers in order to select the best weak classifier .
120
Many measurements have been proposed ; for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] and , recently , Jensen-Shannon divergence [8] and mutual information [9] ( cf . Table 1 .
Many measurements have been proposed , for example , Bhattacharyya distance [6] , Kullback-Leibler divergence [5] , and recently , Jensen-Shannon divergence [8] and mutual information [9] ( Table 1 ) .
121
Meanwhile , few studies have been made for efficiently partitioning subspaces .
Meanwhile , few studies have been made on efficiently partitioning subspaces .
122
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by above measurements give comparable performance .
As shown in Figure 1 , using a fixed number of bins , strong classifiers trained by the above measures give similar performances . //[measurements / measures?]
123
However , it will be shown in section 5 , these performances are affected seriously if different subspace splitting methods are used .
However , as section 5 will show , these performances are affected dramatically if different subspace splitting methods are used .
124
</p>
</p>
125
</section>
</section>
126
<section label " Ent-Boost Algorithm " >
<section label " Ent-Boost Algorithm " >
127
<p>
<p>
128
The proposed boosting scheme Ent-Boost is an integration of adaptive entropy-based subspace splitting and the symmetric KL divergence-based weak classifier selection .
The proposed boosting scheme , Ent-Boost , is an integration of adaptive entropy-based subspace splitting and the symmetric KL divergence-based weak classifier selection .
129
</p>
</p>
130
<p>
<p>
131
In Ent-Boost , each weak classifier is constructed from one feature and trained on the weighted training samples similar to Real AdaBoost .
In Ent-Boost , each weak classifier is constructed from one feature and trained on weighted training samples similar to [those used in?] Real AdaBoost .
132
However , instead of using equal-width binning method like Real AdaBoost [6] ,[17] which is hard to know the suitable number of bins in advance , we use entropy-based discretization method [19] to split the input space into subspaces .
However , instead of using the equal-width binning method used in Real AdaBoost [6] , [17] which has a hard time predicting the suitable number of bins in advance , we use the entropy-based discretization method [19] to split the input space into subspaces .
133
This subspace splitting process is totally automatically in which the stopping criteria of splitting process is determined through using Minimum Description Length Principles ( MDLP ) ( see the next section ) .
This subspace splitting process is totally automatic ; the stopping criteria of the splitting process are determined using minimum description length principles ( MDLP ) . This process will be described in greater detail in the next section .
134
</p>
</p>
135
<p>
<p>
136
To select the best weak classifier from the input weak classifier set , we use symmetric KL divergence as in [5] which measures the distance between two distributions as follows : \MATH where \MATH and \MATH are probability distributions of a discrete random variable .
To select the best weak classifier from the input weak classifier set , we use symmetric KL divergence as in [5] , which measures the distance between two distributions as follows : \MATH where \MATH and \MATH are probability distributions of a discrete random variable .
137
</p>
</p>
138
<p>
<p>
139
This formula can be rewritten in entropy terms : \MATH or \MATH where \MATH and \MATH are entropy , and \MATH is cross entropy of \MATH and \MATH .
This formula can be rewritten in entropy terms : \MATH or \MATH where \MATH and \MATH are entropy and \MATH is cross entropy of \MATH and \MATH .
140
</p>
</p>
141
<p>
<p>
142
The outline of Ent-Boost is shown in Algorithm 2 .
The outline of Ent-Boost is shown in Algorithm 2 .
143
Note that the discretization process is performed in every round of boosting to adapt to new distributions of samples .
Note that the discretization process is performed in every round of boosting to adapt to new distributions of samples .
144
As a result , the number of intervals of selected weak classifier varies .
As a result , the number of intervals of the selected weak classifier varies . //[classifier varies / classifiers vary?]
145
This is different from previous methods that fix the number of equal-width intervals in advance .
This is different from previous methods , which fix the number of equal-width intervals in advance .
146
</p>
</p>
147
</section>
</section>
148
<section label " Entropy Based Subspace Splitting " >
<section label " Entropy-based Subspace Splitting " >
149
<p>
<p>
150
This section gives a brief introduction on automatic subspace splitting using entropy-based discretization .
This section briefly describes automatic subspace splitting using entropy-based discretization .
151
Basically , discretization is a quantizing process that converts continuous values into discrete values ; it typically consists of four steps [18] :
Discretization is a quantizing process that converts continuous values into discrete values . It typically consists of four steps [18] .
152
</p>
</p>
153
<p>
<p>
154
Step 1 : Sorting the continuous values of the feature to be discretized .
Step 1 : Sorting the continuous values of the feature to be discretized .
155
</p>
</p>
156
<p>
<p>
157
Step 2 : valuating candidate cut-points and selecting the best cut-point for splitting .
Step 2 : Evaluating candidate cut-points and selecting the best cut-point for splitting .
158
A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold .
A cut-point is a threshold value that divides the range of continuous values into two intervals ; one interval is less than or equal to the threshold , and the other interval is greater than the threshold .
159
</p>
</p>
160
<p>
<p>
161
Step 3 : Splitting the data into two intervals using the selected cut-point in step 2 .
Step 3 : Splitting the data into two intervals using the cut-point selected in step 2 .
162
</p>
</p>
163
<p>
<p>
164
Step 4 : Continuing discretization with each interval until a stopping criteria is satisfied .
Step 4 : Continuing discretization with each interval until a stopping criteria is satisfied .
165
The stopping criteria are usually selected according to a trade-off between lower arity ( the number of intervals or the number of bins ) and its effect on the accuracy of classification tasks .
The stopping criteria are usually selected by considering a trade-off between lower arity ( the number of intervals or the number of bins ) and its effect on the accuracy of classification tasks .
166
A higher arity can make the understanding of an attribute more difficult , while a very low arity may affect predictive accuracy negatively .
A higher arity can make the complicate the understanding of an attribute , while a very low arity may damage predictive accuracy .
167
</p>
</p>
168
<subsection label= " Entropy Based Subspace Splitting " >
<subsection label= " Entropy-based Subspace Splitting " >
169
<p>
<p>
170
Given a set \MATH of sorted continuous values \MATH , candidate cut-points are usually selected as mid-points of every successive pair of \MATH .
Given a set \MATH of sorted continuous values \MATH , candidate cut-points are usually selected as mid-points of every successive pair of \MATH .
171
On the other hand , candidate cut-points are \MATH .
On the other hand , candidate cut-points are \MATH .
172
</p>
</p>
173
<p>
<p>
174
For each cut-point \MATH that splits set \MATH into two subsets \MATH , the class entropy of a subset \MATH is defined as \MATH where \MATH is the number of classes \MATH , and \MATH is the proportion of examples in \MATH that have class \MATH .
For each cut-point \MATH that splits set \MATH into two subsets \MATH , the class entropy of a subset \MATH is defined as \MATH where \MATH is the number of classes \MATH , and \MATH is the proportion of examples in \MATH that have class \MATH .
175
</p>
</p>
176
<p>
<p>
177
To evaluate the resulting class entropy after set \MATH is partitioned into two sets \MATH and \MATH , the class-information entropy of the partition induced by cut-point T is defined by taking the weighted average of their resulting class entropies \MATH he best cut-point selected in step 2 is the cut-point \MATH for which \MATH is minimal amongst all the candidate cut-points .
To evaluate the resulting class entropy after set \MATH is partitioned into two sets \MATH and \MATH , the class-information entropy of the partition induced by cut-point T is defined by taking the weighted average of their resulting class entropies \MATH he best cut-point selected in step 2 is the cut-point \MATH for which \MATH is minimal amongst all the candidate cut-points .
178
</p>
</p>
179
</subsection>
</subsection>
180
<subsection label= " Stopping Criteria " >
<subsection label= " Stopping Criteria " >
181
<p>
<p>
182
Given set S and a potential binary partition , \MATH , specified on S by the given cut-point \MATH , a stopping criteria is used to decide whether or not this partition should be accepted .
Given set S and a potential binary partition \MATH , specified on S by the given cut-point \MATH , a stopping criteria is used to decide whether or not this partition should be accepted .
183
If the answer is YES , the discretization will continue with each partition given by \MATH ; otherwise , the discretization process will stop .
If the answer is YES , the discretization will continue with each partition given by \MATH ; otherwise , the discretization process will stop .
184
</p>
</p>
185
<p>
<p>
186
Suppose \MATH is the probability of a \MATH answer , and \MATH is the probability of the \MATH answer .
Suppose \MATH is the probability of a \MATH answer , and \MATH is the probability of a \MATH answer .
187
Partition \MATH is only accepted if \MATH .
Partition \MATH is only accepted if \MATH .
188
</p>
</p>
189
<p>
<p>
190
However , in practice , there is no easy way to estimate these probabilities directly .
However , in practice , there is no easy way to estimate these probabilities directly .
191
Instead , Fayyad and Irani [19] proposed using MDLP to indirectly estimate them .
Instead , Fayyad and Irani [19] proposed using MDLP to indirectly estimate them .
192
</p>
</p>
193
<p>
<p>
194
Originally , the minimum description length of an object is defined as the minimum number of bits required to uniquely specify that object out of the universe of all objects .
The minimum description length of an object is defined as the minimum number of bits required to uniquely specify that object out of the universe of all objects .
195
To employ MDLP in choosing the stopping criteria , Fayyad and Irani formulated the above problem as a communication problem between a sender and a receiver .
To employ MDLP in choosing the stopping criteria , Fayyad and Irani formulated the above problem as a communication problem between a sender and a receiver .
196
It is assumed that the sender has the entire set of training examples , while the receiver has the examples without their class labels .
It is assumed that the sender has the entire set of training examples , while the receiver has the examples without their class labels .
197
The sender needs to convey to proper class labeling of the example set to the receiver .
The sender needs to convey needed information for the proper class labeling of the example set to the receiver .
198
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to send before partition is more than the length of the message required to send after partition .
It says that the partition induced by a cut-point is accepted if and only if the length of the message required to be sent before the partition is more than the length of the message required to be sent after the partition .
199
</p>
</p>
200
<p>
<p>
201
By inferring from coding hypothesis , the stopping criteria is defined as follows : MDLP Criteria :A partition induced by cut-point \MATH for a set \MATH of \MATH examples is accepted iff :\MATH
By inferring from coding hypothesis , the stopping criteria is defined as follows : MDLP Criteria :A partition induced by cut-point \MATH for a set \MATH of \MATH examples is accepted iff :\MATH
202
where \MATH and \MATH \MATH is the number of classes in \MATH Extensive experiments [19] ,[18] recommended that this method should be the first choice for variable discretization because it gives small number of cut-points while maintaining consistency .
where \MATH and \MATH where\MATH is the number of classes in \MATH Extensive experiments [18] , [19] recommended that this method should be the first choice for variable discretization because it gives a small number of cut-points while maintaining consistency .
203
</p>
</p>
204
</subsection>
</subsection>
205
</section>
</section>
206
<section label " Experiments " >
<section label " Experiments " >
207
<subsection label= " Training Data " >
<subsection label= " Training Data " >
208
<p>
<p>
209
For experiments , face and non-face patterns are of size 24x24 .
For our experiments , face and non-face patterns were of size 24x24 . //[what is the unit here?]
210
A set of 10 ,000 face patterns were collected from the Internet .
A set of 10 ,000 face patterns were collected from the Internet .
211
Another set of 10 ,000 hard non-face patterns were false positives collected by running a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
Another set of 10 ,000 hard non-face patterns were false positives collected by running a cascade of 17 AdaBoost classifiers at different locations and scales on 8 ,440 images with various subjects , such as rocks , trees , buildings , scenery , and flowers , containing no faces .
212
The 10 ,000 patterns in each set are divided into a training set of 6 ,000 patterns and a test set of 4 ,000 examples .
The 10 ,000 patterns in each set were divided into a training set of 6 ,000 patterns and a test set of 4 ,000 examples .
213
Some examples of the collected 24x24 face and non-face patterns are shown in Figure 2 .
Some examples of the collected 24x24 face and non-face patterns are shown in Figure 2 .
214
</p>
</p>
215
<p>
<p>
216
Haar wavelet feature that has been widely used in many face detection systems [4] ,[6] ,[14] is used in our experiments .
Haar wavelet features , which have been used in many face detection systems [4] , [6] , [14] , were used in our experiments .
217
It consists of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
These consisted of four kinds of features modeled from adjacent basic rectangles with the same size and shape .
218
The feature value is defined as the difference of sum of the pixels within rectangles ( cf . Figure 3 ) .
The feature value was defined as the difference of the sum of the pixels within rectangles ( Figure 3 ) .
219
In total , 134 ,736 features were used for training classifiers .
In total , 134 ,736 features were used for training classifiers .
220
</p>
</p>
221
</subsection>
</subsection>
222
<subsection label= " Results " >
<subsection label= " Results " >
223
<p>
<p>
224
Figure 4 shows a comparison of performances of strong classifiers trained by different boosting schemes that are AdaBoost [4] , Real AdaBoost [17] and Ent-Boost .
Figure 4 shows a comparison of the performances of strong classifiers trained by the different boosting schemes : AdaBoost [4] , Real AdaBoost [17] , and Ent-Boost .
225
Each strong classifier is a combination of 80 weak classifiers ( using more weak classifiers does not improve much the performance ) .
Each strong classifier is a combination of 80 weak classifiers ( using more weak classifiers does not much improve the performance ) .
226
As for Real AdaBoost , the subspace splitting is done by equal width binning in which the number of bins is arbitrarily selected to be 64 and 128 .
For Real AdaBoost , subspace splitting is done by equal-width binning in which the number of bins is arbitrarily selected to be 64 and 128 .
227
The curves indicate that the performances of Real AdaBoost and Ent-Boost are better than that of AdaBoost .
The curves indicate that the performances of Real AdaBoost and Ent-Boost were better than that of AdaBoost .
228
In addition , the performance of Real AdaBoost classifiers varies when using different number of bins .
In addition , the performance of Real AdaBoost classifiers varied when using different numbers of bins .
229
Overall , Ent-Boost has the best result .
Overall , Ent-Boost produced the best result .
230
As for storage space , the Ent-Boost based classifier only employs 6 .79 bins on average which is much smaller than that of Real AdaBoost-based classifiers .
As for storage space , the Ent-Boost-based classifier only uses 6 .79 bins on average , which is much fewer than the number used by Real AdaBoost-based classifiers .
231
</p>
</p>
232
<p>
<p>
233
Using Ent-Boost , a robust face detector was built .
Using Ent-Boost , a robust face detector was built .
234
It was a cascade of Ent-Boost based classifiers that were trained similar to [4] .
It was a cascade of Ent-Boost-based classifiers that were trained [through a process similar to that used in] [4] .
235
</p>
</p>
236
<p>
<p>
237
The result cascade has 25 layers employing 3 ,850 features .
The resulting cascade has 25 layers using 3 ,850 features .
238
Performances of AdaBoost-based face detector [4] and Ent-Boost based face detector on MIT+CMU test set [1] shown in Table 2 has confirmed the effectiveness of our proposed boosting scheme .
The performances of the AdaBoost-based face detector [4] and our Ent-Boost-based face detector on the MIT+CMU test set [1] confirmed the effectiveness of our proposed boosting scheme ( Table 2 ) .
239
Some detection results are given in Figure 5 .
Some detection results are given in Figure 5 .
240
</p>
</p>
241
</subsection>
</subsection>
242
</section>
</section>
243
<section label " Conclusions " >
<section label " Conclusions " >
244
<p>
<p>
245
We have presented Ent-Boost , a variant of AdaBoost , which uses entropy measure for automatic subspace splitting and optimal weak classifier selection .
We have described Ent-Boost , a variant of AdaBoost , which uses entropy measures for automatic subspace splitting and optimal weak classifier selection .
246
The resulted strong classifier has good performance and compact storage .
The resultant strong classifier has good performance and achieves compact storage .
247
Furthermore , it overcomes the main limitation of Real AdaBoost which is hard to determine the suitable number of bins for subspace splitting .
Furthermore , this new boosting scheme overcomes the main limitation of Real AdaBoost , which is difficulty in determining the suitable number of bins for subspace splitting .
248
By considering the class information and the distribution of the input data in splitting process , this method is generic and can be applied to other applications .
Because it considers the class information and the distribution of the input data in the splitting process , this method is generic and can be used for other applications .
249
Experiments have shown promising results , especially in building a robust face detector .
Experiments have shown promising results , especially in the building of a robust face detector .
250
</p>
</p>
251
</section>
</section>
252
</document>
</document>
