0
<document>
<document>
1
<title>
<title>
2
Unsupervised Face Re-Ranking By Mining the Web and Video Archives
Unsupervised Face Re-Ranking By Mining the Web and Video Archives
3
</title>
</title>
4
<abstract>
<abstract>
5
<p>
<p>
6
to improve the retrieval performance of image search engines that use textual information for indexing , it is necessary to utilize visual information .
It is necessary to utilize visual information to improve the efficiency of retrieval in image-search engines that use textual information for indexing .
7
</p>
</p>
8
<p>
<p>
9
One popular approach is to learn visual consistency among the images returned by these search engines .
One popular approach has been to learn visual consistency between images returned by these search engines .
10
Most of the state of the art methods for learning the visual consistency usually learn one specific classifier for each query for re-ranking the returned images .
Most state-of-the-art methods of learning visual consistency usually learn one specific classifier for each query to re-rank the returned images .
11
The drawback of these methods is it requires computational cost and processing time that are unsuitable for handling a large number of queries .
The main drawback with these methods is that they require computational cost and processing time that are unsuitable for handling a large number of queries .
12
We propose a method in which one generic classifier is learned and then is used for all queries .
We propose a method in which one generic classifier is learned and is then used for all queries .
13
Different from query-specific based methods that learn classifiers for recognition concepts encoded in each query , the generic classifier of our method learns relevancy between images and the query for re-ranking purpose .
Different from query-specific based methods that learn classifiers for recognition concepts encoded in each query , the generic classifier in our method learns relevance between images and the query for re-ranking purposes .
14
The key contribution of this paper is to introduce a query-dependent feature to represent this relevancy and an unsupervised method to collect training samples for learning the generic classifier .
The key contribution of this research is to introduce a query-dependent feature to represent this relevance and an unsupervised method of collecting training samples to learn the generic classifier .
15
The generic classifier is built automatically and independent with existing ranking algorithms of input search engines .
The generic classifier is built automatically and is independent of existing ranking algorithms for input search engines .
16
experimental results show that the proposed method achieves good performance in various datasets .
The experimental results demonstrated that the proposed method performed very well in various datasets .
17
</p>
</p>
18
</abstract>
</abstract>
19
<section label=" Introduction ">
<section label= " Introduction " >
20
<p>
<p>
21
Image search is essential for many search engines .
Image searches are essential for many search engines .
22
Most of existing image search engines usually use text information for judging relevancy , resulting low precision performance .
Most existing image-search engines usually use text information to determine relevance , resulting in poor precision .
23
To improve the retrieval performance , it is necessary to use visual information of images for re-ranking .
To improve the accuracy of retrieval , it is necessary to use visual information from images to re-rank them .
24
However , content-based image understanding is a challenging and unsolved problem .
However , understanding content-based images remains a challenging and unsolved problem .
25
In addition , using visual information requires huge computational cost compared with using text .
In addition , using visual information requires much greater computational cost than using text .
26
</p>
</p>
27
<p>
<p>
28
One popular approach \CITE combining both text and visual information is to use text information to quickly retrieve a set of candidates and then do post-processing (i . e . re-rank) on this set to improve the precision .
One popular approach \CITE combining both text and visual information has been to use text information to quickly retrieve a set of candidates and then do post-processing ( i . e . , re-ranking ) on this set to improve precision .
29
</p>
</p>
30
<p>
<p>
31
There are two ways for post-processing : The first way \CITE is to build a ranker or a classifier specific to the given query using the returned images .
There are two ways of doing post-processing : The first \CITE has been to build a ranker or a classifier specific to the given query using the returned images .
32
Building such classifiers requires large computational cost and time .
Building such classifiers involves large computational cost and time .
33
As a result , this way is not scalable for applications processing very large number of queries .
As a result , this way is not scalable for applications that process very large numbers of queries .
34
The second way \CITE is to build a generic classifier once and then use it for all new queries .
The second way \CITE has been to build a generic classifier once and then use it for all new queries .
35
This way is more scalable and can be used for practical applications such as meta search engines .
This is more scalable and can be used for practical applications such as meta-search engines .
36
</p>
</p>
37
<p>
<p>
38
We follow the latter way for the problem of face retrieval in which the system enables users to search persons's appearance by their names .
We pursued the latter way to solve the problem with face retrieval in which the system enables users to search people's appearances by their names .
39
Our system re-ranks the faces returned by text-based search engines by a generic classifier that is trained in advance using visual information before returning to the user .
Our system re-ranks the faces returned by text-based search engines with a generic classifier that is trained in advance using visual information before returning them to the user .
40
</p>
</p>
41
Building such generic classifiers requires solving two problems : finding good query-relative representation of faces and collecting a large labeled dataset for training the classifier .
Building such generic classifiers requires two problems to be solved : finding a good query-relative representation of faces and collecting a large labeled dataset to train the classifier .
42
By addressing these problems , Our contribution is two-fold :
Our contribution by addressing these problems is two-fold :
43
</p>
</p>
44
<p>
<p>
45
-We propose a general framework for re-ranking faces returned by existing text-based search engine .
-We propose a general framework for re-ranking faces returned by existing text-based search engines .
46
In this framework , We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not .
We learn a relevance classifier that classifies whether an input face is relevant to the associated query or not in this framework .
47
The output scores returned by this classifier are used to re-rank the faces .
The output scores returned by this classifier are used to re-rank the faces .
48
The more relevant a face to the query , the higher score is .
The more relevant a face is to the query , the higher score is .
49
This approach is different from existing approaches such as \CITE that learn a classifier to recognize the identity of the returned faces .
This approach is different from existing ones \CITE that learn a classifier to recognize the identity of the returned faces .
50
For example , it recognizes a face as the appearance of 'personX' or not the appearance of 'personX' .
For example , it recognizes a face as the appearance of 'personX' or not the appearance of 'personX' .
51
Instead , the relevance classifier is learned to classify a face being relevant or irrelevant to the query .
Instead , the relevance classifier is learned to classify a face being relevant or irrelevant to the query .
52
this classifier is independent with the identity of faces , so it can be shared for multiple queries (cf . Figure \REF) .
As this classifier is independent of the identity of faces , it can be shared for multiple queries ( cf . Figure \REF ) .
53
We propose a novel representation for each face that models relevance between that face and the query .
We propose a novel representation for each face that models the relevance between that face and the query .
54
Once this query-dependent feature for each face is extracted , one relevance classifier can be shared by faces of various queries .
Once this query-dependent feature for each face is extracted , one relevance classifier can be shared by the faces of various queries .
55
experimental results show that the relevance classifier that is independent with underlying ranking algorithm of existing search engines can significantly boost the performance .
The experimental results demonstrated that the relevance classifier that is independent of the underlying ranking algorithms of existing search engines could significantly boost performance .
56
</p>
</p>
57
<p>
<p>
58
-We propose a simple yet efficient mining technique for automatically collecting labeled data for training the generic classifier .
-We propose a simple yet efficient mining technique of automatically collecting labeled data to train the generic classifier .
59
Specifically , We detect and group faces of persons appearing in video programs in face tracks in which each face track contains of the faces of one person .
We specifically detected and grouped faces of people appearing in video programs in face tracks in which each face track contained the faces of one person .
60
To distinguish face tracks of different persons , we assume that if multiple faces are detected at different locations in one frame , they are of different persons (cf . Figure \REF) .
To distinguish the face tracks of different people , we assumed that if multiple faces were detected at different locations in one frame , they would be of different people ( cf .
61
Using this assumption , we collect the face tracks whose faces are detected in the same frames to guarantee that each face track is associated to one unique person .
Using this assumption , we collected face tracks whose faces were detected in the same frames to guarantee that each face track was associated with one unique person .
62
To enlarge the number of such face tracks , We use video programs of multiple genres and channels .
We used video programs from multiple genres and channels to increase the number of such face tracks .
63
From these faces , We can artificially generate face sets similar to the sets returned by search engines given person names .
We could artificially generate face sets from these faces similar to the sets returned by search engines given people's names .
64
Since we know the relevance of these faces to the artificial sets , the labels of each face can be easily generated and no human intervention is needed for this process .
Since we knew the relevance of these faces to the artificial sets , the labels for each face could be easily generated and no human intervention was needed in this process .
65
Note that the label of faces in our approach is not identity of that face . It is the relevance between the face and the associated query .
Note that the labels of faces in our approach did not identity those faces but the relevance between the faces and the associated query .
66
Collecting training sets from such external sources as video archives is easy and efficient because : firstly , a large number of videos can be easy to obtain .
Collecting training sets from such external sources as video archives is easy and efficient because , first , a large number of videos can be easily obtained .
67
For example , people can record broadcast videos of different channels in a certain period .
For example , people can record broadcast videos from different channels within a certain period .
68
Secondly , a huge number of faces can be obtained by applying the face detector in every frame .
Second , a huge number of faces can be obtained by applying a face detector to all frames .
69
In addition , using temporal information , faces of one person appearing in consecutive frames can be automatically grouped with high accuracy .
In addition , the faces of one person appearing in consecutive frames can be automatically grouped with a high degree of accuracy using temporal information .
70
</p>
</p>
71
</section>
</section>
72
<section label=" Related Work ">
<section label= " Related Work " >
73
<p>
<p>
74
given a query described by text , for example , 'airplane' or 'George Bush' , finding relevant images with high precision is essential for image search engines .
It is essential for image-search engines to find relevant images with a high degree of precision given queries described by text , e.g. , 'airplane' or 'George Bush' .
75
Existing image search engines usually use textual information associated with the images such as filename , image caption , and surrounding text for ranking that leads to poor precision .
Existing image-search engines usually use textual information associated with images such as filenames , image captions , and surrounding text for ranking that leads to poor precision .
76
To improve the precision , visual information is used to re-rank the returned images .
To improve precision , visual information is used to re-rank the returned images .
77
The idea is to rely on the visual consistency among these images to learn visual classifiers that measure the relevancy between an image and the input query .
The idea is to rely on the visual consistency between these images to learn visual classifiers that measure the relevance between an image and the input query .
78
</p>
</p>
79
<p>
<p>
80
There are different approaches described in \CITE for re-ranking images containing general objects and faces returned from text-based search engines .
There have been different approaches \CITE to re-ranking images containing general objects and faces returned from text-based search engines .
81
Work such as \CITE extend topics models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or Hierarchical Dirichlet Process to learn generative model based classifiers .
Work \CITE has extended to topics on models using probabilistic Late Semantic Analysis , Latent Dirichlet Allocation , or the Hierarchical Dirichlet Process to learn generative model-based classifiers .
82
These models can handle noisy image data in some degree .
These models can handle noisy image data to some degree .
83
However , they have many parameters needed to be tuned such as number of topics and feature configurations .
However , they have many parameters that need to be tuned such as the number of topics and feature configurations .
84
In addition , how to select the best topic associated with the input query for identifying target label is still challenging \CITE .
In addition , how the best topic is selected associated with the input query to identify the target label is still a difficult issue \CITE .
85
</p>
</p>
86
<p>
<p>
87
In \CITE , Textual information is used to build a text ranker to re-rank the returned images \CITE .
Textual information has been used to build a text ranker to re-rank the returned images \CITE .
88
The top images in this ranked list are used as positive samples to train visual classifiers using SVM (Support vector machines) .
The top images in this ranked list were used as positive samples to train visual classifiers using SVM ( Support vector machines ) .
89
This method makes the training data cleaner that leads to performance improvement .
This method made the training data cleaner and led to improved performance .
90
</p>
</p>
91
<p>
<p>
92
In \CITE , A multiple instance learning framework is used to learn category models from images associated with keywords \CITE .
A multiple-instance learning framework has been used to learn category models from images associated with keywords \CITE .
93
The returned images are treated as positive bag .
The returned images were treated as a positive bag .
94
Negative bags are collected from image sets corresponding to unrelated keywords .
Negative bags were collected from image sets corresponding to unrelated keywords .
95
The learned model is used to re-rank the images .
The learned model was used to re-rank the images .
96
</p>
</p>
97
<p>
<p>
98
The work mentioned above are for re-ranking images containing general objects .
These researchers re-ranked images containing general objects .
99
For re-ranking faces , work described in \CITE use Gaussian mixture models to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
Gaussian mixture models have been used for re-ranking faces to build face recognizers and apply these recognizers back to the input faces for re-ranking \CITE .
100
In \CITE , Discriminative approach based models such as SVM and linear discriminant analysis are used instead of Gaussian mixture models \CITE .
Discriminative-approach-based models such as SVM and linear discriminant analysis have been used instead of Gaussian mixture models \CITE .
101
In \CITE , A densest graph based method is used for finding the face group relevant to the query \CITE .
A densest-graph-based method has been used for finding the face group relevant to the query \CITE .
102
</p>
</p>
103
<p>
<p>
104
As for these approaches , One specific classifier is built for each query .
One specific classifier is built for each query in these approaches .
105
Therefore , to handle a large number of queries , many classifiers must be built which are not suitable in practice .
Therefore , many classifiers must be built , which are not suitable in practice , to handle a large number of queries .
106
In \CITE{Krapac10CVPR} , Only one generic classifier is built in advance \CITE and then used for all queries .
Only one generic classifier has been built in advance \CITE and then used for all queries .
107
This generic classifier is a relevance classifier that learns relevancy between an image and the query .
This generic classifier was a relevance classifier that learned relevance between an image and the query .
108
As for specific classifiers , Each image is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , for example , 'airplane' .
Each image for specific classifiers is classified as 'class-A' or 'non-class-A' , where 'class-A' is the category associated with the query , e.g. , 'airplane' .
109
In other words , each specific classifier is associated with one class label implied by the query .
In other words , each specific classifier is associated with one class label implied by the query .
110
In generic classifier , Each image is classified as relevant or irrelevant to the query .
Each image in a generic classifier is classified as relevant or irrelevant to the query .
111
Therefore , it is independent to class labels and can be used for any query .
Therefore , it is independent of class labels and can be used for any query .
112
This method works well for objects such as car , flag , but fails to handle faces .
This method works well for objects such as cars and flags , but fails to handle faces .
113
</p>
</p>
114
<p>
<p>
115
Our method is inspired by the generic classifier based approach .
Our method was inspired by the generic-classifier-based approach .
116
We extend it by two means : first , query-dependent features specific for faces are proposed , and second , the training data for learning the generic classifier is collected automatically by mining video archives .
We extended it in two ways : first , query-dependent features specific to faces are proposed , and second , the training data for learning the generic classifier are collected automatically by mining video archives .
117
</p>
</p>
118
</section>
</section>
119
<section label= " Method " >
<section label= " Overview of Method " >
120
<p>
<p>
121
Given a set of faces returned by any search engine for a queried person ( e .g . 'George Bush' ) , our task is to re-rank these faces to improve the precision .
Given a set of faces returned by any search engine for a queried person ( e.g. , 'George Bush' ) , our task is to re-rank these faces to improve precision .
122
To this end , we extract query-dependent feature for each face and then use the generic classifier trained in advance to predict scores representing the relevance between that face and the query .
To this end , we extract query-dependent features for each face and then use the generic classifier trained in advance to predict scores representing the relevance between that face and the query .
123
These scores are sorted and used for re-ranking .
These scores are sorted and used for re-ranking .
124
The ranked list is then return to users as shown in Figure \REF( b ) .
The ranked list is then returned to users as shown in Figure \REF( b ) .
125
</p>
</p>
126
<p>
<p>
127
This approach is different from existing approaches such as \CITE as shown in Figure \REF( a ) in which one specific classifier is built for each query .
This approach is different from the existing approaches \CITE shown in Figure \REF( a ) in which one specific classifier is built for each query .
128
To build the specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by the query-independent feature such as pixel intensity around facial features such as eyes , nose , and mouth \CITE .
To build a specific classifier for re-ranking faces returned by the query of 'personX' , each face is represented by a query-independent feature such as pixel intensity around facial features such as the eyes , nose , and mouth \CITE .
129
The label for each face is 'personX' or 'non-personX' meaning that it is relevant or irrelevant to 'personX' .
The label for each face is 'personX' or 'non-personX' meaning that it is relevant or irrelevant to 'personX' .
130
Meanwhile , to build the generic classifier which is independent with any \textit{'personX'} , each face is represented by the query-dependent feature .
Further , each face is represented by the query-dependent feature to build a generic classifier that is independent of any 'personX' .
131
The label for each face is relevant or irrelevant to the query .
The label for each face is relevant or irrelevant to the query .
132
The query-dependent feature is used to encode this relevancy .
The query-dependent feature is used to encode this relevance .
133
In \CITE , the Query-dependent features using textual information are proposed \CITE .
Query-dependent features using textual information has been proposed \CITE .
134
Each feature is treated as binary indicating the presence or absence of the query terms in textual data associated with the input image , for example , filename , image title , and nearby text .
Each feature was treated as binary indicating the presence or absence of query terms in the textual data associated with the input image , e.g. , filenames , image titles , and nearby text .
135
</p>
</p>
136
<p>
<p>
137
Extending this query-dependent feature for using visual information is not trivial since we can not compute the presence and absence of the query term such as 'George Bush' in each face .
Extending this query-dependent feature to use visual information is not trivial since we cannot compute the presence or absence of query terms such as 'George Bush' in each face .
138
In \CITE , Each image \CITE is represented as a set of visual words .
Each image in \CITE is represented as a set of visual words .
139
The top- \MATH visual words that are strongly associated with the set of the returned images for the query are selected .
The top- \MATH visual words that are strongly associated with the set of returned images for the query are selected .
140
The binary features for each image are computed by evaluating the presence and absence of these visual words in that image .
The binary features for each image are computed by evaluating the presence and absence of these visual words in that image .
141
Since this method is suitable for general objects rather than faces , we proposed another method described below for extracting query-dependent features to train the generic classifier .
Since this method is suitable for general objects rather than faces , we propose another method of extracting query-dependent features to train the generic classifier that is described below .
142
</p>
</p>
143
<subsection label= " Query-Dependent Feature " >
<subsection label= " Query-Dependent Feature " >
144
<p>
<p>
145
To be able to model the relevance between a face and the given query , We assume that there is visual consistency among faces returned by search engines for that query .
We assumed that there would be visual consistency between faces returned by search engines for a query to be able to model the relevance between a face and that given query .
146
In the other word , we assume faces that are relevant to the query form the largest cluster .
In the other words , we assumed faces that were relevant to the query would form the largest cluster .
147
Note that finding such cluster is still difficult since the number of clusters is not known in advance and the accuracy of clustering algorithms always depends on the discriminative power of feature representation .
Note that finding such clusters is still difficult since the number of clusters is not known in advance and the accuracy of clustering algorithms always depends on the discriminative power of feature representation .
148
This assumption is widely accepted in most of the work of this field \CITE .
This assumption is widely accepted in most of the work in this field \CITE .
149
</p>
</p>
150
<p>
<p>
151
We consider the problem of finding relevant and irrelevant faces in the input set as the problem of outlier detection \CITE that is popular in data mining community .
We consider the problem of finding relevant and irrelevant faces in the input set to be the problem of outlier detection \CITE that is popular in the data-mining community .
152
We first describe several distance based outlier detection methods that use the distance to the \MATH -nearest neighbors to determine observations as outliers or non-outliers .
We first describe several distance-based methods of outlier detection that use the distance to the \MATH -nearest neighbors to determine observations as outliers or non-outliers .
153
Then the adaptation is proposed to form the query-dependent feature .
Then , adaptation is proposed to form a query-dependent feature .
154
</p>
</p>
155
</subsection>
</subsection>
156
<subsection label= " Neighborhood score " >
<subsection label= " Neighborhood score " >
157
<p>
<p>
158
Given a threshold \MATH , for each point \MATH , we examine number of points \MATH so that \MATH , where \MATH is the distance ( e .g . Euclidean distance ) between \MATH and \MATH in the feature space .
Given threshold \MATH , for each point \MATH , we examine the number of points \MATH so that \MATH , where \MATH is the distance ( e.g. , Euclidean distance ) between \MATH and \MATH in the feature space .
159
This number of points \MATH is called the neighborhood score of \MATH and is defined as follows : \MATH where \MATH is the total number of points of the input dataset .
This number of points \MATH is called the neighborhood score of \MATH and is defined as : \MATH where \MATH is the total number of points in the input dataset .
160
</p>
</p>
161
<p>
<p>
162
A low value of \MATH indicates \MATH is a candidate of outliers , while a high value of \MATH indicates \MATH is a member of one strong association cluster .
A low value for \MATH indicates \MATH is a candidate of outliers , while a high value for \MATH indicates \MATH is a member of one strong association cluster .
163
In practice , it is difficult to know \MATH because it depends on underlying distribution of the input dataset .
In practice , it is difficult to know \MATH because this depends on the underlying distribution of the input dataset .
164
</p>
</p>
165
</subsection>
</subsection>
166
<subsection label= " Distance score " >
<subsection label= " Distance score " >
167
<p>
<p>
168
For each point \MATH , find its \MATH -nearest neighbors \MATH , the distance score of \MATH is the sum of the distances between \MATH and its \MATH -nearest neighbors \MATH and is defined as follows : \MATH
For each point \MATH , find its \MATH -nearest neighbors \MATH ; the distance score of \MATH is the sum of the distances between \MATH and its \MATH -nearest neighbors \MATH and is defined as : \MATH
169
</p>
</p>
170
<p>
<p>
171
Points with larger values for \MATH have more sparse neighborhoods and are likely outliers than points belonging to dense clusters which usually have lower values of \MATH .
Points with larger values for \MATH have sparser neighborhoods and are more likely outliers than points belonging to dense clusters , which usually have lower values for \MATH .
172
Similar to nearest neighbor score , it is difficult to determine the appropriate \MATH value for each dataset .
Similar to the nearest neighbor score , it is difficult to determine the appropriate \MATH value for each dataset .
173
</p>
</p>
174
</subsection>
</subsection>
175
<subsection label= " Nearest neighbor based query-dependent feature " >
<subsection label= " Nearest-neighbor-based query-dependent feature " >
176
<p>
<p>
177
We consider the generic classifier as an outlier classifier that classifies an input sample as outlier or non-outlier .
We consider the generic classifier as an outlier classifier that classifies an input sample as an outlier or a non-outlier .
178
In our framework , Each face is an sample , and non-outliers / outliers mean faces relevant / irrelevant to the query ( i .e . target person ) .
Each face in our framework is a sample , and non-outliers / outliers mean faces are relevant / irrelevant to the query ( i.e. , target person ) .
179
</p>
</p>
180
<p>
<p>
181
As described above , \MATH and \MATH of outliers and non-outliers might have distributions shown in Figure \REF , these scores can be used as feature values to discriminate non-outliers and outliers .
As described above , the \MATH and \MATH of outliers and non-outliers might have the distributions in Figure \REF ; these scores can be used as feature values to discriminate non-outliers from outliers .
182
From this observation , the feature vector is formed by varying parameters such as \MATH and \MATH in formula of \MATH and \MATH as follows : \MATH .
From this observation , the feature vector is formed by varying parameters such as \MATH and \MATH in the formula of \MATH and \MATH as follows : \MATH .
183
</p>
</p>
184
</subsection>
</subsection>
185
<subsection label= " Collecting training samples " >
<subsection label= " Collecting training samples " >
186
<p>
<p>
187
In order to train the relevance classifier using supervised learning methods such as SVM , it requires a sufficient number of training samples .
It requires a sufficient number of training samples to train the relevance classifier using supervised learning methods such as SVM .
188
To collect training samples , The simplest way \CITE is we pick many names , and pass them to search engines .
The simplest way \CITE of collecting training samples is to pick many names , and pass them to search engines .
189
After collecting the returned faces , we manually label each face whether it is relevant to the input query or not .
After collecting the returned faces , we manually label each face as to whether it is relevant to the input query or not .
190
It is a tedious task and requires human labor cost .
This is a tedious task and involves a human-labor cost .
191
</p>
</p>
192
<p>
<p>
193
We propose another approach to automatically collect training samples for training the relevant classifier .
We propose another approach to automatically collecting training samples to train the relevant classifier .
194
This approach consists of two steps :
This approach consists of two steps .
195
First , by mining video archives , we automatically collect a set of faces of \MATH different persons \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of persons; and
First , by mining video archives , we automatically collect a set of faces of \MATH different people \MATH , where \MATH is the set of faces of person \MATH , and \MATH is the number of people .
196
Second , we generate a set of subsets \MATH , where \MATH is the set of faces that is picked from \MATH , and \MATH is the number of subsets .
Second , we generate a set of subsets \MATH , where \MATH is the set of faces that is picked from \MATH , and \MATH is the number of subsets .
197
The restriction is the assumption of visual consistency is satisfied .
The restriction is that the assumption of visual consistency is satisfied .
198
In other words , as shown in Figure \REF , \MATH might have several face clusters and the largest cluster is equivalent to the faces relevant to the query if returning by a search engine .
In other words , as seen in Figure \REF , \MATH might have several face clusters and the largest cluster is equivalent to the faces relevant to the query if they are returned by a search engine .
199
</p>
</p>
200
<p>
<p>
201
As a result , this method can stimulate face sets returned by search engines using many names mentioned above .
As a result , this method can be used to stimulate face sets returned by search engines using many names as mentioned above .
202
</p>
</p>
203
</subsection>
</subsection>
204
<subsection label= " Collecting \MATH " >
<subsection label= " Collecting \MATH " >
205
<p>
<p>
206
To obtain \MATH , we use a simple technique for faces extracted from video archives .
To obtain \MATH , we use a simple technique for faces extracted from video archives .
207
Specifically , We use the following heuristics to pick a set of different persons appearing in video archives :
We specifically use the following heuristics to pick a set of different people appearing in video archives :
208
</p>
</p>
209
<p>
<p>
210
-If there are more than one face appearing in different locations in one frame , they likely belong to different persons .
-If there is more than one face appearing in different locations in one frame , they are likely to belong to different people .
211
Figure \REF shows an example of this case .
Figure \REF shows an example where this has occurred .
212
<p>
<p>
213
-If two persons appear in video programs broadcast by different broadcast stations ( e .g . , CNN , MSNBC , and CCTV ) , they are likely different .
-If two people appear in video programs broadcast by different broadcast stations ( e.g. , CNN , MSNBC , and CCTV ) , they are likely to be different .
214
</p>
</p>
215
<p>
<p>
216
If we have large video archives , using these heuristics we can collect a sufficient number of training samples for learning the relevance classifier .
If we have large video archives , we can collect a sufficient number of training samples to learn the relevance classifier by using these heuristics .
217
</p>
</p>
218
</subsection>
</subsection>
219
<subsection label= " Generating \MATH " >
<subsection label= " Generating \MATH " >
220
<p>
<p>
221
We form a face set Generating \MATH by picking a subset of faces of Generating \MATH and adding randomly faces from other sets Generating \MATH .
We form face set Generating \MATH by picking a subset of faces of Generating \MATH and randomly adding faces from other sets Generating \MATH .
222
To keep the assumption of visual consistency satisfied , the number of faces selected in each set Generating \MATH must be smaller than the number of faces in set Generating \MATH .
To keep satisfying the assumption of visual consistency , the number of faces selected in each set Generating \MATH must be smaller than the number of faces in set Generating \MATH .
223
We then label faces in set Generating \MATH as relevant to the query associated with Generating \MATH , and the other faces of Generating \MATH as irrelevant to the query .
We then label faces in set Generating \MATH as relevant to the query associated with Generating \MATH , and the other faces of Generating \MATH as irrelevant to the query .
224
</p>
</p>
225
<p>
<p>
226
Once the training samples are collected , we use SVM with linear kernel to learn the relevance classifier .
Once the training samples are collected , we use SVM with a linear kernel to learn the relevance classifier .
227
</p>
</p>
228
</subsection>
</subsection>
229
</section>
</section>
230
<section label= " Experiments " >
<section label= " Experiments " >
231
<subsection label= " Data Sets " >
<subsection label= " Data Sets " >
232
TRECVID dataset : We collected all video programs of TRECVID 2006 dataset \CITE .
TRECVID dataset : We collected all video programs from the TRECVID 2006 dataset \CITE .
233
There are 527 video programs broadcast on 7 channels in 3 languages including English , Chinese and Arabic .
There were 527 video programs broadcast on seven channels in three languages including English , Chinese , and Arabic .
234
We extracted faces from these video programs and grouped faces belonging to one person in each shot in one face track using a similar method described in \CITE .
We extracted faces from these video programs and grouped faces belonging to one person in each shot in one face track using a similar method to that described in \CITE .
235
For each channel , We scanned all face tracks extracted from the videos broadcast by this channel , and picked face tracks extracted from keyframes that several faces were detected at different locations .
We scanned all face tracks for each channel extracted from the videos broadcast by this channel , and picked face tracks extracted from key frames where several faces were detected at different locations .
236
To guarantee selected face tracks representing different persons , for one channel , only face tracks of one shot was picked .
To guarantee selected face tracks representing different people , only the face tracks from one shot were picked for one channel .
237
As a result , there are 5 ,126 faces of 19 face tracks picked from the 7 channels corresponding to 19 different persons .
As a result , there were 5 ,126 faces of 19 face tracks selected from the seven channels corresponding to 19 different people .
238
Note that , the system does not know the identity of these faces .
Note that the system did not know the identity of these faces .
239
It only knows any two face tracks represent different persons .
It only knew any two face tracks represented different people .
240
The number of faces of these face tracks is shown in Figure \REF .
The number of faces in these face tracks is shown in Figure \REF .
241
</p>
</p>
242
<p>
<p>
243
Using these face tracks , We generated 133 labeled sets described in Section \REF and used them for training the relevance classifier .
We generated the 133 labeled sets described in Section \REF using these face tracks and used them to train the relevance classifier .
244
</p>
</p>
245
<p>
<p>
246
Yahoo News Images : This dataset consists of approximately half a million news photos and captions from Yahoo News collected over a period of roughly two years \CITE .
Yahoo News Images : This dataset consists of approximately half a million news photos and captions from Yahoo News collected over a period of roughly two years \CITE .
247
Using person names as queries , we applied simple string search to the captions this dataset to return a list of faces for each queried name .
Using people�fs names as queries , we applied a simple string search to the captions in this dataset to return a list of faces for each queried name .
248
We used 23 names of celebrities such as George W .
We used 23 names of celebrities such as George W .
249
Bush , Vladimir Putin , Ziang Jemin , Tony Blair , and Abdullah Gul .
Bush , Vladimir Putin , Ziang Jemin , Tony Blair , and Abdullah Gul .
250
These names are widely used in experiments such as \CITE .
These names have widely been used in experiments \CITE .
251
In total , 9 ,136 faces were retrieved in which 3 ,909 faces were relevant .
A total of 9 ,136 faces were retrieved in which 3 ,909 faces were relevant .
252
On average , The accuracy was \MATH .
The accuracy was \MATH on average .
253
</p>
</p>
254
<p>
<p>
255
Google Images : We used the same set of person names used in Yahoo News Images dataset and put to Google Image Search Engine .
Google Images : We used the same set of people�fs names used in the Yahoo News Images dataset and input them into the Google Image Search Engine .
256
For each query , We crawled a maximum of 500 images from URLs returned by Google .
We crawled a maximum of 500 images from URLs returned by Google for each query .
257
In total , 9 ,516 faces were extracted in which 5 ,816 faces were relevant .
A total of 9 ,516 faces were extracted in which 5 ,816 faces were relevant .
258
On average , The accuracy was \MATH .
The accuracy was \MATH on average .
259
</p>
</p>
260
<p>
<p>
261
The TRECVID dataset was used for training the generic classifier .
The TRECVID dataset was used for training the generic classifier .
262
The datasets , Yahoo News Images and Google Images as shown in Figure \REF , were used for testing .
The datasets for Yahoo News Images and Google Images , as shown in Figure \REF , were used for testing .
263
</p>
</p>
264
</subsection>
</subsection>
265
<subsection label= " Face Processing " >
<subsection label= " Face Processing " >
266
<p>
<p>
267
We used the Viola-Jones face detector \CITE to detect frontal faces in images and video frames .
We used the Viola-Jones face detector \CITE to detect frontal faces in images and video frames .
268
To group faces belonging to one person in one video shot , We simply used a similar technique described in \CITE .
We simply used a similar technique to that described in \CITE to group faces belonging to one person in one video shot .
269
Using the prior knowledge that faces of the same person in consecutive frames do not change much in locations and appearance , the technique used tracked points to robustly associate these faces into face tracks with the precision of \MATH .
Using prior knowledge that faces of the same person in consecutive frames do not change much in locations and appearance , the technique used tracked points to robustly associate these faces in face tracks with a precision of \MATH .
270
</p>
</p>
271
<p>
<p>
272
Once faces were extracted , we used the code provided by the authors \CITE to extract features .
Once faces were extracted , we used the code provided by the authors \CITE to extract features .
273
Each face is then represented as a point in a very high dimensional feature space .
Each face was then represented as a point in a very high dimensional feature space .
274
Specifically , for each face , 9 facial feature points were detected , and 4 more facial feature points were inferred from these 9 points .
Nine facial-feature points were specifically detected for each face , and four more facial feature points were inferred from these nine points .
275
In total , There were 13 feature points from which features are extracted .
There were a total of 13 feature points from which features were extracted .
276
The features are intensity values lying within the circle with radius of 15 pixels .
The features were intensity values lying within a circle with a radius of 15 pixels .
277
The output feature has 13x149 = 1 ,937 dimensions .
The output feature had 13x149 = 1 ,937 dimensions .
278
Figure \REF shows illustration of this feature .
Figure \REF illustrates this feature .
279
</p>
</p>
280
</subsection>
</subsection>
281
<subsection label= " Method " >
<subsection label= " Method of Evaluation " >
282
<p>
<p>
283
We evaluated the retrieval performance with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
We evaluated the efficiency of retrieval with measures that are commonly used in information retrieval , such as precision , recall , and average precision .
284
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as follows : \MATH .
Given a queried person and letting \MATH be the total number of faces returned , \MATH the number of relevant faces , and \MATH the total number of relevant faces , recall and precision can be calculated as : \MATH .
285
</p>
</p>
286
<p>
<p>
287
Precision and recall only evaluate the quality of an unordered set of retrieved faces .
Precision and recall were only used to evaluate the quality of an unordered set of retrieved faces .
288
To evaluate ranked lists in which both recall and precision are taken into account , the average precision is usually used .
Average precision is usually used to evaluate ranked lists in which both recall and precision are taken into account .
289
The average precision is computed by taking the average of the interpolated precision measured at the 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
The average precision is computed by taking the average of the interpolated precision measured at 11 recall levels of 0 .0 , 0 .1 , 0 .2 , . . . , 1 .0 .
290
The interpolated precision \MATH at a certain recall level \MATH is defined as the highest precision found for any recall level \MATH :
The interpolated precision , \MATH , at a certain recall level , \MATH , is defined as the highest precision found for any recall level \MATH :
291
</p>
</p>
292
<p>
<p>
293
In addition , to evaluate the performance of multiple queries , we used mean average precision , which is the mean of average precisions computed from queries .
In addition , we used the mean average precision to evaluate the performance of multiple queries , which is the mean of average precisions computed from queries .
294
</p>
</p>
295
</subsection>
</subsection>
296
<subsection label= " Results " >
<subsection label= " Results " >
297
<subsubsection label= " Comparison on YahooNews Images " >
<subsubsection label= " Comparison of Performance on YahooNews Images " >
298
<p>
<p>
299
In this experiment , We compare the MAP performance of the following systems testing on YahooNews Images :
We compared the performance of the Maximum A-Posteriori ( MAP ) algorithm in seven systems in this experiment by testing it on YahooNews Images :
300
</p>
</p>
301
<p>
<p>
302
-DistScore-TrainGoogleImages : The training set is the set of annotated faces returned by Google Images Search for 23 person names .
-DistScore-TrainGoogleImages : The training set was the set of annotated faces returned by Google Images Search for 23 people�fs names .
303
The feature vector is computed using \MATH .
The feature vector was computed using \MATH .
304
</p>
</p>
305
<p>
<p>
306
-NNScore-TrainGoogleImages : The training set is the same as DistScore-TrainGoogleImages .
-NNScore-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
307
The feature vector is computed using \MATH .
The feature vector was computed using \MATH .
308
DistScore-TrainTRECVID : The feature vector is computed using .
DistScore-TrainTRECVID : The feature vector was computed using .
309
The training set is the set of annotated faces artificially generated by our method described in Section \REF .
The training set was the set of annotated faces artificially generated with our method described in Section \REF .
310
</p>
</p>
311
<p>
<p>
312
-NNScore-TrainTRECVID : The training set is the same as DistScore-TrainTRECVID .
-NNScore-TrainTRECVID : The training set was the same as DistScore-TrainTRECVID .
313
The feature vector is computed using \MATH .
The feature vector was computed using \MATH .
314
</p>
</p>
315
<p>
<p>
316
-Krapac[11]-TrainGoogleImages : The training set is the same as DistScore-TrainGoogleImages .
-Krapac[11]-TrainGoogleImages : The training set was the same as DistScore-TrainGoogleImages .
317
We re-implemented the method proposed by Krapac et al . \CITE for extracting query-dependent feature .
We re-implemented the method proposed by Krapac et al. \CITE of extracting the query-dependent feature .
318
Since this method was proposed to handle images , not for faces , we modified it for handling faces .
Since this method was proposed to handle images , not faces , we modified it to handle faces .
319
Specifically , Each face is represented as a bag of visual words .
Each face was specifically represented as a bag of visual words .
320
We used 13 facial feature points detected in each face and their descriptors using pixel intensity as visual words .
We used 13 facial-feature points detected in each face and their descriptors using pixel intensity as visual words .
321
The codebook is formed by clustering all visual words extracted from all faces of the training set into 200 clusters .
The codebook was formed by clustering all visual words extracted from all faces of the training set into 200 clusters .
322
top-$k$ visual words strongly related to the returned faces of each query and the binary feature vector are computed as described in \CITE .
The top-$k$ visual words strongly related to the returned faces of each query and the binary feature vector were computed as described in \CITE .
323
</p>
</p>
324
<p>
<p>
325
-Mensink[15]-GaussianModels : This method proposed by Mensink et al . \CITE models the returned faces by using two Gaussians , one for the faces relevant to the target person and one for the remaining faces .
-Mensink[15]-GaussianModels : This method proposed by Mensink et al. \CITE modeled the returned faces by using two Gaussians , the first for the faces relevant to the target person and the second for the remaining faces .
326
</p>
</p>
327
<p>
<p>
328
-Mensink[15]-Friends : This method proposed by Mensink et al . \CITE uses linear discriminant analysis to train a specific classifier for each query .
-Mensink[15]-Friends : This method proposed by Mensink et al. \CITE used linear discriminant analysis to train a specific classifier for each query .
329
This method uses detected person names in captions associated with faces for query expansion to model faces of the target person 's friends .
This method used detected people�fs names in captions associated with faces for query expansion to model faces of the target person 's friends .
330
</p>
</p>
331
<p>
<p>
332
The Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are the state of the art methods that learn a specific classifier for each query .
Methods such as Mensink[15]-GaussianModels and Mensink[15]-Friends are state-of-the-art that learn a specific classifier for each query .
333
The method Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then is used for new queries .
Krapac[11]-TrainGoogleImages is similar to our method in which one generic classifier is trained in advance and then used for new queries .
334
</p>
</p>
335
<p>
<p>
336
Figure \REF shows the performance comparison of these systems when testing on YahooNews Images dataset .
Figure \REF compares the performance of these systems when they were tested on the YahooNews Images dataset .
337
As for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID , the curves show the correlation between the performance and the number of features .
The curves plot the correlation between performance and the number of features for systems such as DistScore-TrainGoogleImages , NNScore-TrainGoogleImages , DistScore-TrainTRECVID , and NNScore-TrainTRECVID .
338
</p>
</p>
339
<p>
<p>
340
-DistScore is significantly better than that of NNScore .
-DistScore performed significantly better than NNScore .
341
</p>
</p>
342
<p>
<p>
343
-The performance of DistScore and NNScore are not affected by selecting the number of features .
-The performance of DistScore and NNScore was not affected by selecting the number of features .
344
Therefore , we can use small number of features for reducing the computational cost .
Therefore , we could use small numbers of features to reduce the computational cost .
345
</p>
</p>
346
<p>
<p>
347
-The performance of the system using the training data generated artificially by our method is comparable with that of the system using the training data returned by search engines .
-The performance of the system using training data generated artificially with our method was comparable to that of the system using training data returned by search engines .
348
</p>
</p>
349
<p>
<p>
350
-Our proposed method DistScore-TrainTRECVID has comparable performance to the state of the art method in specific classifier-based approach Mensink[15]-Friends .
-The method of DistScore-TrainTRECVID we propose performed comparably to the state-of-the-art method in the specific classifier-based approach of Mensink[15]-Friends .
351
It outperforms the method using only visual information Mensink[15]-GaussianModels .
It outperformed the method where only visual information was used , i.e. , Mensink[15]-GaussianModels .
352
</p>
</p>
353
<p>
<p>
354
-Our proposed method DistScore-TrainTRECVID outperforms the method proposed by Krapac et al . customized for handling faces .
-Our proposed method DistScore-TrainTRECVID outperformed the method proposed by Krapac et al. , which was customized to handle faces .
355
</p>
</p>
356
</subsubsection>
</subsubsection>
357
<subsubsection label= " Comparison on Google Images " >
<subsubsection label= " Comparison of Performance on Google Images " >
358
<p>
<p>
359
As shown in Figure \REF , DistScore-TrainTRECVID outperforms original ranking of Google Images Search Engine if using from 20 to 50 features .
As seen in Figure \REF , DistScore-TrainTRECVID outperformed the original ranking of the Google Images Search Engine if from 20 to 50 features were used .
360
The result of DistScore-TrainTRECVID on YahooNews Images set and Google Images set indicates that the relevance classifier of our proposed method is able to generalize well on different queries and independent with underlying ranking algorithms of search engines .
The results for DistScore-TrainTRECVID on the YahooNews Images set and Google Images set indicate that the relevance classifier with our proposed method was able to generalize well on different queries and was independent of underlying ranking algorithms used in search engines .
361
</p>
</p>
362
<p>
<p>
363
Figure \REF shows an example of re-ranking result of top-30 faces for the query John Paul that is one of the most difficult cases of the YahooNews Images set .
Figure \REF shows an example of re-ranking results for the top-30 faces for the query John Paul , which is one of the most difficult cases in the YahooNews Images set .
364
The result clearly shows that our proposed method outperforms the other state of the art methods .
The results clearly demonstrate that our proposed method outperformed the other state-of-the-art methods .
365
</p>
</p>
366
</subsubsection>
</subsubsection>
367
<subsubsection label= " Scalability Issue " >
<subsubsection label= " Scalability Issue " >
368
<p>
<p>
369
Our query-dependent feature is based on nearest neighbors of the images in the returned image set that usually have complexity of \MATH , where \MATH is the total number of images in the set .
Our query-dependent feature was based on the nearest neighbors of images in the returned image set that usually have a complexity of \MATH , where \MATH is the total number of images in the set .
370
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and SASH \CITE can speed up the nearest neighbor search significantly .
However , recent studies on indexing techniques such as \MATH -d tree , locality sensitive hashing ( LSH ) , and a Self Adaptive Set of Histograms ( SASH ) \CITE could significantly speed up the nearest neighbor search .
371
For example , as described in \CITE , the complexity of fast lookup of $k$ approximate nearest neighbors is \MATH \CITE .
For example , the complexity of the fast lookup of $k$ approximate nearest neighbors is \MATH \CITE .
372
Studying other techniques to speedup the query-feature extraction process is our next step in future work .
Studying other techniques to speed up the process of query-feature extraction is our next step in future work .
373
</p>
</p>
374
</subsubsection>
</subsubsection>
375
</subsection>
b</subsection>
376
</section>
</section>
377
<section label= " Conclusion " >
<section label= " Conclusion " >
378
<p>
<p>
379
We have presented a novel method for re-ranking face images returned by existing search engines .
We have presented a novel method of re-ranking face images returned by existing search engines .
380
Instead of training a specific classifier for each new query , we train only one generic classifier and use it for ranking new queries .
Instead of training a specific classifier for each new query , we only trained one generic classifier and used it for ranking new queries .
381
This helps to make the ranking application more scalable .
This helped make the ranking application more scalable .
382
To train the generic classifier , We propose a simple unsupervised method to obtain a large number of labeled faces from video archives .
We propose a simple unsupervised method to train the generic classifier to obtain a large number of labeled faces from video archives .
383
It uses temporal information to group faces belonging to one person in one shot into one track .
It uses temporal information to group faces belonging to one person in one shot into one track .
384
Several heuristics are employed to guarantee that a subset of face tracks has the correct labels used in the training process .
Several heuristics are employed to guarantee that a subset of face tracks has the correct labels used in the training process .
385
Experiments shown that although our method is unsupervised and independent with underlying algorithms of existing search engines but successfully learns visual consistency among returned faces to boosts retrieval performance .
Experiments revealed that although our method is unsupervised and independent of underlying algorithms in existing search engines , it successfully learned visual consistency between returned faces to boost efficiency of retrieval .
386
</p>
</p>
387
</document>
</document>
