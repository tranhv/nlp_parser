0
<document>
<document>
1
<abstract>
<abstract>
2
<p>
<p>
3
This paper analyzes the effects of structural variation of sentences on parsing performances .
This paper analyzes the effect of the structural variation of sentences on parsing performance .
4
We examined the performances of both shallow and deep parsers for two sentence constructions : imperatives and questions .
We examine the performance of both shallow and deep parsers for two sentence constructions : imperatives and questions .
5
The target parsers are adapted to the sentences for these constructions extracted from fiction and query texts .
The target parsers are adapted to sentences of these constructions extracted from fiction and query texts .
6
The analysis of the experimental results will illustrate the necessity for handling various sentence constructions by fundamental improvement of parsers such as re-construction of feature designs .
Analysis of the experimental results illustrates the need to handle different sentence constructions through fundamental improvement of the parsers such as re-construction of feature designs .
7
</p>
</p>
8
</abstract>
</abstract>
9
<section label = " Introduction " >
<section label = " Introduction " >
10
<p>
<p>
11
Parsing is a fundamental natural language processing task and essential for various NLP applications .
Parsing is a fundamental natural language processing task and essential to various NLP applications .
12
Recent research on parsing technologies has achieved high parsing accuracies on the same domains as the training data , but once we move to unfamiliar domains , the performances decrease at unignorable levels .
Recent research on parsing technologies has achieved high parsing accuracy in the same domain as the training data , but once we move to unfamiliar domains , the performance decreases to unignorable levels .
13
</p>
</p>
14
<p>
<p>
15
To address this problem , previous work has mainly focused on adapting lexical or syntactic preferences to the target domain , that is , on adding lexical knowledge or adjusting probabilistic models for the target domain using available in-domain resources \CITE .
To address this problem , previous work has focused mainly on adapting lexical or syntactic preferences to the target domain , that is , on adding lexical knowledge or adjusting probabilistic models for the target domain using available in-domain resources \CITE .
16
Behind their approaches , there seems to be an assumption that grammatical constructions are not largely different among domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
Underlying these approaches , there seems to be the assumption that grammatical constructions are not largely different between domains or do not affect parsing systems , and therefore the same parsing system can be applied to a novel domain .
17
</p>
</p>
18
<p>
<p>
19
However , there are some cases where we cannot achieve as high parsing accuracies as parsing the Penn Treebank just by re-training or adaptation .
However , there are some cases where we cannot achieve such high parsing accuracy as parsing the Penn Treebank ( PTB ) merely by re-training or adaptation .
20
For example , the parsing accuracy for the Brown corpus is significantly lower than for the WSJ portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .
For example , the parsing accuracy for the Brown corpus is significantly lower than that for the Wall Street Journal ( WSJ ) portion of the Penn Treebank , even when re-training the parser with much more in-domain training data than other successful domains .
21
</p>
</p>
22
<p>
<p>
23
This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions which were not extensively studied in the recent parsing research : imperatives and questions .
This research attempts to identify the cause of these difficulties , and focuses on two types of sentence constructions that have not been extensively studied in recent parsing research : imperatives and questions .
24
In these constructions , words in some syntactic positions disappear or the orders of words change .
In these constructions , words in certain syntactic positions disappear or the order of the words changes .
25
We analyze how such sentences affect the parsing behavior and then attempt to clarify the difficulties in parsing imperatives and questions .
We analyze how such sentences affect the parsing behavior and then attempt to clarify the difficulties in parsing imperatives and questions .
26
In order to do so , we prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .
To do so , we first prepare an annotated corpus for each of the two sentence constructions by borrowing sentences from fiction and query domains .
27
</p>
</p>
28
<p>
<p>
29
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracies of a part-of-speech tagger for them .
In the experiments , parsing accuracies of two shallow dependency parsers and a deep parser are examined for imperatives and questions , as well as the accuracy of their part-of-speech ( POS ) tagger .
30
A conventional supervised adaptation technique was applied to these parsers and to the POS tagger .
A conventional supervised adaptation technique was applied to these parsers and to the POS tagger .
31
</p>
</p>
32
</section>
</section>
33
<section label = " Related work " >
<section label = " Related work " >
34
<p>
<p>
35
Since domain adaptation has been an extensive research area in parsing research \CITE , a lot of ideas have been proposed , including un- / semi-supervised approaches \CITE and supervised approaches \CITE .
Since domain adaptation is an extensive research area in parsing research \CITE , many ideas have been proposed , including un- or semi-supervised approaches \CITE and supervised approaches \CITE .
36
Their main focus was on adapting parsing models trained with a specific genre of text ( in most cases Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
The main focus of these works is on adapting parsing models trained with a specific genre of text ( in most cases the Penn Treebank WSJ ) to other genres of text , such as biomedical research papers and broadcast news .
37
A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions .
The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression .
38
However , parsing imperatives and questions involves a significantly different problem ; even when all words in a sentence are known , the sentence has a very different structure from declarative sentences .
However , parsing imperatives and questions involves a significantly different problem ; even when all words in a sentence are known , the sentence has a very different structure from declarative sentences .
39
</p>
</p>
40
<p>
<p>
41
Compared to domain adaptation , structural types of sentences have gained little attention to date .
Compared to domain adaptation , structural types of sentences have received little attention to date .
42
A notable exception is the work on QuestionBank \CITE .
A notable exception is the work on QuestionBank \CITE .
43
The work pointed out low accuracy of state-of-the-art parsers on questions , and proposed supervised parser adaptation by manually creating a treebank of questions .
This work highlighted the low accuracy of state-of-the-art parsers on questions , and proposed a supervised parser adaptation by manually creating a treebank of questions .
44
The question sentences are annotated with phrase structure trees in the Penn Treebank scheme , although function tags and empty categories are omitted .
The question sentences are annotated with phrase structure trees in the Penn Treebank scheme , although function tags and empty categories are omitted .
45
</p>
</p>
46
<p>
<p>
47
QuestionBank was used for the supervised training of an LFG parser , and achieved a significant improvement in parsing accuracy .
QuestionBank was used for the supervised training of an LFG parser , resulting in a significant improvement in parsing accuracy .
48
They collected question sentences from TREC 9-12 competitions , and annotated these sentences with POSs and CCG lexical categories .
In this work , question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories .
49
They reported a significant improvement in CCG parsing without phrase structure annotations .
The authors reported a significant improvement in CCG parsing without phrase structure annotations .
50
</p>
</p>
51
<p>
<p>
52
Our work further extends \CITE and \CITE , while covering a wider range of sentence constructions .
Our work further extends \CITE and \CITE , while covering a wider range of sentence constructions .
53
Although QuestionBank and the resource of \CITE are claimed to be a corpus of questions , they are biased because the sentences come from QA queries .
Although QuestionBank and the resource of \CITE claim to be corpora of questions , they are biased because the sentences come from QA queries .
54
For example , such queries rarely include yes / no questions and tag questions .
For example , such queries rarely include yes / no questions or tag questions .
55
In our work , sentences are collected from the Brown corpus , which includes a wider range of types of questions and imperatives .
For our study , sentences were collected from the Brown corpus , which includes a wider range of types of questions and imperatives .
56
In the experiments , we will additionally use QuestionBank for comparison .
In the experiments , we also used QuestionBank for comparison .
57
</p>
</p>
58
</section>
</section>
59
<section label = " Target Parsers and POS tagger " >
<section label = " Target Parsers and POS tagger " >
60
<p>
<p>
61
We examined the performance of two dependency parsers and a deep parser on the target text sets .
We examined the performance of two dependency parsers and a deep parser on the target text sets .
62
All parsers assume that the input is already POS-tagged .
All parsers assumed that the input was already POS-tagged .
63
We use a tagger in \CITE .
We used the tagger in \CITE .
64
</p>
</p>
65
<subsection label = " MST and Malt parsers " >
<subsection label = " MST and Malt parsers " >
66
<p>
<p>
67
The MST parser and Malt parser are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \CITE and transition-based algorithm \CITE respectively .
The MST and Malt parsers are dependency parsers that produce non-projective dependency trees , using the spanning tree algorithm \CITE and transition-based algorithm \CITE , respectively .
68
Although the publicly available implementation of each parser also has an option to restrict the output to be a projective dependency tree , we used the non-projective version because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .
Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree , we used the non-projective versions because the dependency structures converted from the question sentences in the Brown corpus included many non-projective dependencies .
69
We used pennconverter \CITE to convert a PTB-style treebank to dependency trees .
We used the pennconverter \CITE to convert a PTB-style treebank into dependency trees .
70
For the evaluation of the output from each of the MST and Malt parser , we used the labeled attachment accuracy excluding the punctuations .
To evaluate the output from each of the parsers , we used the labeled attachment accuracy excluding punctuation .
71
</p>
</p>
72
</subsection>
</subsection>
73
<subsection label = " HPSG parser " >
<subsection label = " HPSG parser " >
74
</p>
<p>
75
The Enju parser \CITE is a deep parser based on the HPSG formalism .
The Enju parser \CITE is a deep parser based on the HPSG ( Head Driven Phrase Structure Grammar ) formalism .
76
It produces an analysis of a sentence that includes the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .
It produces an analysis of a sentence including the syntactic structure ( i.e. , parse tree ) and the semantic structure represented as a set of predicate-argument dependencies .
77
We used a toolkit distributed with the Enju parser for training the parser with a PTB-style treebank .
We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank .
78
The toolkit initially converts the PTB-style treebank into an HPSG treebank and then trains the parser on it .
The toolkit initially converts a PTB-style treebank into an HPSG treebank and then trains the parser on this .
79
The HPSG treebank converted from the test section was used as the gold-standard in the evaluation .
The HPSG treebank converted from the test section was used as the gold-standard in the evaluation .
80
As the evaluation metrics of the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .
As evaluation metrics for the Enju parser , we used labeled and unlabeled precision / recall / F-score of the predicate-argument dependencies produced by the parser .
81
</p>
</p>
82
</subsection>
</subsection>
83
</section>
</section>
84
<section label = " Preparing treebanks of imperatives and questions " >
<section label = " Preparing treebanks of imperatives and questions " >
85
<p>
<p>
86
This section explains how we collected the treebanks of imperatives and questions , which were used in the experiments in Section \REF .
This section explains how we collected the treebanks of imperatives and questions used in the experiments in Section \REF .
87
</p>
</p>
88
<subsection label = " Extracting imperatives and questions from Brown Corpus " >
<subsection label = " Extracting imperatives and questions from Brown Corpus " >
89
<p>
<p>
90
Penn Treebank 3 contains treebanks of several genres of texts .
The Penn Treebank 3 contains treebanks of several genres of texts .
91
While the Wall Street Journal ( WSJ ) treebank has extensively been used for parsing experiments , we use the treebank of the Brown Corpus in our experiments .
Although the WSJ treebank has been used extensively for parsing experiments , we used the treebank of the Brown Corpus in our experiments .
92
Because the Brown Corpus portion includes texts of literary works , it is expected that it inherently contains a larger number of imperatives and questions than the WSJ portion .
As the Brown Corpus portion includes texts of literary works , it is expected to contain inherently a larger number of imperatives and questions than the WSJ portion .
93
</p>
</p>
94
<p>
<p>
95
The Brown Corpus portion of Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .
The Brown Corpus portion of the Penn Treebank 3 is annotated with phrase structure trees as in the Penn Treebank WSJ .
96
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " represents wh-questions , while " SQ " denotes yes / no questions .
Interrogative sentences are annotated with the phrase label " SBARQ " or " SQ " , where " SBARQ " denotes wh-questions , while " SQ " denotes yes / no questions .
97
Imperative sentences are annotated with the phrase label " S-IMP " .
Imperative sentences are annotated with the phrase label " S-IMP " .
98
We extracted those sentences annotated with these phrase labels .
All sentences annotated with these phrase labels were extracted .
99
Imperatives and questions appear not only at the top level but also appear as embedded clauses .
Imperatives and questions appear not only at the top level but also as embedded clauses .
100
We extracted such embedded questions and imperatives as well .
We extracted such embedded questions and imperatives as well .
101
When they are embedded in another imperative or question , we only extracted the outermost one .
However , is these were embedded in another imperative or question , we only extracted the outermost one .
102
Extracted sentences are post-processed so that they have natural sentence forms : first characters are capitalized , and question marks or periods are added when appropriate .
Extracted sentences were post-processed to fit the natural sentence form ; that is , with first characters capitalized and question marks or periods added as appropriate .
103
</p>
</p>
104
<p>
<p>
105
As a result , we extracted 750 imperative sentences and 1 ,241 question sentences from 24 ,243 sentences .
As a result , we extracted 750 imperative sentences and 1 ,241 question sentences from 24 ,243 sentences .
106
Examples of extracted sentences are shown in Figure \REF .
Examples of extracted sentences are shown in Figure \REF .
107
The number of sentences for each section is shown in Table \REF .
The numbers of sentences for each section are given in Table \REF .
108
</p>
</p>
109
<p>
<p>
110
Although we also applied a similar method to the WSJ portion , we could obtain only 115 imperatives and 432 questions .
Although we also applied a similar method to the WSJ portion , we only obtained 115 imperatives and 432 questions .
111
We will not use this data in the experiments .
This data was not used in the experiments .
112
</p>
</p>
113
</subsection >
</subsection >
114
<subsection label = " Extracting questions from QuestionBank " >
<subsection label = " Extracting questions from QuestionBank " >
115
<p>
<p>
116
As we will describe below , we additionally use QuestionBank in experiments .
As described below , we also used QuestionBank in the experiments .
117
However , an advantage of using the Brown treebank is that it includes annotations of function tags and empty categories . Therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \CITE , which relies on function tags and empty categories .
The advantage , however , of using the Brown treebank is that it includes annotations of function tags and empty categories , and therefore , we can apply the Penn Treebank-to-HPSG conversion program of Enju \CITE , which relies on function tags and empty categories .
118
Hence , we will show experimental results on Enju only with the Brown data .
Hence , we show experimental results for Enju only with the Brown data .
119
It should also be noted that , a constituency-to-dependency converter , pennconverter \CITE , provides more accurate conversion when function tags and empty categories are available ( See footnote 6 ) .
It should also be noted that , a constituency-to-dependency converter , pennconverter \CITE , provides a more accurate conversion when function tags and empty categories are available ( see footnote 6 ) .
120
QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .
QuestionBank consists of question sentences as well as a small number of imperative and declarative sentences .
121
We extracted 3 ,859 sentences that are annotated with " SBARQ " or " SQ " .
We extracted 3 ,859 sentences annotated with " SBARQ " or " SQ " .
122
During experiments , we found several annotation errors that caused fatal errors of treebank conversion .
During the experiments , we found several annotation errors that caused fatal errors in the treebank conversion .
123
We therefore corrected annotations of twelve sentences manually .
We manually corrected the annotations of twelve sentences .
124
We plan to make these corrections publicly available .
We intend making these corrections publicly available .
125
Examples of the annotation errors include brackets enclosing empty words and undefined or empty tags .
Examples of the annotation errors include brackets enclosing empty words and undefined or empty tags .
126
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged not with " . " but with " ? " ( 2 ,051 sentences ) , and phrase labels annotated as POS ( one sentence ) .
We also found and corrected obvious inconsistencies in the corpus : character " ' " replaced by " $<$ " ( 737 sentences ) , token " ? " tagged with " ? " instead of " . " ( 2 ,051 sentences ) , and phrase labels annotated as the POS ( one sentence ) .
127
</p>
</p>
128
</subsection>
</subsection>
129
</section>
</section>
130
<section label = " Experiments " >
<section label = " Experiments " >
131
<p>
<p>
132
We examined performances of the three parsers and the POS tagger for Brown imperatives and questions , and QuestionBank questions .
We examined the performance of the three parsers and the POS tagger with Brown imperatives and questions , and QuestionBank questions .
133
By observing the effects of parser or tagger adaptation to each domain , we would like to see the difficulties in parsing imperative and question sentences .
By observing the effect of the parser or tagger adaptation in each domain , we can identify the difficulties in parsing imperative and question sentences .
134
We also examined the portability of sentence construction properties between two similar domains : questions in Brown and QuestionBank .
We also examined the portability of sentence construction properties between two similar domains : questions in Brown and in QuestionBank .
135
</p>
</p>
136
<subsection label = " Experimental settings " >
<subsection label = " Experimental settings " >
137
<subsubsection label = " Dividing corpora " >
<subsubsection label = " Dividing corpora " >
138
<p>
<p>
139
We made experimental datasets for five domains : Wall Street Journal ( WSJ ) , Brown overall sentences , Brown imperatives , Brown questions , and QuestionBank questions .
We created experimental datasets for five domains : WSJ , Brown overall , Brown imperatives , Brown questions , and QuestionBank questions .
140
</p>
</p>
141
<p>
<p>
142
WSJ ( 43 ,948 sentences )
WSJ ( 43 ,948 sentences )
143
</p>
</p>
144
<p>
<p>
145
- Divided into three parts for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .
- Divided into three parts , for training ( Section 02 - 21 , 39 ,832 sentences ) , development test ( Section 22 , 1 ,700 sentences ) , and final test ( Section 23 , 2 ,416 sentences ) .
146
</p>
</p>
147
<p>
<p>
148
Brown overall ( 24 ,243 sentences )
Brown overall ( 24 ,243 sentences )
149
</p>
</p>
150
<p>
<p>
151
- Randomly divided into three parts for training ( 19 ,395 sentences ) , development set ( 2 ,424 sentences ) , and final test ( 2 ,424 sentences )
- Randomly divided into three parts for training ( 19 ,395 sentences ) , development set ( 2 ,424 sentences ) , and final test ( 2 ,424 sentences ) .
152
</p>
</p>
153
<p>
<p>
154
Brown imperatives ( 750 sentences )
Brown imperatives ( 750 sentences )
155
</p>
</p>
156
<p>
<p>
157
- divided into two parts : one for ten-folds cross validation test ( 65 $\times$ 10 sentences ) and the other for error analysis ( 100 sentences )
- divided into two parts : one for ten-fold cross validation test ( 65 $\times$ 10 sentences ) and the other for error analysis ( 100 sentences ) .
158
</p>
</p>
159
<p>
<p>
160
Brown questions ( 1 ,241 sentences )
Brown questions ( 1 ,241 sentences )
161
</p>
</p>
162
<p>
<p>
163
- divided into two parts : one for ten-folds cross validation test ( 112 $\times$ 10 sentences ) and the other for error analysis ( 141 sentences )
- divided into two parts : one for ten-fold cross validation test ( 112 $\times$ 10 sentences ) and the other for error analysis ( 141 sentences ) .
164
</p>
</p>
165
<p>
<p>
166
QuestionBank questions ( 3 ,859 sentences )
QuestionBank questions ( 3 ,859 sentences )
167
</p>
</p>
168
<p>
<p>
169
- from the top of the corpus divided into three parts for final test ( 1 ,000 sentences ) , training ( 2 ,560 sentences ) , and analysis ( 299 sentences )
- from the top of the corpus divided into three parts for final test ( 1 ,000 sentences ) , training ( 2 ,560 sentences ) , and analysis ( 299 sentences ) .
170
</p>
</p>
171
</subsubsection >
</subsubsection >
172
<subsubsection label = " Methods for parser and POS tagger adaptation " >
<subsubsection label = " Methods for parser and POS tagger adaptation " >
173
<p>
<p>
174
In order to adapt each parser or POS tagger to a target domain , we trained the parser on combined training data for the target domain and for the original parser .
To adapt each parser and the POS tagger to a target domain , we trained the parser using combined training data for the target domain and the original parser .
175
For a domain which contains only small training data , we replicated the training data for certain times and just utilized the concatenated replicas for training .
For a domain containing only a small amount of training data , we replicated the training data a certain number of times and utilized the concatenated replicas for training .
176
</p>
</p>
177
<p>
<p>
178
POS tagger
POS tagger
179
</p>
</p>
180
<p>
<p>
181
- For Brown overall , we trained the model with the combined training data for the target domain and for the original model .
- For Brown overall , we trained the model with the combined training data for the target domain and the original model .
182
For Brown imperatives / questions and QuestionBank , we replicated the training data for certain times and utilized the concatenated replicas and WSJ training data for training .
For Brown imperatives / questions and QuestionBank , we replicated the training data a certain number of times and utilized the concatenated replicas and WSJ training data for training .
183
For POS tagger , the number of replicas of training data was determined among 1 , 2 , 4 , 8 , 16 , 32 , 64 , and 128 , by testing these numbers on development test sets in three of ten datasets of cross validation .
For the POS tagger , the number of replicas of training data was determined as either 1 , 2 , 4 , 8 , 16 , 32 , 64 , or 128 , by testing these numbers on the development test sets in three of the ten datasets for cross validation .
184
</p>
</p>
185
<p>
<p>
186
MST and Malt parser
MST and Malt parser
187
</p>
</p>
188
<p>
<p>
189
- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and for the original model .
- For Brown overall and QuestionBank questions , we trained the model on combined data for the target domain and the original model .
190
For Brown imperatives and questions , we replicated the training data for ten times and utilized the concatenated replicas and WSJ training data for training .
For Brown imperatives and questions , we replicated the training data ten times and utilized the concatenated replicas and WSJ training data for training .
191
</p>
</p>
192
<p>
<p>
193
Enju parser
Enju parser
194
</p>
</p>
195
<p>
<p>
196
- We used a toolkit in the Enju parser \CITE
- We used the toolkit in the Enju parser \CITE .
197
</p>
</p>
198
</subsubsection>
</subsubsection>
199
</subsection>
</subsection>
200
<subsection label = " Exploring difficulties in POS tagging " >
<subsection label = " Exploring difficulties in POS tagging " >
201
<subsubsection label = " Overview of POS tagging accuracy " >
<subsubsection label = " Overview of POS tagging accuracy " >
202
<p>
<p>
203
Table \REF shows the POS tagging accuracies for the target domains .
Table \REF gives the POS tagging accuracy for the target domains .
204
When we applied WSJ tagger to other domains , the tagging accuracy more or less decreased .
When we applied the WSJ tagger to other domains , the tagging accuracy basically decreased .
205
Among them , for Brown overall sentences , the accuracy did not decrease much from WSJ .
For Brown overall , compared with the WSJ , the accuracy did not decrease much .
206
However , for imperatives and questions , the POS tagger accuracy decreased significantly .
However , for imperatives and questions , the POS tagger accuracy decreased significantly .
207
The table shows the adaptation could improve the tagging accuracy to some extent , while the table also shows that the improved accuracy for the imperatives and questions could not reach eventhe accuracy of adapted tagger for Brown overall .
The table shows that the adaptation improved the tagging accuracy to some extent , but that the improved accuracy for imperatives and questions was still below that of the adapted tagger for Brown overall .
208
</p>
</p>
209
<p>
<p>
210
Figure \REF shows the POS tagging accuracy for the target domains given by changing the size of the target training data .
Figure \REF shows the POS tagging accuracy for the target domains for varying sizes of the target training data .
211
This graph shows that for both types of sentences , first 300 training sentences improved the accuracy rapidly , and after that , the effect of adding training corpus declined .
This graph shows that for both types of sentences , the first 300 training sentences greatly improved the accuracy , but thereafter , the effect of adding training data declined .
212
In order to recover the tagging accuracy of the WSJ tagger for WSJ ( 97 .53\% in Table \REF ) , it would not seem to be enough only to prepare much more training data .
To match the tagging accuracy of the WSJ tagger for the WSJ ( 97 .53\% in Table \REF ) , preparing much more training data does not appear to be enough .
213
Especially , the problem would be more serious for imperatives .
In particular , the problem is more serious for imperatives .
214
</p>
</p>
215
</subsubsection>
</subsubsection>
216
<subsubsection label = " Error analysis in POS tagging " >
<subsubsection label = " Error analysis in POS tagging " >
217
<p>
<p>
218
We then explored the tagging errors in each domain in order to observe what types of errors the WSJ tagger gave and what types of errors were solved or still unsolved by the adapted taggers .
Next , we explored the tagging errors in each domain to observe the types of errors from the WSJ tagger and which of these were either solved by the adapted taggers or remain unsolved .
219
</p>
</p>
220
<p>
<p>
221
Table \REF , \REF , and \REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank respectively .
Tables \REF , \REF , and \REF show the most frequent tagging errors given by the WSJ tagger / adapted tagger for Brown questions , Brown imperatives , and QuestionBank , respectively .
222
</p>
</p>
223
<p>
<p>
224
In the tables , we could find that the major errors of the WSJ tagger for the Brown domains were the mis-tagging to verbs , that is , " VB \SPEC " .
From the results , we found that the main errors of the WSJ tagger for the Brown domains were mistagging of verbs , that is , " VB \SPEC " .
225
We then analyzed why each of such errors had occurred .
We then analyzed why each of these errors had occurred .
226
For Brown imperatives , the WSJ tagger gave two major tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
For Brown imperatives , the WSJ tagger gave two main tagging errors : " VB \SPEC NN( P ) " and " VB \SPEC VBP " .
227
These two types of errors would respectively come from the following differences in sentence constructions between WSJ declarative and the Brown imperative sentences .
These two types of errors arise from the following differences in sentence constructions between the WSJ declarative and Brown imperative sentences .
228
</p>
</p>
229
<p>
<p>
230
Firstly , declarative sentences normally begin with noun phrases while imperative sentences normally begin with verb phrases .
First , a declarative sentence normally begins with a noun phrase , whereas an imperative sentence normally begins with a verb phrase .
231
The WSJ tagger was trained on the domain mainly consisting of declarative sentences , and the training was based on N-gram sequences of words or POSs . The tagger therefore preferred to give noun phrase-derived tags to the beginning of a sentence .
Since The WSJ tagger was trained on a domain consisting mainly of declarative sentences , with the training based on N-gram sequences of words or POSs , preference was given to noun phrase-derived tags at the beginning of a sentence .
232
</p>
</p>
233
<p>
<p>
234
Secondly , main verbs in imperative sentences take base forms while main verbs in declarative sentences take the forms according to tense .
Second , the main verb in an imperative sentence takes a base form , whereas the main verb in a declarative sentence takes a form based on tense .
235
The problem is that , for present tense except for third person singular , verbs in the declarative sentences always take the same appearances as the base forms , while the tags are different : VBP and VB .
A problem arises in that , for the present tense , except for third person singular , the verb in a declarative sentence always has the same appearance as the base form , although the tags are different : VBP and VB , respectively .
236
The WSJ tagger mainly based on declarative sentences therefore prefer to give VBP tags to main verbs .
Since the WSJ tagger is predominantly based on declarative sentences , it prefers to give VBP tags to main verbs .
237
</p>
</p>
238
<p>
<p>
239
After adapting the tagger to Brown imperatives , the N-gram model of tagger would have learned that the first word in a sentence tends to be a verb , and the main verb tends to take base form ( VB ) .
After adapting the tagger to Brown imperatives , the N-gram model of the tagger would have learned that the first word in a sentence tends to be a verb , and that the main verb tends to take the base form ( VB ) .
240
Table \REF shows that the above two types of errors did decrease to some extent . However , we can also observe that not a few mis-tags to verbs were still left after the adaptation .
Table \REF shows that after adaptation the above two types of errors decreased to some extent , although a few mistags of verbs still remained .
241
</p>
</p>
242
<p>
<p>
243
When we observe each of the left errors around VB , we found that several errors still occurred even in simple imperative sentences such as " VB \SPEC NN " for " Charge " in " Charge something for it . " , and that some errors tended to occur after to-infinitive phrase or conjunction , such as " VB \SPEC NN " for " subtract " in " To find estimated net farm income , subtract . . . "
By investigating the remaining errors associated with VB , we found that several errors still occurred even in simple imperative sentences such as " VB \SPEC NN " for " Charge " in " Charge something for it " , and that some errors tended to occur after a to-infinitive phrase or conjunction , such as " VB \SPEC NN " for " subtract " in " To find the estimated net farm income , subtract . . . " .
244
The former type of errors might be solved by increasing the training data , while the latter type of errors would not be easily solved with the model based on word N-gram which cannot detect the existence of long phrases .
The former type could be solved by increasing the training data , whereas the latter error type cannot easily be solved with a model based on a word N-gram that cannot detect the existence of long phrases .
245
</p>
</p>
246
<p>
<p>
247
We also analyzed the errors in Brown questions and QuestionBank , and again found that the WSJ tagger seems to make many errors due to the fact that the tagger was trained on a corpus mainly consisting of declarative sentences .
We also analyzed the errors in Brown questions and QuestionBank , and again found that many errors were due to the fact that the WSJ tagger was trained on a corpus consisting mainly of declarative sentences .
248
After the adaptation , while some of the errors such as special usage of wh-words , i.e. , " WDT \SPEC WP " , were corrected , we found that some kinds or errors related to the global change of sentence structures still remained .
After the adaptation , although some of the errors such as the special use of wh-words , i.e. , " WDT \SPEC WP " , were corrected , other kinds or errors related to the global change in sentence structure still remained .
249
</p>
</p>
250
<p>
<p>
251
In order to give correct tags to words both in imperatives and questions , we might have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
To tag words correctly both in imperatives and questions , we may have to consider richer information than only N-gram based features , such as long distance dependencies or phrases .
252
</p>
</p>
253
</subsubsection>
</subsubsection>
254
</subsection>
</subsection>
255
<subsection label = " Parsing with gold POS tags " >
<subsection label = " Parsing with gold POS tags " >
256
<p>
<p>
257
Table \REF shows the parsing accuracies of MST( first order ) , MST( second order ) , Malt , and Enju parser for WSJ , Brown overall , Brown imperatives and Brown questions .
Table \REF gives the parsing accuracy of MST ( first order ) , MST ( second order ) , Malt , and the Enju parser for WSJ , Brown overall , Brown imperatives , and Brown questions .
258
Figure \REF shows the parsing accuracies against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .
Figure \REF plots the parsing accuracy against the training data size of the four parsers for WSJ , Brown imperatives , Brown questions , and QuestionBank .
259
Note that , since training MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environments , the parsing accuracies represented by the bracketed hyphens in Table \REF could not be measured and we could not draw full graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
Note that , since the training of the MST parser ( second order ) on Brown overall , Brown questions , and QuestionBank could not be completed in our experimental environment , the corresponding parsing accuracies denoted by bracketed hyphens in Table \REF could not be measured , Consequently , we could not plot complete graphs of second order MST for Brown questions and QuestionBank in Figure \REF .
260
</p>
</p>
261
<p>
<p>
262
When we adapted the parser model ( see fifth column in Table \REF ) , the parser could give two to four points higher accuracies for each of the Brown domains than the WSJ parser .
After adaptation ( see " Adapted " column in Table \REF ) , the parser achieved two to four percent higher accuracy for each of the Brown domains compared to the WSJ parser .
263
For the QuestionBank , 25 to 35 points accuracy improvements were observed .
For QuestionBank , 25 to 35 percent improvement in accuracy was observed .
264
Figure \REF shows that , the improvements increased according to the size of the training data , and the tendencies would not seem to converge .
Figure \REF shows that the improvement is proportional to the size of the training data and that this tendency does not seem to converge .
265
This would suggest that lower accuracies than the WSJ parser for WSJ would be still brought by the lack of training data .
This would suggest that lower accuracy than that of the WSJ parser for the WSJ could still be as a result of a lack of training data .
266
In Figure \REF , when we focus on the QuestionBank where we could use much more training data than Brown questions , the parser accuracies were approaching the accuracies of WSJ parser for WSJ or exceeded the accuracy .
In Figure \REF , the parser accuracy for QuestionBank , for which we could use much more training data than for Brown questions , approaches or even exceeds that of the WSJ parser for WSJ .
267
However , we have no more training data for Brown imperatives and questions . We should prepare more training data or explore approaches to enable us to sufficiently adapt parsers with small training data .
However , as there is no more training data for Brown imperatives and questions , we need to either prepare more training data or explore approaches that enable the parsers to be adapted with small amounts of training data .
268
</p>
</p>
269
</subsection>
</subsection>
270
<subsection label = " Error analysis on parsing " >
<subsection label = " Error analysis on parsing " >
271
<p>
<p>
272
In order to capture the outline of the adaptation effects , we observed error reduction for the Malt parser .
To capture an overview of the adaptation effects , we observed the error reduction in the Malt parser .
273
Table \REF and \REF show the recall errors on labeled dependencies which were observed more than ten times for 100 analysis sentences of each domain .
Tables \REF and \REF give the recall errors on labeled dependencies , which were observed more than ten times for 100 analysis sentences in each domain .
274
For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags .
For each dependency shown in the first column , the second and third columns show the number of parsing errors by the WSJ parser with gold tags and the adapted parser with gold tags , respectively .
275
Since ROOT dependencies , that is , heads of sentences would be critical to construction of sentences , we mainly focus on that type of errors .
Since ROOT dependencies , that is , heads of sentences , are critical to the construction of sentences , we focus mainly on this type of error .
276
</p>
</p>
277
<p>
<p>
278
For Brown imperatives and questions , we could observe that the reduction of ROOT dependency was prominent .
For Brown imperatives and questions , the reduction in ROOT dependencies was prominent .
279
When we focus on this type of errors , we could find that the WSJ parser could often make mistakes in parsing sentences which began or ended with the names of persons who were talk to .
On investigation , we found that the WSJ parser often made mistakes in parsing sentences which began or ended with the name of the person being addressed .
280
For example in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser regarded the person name " Zion " as ROOT , and the main verb " See " as modifiers of the name .
For example , in Brown imperatives , for the sentence " See for yourself , Miss Zion . " , the WSJ parser mistook the name " Zion " to be ROOT , and the main verb " See " to be a modifier of the name .
281
The adapted parser could then correctly give ROOT to the main verb .
The adapted parser correctly assigned ROOT to the main verb .
282
</p>
</p>
283
<p>
<p>
284
We could also often find that the WSJ parser could often make mistakes in parsing sentences containing quotation , exclamation , and question marks , such as " " Hang on " !! " " or " Why did you kill it " ? ? " or " " " " .
We also found that the WSJ parser often made mistakes in parsing sentences containing quotation , exclamation , or question marks , such as " " Hang on " !! " " or " Why did you kill it " ? ? " or " " " " .
285
For such sentences , the WSJ parser regarded the first " ! " or " ? " as ROOT , and " Hang " or " did " as the modifier of the marks .
For such sentences , the WSJ parser regarded the first " ! " or " ? " as ROOT , and " Hang " or " did " as the modifier of the punctuation .
286
We thought that this kind of errors would partly come fromthe Brown corpus itself . The exclamation or question marks should be inside the quotation , while the Brown corpus usually put the marks outside .
A possible reason for this type of error could be that the Brown corpus places exclamation or question marks outside , instead of inside the quotation .
287
However , the adapted parser could take in such doubtful construction and gave ROOT to the main verbs as the corpus required .
The adapted parser could handle this dubious construction and assigned ROOT to the main verbs as the corpus required .
288
</p>
</p>
289
<p>
<p>
290
On the other hand , we also observed some still unsolved errors . We would show the two kinds of major errors among them .
On the other hand , we also observed some unsolved errors , of which we discuss two .
291
First , Brown imperatives and questions , include many conversation sentences , and therefore rather flexible constructions could be observed especially for imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
First , Brown imperatives and questions , include many colloquial sentences , which have rather flexible constructions , especially imperatives , such as " Lift , don't shove lift! " , " Come out , come out in the meadow! " , etc.
292
The parsing models based on the plausibility of constructions could hardly capture such sentences .
The parsing models based on the plausibility of constructions were not able to capture such sentences .
293
</p>
</p>
294
<p>
<p>
295
Second , when the different constructions of sentences were in one sentence , such as , the case where to-infinitive phrases or subordinate clauses precede imperatives and questions , the parser would often be confused .
Second , having different sentence constructions within a single sentence , such as , where a to-infinitive phrase or subordinate clause precedes an imperative or question , often confused the parser .
296
For example , for the imperative sentence " To find estimated net farm income , subtract estimated annual farming expenditures . . . " , both of the WSJ and adapted parsers regarded " find " as ROOT , because the parsers regarded the words following " find " as a that-clause complement for the " find " , like " To find [ ( that ) estimated net farm income , subtract estimated annual farming . . .] " .
For example , for the imperative sentence , " To find the estimated net farm income , subtract the estimated annual farming expenditure . . . " , both the WSJ and adapted parsers regarded " find " as ROOT , because the parsers regarded the words following " find " as a that-clause complementing " find " , as in " To find [ ( that ) the estimated net farm income , subtract the estimated annual farming . . .] " .
297
It would be difficult for the parsers to know where the main clause in such complex sentences .
It would be difficult for the parsers to know which is the main clause in such complex sentences .
298
This type of errors would hardly be solved only by increasing the training data .
This type of error cannot be solved merely by increasing the training data .
299
</p>
</p>
300
<p>
<p>
301
Imperatives or questions sentences consist not only of pure imperative or question clause , but also of other constructions of phrases or clauses .
Imperative or question sentences typically consist not only of a pure imperative or question clause , but also of other constructions of phrases or clauses .
302
The parser would parse such complex sentences without partition into each construction , and therefore it would sometimes be confused .
These complex sentences were parsed without being partitioned into separate constructions , and as a result the parser sometimes became confused .
303
</p>
</p>
304
</subsection>
</subsection>
305
<subsection label = " QuestionBank vs. Brown questions " >
<subsection label = " QuestionBank vs. Brown questions " >
306
<p>
<p>
307
Both of Brown questions and QuestionBank are in the domain of question .
Both the Brown questions and QuestionBank are in the question domain .
308
In this section , we examined whether the parser adapted to one domain would be portable to the other domain .
In this section , we examine whether a parser adapted to one domain could be ported to another domain .
309
</p>
</p>
310
<p>
<p>
311
QuestionBankdoes not give function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
QuestionBank does not provide function tags , and therefore in training and evaluation of the parsers , abstracted dependencies were extracted from the corpus .
312
Therefore , the parser adapted to one domain could not give correct dependency labels on such functions for the other domain .
As a result , a parser adapted to one domain could not provide correct dependency labels on functions for the other domain .
313
However , we would be able to expect that sentence constructions would be basically common and portable between two domains , which would contribute to give correct boundary for phrases and therefore the correct dependencies in phrases would be introduced by the adaptation .
However , we would expect that sentence constructions are basically common and portable between two domains , which would provide a correct boundary for phrases and therefore , the correct dependencies in phrases would be introduced by the adaptation .
314
</p>
</p>
315
<p>
<p>
316
Table \REF shows the parsing or tagging accuracies of each parser and the POS tagger for Brown questions and QuestionBank .
Table \REF gives the parsing or tagging accuracy of each parser and the POS tagger for Brown questions and QuestionBank .
317
the difference from Table \REF was that the parsers and the tagger were adapted to another question domain .
These results differ from those in Table \REF in that the parsers and the tagger have been adapted to another question domain .
318
The table shows that the parsers adapted to Brown questions improved the parsing accuracies for QuestionBank , while the parsers adapted to QuestionBank decreased .
The table shows that the parsers adapted to the Brown questions improved their parsing accuracy with QuestionBank , whereas the parsers adapted to QuestionBank decreased in accuracy .
319
Table \REF could explain the result .
Table \REF could explain this result .
320
With Brown questions , we could learn wh-questions which QuestionBank mainly contain , while with QuestionBank , we could not we could not learn yes-no questions which more than half of Brown corpus contain .
Using Brown questions , many wh-questions were learnt , which is what QuestionBank mainly contains . On the other hand , despite yes-no questions constituting more than half the Brown corpus , these were not learnt using QuestionBank for training .
321
</p>
</p>
322
<p>
<p>
323
A question domain contains various types of questions and gives various sentence constructions .
A question domain contains various types of questions with various sentence constructions .
324
In order to parse questions correctly , we should capture each of them correctly .
In order to parse questions correctly , we need to capture each of these correctly .
325
This type of problem would not be noticed so much when we were working mainly on declarative sentences .
This type of problem was not so obvious when we were working mainly with declarative sentences .
326
</p>
</p>
327
</subsection>
</subsection>
328
</section>
</section>
329
<section label = " Conclusion " >
<section label = " Conclusion " >
330
<p>
<p>
331
Through the experiments on various parsers we observed that simple supervised adaptation methods are insufficient to arrive at theparsing accuracy comparable to that of declarative sentences .
Through experiments with various parsers we observed that simple supervised adaptation methods are insufficient to achieve parsing accuracy comparable with that of declarative sentences .
332
This observation holds both for POS tagging and syntactic parsing , and itindicates that we need fundamental improvement of parsers , such as re-constructing feature designs or changing parsing models .
This observation holds both for POS tagging and syntactic parsing , and indicates that the parsers need to be fundamentally improved , such as re-constructing feature designs or changing parsing models .
333
</p>
</p>
334
<p>
<p>
335
Following the present work , future work should include investigating parsing frameworks that are robust for sentences with various sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives , questions , and more .
Following on from this study , future work includes investigating parsing frameworks that are robust for sentences with different sentence constructions , and / or methods that can effectively adapt a parser to different sentence constructions including imperatives and questions , among others .
336
</p>
</p>
337
</section>
</section>
338
</document>
</document>
