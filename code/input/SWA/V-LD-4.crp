0
<document>
<document>
1
<title>
<title>
2
A Text Segmentation Based Approach to Video Shot Boundary Detection
A Text Segmentation Based Approach to Video Shot Boundary Detection
3
</title>
</title>
4
<abstract>
<abstract>
5
<p>
<p>
6
Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .
Video shot boundary detection is one of the fundamental tasks of video indexing and retrieval applications .
7
</p>
</p>
8
<p>
<p>
9
Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle various transition types caused by photo flashes , rapid camera movement and object movement is still challenging .
Although many methods have been proposed for this task , finding a general and robust shot boundary method that is able to handle the various transition types caused by photo flashes , rapid camera movement , and object movement is still challenging .
10
</p>
</p>
11
<p>
<p>
12
In this paper , we present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing .
We present a novel approach for detecting video shot boundaries in which we cast the problem of shot boundary detection into the problem of text segmentation in natural language processing . //detecting / determining?
13
</p>
</p>
14
<p>
<p>
15
By the formulation that each frame is considered as a word and shot boundaries are treated as boundaries of text segments ( e .g topics ) .
This is possible by assuming that each frame is a word and then the shot boundaries are treated as text segment boundaries ( e.g. topics ) .
16
</p>
</p>
17
<p>
<p>
18
Text segmentation based approaches that have been well studied in natural language processing can be adopted .
The text segmentation based approaches in natural language processing can be used .
19
</p>
</p>
20
<p>
<p>
21
Experimental results on various long video sequences show the effectiveness of our approach .
The experimental results from various long video sequences have proven the effectiveness of our approach .
22
</p>
</p>
23
</abstract>
</abstract>
24
<section label " Introduction " >
<section label " Introduction " >
25
<p>
<p>
26
Recent advances in digital technology have made many video archives available .
Recent advances in digital technology have made many video archives readily available .
27
Therefore scalable , efficient and effective tools for indexing and retrieving video are needed .
Therefore scalable , efficient , and effective tools for indexing and retrieving video are needed .
28
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as continuous frames from a single camera at a time .
With a large amount of information encoded in one video , typically the first step of any video processing tools is to segment the input video into elementary shots in which each shot is defined as a continuous frame from a single camera at a given moment .
29
By decomposing a video into shots and then extracting keyframes from these shots , a 30-minute video with 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) which are easily manageable for many video applications in indexing , browsing , summarization , retrieval and so on .
By breaking down a video into individual shots and then extracting the keyframes from these shots , a 30-minute video containing 54 ,000 frames can be represented by around 500 keyframes ( 108 times smaller ) that are easily manageable for many video applications [in / such as? / including?] indexing , browsing , summarization , and retrieval .
30
</p>
</p>
31
<p>
<p>
32
There are many types of transitions between shots .
There are many types of transitions between shots .
33
According to TRECVID 's categorization \CITE , shot boundaries can be classified into two main categories : cut and gradual .
According to TRECVID 's categorization \CITE , shot boundaries can be classified into two main categories : cut and gradual .
34
A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs in a number of consecutive frames .
A cut is an abrupt shot change that occurs in a single frame while a gradual is a slow change that occurs over a number of consecutive frames .
35
</p>
</p>
36
With the gradual type , fades and dissolves are common .
With the gradual type , fades and dissolves are common .
37
A fade is usually a change in brightness with one or several solid black frames in between , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
A fade is usually a change in brightness with one or several solid black frames in between the key frames , while a dissolve occurs when the images in the current shot get dimmer and the images of the next shot get brighter \CITE .
38
Figure \REF shows examples of shot boundary types .
Figure \REF shows examples of shot boundary types .
39
</p>
</p>
40
<p>
<p>
41
Many approaches have been proposed for shot boundary detection .
Many approaches have been proposed for shot boundary detection .
42
The simplest approach is to compute the differences between color distributions of consecutive frames and use a threshold to classify whether a hard cut occurs .
The simplest approach is to compute the differences between the color distributions of consecutive frames and use a threshold to classify whether a hard cut occurs .
43
In order to detect gradual transitions , edge change ratio or motion vectors can be used \CITE .
In order to detect gradual transitions , edge change ratios or motion vectors can be used \CITE .
44
Since these approaches use threshold-based models for detection , their advantage is fast speed .
Since these approaches use threshold-based models for detection , their advantage is they are fast .
45
Nevertheless , they are sensitive to changes of illumination and motion .
Nevertheless , they are sensitive to changes in illumination and motion .
46
Furthermore , they are difficult to generalize for new datasets .
Furthermore , they are difficult to generalize for new datasets .
47
</p>
</p>
48
<p>
<p>
49
Recent works \CITE use machine learning methods for making decision and show impressive results on test videos of TRECVID \CITE which is a de-facto benchmark for evaluation of various techniques in shot boundary detection .
Recent works \CITE use machine learning methods for making decisions and have received impressive results on the test videos of TRECVID \CITE , which is a de-facto benchmark for evaluating the various techniques used in shot boundary detection .
50
</p>
</p>
51
<p>
<p>
52
In this study , we propose a new approach inspired from natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem of text segmentation .
In this study , we propose a new approach that was inspired by the natural language processing text segmentation techniques in which the problem of shot boundary detection is treated similarly to the problem in text segmentation .
53
Specifically , each frame is considered as a word and a set of consecutive frames , forming a shot , is considered as a text segment .
Specifically , each frame is considered a word and a set of consecutive frames , forming a shot , is considered a text segment .
54
</p>
</p>
55
<p>
<p>
56
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of labels such as
Then , the text segmentation problem can be considered a sequential tagging problem in which each word is labeled by one of the following labels :
57
</p>
</p>
58
<p>
<p>
59
PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) and POSTSEG ( word outside a segment ) .
PRESEG ( word beginning of a segment ) , INSEG ( word inside a segment ) , and POSTSEG ( word outside a segment ) .
60
Given a sequence of labeled words , the boundary between text segments can be identified .
Given a sequence of labeled words , the boundary between text segments can be identified .
61
</p>
</p>
62
<p>
<p>
63
The remaining of the paper is organized as follows .
The remainder of this paper is organized as follows .
64
In section \REF , we present an overview of our framework .
In section \REF , we present an overview of our framework .
65
Section \REF introduces experiments on different long video sequences from TRECVID dataset .
Section \REF introduces our experiments on different long video sequences from the TRECVID dataset .
66
Section \REF concludes the paper .
Section \REF concludes the paper .
67
</p>
</p>
68
</section>
</section>
69
<section label " Framework Overview " >
<section label " Framework Overview " >
70
<p>
<p>
71
Given a video , the shot boundary detection process is carried out through two main stages .
The shot boundary detection process for a given video is carried out through two main stages .
72
In the first stage , frames are extracted and labeled by pre-defined labels .
In the first stage , frames are extracted and labeled with pre-defined labels .
73
In the second stage , shot boundaries are identified by grouping labeled frames into segments .
In the second stage , the shot boundaries are identified by grouping the labeled frames into segments .
74
</p>
</p>
75
<p>
<p>
76
We use the following six labels to label frames in video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , POST -GRAD ( post-frame of a GRADUAL transition ) .
We use the following six labels to label frames in a video : NORM -FRM ( frame of a normal shot ) , PRE -CUT ( pre-frame of a CUT transition ) , POST -CUT ( post-frame of a CUT transition ) , PRE -GRAD ( pre-frame of a GRADUAL transition ) , IN -GRAD ( frame inside a GRADUAL transition ) , and POST -GRAD ( post-frame of a GRADUAL transition ) .
77
</p>
</p>
78
<p>
<p>
79
Given a sequence of labeled frames , shot boundaries and transition types are identified by looking up and processing frames marked by non NORM -FRM label .
Given a sequence of labeled frames , the shot boundaries and transition types are identified by looking up and processing the frames marked with a non NORM -FRM label .
80
For example , if we encounter two consecutive frames marked by IN-CUT and POST-CUT respectively , we can declare that a shot boundary occurs at these frames and the transition type is CUT .
For example , if we encounter two consecutive frames respectively marked by IN-CUT and POST-CUT , we can declare that a shot boundary occurs at these frames and the transition type is a CUT .
81
In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare a GRADUAL shot boundary occurs at these frames .
In another case , if we encounter a number of frames marked by xxx-GRAD , we can declare that a GRADUAL shot boundary occurs at these frames .
82
</p>
</p>
83
<p>
<p>
84
Figure \REF shows an example of labeled frames of a shot transition .
Figure \REF shows an example of the labeled frames of a shot transition .
85
</p>
</p>
86
<p>
<p>
87
To label a frame in video , firstly we extract features for that frame and then use a classifier , that has been trained by annotated frames in advance , to classify it into one of six categories mentioned above .
To label a frame in a video , we must firstly extract the features for that frame and then use a classifier , which has been trained in advance by the annotated frames , to classify it into one of the six categories mentioned above .
88
The feature extraction process and classifier learning using support vector machine ( SVM ) are described in details below .
The feature extraction process and classifier learning using a support vector machine ( SVM ) are described in detail below .
89
</p>
</p>
90
<subsection label= " Feature Extraction " >
<subsection label= " Feature Extraction " >
91
<p>
<p>
92
We use two typical features that are color moments , edge direction histogram for representing visual information of each frame .
We use two typical features , which are the color moments and edge direction histogram , to represent the visual information of each frame .
93
However , using this representation is not discriminative enough for frame categorization since frames of a shot transition usually have strong relation to their neighbor frames .
However , using this representation is not discriminative enough for frame categorization since the frames of a shot transition usually strongly relate to their neighboring frames .
94
For example , an abrupt change in illumination between two consecutive frames is a strong cue for a hard cut , or one solid black frames in between dark and then bright frames might help to identify a fade shot transition .
For example , an abrupt change in illumination between two consecutive frames is a strong cue for a hard cut , or one solid black frame in between dark and then bright frames might help to identify a fade shot transition .
95
</p>
</p>
96
<p>
<p>
97
Therefore , in this study , we do not directly use above features .
Therefore , in this study , we do not directly use the above features .
98
Instead , we use them indirectly to model the difference and motion between the current frame and its neighbor frames .
Instead , we use them indirectly to model the difference and motion between the current frame and its neighboring frames .
99
Specifically , for each frame , we compute \MATH distances between the current frame \MATH and neighbor frames ranging from \MATH .
In particular , for each frame , we compute \MATH distances between the current frame \MATH and neighboring frames ranging from \MATH .
100
These distances are used to form a feature vector for frame \MATH in training and testing process later .
These distances are used to form a feature vector for frame \MATH in the training and testing process later .
101
By this way , we can have a unified framework for shot boundary detection and consequently avoid to have special treatments for different shot boundary types as described in many works participated the TRECVID benchmark \CITE .
In this way , we can have a unified framework for the shot boundary detection and consequently avoid having to give special treatment to the different shot boundary types as described in many of the works that participated the TRECVID benchmark \CITE .
102
</subsection>
</subsection>
103
<subsection label= " Color Moments ( GCM ) " >
<subsection label= " Color Moments ( GCM ) " >
104
<p>
<p>
105
Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing color distributions of images \CITE .
Color moments have been successfully used in retrieval systems and proved to be efficient and effective in representing the color distributions of images \CITE .
106
The first order ( mean ) , the second order ( variance ) and the third order ( skewness ) color moments are defined as :
The first order ( mean ) , the second order ( variance ) , and the third order ( skewness ) color moments are defined as :
107
where \MATH is the value of the \MATH -th color component of the image pixel \MATH , and \MATH is the number of pixels in the image .
where \MATH is the value of the \MATH -th color component of image pixel \MATH , and \MATH is the number of pixels in the image .
108
<subsubsection label= " Edge Orientation Histogram ( EOH ) " >
<subsubsection label= " Edge Orientation Histogram ( EOH ) " >
109
<p>
<p>
110
Edge orientation histogram has also been used widely in shot boundary detection \CITE .
Edge orientation histogram has also been widely used in shot boundary detection \CITE .
111
The basic steps to compute edge orientation histogram feature are as follows :
The basic steps for computing the edge orientation histogram features are as follows :
112
</p>
</p>
113
<p>
<p>
114
Extract edges from the input image by using Canny edge detector .
Extract edges from the input image by using Canny edge detector .
115
</p>
</p>
116
<p>
<p>
117
Compute a \MATH -bin histogram of edge and non-edge pixels .
Compute a \MATH -bin histogram of edge and non-edge pixels .
118
The first \MATH bins are used to represent edge directions quantized at \MATH interval and the remaining bin is used for counting non-edge pixels .
The first \MATH bins are used to represent the edge directions quantized at a \MATH interval and the remaining bin is used for counting the non-edge pixels .
119
The histogram is normalized by the number of all pixels to compensate for different image sizes .
The histogram is normalized by the total number of all the pixels to compensate for different image sizes .
120
</p>
</p>
121
<subsubsection label= " Distance-based Neighbor Frame Feature " >
<subsubsection label= " Distance-based Neighbor Frame Feature " >
122
<p>
<p>
123
We use color moments and edge orientation histogram to compute distances between the current frame \MATH and it neighbor frames as follows :
We use color moments and an edge orientation histogram to compute the distances between the current frame \MATH and its neighboring frames as follows :
124
</p>
</p>
125
<p>
<p>
126
The input image is converted to LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \MATH grid .
The input image is converted to a LUV color space ( for GCM ) or grayscale ( for EOH ) and then divided into sub-images by a \MATH grid .
127
</p>
</p>
128
<p>
<p>
129
The color moments and edge orientation histogram are extracted from these sub-images .
The color moments and edge orientation histogram are extracted from these sub-images .
130
For color moments , there are \MATH values .
For color moments , there are \MATH values .
131
For edge orientation histogram , there are \MATH values for each input frame image .
For the edge orientation histogram , there are \MATH values for each input frame image .
132
Compute \MATH values which are the Euclidean distance between current frame \MATH and its neighbor frames ranging from \MATH .
Compute \MATH values , which are the Euclidean distances between the current frame \MATH and its neighboring frames ranging from \MATH .
133
In other words , we compute \MATH where \MATH .
In other words , we compute \MATH , where \MATH .
134
These values \MATH are then used to form the feature vector for frame \MATH .
These values \MATH are then used to form the feature vector for frame \MATH .
135
</p>
</p>
136
</subsubsection>
</subsubsection>
137
</subsection>
</subsection>
138
<subsection label= " Support Vector Machines " >
<subsection label= " Support Vector Machines " >
139
<p>
<p>
140
The Support Vector Machines ( SVM ) is a statistical learning method based on the structure risk minimization principle \CITE .
Support Vector Machines ( SVM ) are a statistical learning method based on the structure risk minimization principle \CITE .
141
It has been very efficiently proved in many pattern recognition applications \CITE .
They have been very efficiently proved to be useful in many pattern recognition applications \CITE .
142
In the binary classification case , the objective of the SVM is to find a best separating hyperplane with a maximum margin .
In the case of binary classification , the objective of the SVM is to find the best separating hyperplane with a maximum margin .
143
</p>
</p>
144
<p>
<p>
145
The form of SVM classifiers is : \MATH
The form of the SVM classifiers is : \MATH
146
</p>
</p>
147
<p>
<p>
148
where \MATH is the d-dimensional vector of an observation example , \MATH is a class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
where \MATH is the d-dimensional vector of an observation example , \MATH is the class label , \MATH is the vector of the \MATH training example , \MATH is the number of training examples , and \MATH is a kernel function , \MATH is learned through the learning process .
149
</p>
</p>
150
<p>
<p>
151
SVM were originally designed for binary classification .
SVMs were originally designed for binary classification .
152
To handle the case of multi-class classification , there are two common approaches .
There are two common approaches for handling multi-class classification .
153
The first one is the one-against-all method that combines \MATH binary classifiers where \MATH is the number of classes .
The first one is the one-against-all method that combines \MATH binary classifiers , where \MATH is the number of classes .
154
The \MATH SVM classifier is trained by positive samples being examples of the \MATH class and negative samples being examples of the other classes .
The \MATH SVM classifier is trained using positive samples as examples of the \MATH class and negative samples as the examples of the other classes .
155
</p>
</p>
156
<p>
<p>
157
The second one is the one-against-one method that combines \MATH binary classifiers in which each classifier is trained on examples of two classes .
The second one is the one-against-one method that combines \MATH binary classifiers in which each classifier is trained on examples from the two classes .
158
</p>
</p>
159
<p>
<p>
160
There are seven classes in our framework : NORM FRM ( frame of a normal shot ) , PRE CUT ( pre-frame of a CUT transition ) , POST CUT ( postframe
There are seven classes in our framework : NORM FRM ( frame of a normal shot ) , PRE CUT ( pre-frame of a CUT transition ) , POST CUT ( postframe
161
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) and NORM-FRM ( normal frame which does not belong to any shot transitions ) .
of a CUT transition ) , PRE GRAD ( pre-frame of a GRADUAL transition ) , IN GRAD ( frame inside a GRADUALtransition ) , POST GRAD ( post-frame of a GRADUAL transition ) , and NORM-FRM ( normal frame that does not belong to any shot transitions ) .
162
To learn this classifier , we manually annotate frames in the training data .
To train this classifier , we manually annotated frames in the training data . //learn / learn about? / find? / educate? / develop? / train?
163
</p>
</p>
164
</subsection>
</subsection>
165
<subsection label= " Shot Boundary Detection Based on Tagged Sequences " >
<subsection label= " Shot Boundary Detection Based on Tagged Sequences " >
166
<p>
<p>
167
Using the trained classifier , we can label a sequence of frames with tags mentioned above .
Using the trained classifier , we can label a sequence of frames with the tags mentioned above .
168
A gradual transition usually has the pattern " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' and a cut transition usually has the pattern " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " ' .
A gradual transition usually has a " ` . . . , PRE-GRAD , IN-GRAD , IN-GRAD , . . . , IN-GRAD , POS-GRAD , . . . " ' pattern and a cut transition usually has a " ` . . . , PRE-CUT , IN-CUT , . . . , IN-CUT , POST-CUT , . . . " 'pattern .
169
The shot boundary detection process is started by checking these transition patterns in the tagged sequence .
The shot boundary detection process is started by checking for these transition patterns in the tagged sequence .
170
Once a pattern is encountered , PRE-xxx and POST-xxx tags are used to identify the shot boundary and the two ends of the shot transition .
Once a pattern is encountered , PRE-xxx and POST-xxx tags are used to identify the shot boundary and the two ends of the shot transition .
171
</p>
</p>
172
<p>
<p>
173
Since the classifier occasionally produce false predictions due to variations caused by photo flashes , rapid camera movement and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many truth shot boundaries .
Since the classifier occasionally produces false predictions due to the variations caused by photo flashes , rapid camera movement , and object movement , only using the perfect match between the predefined patterns and sub-sequences usually skips many of the true shot boundaries .
174
Instead , we use a more flexible matching algorithm in which a match is declared if a portion of the predefined pattern is found in the input sub-sequence .
Instead , we use a more flexible matching algorithm in which a match is declared if a portion of the predefined pattern is found in the input sub-sequence .
175
</p>
</p>
176
</subsection>
</subsection>
177
</section>
</section>
178
<section label " Experiments " >
<section label " Experiments " >
179
<subsection label= " Data Sets " >
<subsection label= " Data Sets " >
180
<p>
<p>
181
We used annotated data sets from TRECVID 2003 test sets for training and testing .
We used annotated data sets from the TRECVID 2003 test sets for the training and testing .
182
We divided 8 videos , each 30-minute length , into two sets : training set and testing set .
We divided eight videos , each 30-minute long , into two sets : a training set and a test set .
183
The number of frames , the number of shot boundaries and types of these sets are shown in Table \REF .
The number of frames , the number of shot boundaries , and the types of these sets are shown in Table \REF .
184
Note that , the number of shot boundaries is equal to the number of frames with PRE-CUT / GRAD label and the number of frames with PRE-CUT / GRAD label is equal to the number of frames with POST-CUT / GRAD label .
Note that , the number of shot boundaries is equal to the number of frames with a PRE-CUT / GRAD label and the number of frames with a PRE-CUT / GRAD label is equal to the number of frames within a POST-CUT / GRAD label .
185
</p>
</p>
186
</subsection>
</subsection>
187
<subsection label= " Classifier Learning " >
<subsection label= " Classifier Learning " >
188
<p>
<p>
189
We used \MATH grid for dividing the input image into sub-images .
We used \MATH grid to divide the input image into sub-images .
190
As for edge orientation histogram , we used 12-bins for edge pixels and one bin for non-edge pixels .
As for the edge orientation histogram , we used 12-bins for the edge pixels and one bin for the non-edge pixels .
191
Furthermore , we used 20 neighbor frames before and after the current frame ( \MATH ) for computing the distances .
Furthermore , we used 20 neighboring frames before and after the current frame ( \MATH ) for computing the distances .
192
These parameters were selected from our empirical studies when participating TRECVID 's tasks .
These parameters were selected from our empirical studies when participating in TRECVID 's tasks .
193
</p>
</p>
194
<p>
<p>
195
The extracted features are normalized to zero mean and unit standard deviation and then stored for training and testing .
The extracted features are normalized to zero mean and a unit standard deviation and then stored for training and testing .
196
Specifically , the normalized vector \MATH
Specifically , the normalized vector \MATH
197
</p>
</p>
198
<p>
<p>
199
where \MATH is the \MATH-th element of the feature vectors \MATH respectively , \MATH is the number of dimensions .
where \MATH is the \MATH-th element of the feature vectors \MATH , respectively , and \MATH is the number of dimensions .
200
</p>
</p>
201
<p>
<p>
202
</p>
</p>
203
<p>
<p>
204
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
In order to handle the problem of imbalanced training sets where the number of NORM-FRM frames is much larger than other frames , we randomly take the \MATH of NORM-FRM frames and 100\% of the other frames to form the training set .
205
</p>
</p>
206
<p>
<p>
207
We use LibSVM \CITE to train SVM classifiers with RBF kernel .
We use LibSVM \CITE to train the SVM classifiers with a RBF kernel .
208
The optimal \MATH parameters are found by conducting a grid search with 5-fold cross validation on a subset 10 ,000 samples stratified selected from the original dataset .
The optimal \MATH parameters are found by conducting a grid search with a 5-fold cross validation on a subset of 10 ,000 samples stratified selected from the original dataset .
209
As for multi-class classification , LibSVM used the one-against-one approach .
As for the multi-class classification , LibSVM used the one-against-one approach .
210
</p>
</p>
211
</subsection>
</subsection>
212
<subsection label= " Evaluation " >
<subsection label= " Evaluation " >
213
<p>
<p>
214
The results that were evaluated by a tool provided by TRECVID with standard measurement such as precision , recall and F1 score clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
The results that were evaluated by a tool provided by TRECVID with a standard measurements , such as the precision , recall , and F1 score , clearly show that our proposed method significantly outperforms the baseline method and the combination of GCM+EOH obtains the best result .
215
</p>
</p>
216
<p>
<p>
217
We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process .
We evaluated the performance of our system with different choices for taking the number of NORM -FRM frames used in training process . //for / by?
218
Specifically , we selected three sampling rates \MATH which are \MATH and \MATH .
Specifically , we selected three sampling rates \MATH , which were \MATH and \MATH .
219
</p>
</p>
220
<p>
<p>
221
As shown in Figure \REF , the best performance is obtained with the sampling rate of \MATH .
As shown in Figure \REF , the best performance was obtained at a sampling rate of \MATH .
222
</p>
</p>
223
<p>
<p>
224
In Table \REF we show the evaluation of using different features for forming the feature vector using distances between current frames and its neighbors .
In Table \REF , we list the evaluation results when using different features to form the feature vector using the distances between the current frames and their neighbors .
225
The first one is GCM , the second one is EOH and the last one GCM+EOH is combination of distances using GCM and distances using EOH .
The first one is GCM , the second one is EOH , and the last one GCM+EOH is a combination of the distances using GCM and the distances using EOH .
226
The number of dimensions of feature vectors using GCM and EOH is 20 while that of feature vectors using GCM+EOH is 40 .
The number of dimensions of the feature vectors using GCM and EOH was 20 , while that of feature vectors using GCM+EOH was 40 .
227
We also compare the proposed method with the baseline method that computes differences in color histograms between two consecutive frames and then decides a shot transitition by using a predefined threshold .
We also compared the proposed method with the baseline method that computes the differences in the color histograms between two consecutive frames , and then decides the shot transition by using a predefined threshold .
228
</p>
</p>
229
<p>
<p>
230
In Figure \REF , we superimpose our result on the results reported in the shot boundary detection task of TRECVID 2003 .
In Figure \REF , we superimposed our result on the results reported in the shot boundary detection task of TRECVID 2003 .
231
Our system achieves high precision and recall for the CUT transition and the result is comparable with the third-ranked system .
Our system achieves a high precision and recall for the CUT transition and this result is comparable to the third-ranked system .
232
Note that our system is general and has no special treatment for particular shot transition .
Note that our system is general and has no special treatment for particular shot transitions .
233
</p>
</p>
234
</subsection>
</subsection>
235
</section>
</section>
236
<section label= " Conclusion " >
<section label= " Conclusion " >
237
<p>
<p>
238
Many previous shot boundary detectors usually divided the system into sub-systems in which special treatments were proposed to handle different types of shot transitions .
Many previous shot boundary detectors usually divide the system into sub-systems in which special treatments are proposed to handle different types of shot transitions .
239
Therefore , it is difficult to generalize for new test sets .
Therefore , it is generalization is difficult for new test sets .
240
</p>
</p>
241
<p>
<p>
242
Different from these approaches , in this paper , we have proposed a unified and general framework for shot boundary detection using a text segmentation based approach .
We have proposed a unified and general framework for shot boundary detection that uses a text segmentation based approach .
243
</p>
</p>
244
Firstly , we label frames by one of six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD and POST -GRAD .
Firstly , we label the frames with one of the six labels defined for different types of frames : NORM -FRM , PRE -CUT , POST -CUT , PRE -GRAD , IN -GRAD , and POST -GRAD .
245
Then we extract shot boundaries and types from these labeled frames .
Then we extract the shot boundaries and types from these labeled frames .
246
</p>
</p>
247
<p>
<p>
248
In order to label frames , we proposed a new feature type to model the difference and motion in color and edge between frames and used it in classification with SVM classifiers .
In order to label frames , we proposed a new feature type to model the difference and motion in color and the edges between the frames and used it in the classification with SVM classifiers .
249
Experiments on various videos of TRECVID 2003 have shown that our approach is effective .
The experiments we conducted on various videos from TRECVID 2003 have shown that our approach is effective .
250
</p>
</p>
251
</section>
</section>
252
</document>
</document>
